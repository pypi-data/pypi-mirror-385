"""
Workflow management.

Provides the main interface for managing workflows and remote flows
"""

import json
import logging
from datetime import datetime, timezone
from typing import Dict, List, Optional, Any
from uuid import UUID, uuid4

import httpx
from sqlalchemy import select, delete, cast, String, or_, func
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.orm import Session, joinedload

from ..database.models import (
    Workflow,
    Schedule,
    Task,
    WorkflowComponent,
    TaskLog,
    WorkflowSource,
)
from ..schemas.source import SourceType
from .remote import LangFlowManager
from .task import TaskManager
from .automagik_agents import AutoMagikAgentManager
from .automagik_hive import AutomagikHiveManager
from .adapters import AdapterRegistry

import os
import asyncio

LANGFLOW_API_URL = os.environ.get("LANGFLOW_API_URL")
LANGFLOW_API_KEY = os.environ.get("LANGFLOW_API_KEY")

logger = logging.getLogger(__name__)


class WorkflowManager:
    """Workflow management class."""

    def __init__(self, session: AsyncSession):
        """Initialize workflow manager."""
        self.session = session
        self.source_manager = None  # Initialize lazily based on workflow source
        self.task = TaskManager(session)

    async def __aenter__(self):
        """Enter context manager."""
        if self.source_manager:
            await self.source_manager.__aenter__()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Exit context manager."""
        if self.source_manager:
            await self.source_manager.__aexit__(exc_type, exc_val, exc_tb)

    async def _get_workflow_source(self, workflow_id: str) -> Optional[WorkflowSource]:
        """Get the workflow source for a given workflow ID."""
        workflow = await self.get_workflow(workflow_id)
        if not workflow:
            return None

        # Return the associated workflow source
        return workflow.workflow_source

    async def _get_source_manager(
        self, source_url: Optional[str] = None, source: Optional[WorkflowSource] = None
    ) -> Any:
        """Get the appropriate source manager based on source type."""
        if not source and source_url:
            # Try to find source by URL
            source = (
                await self.session.execute(
                    select(WorkflowSource).where(WorkflowSource.url == source_url)
                )
            ).scalar_one_or_none()
            if not source:
                raise ValueError(f"No source found with URL {source_url}")

        if not source:
            raise ValueError("Either source or source_url must be provided")

        # Get decrypted API key
        api_key = WorkflowSource.decrypt_api_key(source.encrypted_api_key)

        # Initialize appropriate manager based on source type
        if source.source_type == SourceType.LANGFLOW:
            # Don't pass session to avoid async/sync issues
            return LangFlowManager(api_url=source.url, api_key=api_key)
        elif source.source_type == SourceType.AUTOMAGIK_AGENTS:
            return AutoMagikAgentManager(source.url, api_key, source_id=source.id)
        elif source.source_type == SourceType.AUTOMAGIK_HIVE:
            return AutomagikHiveManager(source.url, api_key, source_id=source.id)
        else:
            raise ValueError(f"Unsupported source type: {source.source_type}")

    async def list_remote_flows(
        self, workflow_id: Optional[str] = None, source_url: Optional[str] = None
    ) -> List[Dict[str, Any]]:
        """List remote flows from all sources, or a specific source if provided.

        Args:
            workflow_id: Optional workflow ID to filter by
            source_url: Optional source URL or instance name to filter by

        Returns:
            List[Dict[str, Any]]: List of flows matching the criteria
        """
        if source_url:
            # Try to find source by URL or instance name
            sources_query = select(WorkflowSource).where(
                or_(
                    WorkflowSource.url == source_url,
                    # Extract instance name from URL and compare
                    func.split_part(
                        func.split_part(WorkflowSource.url, "://", 2), "/", 1
                    ).ilike(f"{source_url}%"),
                )
            )
            sources = (await self.session.execute(sources_query)).scalars().all()

            if not sources:
                logger.warning(f"No sources found matching {source_url}")
                return []

            all_flows = []
            for source in sources:
                try:
                    # Use consistent pattern with _get_source_manager
                    manager = await self._get_source_manager(source=source)

                    if source.source_type == SourceType.LANGFLOW:
                        with manager:  # Use context manager for proper cleanup
                            flows = manager.list_flows_sync()
                    elif source.source_type == SourceType.AUTOMAGIK_AGENTS:
                        flows = manager.list_flows_sync()
                    elif source.source_type == SourceType.AUTOMAGIK_HIVE:
                        flows = manager.list_flows_sync()
                    else:
                        logger.warning(f"Unsupported source type: {source.source_type}")
                        continue

                    # Add source info to each flow
                    for flow in flows:
                        flow["source_url"] = source.url
                        instance = source.url.split("://")[-1].split("/")[0]
                        instance = instance.split(".")[0]
                        flow["instance"] = instance
                    all_flows.extend(flows)
                except Exception as e:
                    logger.error(
                        f"Failed to list flows from source {source.url}: {str(e)}"
                    )
                    continue

            return all_flows
        else:
            # Get all active sources (skip inactive ones to avoid connection errors)
            all_sources = (
                (await self.session.execute(select(WorkflowSource))).scalars().all()
            )
            if not all_sources:
                return []

            aggregated_flows: List[Dict[str, Any]] = []
            for src in all_sources:
                # Skip inactive sources
                if getattr(src, "status", "active") != "active":
                    continue
                flows_for_src = await self.list_remote_flows(
                    workflow_id=workflow_id, source_url=src.url
                )
                aggregated_flows.extend(flows_for_src)
            return aggregated_flows

    async def get_remote_flow(
        self, flow_id: str, source_url: Optional[str] = None
    ) -> Optional[Dict[str, Any]]:
        """Get a remote flow by ID from any source or a specific source.

        Args:
            flow_id: ID of the flow to get
            source_url: Optional URL of the source to get the flow from

        Returns:
            Optional[Dict[str, Any]]: The flow data if found, None otherwise
        """
        if source_url:
            # Try specific source
            try:
                # Get source from database to check type consistently
                source = (
                    await self.session.execute(
                        select(WorkflowSource).where(WorkflowSource.url == source_url)
                    )
                ).scalar_one_or_none()
                if not source:
                    logger.warning(f"No source found with URL {source_url}")
                    return None

                self.source_manager = await self._get_source_manager(source=source)
                async with self.source_manager:
                    if source.source_type == SourceType.LANGFLOW:
                        flow = await self.source_manager.get_flow(flow_id)
                    elif source.source_type == SourceType.AUTOMAGIK_AGENTS:
                        flow = await self.source_manager.get_agent(flow_id)
                    elif source.source_type == SourceType.AUTOMAGIK_HIVE:
                        flow = await self.source_manager.get_flow(flow_id)
                    else:
                        logger.warning(f"Unsupported source type: {source.source_type}")
                        return None

                    if flow:
                        flow["source_url"] = self.source_manager.api_url
                        instance = self.source_manager.api_url.split("://")[-1].split(
                            "/"
                        )[0]
                        instance = instance.split(".")[0]
                        flow["instance"] = instance
                        return flow
            except Exception as e:
                logger.error(
                    f"Failed to get flow {flow_id} from source {source_url}: {str(e)}"
                )
                return None
        else:
            # Try all sources
            sources = (
                (await self.session.execute(select(WorkflowSource))).scalars().all()
            )
            for source in sources:
                flow = await self.get_remote_flow(flow_id, source.url)
                if flow:
                    return flow
            return None

    async def _get_langflow_manager(self, api_url: str = None, api_key: str = None):
        """Deprecated helper for tests â€“ returns a LangFlowManager instance synchronously/async."""
        from .remote import LangFlowManager  # local import to avoid circular

        return LangFlowManager(
            api_url=api_url or LANGFLOW_API_URL, api_key=api_key or LANGFLOW_API_KEY
        )

    async def get_flow_components(self, flow_id: str) -> Dict[str, Any]:
        """Get flow components from LangFlow API.

        For legacy tests, lazily initialize a LangFlowManager via the (re-added) `_get_langflow_manager` helper
        if `source_manager` is still None. The tests monkey-patch this helper, so we must make sure to call it.
        """
        # Choose a manager (existing source_manager or fallback one that tests patch).
        mgr = self.source_manager
        if mgr is None:
            mgr = await self._get_langflow_manager()

        # First, legacy path: use sync_flow then extract nodes (tests rely on this)
        flow_data = None
        if hasattr(mgr, "sync_flow"):
            sync_fn = mgr.sync_flow
            flow_data = (
                await sync_fn(flow_id)
                if asyncio.iscoroutinefunction(sync_fn)
                else sync_fn(flow_id)
            )
            if isinstance(flow_data, dict) and "flow" in flow_data:
                flow_data = flow_data["flow"]

        if flow_data:
            from ..workflows.analyzer import FlowAnalyzer

            components = FlowAnalyzer.get_flow_components(flow_data)
            # Ensure description key exists for tests
            for node, comp in zip(
                flow_data.get("data", {}).get("nodes", []), components
            ):
                desc = node.get("data", {}).get("node", {}).get("description", "")
                comp["description"] = desc
                if comp.get("type") == "Unknown":
                    comp["type"] = node.get("type")
            return components

        # Next, try direct helper if available
        if hasattr(mgr, "get_flow_components"):
            comp_fn = mgr.get_flow_components
            result = (
                await comp_fn(flow_id)
                if asyncio.iscoroutinefunction(comp_fn)
                else comp_fn(flow_id)
            )
            # Ensure list structure
            if isinstance(result, list):
                return result

        # If nothing else, return empty list
        return []

    async def sync_flow(
        self,
        flow_id: str,
        input_component: Optional[str] = None,
        output_component: Optional[str] = None,
        source_url: Optional[str] = None,
    ) -> Optional[Dict[str, Any]]:
        """Sync a flow from a remote source.

        Args:
            flow_id: ID of the flow to sync
            input_component: Optional ID of the input component (will use source defaults if not provided)
            output_component: Optional ID of the output component (will use source defaults if not provided)
            source_url: Optional URL of the source to sync from

        Returns:
            Optional[Dict[str, Any]]: The synced workflow data if successful
        """
        if source_url:
            # If source URL is provided, use that source
            source = (
                await self.session.execute(
                    select(WorkflowSource).where(WorkflowSource.url == source_url)
                )
            ).scalar_one_or_none()
            if not source:
                raise ValueError(f"No source found with URL {source_url}")
            sources = [source]
        else:
            # Get all sources
            sources = (
                (await self.session.execute(select(WorkflowSource))).scalars().all()
            )

        # Try each source until we find the flow
        for source in sources:
            # Skip inactive sources
            if getattr(source, "status", "active") != "active":
                continue

            # Get adapter for this source
            api_key = WorkflowSource.decrypt_api_key(source.encrypted_api_key)
            try:
                adapter = AdapterRegistry.get_adapter(
                    source_type=source.source_type,
                    api_url=source.url,
                    api_key=api_key,
                    source_id=source.id,
                )
            except ValueError as e:
                logger.warning(f"No adapter for source type {source.source_type}: {e}")
                continue

            # Check if flow exists and get it
            try:
                flows = adapter.list_flows_sync()
                flow_exists = any(flow.get("id") == flow_id for flow in flows)
                if not flow_exists:
                    continue

                # Get flow data
                flow_data = adapter.get_flow_sync(flow_id)
                if not flow_data:
                    continue

                # Use provided params or get defaults from adapter
                if not input_component or not output_component:
                    defaults = adapter.get_default_sync_params(flow_data)
                    input_component = input_component or defaults.get("input_component")
                    output_component = output_component or defaults.get(
                        "output_component"
                    )

                # Normalize flow data and save
                flow_data = adapter.normalize_flow_data(flow_data)
                flow_data["input_component"] = input_component
                flow_data["output_component"] = output_component

                # Store adapter for _create_or_update_workflow
                self.source_manager = adapter

                return await self._create_or_update_workflow(flow_data)

            except Exception as e:
                logger.error(f"Failed to sync flow from source {source.url}: {e}")
                continue

        raise ValueError(f"No source found containing flow {flow_id}")

    async def list_workflows(self, options: dict = None) -> List[Dict[str, Any]]:
        """List all workflows from the local database."""
        query = select(Workflow)
        options = options or {}

        # Always load schedules and tasks by default
        if "joinedload" not in options:
            options["joinedload"] = []
        if isinstance(options["joinedload"], list):
            options["joinedload"].extend(["schedules", "tasks"])

        # Add other relationships if requested
        if options.get("with_source"):
            if isinstance(options["joinedload"], list):
                options["joinedload"].append("workflow_source")
            else:
                options["joinedload"] = ["workflow_source"]

        # Apply joinedload options
        if options.get("joinedload"):
            for relationship in options["joinedload"]:
                query = query.options(joinedload(getattr(Workflow, relationship)))

        result = await self.session.execute(query)
        # Call unique() to handle collection relationships
        workflows = result.unique().scalars().all()
        return [workflow.to_dict() for workflow in workflows]

    async def get_workflow(self, workflow_id: str) -> Optional[Workflow]:
        """Get a workflow by ID.

        Args:
            workflow_id: Can be either the local workflow ID or the remote flow ID

        Returns:
            Optional[Workflow]: The workflow if found, None otherwise
        """
        # Try to find by local ID first
        query = (
            select(Workflow)
            .options(joinedload(Workflow.workflow_source))
            .where(
                or_(
                    cast(Workflow.id, String) == workflow_id,
                    Workflow.remote_flow_id == workflow_id,
                )
            )
        )
        result = await self.session.execute(query)
        return result.scalar_one_or_none()

    async def delete_workflow(self, workflow_id: str) -> bool:
        """Delete a workflow and all its related objects."""
        # Try exact match first
        workflow = await self.get_workflow(workflow_id)

        # If no exact match and workflow_id looks like UUID prefix, search by prefix
        if not workflow and len(workflow_id) < 36:
            result = await self.session.execute(
                select(Workflow).where(
                    cast(Workflow.id, String).like(f"{workflow_id}%")
                )
            )
            workflows = result.scalars().all()
            if len(workflows) > 1:
                raise ValueError("Prefix matches multiple workflows")
            workflow = workflows[0] if workflows else None

        # Try again with sanitized id (remove dashes) for SQLite UUID representation
        if not workflow:
            sanitized = workflow_id.replace("-", "")
            result = await self.session.execute(
                select(Workflow).where(cast(Workflow.id, String) == sanitized)
            )
            workflow = result.scalar_one_or_none()

        if not workflow:
            # If the id string is not a valid UUID and neither matches remote_flow_id, raise
            import uuid

            try:
                uuid.UUID(workflow_id)
            except ValueError:
                raise ValueError("Invalid UUID format")
            return False

        try:
            # Delete task logs first
            await self.session.execute(
                delete(TaskLog).where(
                    TaskLog.task_id.in_(
                        select(Task.id).where(Task.workflow_id == workflow.id)
                    )
                )
            )

            # Delete tasks
            await self.session.execute(
                delete(Task).where(Task.workflow_id == workflow.id)
            )

            # Delete schedules
            await self.session.execute(
                delete(Schedule).where(Schedule.workflow_id == workflow.id)
            )

            # Delete components
            await self.session.execute(
                delete(WorkflowComponent).where(
                    WorkflowComponent.workflow_id == workflow.id
                )
            )

            # Finally, delete workflow
            await self.session.execute(
                delete(Workflow).where(Workflow.id == workflow.id)
            )

            await self.session.commit()
            return True
        except Exception as e:
            await self.session.rollback()
            raise e

    async def get_task(self, task_id: str) -> Optional[Task]:
        """Get a task by ID."""
        query = select(Task).where(cast(Task.id, String) == task_id)
        result = await self.session.execute(query)
        return result.scalar_one_or_none()

    async def list_tasks(
        self,
        workflow_id: Optional[str] = None,
        status: Optional[str] = None,
        limit: int = 50,
    ) -> List[Dict[str, Any]]:
        """List tasks from database."""
        query = select(Task).order_by(Task.created_at.desc()).limit(limit)

        if workflow_id:
            query = query.where(cast(Task.workflow_id, String) == workflow_id)
        if status:
            query = query.where(Task.status == status)

        result = await self.session.execute(query)
        tasks = result.scalars().all()
        return [task.to_dict() for task in tasks]

    async def retry_task(self, task_id: str) -> Optional[Task]:
        """Retry a failed task."""
        task = await self.get_task(task_id)
        if not task:
            raise ValueError(f"Task {task_id} not found")

        if task.status != "failed":
            raise ValueError(f"Task {task_id} is not in failed state")

        # Reset task status and error
        task.status = "pending"
        task.error = None
        task.tries += 1
        task.started_at = datetime.now(timezone.utc)
        task.finished_at = None

        await self.session.commit()

        # Run the workflow again
        return await self.run_workflow(
            workflow_id=task.workflow_id, input_data=task.input_data, existing_task=task
        )

    def _format_result_for_logging(self, result: Any) -> str:
        """Format result for logging, keeping it concise."""
        if isinstance(result, dict):
            # For automagik-agents, show just the result/message
            if "result" in result:
                return str(result["result"])
            # For other responses, show a summary
            return f"Response with keys: {', '.join(result.keys())}"
        elif isinstance(result, str):
            # Truncate long strings
            max_length = 100
            if len(result) > max_length:
                return result[:max_length] + "..."
            return result
        return str(result)

    async def run_workflow(
        self,
        workflow_id: str | UUID,
        input_data: str,
        existing_task: Optional[Task] = None,
    ) -> Optional[Task]:
        """Run a workflow with input data."""
        workflow = await self.get_workflow(str(workflow_id))
        if not workflow:
            raise ValueError(f"Workflow {workflow_id} not found")

        # Use existing task or create a new one
        task = existing_task or Task(
            id=uuid4(),
            workflow_id=workflow.id,  # Use the local workflow ID
            input_data=input_data,
            status="running",
            started_at=datetime.now(timezone.utc),
        )

        if not existing_task:
            self.session.add(task)
            await self.session.commit()

        try:
            # Get the workflow source
            source = await self._get_workflow_source(str(workflow_id))
            if not source:
                raise ValueError(f"No source found for workflow {workflow_id}")

            logger.info(f"Using workflow source: {source.url}")
            logger.info(f"Remote flow ID: {workflow.remote_flow_id}")

            # Get adapter for this source
            api_key = WorkflowSource.decrypt_api_key(source.encrypted_api_key)
            adapter = AdapterRegistry.get_adapter(
                source_type=source.source_type,
                api_url=source.url,
                api_key=api_key,
                source_id=source.id,
            )

            # Execute workflow using adapter
            try:
                with adapter:
                    execution_result = adapter.run_flow_sync(
                        workflow.remote_flow_id, input_data, str(task.id)
                    )

                # Handle execution result
                if execution_result.success:
                    logger.info(f"Task {task.id} completed successfully")
                    logger.info(
                        f"Result: {self._format_result_for_logging(execution_result.result)}"
                    )

                    # Store the result appropriately based on its type
                    if isinstance(execution_result.result, (dict, list)):
                        task.output_data = json.dumps(execution_result.result)
                    else:
                        task.output_data = str(execution_result.result)

                    task.status = "completed"
                    task.finished_at = datetime.now(timezone.utc)
                else:
                    logger.error(f"Task {task.id} failed: {execution_result.error}")
                    task.status = "failed"
                    task.error = execution_result.error or "No error details provided"
                    task.finished_at = datetime.now(timezone.utc)

            except Exception as e:
                logger.error(f"Error executing flow: {str(e)}")
                if isinstance(e, httpx.HTTPStatusError):
                    logger.error(f"HTTP Status: {e.response.status_code}")
                    logger.error(f"Response text: {e.response.text}")
                raise

        except Exception as e:
            logger.error(f"Failed to run workflow: {str(e)}")
            if isinstance(e, httpx.HTTPStatusError):
                logger.error(f"HTTP Status: {e.response.status_code}")
            task.status = "failed"
            task.error = str(e)
            task.finished_at = datetime.now(timezone.utc)

        await self.session.commit()
        return task

    async def create_task(
        self, workflow_id: str, input_data: Optional[str] = None, max_retries: int = 3
    ) -> Optional[Task]:
        """Create a new task for a workflow.

        Args:
            workflow_id: ID of the workflow to create a task for
            input_data: Optional input data for the task
            max_retries: Maximum number of retries for the task

        Returns:
            Optional[Task]: The created task if successful
        """
        task_data = {
            "workflow_id": workflow_id,
            "input_data": input_data if input_data else "",
            "max_retries": max_retries,
            "status": "pending",
            "tries": 0,
        }
        return await self.task.create_task(task_data)

    async def _create_or_update_workflow(
        self, flow_data: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Create or update a workflow from flow data."""
        # Get existing workflow by remote flow ID
        query = select(Workflow).where(Workflow.remote_flow_id == flow_data["id"])
        result = await self.session.execute(query)
        workflow = result.scalar_one_or_none()

        # Get the source
        source = (
            await self.session.execute(
                select(WorkflowSource).where(
                    WorkflowSource.url == self.source_manager.api_url
                )
            )
        ).scalar_one_or_none()

        if not source:
            raise ValueError(f"Source not found for URL: {self.source_manager.api_url}")

        # Extract workflow fields from flow data
        workflow_fields = {
            "name": flow_data.get("name"),
            "description": flow_data.get("description"),
            "source": self.source_manager.api_url,
            "remote_flow_id": flow_data["id"],
            "data": flow_data.get("data"),
            "flow_raw_data": flow_data,  # Store the complete flow data
            "input_component": flow_data.get("input_component"),
            "output_component": flow_data.get("output_component"),
            "is_component": self.to_bool(flow_data.get("is_component", False)),
            "folder_id": flow_data.get("folder_id") or flow_data.get("project_id"),
            "folder_name": flow_data.get("folder_name")
            or flow_data.get("project_name"),
            "icon": flow_data.get("icon"),
            "icon_bg_color": flow_data.get("icon_bg_color"),
            "liked": self.to_bool(flow_data.get("liked", False)),
            "tags": flow_data.get("tags", []),
            "workflow_source_id": source.id,  # Use the actual source ID
        }

        if workflow:
            # Update existing workflow
            for key, value in workflow_fields.items():
                setattr(workflow, key, value)
        else:
            # Create new workflow
            workflow = Workflow(**workflow_fields)
            self.session.add(workflow)

        # Delete existing components if any
        await self.session.execute(
            delete(WorkflowComponent).where(
                WorkflowComponent.workflow_id == workflow.id
            )
        )

        # Create components from flow data
        if "data" in flow_data and "nodes" in flow_data["data"]:
            for node in flow_data["data"]["nodes"]:
                component = WorkflowComponent(
                    id=uuid4(),
                    workflow_id=workflow.id,
                    component_id=node["id"],
                    type=node.get("data", {}).get("type", "genericNode"),
                    template=node.get("data", {}),
                    tweakable_params=node.get("data", {}).get("template", {}),
                    is_input=workflow.input_component == node["id"],
                    is_output=workflow.output_component == node["id"],
                )
                self.session.add(component)

        await self.session.commit()
        # Detach the workflow from the session to avoid greenlet errors
        self.session.expunge(workflow)
        return {
            "id": str(workflow.id),
            "name": workflow.name,
            "description": workflow.description,
            "data": workflow.data,
            "flow_raw_data": workflow.flow_raw_data,
            "source": workflow.source,
            "remote_flow_id": workflow.remote_flow_id,
            "flow_version": workflow.flow_version,
            "input_component": workflow.input_component,
            "output_component": workflow.output_component,
            "is_component": workflow.is_component,
            "folder_id": workflow.folder_id,
            "folder_name": workflow.folder_name,
            "icon": workflow.icon,
            "icon_bg_color": workflow.icon_bg_color,
            "liked": workflow.liked,
            "tags": workflow.tags,
            "workflow_source_id": (
                str(workflow.workflow_source_id)
                if workflow.workflow_source_id
                else None
            ),
            "created_at": (
                workflow.created_at.isoformat() if workflow.created_at else None
            ),
            "updated_at": (
                workflow.updated_at.isoformat() if workflow.updated_at else None
            ),
            "schedules": [],  # Don't load schedules in async context
        }

    @staticmethod
    def to_bool(value):
        """Convert a value to boolean."""
        if isinstance(value, bool):
            return value
        if isinstance(value, str):
            return value.lower() in ("true", "1", "t", "y", "yes")
        return bool(value)


class SyncWorkflowManager:
    """Synchronous workflow management class."""

    def __init__(self, session: Session):
        """Initialize workflow manager."""
        self.session = session
        self.source_manager = None  # Initialize lazily based on workflow source

    def _get_workflow_source(self, workflow_id: str) -> Optional[WorkflowSource]:
        """Get the workflow source for a given workflow ID."""
        workflow = self.get_workflow(workflow_id)
        if not workflow:
            return None

        # Return the associated workflow source
        return workflow.workflow_source

    def get_workflow(self, workflow_id: str) -> Optional[Workflow]:
        """Get a workflow by ID."""
        query = select(Workflow).where(cast(Workflow.id, String) == workflow_id)
        result = self.session.execute(query)
        return result.scalar_one_or_none()

    def run_workflow_sync(
        self, workflow: Workflow, task: Task, session: Session
    ) -> Optional[Task]:
        """Run a workflow synchronously."""
        try:
            # Get the workflow source
            source = self._get_workflow_source(str(workflow.id))
            if not source:
                raise ValueError(f"No source found for workflow {workflow.id}")

            logger.info(f"Using workflow source: {source.url}")
            logger.info(f"Remote flow ID: {workflow.remote_flow_id}")

            # Get adapter for this source
            api_key = WorkflowSource.decrypt_api_key(source.encrypted_api_key)
            logger.info(f"Decrypted API key length: {len(api_key) if api_key else 0}")

            adapter = AdapterRegistry.get_adapter(
                source_type=source.source_type,
                api_url=source.url,
                api_key=api_key,
                source_id=source.id,
            )

            # Execute workflow using adapter
            with adapter:
                execution_result = adapter.run_flow_sync(
                    workflow.remote_flow_id, task.input_data, str(task.id)
                )

            # Handle execution result
            if execution_result.success:
                logger.info(f"Task {task.id} completed successfully")
                # Store the result appropriately based on its type
                if isinstance(execution_result.result, (dict, list)):
                    task.output_data = json.dumps(execution_result.result)
                else:
                    task.output_data = str(execution_result.result)
                task.status = "completed"
                task.finished_at = datetime.now(timezone.utc)
            else:
                logger.error(f"Task {task.id} failed: {execution_result.error}")
                task.status = "failed"
                task.error = execution_result.error or "No error details provided"
                task.finished_at = datetime.now(timezone.utc)

        except Exception as e:
            logger.error(f"Failed to run workflow: {str(e)}")
            if isinstance(e, httpx.HTTPStatusError):
                logger.error(f"HTTP Status: {e.response.status_code}")
                logger.error(f"Response text: {e.response.text}")
            task.status = "failed"
            task.error = str(e)
            task.finished_at = datetime.now(timezone.utc)

        session.commit()
        return task
