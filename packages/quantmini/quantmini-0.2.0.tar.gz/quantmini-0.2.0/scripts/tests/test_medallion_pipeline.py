#!/usr/bin/env python3
"""
Test Medallion Architecture Pipeline - Complete Flow

Tests the complete data pipeline from Landing ‚Üí Bronze ‚Üí Silver ‚Üí Gold
for 1 week of data across all data types.

Date Range: 2025-10-11 to 2025-10-18 (1 week)

Usage:
    uv run python scripts/tests/test_medallion_pipeline.py
"""

import asyncio
import sys
import logging
from pathlib import Path
from datetime import date, timedelta

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from src.core.config_loader import ConfigLoader
from src.download.polygon_rest_client import PolygonRESTClient
from src.download.news import NewsDownloader
from src.download.reference_data import ReferenceDataDownloader
from src.utils.data_loader import DataLoader

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


async def test_pipeline():
    """Run complete pipeline test for 1 week of data"""

    # Test configuration
    TEST_DIR = Path('/Users/zheyuanzhao/workspace/quantmini/test_pipeline')
    START_DATE = '2025-10-11'
    END_DATE = '2025-10-18'
    TEST_TICKERS = ['AAPL', 'MSFT', 'GOOGL', 'TSLA', 'NVDA']  # 5 tickers for testing

    logger.info("=" * 80)
    logger.info("MEDALLION ARCHITECTURE PIPELINE TEST")
    logger.info("=" * 80)
    logger.info(f"Test Directory: {TEST_DIR}")
    logger.info(f"Date Range: {START_DATE} to {END_DATE}")
    logger.info(f"Test Tickers: {', '.join(TEST_TICKERS)}")
    logger.info("=" * 80)

    # Load config
    config = ConfigLoader()
    credentials = config.get_credentials('polygon')

    if not credentials or 'api_key' not in credentials:
        logger.error("Polygon API key not found in config/credentials.yaml")
        sys.exit(1)

    # =========================================================================
    # PHASE 1: LANDING/BRONZE LAYER - Download Data via REST API
    # =========================================================================
    logger.info("\n" + "=" * 80)
    logger.info("PHASE 1: LANDING/BRONZE LAYER - Download Data")
    logger.info("=" * 80)

    bronze_path = TEST_DIR / 'bronze'
    bronze_path.mkdir(parents=True, exist_ok=True)

    async with PolygonRESTClient(
        api_key=credentials['api_key'],
        max_concurrent=50,
        max_connections=100
    ) as client:

        # 1.1 Download News Data
        logger.info("\nüì∞ Step 1.1: Downloading news articles...")
        news_path = bronze_path / 'news'
        news_path.mkdir(parents=True, exist_ok=True)

        news_downloader = NewsDownloader(
            client=client,
            output_dir=news_path,
            use_partitioned_structure=True
        )

        try:
            news_result = await news_downloader.download_news_batch(
                tickers=TEST_TICKERS,
                published_utc_gte=START_DATE,
                published_utc_lte=END_DATE,
                limit=100  # Limit articles per ticker for test
            )
            logger.info(f"‚úÖ News downloaded: {news_result['total_articles']} articles")
        except Exception as e:
            logger.error(f"‚ùå News download failed: {e}")

        # 1.2 Download Reference Data
        logger.info("\nüìã Step 1.2: Downloading reference data...")
        reference_path = bronze_path / 'reference'
        reference_path.mkdir(parents=True, exist_ok=True)

        ref_downloader = ReferenceDataDownloader(
            client=client,
            output_dir=reference_path,
            use_partitioned_structure=True
        )

        try:
            for ticker in TEST_TICKERS:
                await ref_downloader.download_ticker_details(ticker)
            logger.info(f"‚úÖ Reference data downloaded for {len(TEST_TICKERS)} tickers")
        except Exception as e:
            logger.error(f"‚ùå Reference data download failed: {e}")

        # Print API statistics
        stats = client.get_statistics()
        logger.info(f"\nüìä API Statistics:")
        logger.info(f"   Total requests: {stats['total_requests']:,}")
        logger.info(f"   Total retries: {stats['total_retries']:,}")
        logger.info(f"   Success rate: {stats['success_rate']:.1%}")

    # =========================================================================
    # PHASE 2: BRONZE LAYER - Download Market Data via CLI
    # =========================================================================
    logger.info("\n" + "=" * 80)
    logger.info("PHASE 2: BRONZE LAYER - Download Market Data (CLI)")
    logger.info("=" * 80)

    # Note: Market data download requires CLI which uses actual data_root
    # For test, we'll just document what would be run:
    logger.info("\nüìà Market Data Download Commands (run separately):")
    logger.info(f"   uv run python -m src.cli.main data ingest -t stocks_daily -s {START_DATE} -e {END_DATE}")
    logger.info(f"   uv run python -m src.cli.main data ingest -t options_daily -s {START_DATE} -e {END_DATE}")
    logger.info("   (These write to configured data_root, not test directory)")

    # =========================================================================
    # PHASE 3: SILVER LAYER - Feature Engineering
    # =========================================================================
    logger.info("\n" + "=" * 80)
    logger.info("PHASE 3: SILVER LAYER - Feature Engineering")
    logger.info("=" * 80)

    silver_path = TEST_DIR / 'silver'
    silver_path.mkdir(parents=True, exist_ok=True)

    logger.info("\nüîß Feature Engineering Commands (run separately):")
    logger.info("   uv run python scripts/transformation/transform_add_features.py")
    logger.info("   (Generates Alpha158 features from bronze ‚Üí silver)")

    # =========================================================================
    # PHASE 4: GOLD LAYER - Qlib Binary Conversion
    # =========================================================================
    logger.info("\n" + "=" * 80)
    logger.info("PHASE 4: GOLD LAYER - Qlib Binary Conversion")
    logger.info("=" * 80)

    gold_path = TEST_DIR / 'gold' / 'qlib'
    gold_path.mkdir(parents=True, exist_ok=True)

    logger.info("\nüíé Qlib Conversion Commands (run separately):")
    logger.info("   uv run python scripts/conversion/convert_to_qlib_binary.py")
    logger.info("   (Converts silver ‚Üí gold/qlib binary format)")

    # =========================================================================
    # PHASE 5: VALIDATION - Verify All Layers
    # =========================================================================
    logger.info("\n" + "=" * 80)
    logger.info("PHASE 5: VALIDATION - Verify All Layers")
    logger.info("=" * 80)

    # 5.1 Verify Bronze Layer
    logger.info("\nüîç Step 5.1: Verifying bronze layer...")

    if (bronze_path / 'news').exists():
        news_files = list((bronze_path / 'news').rglob('*.parquet'))
        logger.info(f"‚úÖ News data: {len(news_files)} parquet files")
    else:
        logger.warning("‚ö†Ô∏è  No news data found")

    if (bronze_path / 'reference').exists():
        ref_files = list((bronze_path / 'reference').rglob('*.parquet'))
        logger.info(f"‚úÖ Reference data: {len(ref_files)} parquet files")
    else:
        logger.warning("‚ö†Ô∏è  No reference data found")

    # 5.2 Test Data Loader
    logger.info("\nüîç Step 5.2: Testing data loader...")
    logger.info("   (Data loader uses configured data_root)")

    try:
        loader = DataLoader()
        logger.info("‚úÖ Data loader initialized successfully")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è  Data loader initialization: {e}")

    # =========================================================================
    # SUMMARY
    # =========================================================================
    logger.info("\n" + "=" * 80)
    logger.info("TEST PIPELINE SUMMARY")
    logger.info("=" * 80)

    logger.info(f"\n‚úÖ Test completed for date range: {START_DATE} to {END_DATE}")
    logger.info(f"‚úÖ Test directory: {TEST_DIR}")
    logger.info(f"‚úÖ Tickers tested: {', '.join(TEST_TICKERS)}")

    logger.info("\nüìã Data Layers:")
    logger.info(f"   Landing/Bronze: {bronze_path}")
    logger.info(f"   Silver:         {silver_path}")
    logger.info(f"   Gold:           {gold_path}")

    logger.info("\nüìù Next Steps:")
    logger.info("   1. Run market data download via CLI (stocks_daily, options_daily)")
    logger.info("   2. Run feature engineering: scripts/transformation/transform_add_features.py")
    logger.info("   3. Run Qlib conversion: scripts/conversion/convert_to_qlib_binary.py")
    logger.info("   4. Validate with: scripts/validation/validate_duckdb_access.py")

    logger.info("\n" + "=" * 80)
    logger.info("‚úÖ TEST PIPELINE COMPLETE")
    logger.info("=" * 80)


if __name__ == '__main__':
    asyncio.run(test_pipeline())
