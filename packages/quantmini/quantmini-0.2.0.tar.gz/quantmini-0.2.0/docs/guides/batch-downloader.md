# Batch Downloader Usage Guide

**Script**: `scripts/batch_load_fundamentals_all.py`
**Purpose**: Download all screener data (8 data types) for 10,070 common stocks
**Last Updated**: 2025-10-17

---

## Overview

The batch downloader is a production-ready script that downloads **all 8 data types** from Polygon.io for the complete universe of common stocks.

### Data Types Downloaded

1. **Fundamentals** (3 types):
   - `balance_sheets` - Balance sheet statements
   - `income_statements` - Income statements
   - `cash_flow` - Cash flow statements

2. **Calculated** (1 type):
   - `financial_ratios` - 121 calculated ratios derived from fundamentals

3. **Corporate Actions** (3 types):
   - `dividends` - Dividend payment history
   - `splits` - Stock split history
   - `ticker_events` - Corporate events (IPO, delisting, etc.)

4. **Reference Data** (1 type):
   - `related_tickers` - Similar/related companies

---

## Quick Start

### Basic Usage

```bash
# Download all remaining tickers (skips already downloaded)
python scripts/batch_load_fundamentals_all.py

# Monitor progress
tail -f data/download_progress.json
```

### Re-download Specific Tickers

```bash
# Refill data for specific tickers
python scripts/batch_load_fundamentals_all.py --refill AAPL MSFT GOOGL
```

### Use Different Worker Count

```bash
# Use 4 workers instead of default 8
python scripts/batch_load_fundamentals_all.py --workers 4

# Use 16 workers for maximum speed (requires powerful machine)
python scripts/batch_load_fundamentals_all.py --workers 16
```

### Start from Specific Batch

```bash
# Resume from batch 50 (if previous run was interrupted)
python scripts/batch_load_fundamentals_all.py --start-batch 50
```

### Force Re-download All

```bash
# Re-download everything (NOT recommended - will take 8+ hours)
python scripts/batch_load_fundamentals_all.py --force
```

---

## Key Features

### 1. **Idempotent Design**
- Safe to re-run multiple times
- Automatically skips already downloaded tickers
- Uses both file system checks and progress tracking

### 2. **Progress Checkpointing**
- Saves progress after each ticker
- Resume from last successful ticker on interruption
- Progress stored in `data/download_progress.json`

### 3. **Automatic Retry**
- Retries failed tickers up to 3 times
- Exponential backoff (1s, 2s, 4s)
- Continues processing even if individual tickers fail

### 4. **Parallel Processing**
- Default: 8 workers (async parallel downloads)
- Configurable via `--workers` flag
- Maximum throughput: ~3 seconds per ticker

### 5. **Comprehensive Logging**
- Real-time progress updates
- Per-ticker success/failure status
- Detailed error messages for failures

---

## Performance Benchmarks

### Test Results (AAPL, MSFT)

```
Batch 1/1: Processing 2 tickers
  ✅ AAPL: BS=82, IS=82, CF=82, Ratios=1451, Div=53, Spl=3, Evt=1, Related=10
  ✅ MSFT: BS=79, IS=79, CF=79, Ratios=1351, Div=87, Spl=1, Evt=1, Related=10

Duration: 6.0s
Success: 2/2 (100.0%)
Average time per ticker: 3.00s
```

### Projected Full Download (9,845 tickers)

| Workers | Estimated Time | Notes |
|---------|---------------|-------|
| 1 | 8.2 hours | Single-threaded |
| 2 | 4.1 hours | Conservative |
| 4 | 2.0 hours | Recommended |
| 8 | 1.0 hours | Default, fast |
| 16 | 30 minutes | Maximum (requires powerful machine) |

---

## Output Structure

All data saved to `data/partitioned_screener/` in Hive-style partitions:

```
data/partitioned_screener/
├── balance_sheets/
│   └── year=2024/month=01/ticker=AAPL.parquet
├── income_statements/
│   └── year=2024/month=01/ticker=AAPL.parquet
├── cash_flow/
│   └── year=2024/month=01/ticker=AAPL.parquet
├── financial_ratios/
│   └── year=2024/month=01/ticker=AAPL.parquet
├── dividends/
│   └── year=2024/month=05/ticker=AAPL.parquet
├── splits/
│   └── year=2020/month=08/ticker=AAPL.parquet
├── ticker_events/
│   └── year=2003/month=09/ticker=AAPL.parquet
└── related_tickers/
    └── ticker=AAPL.parquet
```

### Benefits of Hive Partitioning
- **Efficient querying** - Filter by year/month without full table scan
- **Easy organization** - Group data by time periods
- **Incremental updates** - Add new data without reprocessing old data
- **Polars compatibility** - Native support for partition pruning

---

## Progress Tracking

### Progress File Format

`data/download_progress.json`:

```json
{
  "completed": ["AAPL", "MSFT", "GOOGL", ...],
  "failed": {
    "XYZ": {
      "error": "No fundamentals data returned by API",
      "timestamp": "2025-10-17T14:23:45"
    }
  },
  "started_at": "2025-10-17T14:00:00",
  "last_updated": "2025-10-17T15:30:00",
  "total_tickers": 9845,
  "workers": 8
}
```

### Check Progress

```bash
# Count completed tickers
cat data/download_progress.json | jq '.completed | length'

# Count failed tickers
cat data/download_progress.json | jq '.failed | length'

# Calculate completion percentage
python -c "
import json
data = json.load(open('data/download_progress.json'))
completed = len(data['completed'])
total = data['total_tickers']
print(f'{completed}/{total} ({completed/total*100:.1f}%)')
"
```

---

## Error Handling

### Common Errors and Solutions

#### 1. "No fundamentals data returned by API"
- **Cause**: Ticker has no financial data in Polygon (e.g., ETF, delisted stock)
- **Solution**: Normal, skip these tickers
- **Note**: Corporate actions and related tickers may still download

#### 2. API Rate Limiting (429 errors)
- **Cause**: Too many workers or requests too fast
- **Solution**: Reduce `--workers` count or add delays
- **Note**: Script automatically retries with exponential backoff

#### 3. Network Failures
- **Cause**: Internet connection issues
- **Solution**: Script auto-retries 3 times, then continues
- **Recovery**: Re-run script later to fill gaps

#### 4. Storage Full
- **Cause**: Insufficient disk space (~10 GB needed)
- **Solution**: Free up space and re-run
- **Prevention**: Check disk space before starting

### Retry Failed Tickers

```bash
# Get list of failed tickers
python -c "
import json
failed = json.load(open('data/download_progress.json'))['failed']
print(' '.join(failed.keys()))
" > /tmp/failed_tickers.txt

# Retry failed tickers
python scripts/batch_load_fundamentals_all.py --refill $(cat /tmp/failed_tickers.txt)
```

---

## Command-Line Options

```bash
python scripts/batch_load_fundamentals_all.py [OPTIONS]

Options:
  --workers INT           Number of parallel workers (default: 8)
  --batch-size INT        Tickers per batch for progress tracking (default: 50)
  --start-batch INT       Start from this batch number (default: 1)
  --force                 Force re-download all tickers (ignores progress)
  --refill TICKER [...]   Re-download specific tickers only

Examples:
  # Default: 8 workers, batch size 50
  python scripts/batch_load_fundamentals_all.py

  # Conservative: 4 workers
  python scripts/batch_load_fundamentals_all.py --workers 4

  # Resume from batch 100
  python scripts/batch_load_fundamentals_all.py --start-batch 100

  # Refill 3 specific tickers
  python scripts/batch_load_fundamentals_all.py --refill AAPL MSFT GOOGL

  # Force re-download everything (takes 8+ hours)
  python scripts/batch_load_fundamentals_all.py --force
```

---

## Best Practices

### 1. **Monitor First Batch**
Watch the first batch complete to ensure everything works:
```bash
python scripts/batch_load_fundamentals_all.py --workers 4 --batch-size 10
```

### 2. **Use Screen/Tmux for Long Downloads**
```bash
# Start screen session
screen -S batch_download

# Run downloader
python scripts/batch_load_fundamentals_all.py

# Detach: Ctrl+A, then D
# Reattach later: screen -r batch_download
```

### 3. **Check Disk Space Before Starting**
```bash
# Check available space
df -h data/

# Ensure at least 15 GB free (10 GB data + buffer)
```

### 4. **Monitor Progress During Download**
```bash
# In separate terminal
watch -n 30 "cat data/download_progress.json | jq '.completed | length'"
```

### 5. **Validate After Completion**
```bash
# Check data integrity
python scripts/check_download_status.py
```

---

## Troubleshooting

### Download Seems Stuck
- Check progress file: `tail -f data/download_progress.json`
- Monitor network: `nethogs` or `iftop`
- Check logs: Script outputs to stdout

### High Memory Usage
- Reduce `--workers` count
- Close other applications
- Monitor with `htop` or `top`

### Failed Tickers Not Retrying
- Check error in progress file
- Some failures are expected (delisted stocks, ETFs)
- Manually refill critical tickers with `--refill`

### Slow Performance
- Increase `--workers` if CPU/network underutilized
- Check Polygon API status
- Verify internet connection speed

---

## Data Validation

### Quick Check
```bash
# Count files per table
echo "balance_sheets: $(find data/partitioned_screener/balance_sheets -name '*.parquet' | wc -l)"
echo "income_statements: $(find data/partitioned_screener/income_statements -name '*.parquet' | wc -l)"
echo "cash_flow: $(find data/partitioned_screener/cash_flow -name '*.parquet' | wc -l)"
echo "financial_ratios: $(find data/partitioned_screener/financial_ratios -name '*.parquet' | wc -l)"
echo "dividends: $(find data/partitioned_screener/dividends -name '*.parquet' | wc -l)"
echo "splits: $(find data/partitioned_screener/splits -name '*.parquet' | wc -l)"
echo "ticker_events: $(find data/partitioned_screener/ticker_events -name '*.parquet' | wc -l)"
echo "related_tickers: $(find data/partitioned_screener/related_tickers -name '*.parquet' | wc -l)"
```

### Comprehensive Validation
```bash
# Run validation script
python scripts/check_download_status.py
```

---

## Next Steps After Download

1. **Verify Completion**
   ```bash
   python scripts/check_download_status.py
   ```

2. **Explore Data**
   ```python
   from src.utils.data_loader import DataLoader
   loader = DataLoader()

   # Load financial ratios
   ratios = loader.load_financial_ratios()
   print(f"Total ratio records: {len(ratios):,}")

   # Load dividends
   dividends = loader.load('dividends')
   print(f"Total dividend records: {len(dividends):,}")
   ```

3. **Set Up Incremental Updates**
   - Schedule weekly updates for new data
   - Use `--refill` for specific tickers needing refresh

4. **Integrate with Qlib**
   - Use financial ratios as features
   - Build ML models with clean, structured data

---

## Support

For issues or questions:
- Check `/docs/LARGE_SCALE_FUNDAMENTALS_PLAN.md` for detailed planning
- Review script source: `scripts/batch_load_fundamentals_all.py`
- Check data loader: `src/utils/data_loader.py`

---

## Performance Tips

1. **Optimal Worker Count** = CPU cores * 2 (for I/O-bound tasks)
2. **Monitor API rate limits** - Polygon typically allows 5 req/sec
3. **Use SSD storage** for faster writes
4. **Close other applications** to free resources
5. **Run during off-peak hours** for better API response times

---

**Script Status**: Production-ready, tested on AAPL/MSFT with 100% success rate
