#!/usr/bin/env python3
"""
Test all performance optimizations in SOLLOL
"""
import sys
import time
sys.path.insert(0, "/home/joker/SOLLOL/src")

from sollol import OllamaPool

def test_pool_initialization():
    """Test 1: Pool initialization with HTTP/2 and cache"""
    print("\n" + "="*80)
    print("Test 1: Pool Initialization")
    print("="*80)

    pool = OllamaPool.auto_configure(enable_cache=True)

    stats = pool.get_stats()

    print(f"‚úì Pool created with {stats['nodes_configured']} nodes")
    print(f"‚úì HTTP/2 enabled: {stats['http2_enabled']}")
    print(f"‚úì Async I/O enabled: {stats['async_io_enabled']}")
    print(f"‚úì Cache enabled: {stats['cache']['enabled']}")
    print(f"‚úì Intelligent routing: {stats['intelligent_routing_enabled']}")

    if not stats['nodes_configured']:
        print("‚ö†Ô∏è  No nodes found - some tests will be skipped")
        return pool, False

    print("\n‚úÖ Pool initialization: PASSED")
    return pool, True


def test_response_caching(pool):
    """Test 2: Response caching"""
    print("\n" + "="*80)
    print("Test 2: Response Caching")
    print("="*80)

    try:
        # First request (cache miss)
        print("Making first request (cache miss)...")
        start = time.time()
        result1 = pool.embed(model="mxbai-embed-large", input="Test caching")
        latency1 = (time.time() - start) * 1000
        print(f"‚úì First request: {latency1:.1f}ms")

        # Second request (should be cached)
        print("Making second request (cache hit)...")
        start = time.time()
        result2 = pool.embed(model="mxbai-embed-large", input="Test caching")
        latency2 = (time.time() - start) * 1000
        print(f"‚úì Second request: {latency2:.1f}ms")

        # Check cache stats
        cache_stats = pool.get_cache_stats()
        print(f"\nCache stats:")
        print(f"  Size: {cache_stats['size']}/{cache_stats['max_size']}")
        print(f"  Hits: {cache_stats['hits']}")
        print(f"  Misses: {cache_stats['misses']}")
        print(f"  Hit rate: {cache_stats['hit_rate']:.1%}")

        if latency2 < latency1 * 0.5:
            print(f"\n‚úÖ Cache speedup: {latency1/latency2:.1f}x faster")
        else:
            print(f"\n‚ö†Ô∏è  Cache may not be working (latency2 not much faster)")

        print("\n‚úÖ Response caching: PASSED")
        return True

    except Exception as e:
        print(f"\n‚ùå Response caching: FAILED - {e}")
        return False


def test_cache_management(pool):
    """Test 3: Cache management API"""
    print("\n" + "="*80)
    print("Test 3: Cache Management API")
    print("="*80)

    try:
        # Test cache operations
        pool.clear_cache()
        print("‚úì clear_cache() works")

        # Make a request to populate cache
        pool.embed(model="mxbai-embed-large", input="Test management")

        # List keys
        keys = pool.cache.list_keys()
        print(f"‚úì list_keys() works - {len(keys)} keys")

        # Export/import
        exported = pool.export_cache()
        print(f"‚úì export_cache() works - {len(exported.get('cache', {}))} entries")

        pool.clear_cache()
        imported = pool.import_cache(exported)
        print(f"‚úì import_cache() works - {imported} entries imported")

        print("\n‚úÖ Cache management: PASSED")
        return True

    except Exception as e:
        print(f"\n‚ùå Cache management: FAILED - {e}")
        return False


def test_streaming(pool):
    """Test 4: Streaming support"""
    print("\n" + "="*80)
    print("Test 4: Streaming Support")
    print("="*80)

    try:
        print("Testing streaming generation...")
        chunk_count = 0

        for chunk in pool.generate(
            model="llama3.2",
            prompt="Say hello in exactly 5 words",
            stream=True
        ):
            chunk_count += 1
            if chunk_count <= 3:  # Show first 3 chunks
                content = chunk.get("response", "")
                if content:
                    print(f"  Chunk {chunk_count}: '{content}'")

        print(f"\n‚úì Received {chunk_count} chunks")

        if chunk_count > 0:
            print("\n‚úÖ Streaming: PASSED")
            return True
        else:
            print("\n‚ö†Ô∏è  No chunks received")
            return False

    except NotImplementedError:
        print("\n‚ö†Ô∏è  Streaming not supported in this version")
        return False
    except Exception as e:
        print(f"\n‚ùå Streaming: FAILED - {e}")
        return False


def test_model_warming(pool):
    """Test 5: Model warming"""
    print("\n" + "="*80)
    print("Test 5: Model Warming")
    print("="*80)

    try:
        print("Warming model 'llama3.2'...")
        success = pool.warm_model("llama3.2")

        if success:
            print("‚úì Model warmed successfully")
            print("\n‚úÖ Model warming: PASSED")
            return True
        else:
            print("‚ö†Ô∏è  Model warming returned False")
            return False

    except Exception as e:
        print(f"\n‚ùå Model warming: FAILED - {e}")
        return False


def test_async_io(pool):
    """Test 6: Async I/O"""
    print("\n" + "="*80)
    print("Test 6: Async I/O Support")
    print("="*80)

    try:
        import asyncio

        async def test_async():
            print("Testing async chat...")
            result = await pool.chat_async(
                model="llama3.2",
                messages=[{"role": "user", "content": "Say hi"}]
            )
            return result

        # Run async test
        result = asyncio.run(test_async())

        if result:
            print("‚úì Async chat completed")
            print("\n‚úÖ Async I/O: PASSED")
            return True
        else:
            print("‚ö†Ô∏è  Async chat returned empty result")
            return False

    except RuntimeError as e:
        if "httpx" in str(e).lower():
            print(f"‚ö†Ô∏è  Async I/O requires httpx: {e}")
            return False
        raise
    except Exception as e:
        print(f"\n‚ùå Async I/O: FAILED - {e}")
        return False


def test_adaptive_health_checks(pool):
    """Test 7: Adaptive health checks"""
    print("\n" + "="*80)
    print("Test 7: Adaptive Health Checks")
    print("="*80)

    try:
        # Check if adaptive health checks are configured
        if hasattr(pool, '_adaptive_health_checks') and pool._adaptive_health_checks:
            print("‚úì Adaptive health checks enabled")

            # Check health stats
            stats = pool.get_stats()
            if 'node_performance' in stats:
                print(f"‚úì Monitoring {len(stats['node_performance'])} nodes")

                # Show one node's stats
                for node_key, perf in list(stats['node_performance'].items())[:1]:
                    print(f"\n  Sample node: {node_key}")
                    print(f"    Total requests: {perf.get('total_requests', 0)}")
                    print(f"    Failed requests: {perf.get('failed_requests', 0)}")
                    print(f"    Success rate: {perf.get('success_rate', 0):.1%}")

            print("\n‚úÖ Adaptive health checks: PASSED")
            return True
        else:
            print("‚ö†Ô∏è  Adaptive health checks not found")
            return False

    except Exception as e:
        print(f"\n‚ùå Adaptive health checks: FAILED - {e}")
        return False


def main():
    """Run all tests"""
    print("\n" + "="*80)
    print("SOLLOL Performance Optimization Test Suite")
    print("="*80)

    results = {}

    # Test 1: Initialization
    pool, has_nodes = test_pool_initialization()
    results['initialization'] = True

    if not has_nodes:
        print("\n‚ö†Ô∏è  Skipping tests that require Ollama nodes")
        pool.stop()
        return

    # Test 2-7: Features that require nodes
    results['caching'] = test_response_caching(pool)
    results['cache_management'] = test_cache_management(pool)
    results['streaming'] = test_streaming(pool)
    results['model_warming'] = test_model_warming(pool)
    results['async_io'] = test_async_io(pool)
    results['adaptive_health'] = test_adaptive_health_checks(pool)

    # Cleanup
    pool.stop()

    # Summary
    print("\n" + "="*80)
    print("TEST SUMMARY")
    print("="*80)

    passed = sum(1 for v in results.values() if v)
    total = len(results)

    for test_name, result in results.items():
        status = "‚úÖ PASSED" if result else "‚ùå FAILED"
        print(f"  {test_name:20s}: {status}")

    print(f"\nTotal: {passed}/{total} tests passed")

    if passed == total:
        print("\nüéâ All tests passed!")
        sys.exit(0)
    else:
        print(f"\n‚ö†Ô∏è  {total - passed} test(s) failed")
        sys.exit(1)


if __name__ == "__main__":
    main()
