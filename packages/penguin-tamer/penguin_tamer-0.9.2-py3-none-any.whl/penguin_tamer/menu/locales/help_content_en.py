# flake8: noqa: E501
"""
Help content and documentation texts for the configuration menu.
English version.
"""

# Tab help texts - comprehensive documentation for each tab
TAB_HELP = {
    "tab-general": """# PENGUIN TAMER GENERAL SETTINGS
---
`Penguin Tamer` comes with several free models from OpenRouter pre-installed. To use them, obtain an API_KEY from your OpenRouter account.
## Getting **API_KEY** from OpenRouter
- In your account go to "Keys" section [https://openrouter.ai/settings/keys](https://openrouter.ai/settings/keys)
- Click "Create Key" and copy the created `API_KEY`
- In `Penguin Tamer` settings, open the LLM edit dialog and paste the copied `API_KEY` in the corresponding field.
---

`Penguin Tamer` can work with any compatible neural network. To do this, you need to specify **Model ID**, **API_URL** and **API_KEY**.

List of compatible free models on OpenRouter website (51 models) - [https://openrouter.ai/models/?q=free](https://openrouter.ai/models/?q=free)

---

Connecting any LLM in `Penguin Tamer` consists of three steps:

# Getting **Model ID**, **API_URL** and **API_KEY** using OpenRouter provider as example

Register on **OpenRouter** [openrouter.ai](https://openrouter.ai)

## 1. Finding **Model ID**
- Go to Models section [https://openrouter.ai/models/?q=free](https://openrouter.ai/models/?q=free)
- Select any model from the list
- In the model description find the `Model ID` field
- Example: `anthropic/claude-3-sonnet`

## 2. **API_URL**
- Same URL for all OpenRouter models: `https://openrouter.ai/api/v1`

## 3. Getting **API_KEY**
- In your account go to "Keys" section [https://openrouter.ai/settings/keys](https://openrouter.ai/settings/keys)
- Click "Create Key" and copy the created `API_KEY`

> üí° **Tip:** Save the obtained `API_KEY` ‚Äî it won't be available in your account later. This is a security policy of most LLM providers""",

    "tab-params": """# GENERATION PARAMETERS

Configure language model behavior during text generation.

---

## Temperature

**Range:** 0.0 - 2.0 | **Default:** 0.8

Controls creativity and randomness of responses:

- **0.0-0.3** ‚Äî Deterministic, precise answers
  *Ideal for:* technical tasks, data extraction

- **0.4-0.7** ‚Äî Balanced responses
  *Ideal for:* general use, Q&A

- **0.8-1.5** ‚Äî Creative, diverse responses
  *Ideal for:* brainstorming, idea generation

- **1.6-2.0** ‚Äî Very creative (may be unpredictable)

---

## Max Tokens

**Range:** 1 - ‚àû | **Default:** unlimited

Limits the length of model's response:

- **100 tokens** ‚âà 75 words ‚âà 1-2 paragraphs
- **500 tokens** ‚âà 375 words ‚âà 1 page
- **2000 tokens** ‚âà 1500 words ‚âà 3-4 pages

*Leave empty for unlimited response length.*

---

## Top P (Nucleus Sampling)

**Range:** 0.0 - 1.0 | **Default:** 0.95

Alternative way to control randomness. Model selects from top N% most probable tokens.

> üí° **Tip:** Change **either** `temperature` **or** `top_p`, but not both simultaneously.

---

## Frequency Penalty

**Range:** -2.0 to 2.0 | **Default:** 0.0

Reduces probability of repeating the same tokens:

- **0.3-0.5** ‚Äî Light repetition reduction *(recommended)*
- **1.0-2.0** ‚Äî Strong repetition avoidance

---

## Presence Penalty

**Range:** -2.0 to 2.0 | **Default:** 0.0

Increases probability of discussing new topics. Penalizes for the fact of token mention
(unlike frequency_penalty which penalizes for frequency).

---

## Seed

**Type:** integer or empty | **Default:** random

Ensures reproducibility of results. Same seed with same parameters will give identical response.

*Use for:* testing, prompt debugging, demonstrations

---

## Recommendations

Start with **defaults** and adjust gradually

Change **one parameter at a time** to understand the impact

Use **debug mode** to analyze requests

---

## Configuration Examples

### ü§ñ System Administration Configuration (default)

```yaml
global:
  temperature: 0.8
  max_tokens: null
  top_p: 0.95
  frequency_penalty: 0.0
  presence_penalty: 0.0
  stop: null
  seed: null
```

**Characteristics:** Balanced approach with moderate creativity

### üéØ Precise Technical Answers

```yaml
global:
  temperature: 0.2
  max_tokens: 2000
  top_p: 0.9
  frequency_penalty: 0.1
  presence_penalty: 0.0
  stop: null
  seed: null
```

**Use case:** Information extraction, technical documentation, terminal commands

### üí° Creative Brainstorming

```yaml
global:
  temperature: 1.2
  max_tokens: null
  top_p: 0.98
  frequency_penalty: 0.5
  presence_penalty: 0.6
  stop: null
  seed: null
```

**Use case:** Idea generation, alternative solutions, unconventional approaches

### üìù Brief Answers

```yaml
global:
  temperature: 0.5
  max_tokens: 500
  top_p: 0.9
  frequency_penalty: 0.2
  presence_penalty: 0.1
  stop: ["\n\n\n"]
  seed: null
```

**Use case:** Quick Q&A, short explanations

### üß™ Testing and Debugging

```yaml
global:
  temperature: 0.7
  max_tokens: 1000
  top_p: 0.95
  frequency_penalty: 0.0
  presence_penalty: 0.0
  stop: null
  seed: 42  # Fixed result
```

> More details:
> [LLM_PARAMETERS_GUIDE](https://github.com/Vivatist/penguin-tamer/blob/main/docs/LLM_PARAMETERS_GUIDE.md)""",

    "tab-content": """# CUSTOM CONTEXT

Use this field for **system prompts** and **instructions** that should
automatically apply to each of your requests to the LLM.

This allows you to:
- Set the assistant's role and communication style
- Define response format
- Add specific context for your tasks
- Set communication language

---

Note that basic information about your working environment is already available to the model:

- Operating system
- Architecture
- User
- Home directory
- Current directory
- Hostname
- Local IP address
- Python version
- Python executable
- Virtual environment
- Virtual environment path
- System encoding
- Filesystem encoding
- System locale
- Temporary directory
- CPU count
- Current time
- Shell

---

## Examples

### Assistant Role

You are an experienced Python programmer.
Always provide detailed explanations with code examples.


### Response Format

Answer briefly and to the point.
Use bullet lists and headings.


### Communication Style

Communicate in a friendly and informal way,
use real-life examples to explain complex concepts.


### Specialization

You specialize in web development
with focus on React and TypeScript.

---

## Add Execution Results to Context

Controls whether command execution results are added to the conversation context.

- **Enabled** (default): Command outputs and results are sent to the LLM as context
- **Disabled**: Only commands are recorded, not their outputs

> üí° **Token Saving:** Disable this setting if commands produce large outputs
> (logs, file contents, etc.) to reduce token consumption and API costs.

---

> üí° **Tip:** Be specific ‚Äî clear instructions give better results""",

    "tab-system": """# SYSTEM SETTINGS

Application behavior settings.

> ! **WARNING:** Resetting settings will delete all your configurations including API_KEY.

## Stream Delay (0.001-0.1 sec)
Pause between text chunks during streaming generation.
Used for debugging.


## Refresh Rate (1-60 Hz)
Terminal window update speed during generation.
Used for debugging.

---

## Debug Mode

- Information about API requests
- Display of tokens and execution time
- Error and warning details

""",

    "tab-appearance": """# INTERFACE SETTINGS

Application appearance and visual design.

---

### Available Themes

| Theme | Description |
|------|----------|
| **Classic** | Neutral, universal |
| **Monokai** | Bright accents on dark background |
| **Dracula** | Soft purple tones |
| **Nord** | Muted blue shades |
| **Solarized Dark** | Comfortable dark theme for eyes |
| **GitHub Dark** | Familiar theme for developers |
| **Matrix** | Cyberpunk in "Matrix" style |
| **Minimal** | Black and white, no distracting colors |




""",
}


# Widget help texts - detailed help for specific input fields
WIDGET_HELP = {
    "temp-input": """[bold cyan]TEMPERATURE[/bold cyan]

Controls creativity and randomness of responses.

[bold]Range:[/bold] 0.0 - 2.0

[bold]Low values (0.0-0.5):[/bold]
‚Ä¢ More predictable responses
‚Ä¢ Suitable for technical tasks
‚Ä¢ Facts and accuracy

[bold]Medium values (0.6-1.0):[/bold]
‚Ä¢ Balance of creativity and accuracy
‚Ä¢ Suitable for most tasks

[bold]High values (1.1-2.0):[/bold]
‚Ä¢ Very creative responses
‚Ä¢ Suitable for creative tasks
‚Ä¢ May be less accurate""",

    "max-tokens-input": """[bold cyan]MAX TOKENS[/bold cyan]

Limits the length of generated response.

[bold]Values:[/bold]
‚Ä¢ Empty or 0 = unlimited
‚Ä¢ Number > 0 = maximum token count

[bold]Approximately:[/bold]
‚Ä¢ 100 tokens ‚âà 75 words
‚Ä¢ 500 tokens ‚âà 375 words
‚Ä¢ 1000 tokens ‚âà 750 words

[bold]Recommendations:[/bold]
‚Ä¢ Short answers: 100-300
‚Ä¢ Medium answers: 500-1000
‚Ä¢ Long answers: 1500-3000""",

    "top-p-input": """[bold cyan]TOP P (Nucleus Sampling)[/bold cyan]

Controls diversity of word choice.

[bold]Range:[/bold] 0.0 - 1.0

[bold]How it works:[/bold]
Model selects from top N% most probable tokens.

[bold]Low values (0.1-0.5):[/bold]
‚Ä¢ More conservative choice
‚Ä¢ Predictable responses

[bold]Medium values (0.6-0.9):[/bold]
‚Ä¢ Balance of diversity
‚Ä¢ Recommended for most tasks

[bold]High values (0.95-1.0):[/bold]
‚Ä¢ Maximum diversity
‚Ä¢ More unexpected responses""",

    "freq-penalty-input": """[bold cyan]FREQUENCY PENALTY[/bold cyan]

Reduces repetition of the same words.

[bold]Range:[/bold] -2.0 to 2.0

[bold]Negative values:[/bold]
‚Ä¢ Encourages repetitions
‚Ä¢ Rarely used

[bold]Zero value (0.0):[/bold]
‚Ä¢ No penalties
‚Ä¢ Default

[bold]Positive values:[/bold]
‚Ä¢ 0.1-0.5: light repetition reduction
‚Ä¢ 0.6-1.0: noticeable reduction
‚Ä¢ 1.1-2.0: strong reduction (may be unnatural)""",

    "pres-penalty-input": """[bold cyan]PRESENCE PENALTY[/bold cyan]

Encourages discussing new topics.

[bold]Range:[/bold] -2.0 to 2.0

[bold]Negative values:[/bold]
‚Ä¢ Focus on current topic
‚Ä¢ Deep discussion

[bold]Zero value (0.0):[/bold]
‚Ä¢ Natural behavior
‚Ä¢ Default

[bold]Positive values:[/bold]
‚Ä¢ 0.1-0.5: light topic diversity
‚Ä¢ 0.6-1.0: noticeable diversity
‚Ä¢ 1.1-2.0: maximum diversity (may lose focus)""",

    "seed-input": """[bold cyan]SEED[/bold cyan]

Ensures reproducibility of results.

[bold]Values:[/bold]
‚Ä¢ Empty or 0 = random generation
‚Ä¢ Any number = fixed seed

[bold]Use cases:[/bold]
‚Ä¢ Testing: same seed will give same results
‚Ä¢ Debugging: reproducing issues
‚Ä¢ Experiments: comparing different parameters

[bold]Note:[/bold]
Same seed with same parameters will give identical response.""",

    "stream-delay-input": """[bold cyan]STREAM DELAY[/bold cyan]

Pause between text chunks during streaming generation.

[bold]Range:[/bold] 0.001 - 0.1 seconds

[bold]Small values (0.001-0.01):[/bold]
‚Ä¢ Fast display
‚Ä¢ May flicker

[bold]Medium values (0.02-0.05):[/bold]
‚Ä¢ Comfortable speed
‚Ä¢ Recommended

[bold]Large values (0.06-0.1):[/bold]
‚Ä¢ Slow display
‚Ä¢ Easier to read in real-time""",

    "refresh-rate-input": """[bold cyan]REFRESH RATE[/bold cyan]

Interface update speed.

[bold]Range:[/bold] 1-60 Hz (updates per second)

[bold]Low values (1-10):[/bold]
‚Ä¢ Less system load
‚Ä¢ May be less smooth

[bold]Medium values (10-30):[/bold]
‚Ä¢ Optimal balance
‚Ä¢ Recommended (10 by default)

[bold]High values (30-60):[/bold]
‚Ä¢ Very smooth interface
‚Ä¢ Higher CPU load""",

    "debug-switch": """[bold cyan]DEBUG MODE[/bold cyan]

Enables detailed logging.

[bold]Disabled (OFF):[/bold]
‚Ä¢ Normal operation mode
‚Ä¢ Only important messages
‚Ä¢ Recommended for daily use

[bold]Enabled (ON):[/bold]
‚Ä¢ Detailed request information
‚Ä¢ Parameter logging
‚Ä¢ Display of tokens and execution time
‚Ä¢ Useful for diagnosing issues

[bold]Use cases:[/bold]
‚Ä¢ Development and testing
‚Ä¢ Finding API issues
‚Ä¢ Performance analysis""",

    "language-select": """[bold cyan]INTERFACE LANGUAGE[/bold cyan]

Select language for menu and messages.

[bold]Available languages:[/bold]
‚Ä¢ English (en)
‚Ä¢ –†—É—Å—Å–∫–∏–π (ru)

[bold]What will change:[/bold]
‚Ä¢ Menu language
‚Ä¢ System messages
‚Ä¢ Hints and descriptions

[bold]Note:[/bold]
Communication language with LLM depends on your prompt, not this setting.""",

    "theme-select": """[bold cyan]LLM DIALOG THEME[/bold cyan]

Select theme for LLM dialog rendering.

[bold]Available themes:[/bold]
‚Ä¢ Classic (default) - standard theme
‚Ä¢ Monokai - dark with contrasting accents
‚Ä¢ Dracula - popular dark theme with purple tones
‚Ä¢ Nord - cold northern tones
‚Ä¢ Solarized Dark - scientifically selected palette, comfortable for eyes
‚Ä¢ GitHub Dark - familiar theme for developers in GitHub style
‚Ä¢ Matrix - green monochrome in cyberpunk style
‚Ä¢ Minimal - minimalist black and white theme

[bold]Theme selection affects:[/bold]
‚Ä¢ Interface colors
‚Ä¢ Text contrast
‚Ä¢ Overall perception

[bold]Recommendation:[/bold]
Choose a theme that is comfortable for your eyes.""",
}
