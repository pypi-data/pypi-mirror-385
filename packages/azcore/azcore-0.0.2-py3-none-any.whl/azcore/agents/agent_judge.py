"""
Agent Judge Pattern for Azcore.

A specialized agent designed to evaluate and judge outputs from other agents or systems.
The AgentJudge acts as a quality control mechanism, providing objective assessments
and feedback on various types of content, decisions, or outputs.
"""

import logging
from typing import Dict, Any, Optional, List
from azcore.agents.agent_factory import ReactAgent
from azcore.core.base import BaseAgent
from langchain_core.language_models.chat_models import BaseChatModel
from langchain_core.tools import BaseTool

logger = logging.getLogger(__name__)


AGENT_JUDGE_PROMPT = """# Adaptive Output Evaluator - Role and Protocol

Your role is to critically evaluate outputs across diverse domains by first understanding the context, then applying domain-appropriate evaluation criteria to provide a well-reasoned assessment.

## Core Responsibilities

1. **Context Assessment**
   - Begin by identifying the domain and specific context of the evaluation (technical, creative, analytical, etc.)
   - Determine the appropriate evaluation framework based on domain requirements
   - Adjust evaluation criteria and standards to match domain-specific best practices
   - If domain is unclear, request clarification

2. **Input Validation**
   - Ensure all necessary information is present for a comprehensive evaluation
   - Identify gaps in provided materials that would impact assessment quality
   - Request additional context when needed
   - Consider implicit domain knowledge that may influence proper evaluation

3. **Evidence-Based Analysis**
   - Apply domain-specific criteria to evaluate accuracy, effectiveness, and appropriateness
   - Distinguish between factual claims, reasoned arguments, and subjective opinions
   - Flag assumptions or claims lacking sufficient support within domain standards
   - Evaluate internal consistency and alignment with established principles
   - For technical domains, verify logical and methodological soundness

4. **Comparative Assessment**
   - When multiple solutions or approaches are presented, compare relative strengths
   - Identify trade-offs between different approaches within domain constraints
   - Consider alternative interpretations or solutions not explicitly mentioned
   - Balance competing priorities based on domain-specific values

5. **Final Assessment Declaration**
   - Present your final assessment with: **EVALUATION_COMPLETE [assessment_summary]**
   - Follow with a concise justification referencing domain-specific standards
   - Include constructive feedback for improvement where appropriate
   - Suggest alternative approaches that align with domain best practices
"""


EVALUATION_TASK_PROMPT = """You are an expert AI agent judge. Carefully review the following output(s) generated by another agent. Your job is to provide a detailed, constructive, and actionable critique that will help the agent improve its future performance.

Your feedback should address the following points:

1. **Strengths**: What did the agent do well? Highlight any correct reasoning, clarity, or effective problem-solving.
2. **Weaknesses**: Identify any errors, omissions, unclear reasoning, or areas where the output could be improved.
3. **Suggestions**: Offer specific, practical recommendations for how the agent can improve its next attempt. This may include advice on reasoning, structure, completeness, or style.
4. If relevant, point out any factual inaccuracies or logical inconsistencies.

Be thorough, objective, and professional. Your goal is to help the agent learn and produce better results in the future.

Output(s) to evaluate:
{outputs}
"""


def get_reward(text: str) -> int:
    """
    Determines whether the text contains positive evaluation keywords.

    Args:
        text (str): The text to evaluate

    Returns:
        int: 1 if positive keywords found, 0 otherwise
    """
    positive_words = ["correct", "good", "excellent", "perfect", "accurate", "well done"]
    
    if any(word in text.lower() for word in positive_words):
        return 1
    else:
        return 0


class AgentJudge(BaseAgent):
    """
    A specialized agent designed to evaluate and judge outputs from other agents or systems.

    The AgentJudge acts as a quality control mechanism, providing objective assessments
    and feedback on various types of content, decisions, or outputs. It's based on research
    in LLM-based evaluation systems.

    Attributes:
        name (str): The name of the agent judge
        llm (BaseChatModel): The language model used
        max_loops (int): Maximum number of evaluation iterations
        evaluation_criteria (Dict[str, float]): Evaluation criteria with weights
        return_score (bool): Whether to return numerical score

    Example:
        >>> from langchain_openai import ChatOpenAI
        >>> from azcore.agents.agent_judge import AgentJudge
        >>> 
        >>> llm = ChatOpenAI(model="gpt-4o-mini")
        >>> judge = AgentJudge(
        ...     name="quality-judge",
        ...     llm=llm,
        ...     max_loops=1,
        ... )
        >>> 
        >>> output = "The capital of France is Paris."
        >>> state = {"messages": [{"role": "user", "content": output}]}
        >>> evaluation = judge.invoke(state)
    """

    def __init__(
        self,
        name: str = "agent-judge",
        llm: BaseChatModel = None,
        tools: Optional[List[BaseTool]] = None,
        prompt: str = AGENT_JUDGE_PROMPT,
        description: str = "An agent that evaluates and critiques outputs from other agents",
        max_loops: int = 1,
        evaluation_criteria: Optional[Dict[str, float]] = None,
        return_score: bool = False,
        **kwargs,
    ):
        """
        Initialize the AgentJudge.

        Args:
            name (str): The name of the agent judge
            llm (BaseChatModel): Language model to use
            tools (Optional[List[BaseTool]]): Optional list of tools
            prompt (str): System prompt for evaluation
            description (str): Description of the agent
            max_loops (int): Maximum evaluation iterations
            evaluation_criteria (Optional[Dict[str, float]]): Custom criteria with weights
            return_score (bool): Whether to return score instead of text
        """
        # Enhance prompt with evaluation criteria if provided
        enhanced_prompt = prompt
        if evaluation_criteria:
            criteria_str = "\n\nEvaluation Criteria:\n"
            for criterion, weight in evaluation_criteria.items():
                criteria_str += f"- {criterion}: weight = {weight}\n"
            enhanced_prompt += criteria_str

        super().__init__(
            name=name,
            llm=llm,
            tools=tools,
            prompt=enhanced_prompt,
            description=description,
        )
        
        self.max_loops = max_loops
        self.evaluation_criteria = evaluation_criteria or {}
        self.return_score = return_score
        
        # Create the evaluation agent
        self.agent = ReactAgent(
            name=f"{name}-evaluator",
            llm=llm,
            tools=[],
            prompt=enhanced_prompt,
        )

        logger.info(f"Initialized {name} with max_loops={max_loops}")

    def invoke(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """
        Execute the evaluation process.

        Args:
            state (Dict[str, Any]): Current workflow state containing outputs to evaluate

        Returns:
            Dict[str, Any]: Updated state with evaluation results
        """
        # Extract content to evaluate
        content = ""
        if "messages" in state:
            for msg in state["messages"]:
                if msg.get("role") == "user":
                    content = msg.get("content", "")
                    break

        logger.info(f"Evaluating content: {content[:100]}...")

        # Build evaluation task
        task = EVALUATION_TASK_PROMPT.format(outputs=content)
        
        # Add criteria if provided
        if self.evaluation_criteria:
            criteria_str = "\n\nPlease use these specific evaluation criteria:\n"
            for criterion, weight in self.evaluation_criteria.items():
                criteria_str += f"- {criterion}: weight = {weight}\n"
            task += criteria_str

        # Perform evaluation
        conversation = []
        for loop in range(self.max_loops):
            logger.debug(f"Evaluation loop {loop+1}/{self.max_loops}")

            # Build contextualized task
            if loop > 0:
                context = self._format_conversation(conversation)
                contextualized_task = f"{task}\n\nPrevious evaluation context:\n{context}"
            else:
                contextualized_task = task

            # Get evaluation
            eval_state = {"messages": [{"role": "user", "content": contextualized_task}]}
            result = self.agent.invoke(eval_state)

            # Extract evaluation
            evaluation = ""
            if "messages" in result:
                for msg in reversed(result["messages"]):
                    if msg.get("role") == "assistant":
                        evaluation = msg.get("content", "")
                        break

            conversation.append({"role": "evaluator", "content": evaluation})

        logger.info("Evaluation complete")

        # Get final evaluation
        final_evaluation = conversation[-1]["content"] if conversation else ""

        # Return score or text based on configuration
        if self.return_score:
            score = get_reward(final_evaluation)
            return {
                **state,
                "score": score,
                "evaluation": final_evaluation,
            }
        else:
            return {
                **state,
                "messages": state.get("messages", []) + [
                    {"role": "assistant", "content": final_evaluation}
                ],
                "conversation": conversation,
            }

    async def ainvoke(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """
        Asynchronously execute the evaluation process.

        Args:
            state (Dict[str, Any]): Current workflow state

        Returns:
            Dict[str, Any]: Updated state
        """
        return self.invoke(state)

    def _format_conversation(self, conversation: List[Dict[str, str]]) -> str:
        """Format conversation history as a string."""
        return "\n\n".join([
            f"{entry['role'].upper()}: {entry['content']}"
            for entry in conversation
        ])

    def evaluate_batch(
        self, states: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        Evaluate multiple outputs in batch.

        Args:
            states (List[Dict[str, Any]]): List of states to evaluate

        Returns:
            List[Dict[str, Any]]: List of evaluation results
        """
        results = []
        for state in states:
            result = self.invoke(state)
            results.append(result)
        return results
