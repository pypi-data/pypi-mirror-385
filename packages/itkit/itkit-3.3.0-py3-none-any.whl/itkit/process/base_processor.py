import os, json, pdb
import argparse
from abc import ABC, abstractmethod
from multiprocessing import Pool, cpu_count
from tqdm import tqdm
from enum import Enum


"""
Abstract base classes for creating data processing pipelines in ITK.

This module provides a framework for building reusable data processing tools
that operate on medical imaging datasets. It standardizes common tasks such
as file discovery, parallel processing, and metadata collection.

How to Choose a Processor:
- Use `DatasetProcessor`: If your data is structured with 'image' and 'label'
  subdirectories containing corresponding files.
    /path/to/data/
        ├── image/
        │   ├── case01.mha
        │   └── case02.mha
        └── label/
            ├── case01.mha
            └── case02.mha

- Use `SingleFolderProcessor`: If you are processing all files within a
  single directory (e.g., converting all images in a folder to a new
  orientation).
    /path/to/data/
        ├── case01.nii.gz
        ├── case02.mha
        └── ...

- Use `SeparateFoldersProcessor`: If you have two separate folders with
  corresponding files that share the same filenames. This is generic and can
  handle any file pair types (image-label, label-label, image-image, etc.).
    /path/to/folder_A/
        ├── case01.mha
        └── case02.mha
    /path/to/folder_B/
        ├── case01.mha
        └── case02.mha

How to Implement a New Processor:
1. Choose and inherit from one of the three processor classes above.
2. Implement the `process_one` method. This method contains the core logic
   for processing a single file or a pair of files. It should return a
   dictionary if you want to collect metadata, otherwise return `None`.
3. Call the `process()` method on an instance of your new processor to run
   the entire pipeline.
"""


class ProcessingMode(Enum):
    """Define different file processing modes"""
    DATASET_PAIRS = "dataset_pairs"      # image/label subfolders
    SINGLE_FOLDER = "single_folder"      # single folder with files
    SEPARATE_FOLDERS = "separate_folders"  # separate img and lbl folders


class BaseITKProcessor(ABC):
    """
    The abstract base class for all ITK processing pipelines.

    This class provides the core infrastructure for processing datasets, including:
    - A standardized `process()` entry point.
    - Multiprocessing support via `process_items`.
    - Utility methods for finding files (`find_files_recursive`, `find_files_flat`).
    - A mechanism for collecting and saving metadata.

    A subclass MUST implement two abstract methods:
    1. `get_items_to_process()`: Defines how to discover the files or file pairs
       that need to be processed.
    2. `process_one()`: Defines the actual operation to be performed on a single
       item from the list generated by `get_items_to_process`.
    """

    def __init__(self,
                 mp: bool = False,
                 workers: int | None = None,
                 extensions: tuple[str, ...] = ('.mha', '.mhd', '.nii', '.nii.gz')):
        """
        Initializes the base processor.

        Args:
            mp (bool): If True, enables multiprocessing. Defaults to False.
            workers (int | None): The number of worker processes to use. If None,
                it defaults to the number of CPU cores.
            extensions (tuple[str, ...]): A tuple of file extensions to look for.
        """
        self.mp = mp
        self.workers = max(1, workers or (cpu_count() // 2))
        self.meta = {}
        self.extensions = extensions
        self.source_folder: str | None = None
        self.dest_folder: str | None = None

    def find_files_recursive(self, folder: str, extensions: tuple[str, ...] | None = None) -> list[str]:
        """
        Recursively finds all files in a directory matching the given extensions.

        Args:
            folder (str): The directory to search in.
            extensions (tuple[str, ...] | None): A tuple of file extensions.
                If None, uses the processor's default extensions.

        Returns:
            list[str]: A list of absolute paths to the found files.
        """
        if extensions is None:
            extensions = self.extensions
            
        files = []
        for root, dirs, filenames in os.walk(folder):
            for filename in filenames:
                if filename.endswith(extensions):
                    files.append(os.path.join(root, filename))
        return files

    def find_files_flat(self, folder: str, extensions: tuple[str, ...] | None = None) -> list[str]:
        """
        Finds all files in the top-level of a directory (non-recursive).

        Args:
            folder (str): The directory to search in.
            extensions (tuple[str, ...] | None): A tuple of file extensions.
                If None, uses the processor's default extensions.

        Returns:
            list[str]: A list of absolute paths to the found files.
        """
        if extensions is None:
            extensions = self.extensions
            
        files = []
        for f in os.listdir(folder):
            if f.endswith(extensions):
                files.append(os.path.join(folder, f))
        return files

    def _normalize_filename(self, filepath: str) -> str:
        """
        Normalizes a filename for matching by removing common extensions.
        
        This is useful for matching files like 'case1.nii.gz' with 'case1.mha'.

        Args:
            filepath (str): The file path or name.

        Returns:
            str: The base name of the file without its final extension(s).
        """
        base = os.path.splitext(filepath)[0]
        # Handle double extensions like .nii.gz
        if base.endswith('.nii'):
            base = base[:-4]
        return base

    def process(self, desc: str = "Processing"):
        """
        The main entry point to run the processing pipeline.
        
        This method orchestrates the process: it calls `get_items_to_process`
        to discover items and then `process_items` to execute the processing.

        Args:
            desc (str): A description for the progress bar (e.g., "Resampling", "Patching").
        """
        items = self.get_items_to_process()
        return self.process_items(items, desc)

    def process_items(self, items: list, desc: str = "Processing"):
        """
        Processes a list of items, with optional multiprocessing.

        This is the engine of the processor. It iterates over the items,
        calls `process_one` for each, and collects the results.

        Args:
            items (list): The list of items (e.g., file paths or pairs of paths) to process.
            desc (str): The description to show in the tqdm progress bar.

        Returns:
            list: A list of results returned by `process_one` for each item.
        """
        if not items:
            print(f"No items found for {desc.lower()}.")
            return {}
        
        if self.mp:
            with Pool(self.workers) as pool:
                results = list(tqdm(pool.imap_unordered(self.process_one, items),
                                    total=len(items), desc=desc, dynamic_ncols=True))
        else:
            results = []
            for item in tqdm(items, desc=desc, dynamic_ncols=True):
                results.append(self.process_one(item))
        
        # Collect metadata from the results
        for res in results:
            if res:
                self.meta.update(res)
        
        return self.meta

    @abstractmethod
    def get_items_to_process(self) -> list:
        """
        Abstract method to discover all items (files, pairs) to be processed.
        
        This is one of the two core methods a subclass must implement.

        Returns:
            list: A list of items to be processed. Each item will be passed as an
            argument to `process_one`.
        """
        pass

    @abstractmethod
    def process_one(self, args) -> dict | None:
        """
        Abstract method to process a single item.

        This is the other core method a subclass must implement. It contains the
        actual logic for transforming, analyzing, or otherwise processing a single
        data sample.

        Args:
            args: The item to process, as provided by `get_items_to_process`. This
                could be a single file path (str) or a tuple of paths.

        Returns:
            dict | None: A dictionary containing metadata for the processed item.
            The dictionary will be collected and can be saved with `save_meta`.
            Return `None` if no metadata should be recorded for this item.
        """
        pass

    def save_meta(self, meta_path: str | None = None):
        """
        Saves the collected metadata to a JSON file.

        Args:
            meta_path (str | None): The full path to save the metadata file.
                If None, it defaults to 'series_meta.json' inside the `dest_folder`.
        """
        if not meta_path and self.dest_folder:
            meta_path = os.path.join(self.dest_folder, 'series_meta.json')
        if meta_path:
            try:
                os.makedirs(os.path.dirname(meta_path), exist_ok=True)
                with open(meta_path, 'w') as f:
                    json.dump(self.meta, f, indent=4)
                print(f"Metadata saved to {meta_path}")
            except Exception as e:
                print(f"Warning: Could not save metadata: {e}")


class DatasetProcessor(BaseITKProcessor):
    """
    A processor for datasets with a specific 'image'/'label' directory structure.

    Use this class when your data is organized as follows:
    /path/to/data/
        ├── image/
        │   └── case01.mha
        └── label/
            └── case01.mha
    """
    
    def __init__(self, 
                 source_folder: str,
                 dest_folder: str | None = None,
                 mp: bool = False,
                 workers: int | None = None,
                 recursive: bool = False):
        """
        Initializes the DatasetProcessor.

        Args:
            source_folder (str): The root directory containing 'image' and 'label' subfolders.
            dest_folder (str | None): The root directory where results will be saved.
            mp (bool): Enable multiprocessing.
            workers (int | None): Number of worker processes.
            recursive (bool): If True, search for files recursively within the
                'image' and 'label' subdirectories.
        """
        super().__init__(mp, workers)
        self.source_folder = source_folder
        self.dest_folder = dest_folder
        self.recursive = recursive
    
    def get_items_to_process(self) -> list[tuple[str, str]]:
        """
        Finds all corresponding image-label pairs in the dataset.

        Returns:
            list[tuple[str, str]]: A list of (image_path, label_path) tuples.
        """
        img_dir = os.path.join(self.source_folder, 'image')
        lbl_dir = os.path.join(self.source_folder, 'label')
        
        if not (os.path.isdir(img_dir) and os.path.isdir(lbl_dir)):
            raise ValueError(f"Missing 'image' or 'label' subfolders in {self.source_folder}")
        
        if self.recursive:
            # Get all image files with relative paths
            img_files = {}
            for img_path in self.find_files_recursive(img_dir):
                rel_path = os.path.relpath(img_path, img_dir)
                # Normalize extension for matching
                key = self._normalize_filename(rel_path)
                img_files[key] = img_path
            
            lbl_files = {}
            for lbl_path in self.find_files_recursive(lbl_dir):
                rel_path = os.path.relpath(lbl_path, lbl_dir)
                key = self._normalize_filename(rel_path)
                lbl_files[key] = lbl_path
        else:
            # Simple flat matching
            img_files = {self._normalize_filename(f): os.path.join(img_dir, f) for f in os.listdir(img_dir) if f.endswith(self.extensions)}
            lbl_files = {self._normalize_filename(f): os.path.join(lbl_dir, f) for f in os.listdir(lbl_dir) if f.endswith(self.extensions)}
        
        # Find intersection
        common_keys = set(img_files.keys()) & set(lbl_files.keys())
        pairs = [(img_files[key], lbl_files[key]) for key in common_keys]
        return pairs

    @classmethod
    def start_from_arg(cls):
        argparser = argparse.ArgumentParser(description="Dataset Processor")
        argparser.add_argument('source_folder', type=str, help="Root folder containing 'image' and 'label' subfolders")
        argparser.add_argument('--dest-folder', type=str, default=None, help="Destination folder to save results")
        argparser.add_argument('--mp', action='store_true', help="Enable multiprocessing")
        argparser.add_argument('--workers', type=int, default=None, help="Number of worker processes")
        argparser.add_argument('--recursive', action='store_true', help="Recursively search for files in subdirectories")
        args = argparser.parse_args()
        return cls(args.source_folder, args.dest_folder, args.mp, args.workers, args.recursive)


class SingleFolderProcessor(BaseITKProcessor):
    """
    A processor for handling all files within a single folder.

    Use this class for operations that apply to individual files, such as
    resampling or reorienting a collection of images that are not paired with labels.
    """
    
    def __init__(self,
                 source_folder: str,
                 dest_folder: str | None = None,
                 mp: bool = False,
                 workers: int | None = None,
                 recursive: bool = False):
        """
        Initializes the SingleFolderProcessor.

        Args:
            source_folder (str): The directory containing the files to process.
            dest_folder (str | None): The directory where results will be saved.
            mp (bool): Enable multiprocessing.
            workers (int | None): Number of worker processes.
            recursive (bool): If True, search for files recursively.
        """
        super().__init__(mp, workers)
        self.source_folder = source_folder
        self.dest_folder = dest_folder
        self.recursive = recursive
    
    def get_items_to_process(self) -> list[str]:
        """
        Finds all files in the source folder.

        Returns:
            list[str]: A list of file paths to be processed.
        """
        if self.recursive:
            return self.find_files_recursive(self.source_folder)
        else:
            return self.find_files_flat(self.source_folder)


class SeparateFoldersProcessor(BaseITKProcessor):
    """
    A processor for handling file pairs located in two separate folders.

    Use this class when you have two separate folders containing corresponding files
    that can be matched by filename. This is a generic processor that can handle
    any type of file pairs (e.g., image-label, label-label, image-image, etc.).
    
    Examples:
        - Image and label pairs in separate directories
        - Two sets of labels that need to be compared or merged
        - Original and processed versions of the same data
    """
    
    def __init__(self,
                 folder_A: str,
                 folder_B: str,
                 output_folder_A: str | None = None,
                 output_folder_B: str | None = None,
                 mp: bool = False,
                 workers: int | None = None):
        """
        Initializes the SeparateFoldersProcessor.

        Args:
            folder_A (str): The directory containing the first set of files.
            folder_B (str): The directory containing the second set of files.
            output_folder_A (str | None): Directory to save processed files from folder_A.
            output_folder_B (str | None): Directory to save processed files from folder_B.
            mp (bool): Enable multiprocessing.
            workers (int | None): Number of worker processes.
        """
        super().__init__(mp, workers)
        self.folder_A = folder_A
        self.folder_B = folder_B
        self.output_folder_A = output_folder_A
        self.output_folder_B = output_folder_B
        # Set dest_folder for metadata saving purposes
        self.dest_folder = output_folder_A or output_folder_B
    
    def get_items_to_process(self) -> list[tuple[str, str]]:
        """
        Finds all corresponding file pairs from the two separate folders.

        Returns:
            list[tuple[str, str]]: A list of (file_A_path, file_B_path) tuples.
        """
        files_A_paths = self.find_files_flat(self.folder_A)
        files_B_paths = self.find_files_flat(self.folder_B)

        files_A = {self._normalize_filename(os.path.basename(f)): f for f in files_A_paths}
        files_B = {self._normalize_filename(os.path.basename(f)): f for f in files_B_paths}
        
        common_files = set(files_A.keys()) & set(files_B.keys())
        pairs = [(files_A[f], files_B[f]) for f in common_files]
        return pairs
