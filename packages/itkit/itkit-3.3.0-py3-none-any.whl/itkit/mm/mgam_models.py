import pdb
import logging
import copy
from abc import abstractmethod
from tqdm import tqdm
from collections.abc import Sequence

import torch
from torch import Tensor
from dataclasses import dataclass

from mmengine.logging import print_log
from mmengine.registry import MODELS
from mmengine.config import ConfigDict
from mmengine.structures import BaseDataElement, PixelData
from mmengine.model import BaseModel
from mmengine.dist import is_main_process

from .mmseg_Dev3D import VolumeData


@dataclass
class InferenceConfig:
    """Configuration for sliding-window and device settings during inference.

    Attributes:
        patch_size (tuple | None): Sliding window size. None disables sliding window.
        patch_stride (tuple | None): Sliding window stride. None disables sliding window.
        accumulate_device (str): Device for accumulating window results, e.g., 'cpu' or 'cuda'.
        forward_device (str): Device to run the forward pass for each window.
    """
    patch_size: tuple | None = None
    patch_stride: tuple | None = None
    accumulate_device: str = 'cuda'
    forward_device: str = 'cuda'
    forward_batch_windows: int = 1
    # When accumulate and forward devices differ, a chunk size along the last
    # dimension must be provided to avoid OOM during argmax transfer.
    argmax_batchsize: int | None = None


class ArgmaxProcessor:
    """Device-aware argmax with optional chunking along the last dimension.
    
    Advantages:
    - Avoids OOM on device when handling large tensors.
    - ArgMax can utilize GPU acceleration instead of fully relying on CPU.

    Behavior:
    - Always compute argmax on forward_device.
    - If accumulate_device and forward_device are the same, perform argmax on
      the full tensor directly.
    - If they differ, require `argmax_batchsize` to chunk along the last
      dimension to avoid OOM; per-chunk results are transferred back and
      concatenated on accumulate_device.
    """

    def __init__(self, config: InferenceConfig) -> None:
        self.config = config

    def argmax(self, logits: Tensor, dim: int = 1, keepdim: bool = True) -> Tensor:
        acc_dev = torch.device(self.config.accumulate_device)
        fwd_dev = torch.device(self.config.forward_device)

        # Fast path: same device
        if acc_dev.type == fwd_dev.type:
            # Ensure tensor on forward/accum device (same type)
            t = logits.to(fwd_dev)
            preds = torch.argmax(t, dim=dim, keepdim=keepdim).to(torch.uint8)
            # Return on accumulate device (identical type)
            return preds.to(acc_dev)

        # Different devices: require chunk size
        chunk_size = self.config.argmax_batchsize
        if chunk_size is None or not isinstance(chunk_size, int) or chunk_size <= 0:
            raise ValueError(
                "When accumulate_device and forward_device differ, 'argmax_batchsize' "
                f"must be a positive int in InferenceConfig to enable chunked argmax. Got {chunk_size}"
            )

        # Chunk along the last dimension
        # Temp batch is transferred to forward device for argmax,
        # then results are transferred back to accumulate device.
        last_dim = logits.dim() - 1
        L = logits.shape[-1]
        chunks: list[Tensor] = []
        for start in range(0, L, chunk_size):
            end = min(start + chunk_size, L)
            slc = [slice(None)] * logits.dim()
            slc[last_dim] = slice(start, end)
            t_chunk = logits[tuple(slc)].to(fwd_dev)
            preds_chunk = torch.argmax(t_chunk, dim=dim, keepdim=keepdim).to(torch.uint8)
            chunks.append(preds_chunk.to(acc_dev))

        # Concatenate back along the last dimension on accumulate device
        pred = torch.cat(chunks, dim=last_dim if keepdim is False else last_dim)
        return pred


class mgam_Seg_Lite(BaseModel):
    def __init__(self,
                 backbone: ConfigDict,
                 criterion: ConfigDict|list[ConfigDict],
                 num_classes: int,
                 gt_sem_seg_key: str='gt_sem_seg',
                 use_half: bool=False,
                 binary_segment_threshold: float|None=None,
                 inference_config: InferenceConfig | dict | None = None,
                 allow_pbar: bool=False,
                 *args, **kwargs):
        """
        mgam_Seg_Lite is a Lite form of `mmseg` core model implementation,
        without decouple decoder_head, loss, neck design, allowing easier coding experiments.
        Meanwhile, it provides several args to support sliding window inference for large image/volume,
        especially useful for medical image segmentation tasks.
        
        Args:
            backbone (ConfigDict): Configuration of the backbone network, including the merged decode_head. This backbone should directly output the final segmentation logits.
            criterion (ConfigDict): Criterion for computing loss, such as Dice loss or cross-entropy loss.
            gt_sem_seg_key (str): The key name for the ground truth segmentation mask, default is 'gt_sem_seg'.
            use_half (bool): Whether to use half-precision (fp16) for the model, default is False.
            binary_segment_threshold (float | None): Threshold for binary segmentation. If the model outputs a single channel (binary), this must be provided; if the model outputs multiple channels (multi-class), this must be None.
            inference_config (InferenceConfig | dict | None): Inference configuration that groups sliding-window and device options. If dict, keys can be
                {'patch_size', 'patch_stride', 'accumulate_device', 'forward_device'}. If None, sliding-window is disabled.
        """
        super().__init__(*args, **kwargs)
        self.backbone = MODELS.build(backbone)
        self.criterion = [MODELS.build(c) for c in criterion] if isinstance(criterion, list) else [MODELS.build(criterion)]
        self.num_classes = num_classes
        self.gt_sem_seg_key = gt_sem_seg_key
        self.use_half = use_half
        self.binary_segment_threshold = binary_segment_threshold
        # Build and store inference configuration
        if inference_config is None:
            self.inference_config = InferenceConfig()
        elif isinstance(inference_config, InferenceConfig):
            self.inference_config = inference_config
        elif isinstance(inference_config, dict):
            # Accept both new and legacy key names for compatibility
            self.inference_config = InferenceConfig(**inference_config)
        else:
            raise TypeError(f'inference_config must be InferenceConfig, dict or None, but got {type(inference_config)}')
        self.allow_pbar = allow_pbar
        
        if use_half:
            self.half()

    def forward(self,
                inputs: Tensor,
                data_samples:Sequence[BaseDataElement]|None=None,
                mode:str='tensor'):
        """The unified entry for a forward process in both training and test.

        The method should accept three modes: "tensor", "predict" and "loss":

        - "tensor": Forward the whole network and return tensor or tuple of
        tensor without any post-processing, same as a common nn.Module.
        - "predict": Forward and return the predictions, which are fully
        processed to a list of :obj:`SegDataSample`.
        - "loss": Forward and return a dict of losses according to the given
        inputs and data samples.

        Note that this method doesn't handle neither back propagation nor
        optimizer updating, which are done in the :meth:`train_step`.

        Args:
            inputs (torch.Tensor): The input tensor with shape (N, C, ...) in
                general.
            data_samples (list[:obj:`SegDataSample`]): The seg data samples.
                It usually includes information such as `metainfo` and
                `gt_sem_seg`. Default to None.
            mode (str): Return what kind of value. Defaults to 'tensor'.

        Returns:
            The return type depends on ``mode``.

            - If ``mode="tensor"``, return a tensor or a tuple of tensor.
            - If ``mode="predict"``, return a list of :obj:`DetDataSample`.
            - If ``mode="loss"``, return a dict of tensor.
        """
        if mode == 'loss':
            return self.loss(inputs, data_samples)
        elif mode == 'predict':
            return self.predict(inputs, data_samples)
        elif mode == 'tensor':
            return self._forward(inputs, data_samples)
        else:
            raise RuntimeError(f'Invalid mode "{mode}". '
                               'Only supports loss, predict and tensor mode')

    @abstractmethod
    def loss(self, inputs:Tensor, data_samples:Sequence[BaseDataElement]) -> dict:
        ...
    
    @abstractmethod
    def predict(self, inputs:Tensor, data_samples:Sequence[BaseDataElement]|None=None) -> Sequence[BaseDataElement]:
        ...
    
    @abstractmethod
    def _forward(self, inputs: Tensor, data_samples:Sequence[BaseDataElement]|None=None) -> Tensor:
        ...


class mgam_Seg2D_Lite(mgam_Seg_Lite):
    def loss(self, inputs:Tensor, data_samples:Sequence[BaseDataElement]) -> dict:
        """Calculate losses from a batch of inputs and data samples.
        
        Args:
            inputs (Tensor): The input tensor with shape (N, C, H, W)
            data_samples (Sequence[BaseDataElement]): The seg data samples
            
        Returns:
            dict[str, Tensor]: A dictionary of loss components
        """
        # Forward pass to get prediction logits
        seg_logits = self._forward(inputs, data_samples)
        
        # Extract ground truth masks from data_samples
        gt_segs = []
        for data_sample in data_samples:
            gt_segs.append(data_sample.get(self.gt_sem_seg_key).data)
        gt_segs = torch.stack(gt_segs, dim=0).squeeze(1)  # [N, H, W]
        
        return {'loss_' + cri.__class__.__name__: cri(seg_logits, gt_segs) 
                for cri in self.criterion}

    def predict(self, inputs:Tensor, data_samples:Sequence[BaseDataElement]|None=None) -> Sequence[BaseDataElement]:
        """Predict results from a batch of inputs and data samples.

        Args:
            inputs (Tensor): The input tensor with shape (N, C, H, W).
            data_samples (Sequence[BaseDataElement], optional): The seg data samples.
                It usually includes information such as `metainfo`.
                
        Returns:
            Sequence[BaseDataElement]: Segmentation results of the input images.
                Each SegDataSample usually contains:
                - pred_sem_seg (PixelData): Prediction of semantic segmentation.
                - seg_logits (PixelData): Predicted logits of semantic segmentation.
        """
        # Forward pass
        seg_logits = self.inference(inputs, data_samples) # [N, C, H, W]
        
        # Process outputs
        batch_size = inputs.shape[0]
        out_channels = seg_logits.shape[1]
        
        # Validate consistency of binary threshold and output channels
        if out_channels > 1 and self.binary_segment_threshold is not None:
            raise ValueError(
                f"Multi-class model (out_channels={out_channels}) should not set binary_segment_threshold; "
                f"current value is {self.binary_segment_threshold}, expected None"
            )
        if out_channels == 1 and self.binary_segment_threshold is None:
            raise ValueError(f"Binary model (out_channels={out_channels}) must set binary_segment_threshold; current value is None")
        
        if data_samples is None:
            data_samples = [BaseDataElement() for _ in range(batch_size)]
        
        for i in range(batch_size):
            # Process each sample
            i_seg_logits = seg_logits[i] # [C, H, W]
            
            # Generate prediction mask
            if out_channels > 1:  # 多分类情况
                # Argmax on forward_device with optional chunking
                argmax_proc = ArgmaxProcessor(self.inference_config)
                # We want keepdim=True to maintain [1, H, W]
                i_seg_pred = argmax_proc.argmax(i_seg_logits, dim=0, keepdim=True)
            else:  # 二分类情况
                assert self.binary_segment_threshold is not None, \
                    f"二分类模型(输出通道数={out_channels})必须设置binary_segment_threshold，" \
                    f"当前值为None"
                i_seg_logits_sigmoid = i_seg_logits.sigmoid()
                i_seg_pred = (i_seg_logits_sigmoid > self.binary_segment_threshold).to(i_seg_logits)
            
            # Store results into data_samples
            data_samples[i].seg_logits = PixelData(data=i_seg_logits)
            data_samples[i].pred_sem_seg = PixelData(data=i_seg_pred)
        
        return data_samples

    def _forward(self, inputs: Tensor, data_samples:Sequence[BaseDataElement]|None=None) -> Tensor:
        """Network forward process.

        Args:
            inputs (Tensor): The input tensor with shape (N, C, H, W).
            data_samples (Sequence[BaseDataElement], optional): The seg data samples.
            
        Returns:
            Tensor: Output tensor from backbone
        """
        return self.backbone(inputs)

    @torch.inference_mode()
    def inference(self, inputs: Tensor, data_samples:Sequence[BaseDataElement]|None=None) -> Tensor:
        """Perform inference, supporting sliding-window or full-image.

        Args:
            inputs (Tensor): Input tensor of shape (N, C, H, W).
            data_samples (Sequence[BaseDataElement], optional): Data samples.

        Returns:
            Tensor: Segmentation logits.
        """
        # 检查是否需要滑动窗口推理
        if self.inference_config.patch_size is not None and self.inference_config.patch_stride is not None:
            seg_logits = self.slide_inference(inputs, data_samples)
        else:
            # 整体推理
            seg_logits = self._forward(inputs, data_samples)
        
        return seg_logits

    def slide_inference(self, inputs: Tensor, data_samples:Sequence[BaseDataElement]|None=None) -> Tensor:
        """Perform sliding-window inference with overlapping patches.

        Args:
            inputs (Tensor): Input tensor of shape (N, C, H, W).
            data_samples (Sequence[BaseDataElement], optional): Data samples.

        Returns:
            Tensor: Segmentation logits.
        """
        # Retrieve sliding-window parameters
        assert self.inference_config.patch_size is not None and self.inference_config.patch_stride is not None, \
            f"Sliding-window inference requires patch_size({self.inference_config.patch_size}) and patch_stride({self.inference_config.patch_stride})"
        # Validate dimensionality for 2D
        if not (isinstance(self.inference_config.patch_size, (tuple, list)) and len(self.inference_config.patch_size) == 2):
            raise AssertionError(f"For 2D inference, patch_size must be a tuple/list of length 2, got {self.inference_config.patch_size}")
        if not (isinstance(self.inference_config.patch_stride, (tuple, list)) and len(self.inference_config.patch_stride) == 2):
            raise AssertionError(f"For 2D inference, patch_stride must be a tuple/list of length 2, got {self.inference_config.patch_stride}")
        h_stride, w_stride = self.inference_config.patch_stride
        h_crop, w_crop = self.inference_config.patch_size
        batch_size, _, h_img, w_img = inputs.size()
        h_img, w_img = int(h_img), int(w_img)

        # Check if padding is needed for small images
        need_padding = h_img < h_crop or w_img < w_crop
        if need_padding:
            # Compute padding sizes
            pad_h = max(h_crop - h_img, 0)
            pad_w = max(w_crop - w_img, 0)
            # padding: (left, right, top, bottom)
            pad = (pad_w // 2, pad_w - pad_w // 2, pad_h // 2, pad_h - pad_h // 2)
            padded_inputs = torch.nn.functional.pad(inputs, pad, mode='replicate', value=0)
            h_padded, w_padded = padded_inputs.shape[2], padded_inputs.shape[3]
        else:
            padded_inputs = inputs
            h_padded, w_padded = h_img, w_img
            pad = None

        # Compute number of grids based on padded size
        h_grids = max(h_padded - h_crop + h_stride - 1, 0) // h_stride + 1
        w_grids = max(w_padded - w_crop + w_stride - 1, 0) // w_stride + 1

        accumulate_device = torch.device(self.inference_config.accumulate_device)

        preds = torch.zeros(
            size=(batch_size, self.num_classes, h_padded, w_padded),
            dtype=torch.float32,
            device=accumulate_device
        )
        count_mat = torch.zeros(
            size=(batch_size, 1, h_padded, w_padded),
            dtype=torch.uint8,
            device=accumulate_device
        )

        # Sliding-window inference loop
        for h_idx in range(h_grids):
            for w_idx in range(w_grids):
                h1 = h_idx * h_stride
                w1 = w_idx * w_stride
                h2 = min(h1 + h_crop, h_padded)
                w2 = min(w1 + w_crop, w_padded)
                h1 = max(h2 - h_crop, 0)
                w1 = max(w2 - w_crop, 0)

                # Extract patch
                crop_img = padded_inputs[:, :, h1:h2, w1:w2]

                # Run forward on patch (ensure forward device consistency)
                crop_seg_logit = self._forward(crop_img.to(self.inference_config.forward_device))

                # Move results to accumulation device and sum
                crop_seg_logit_on_device = crop_seg_logit.to(accumulate_device)
                preds[:, :, h1:h2, w1:w2] += crop_seg_logit_on_device
                count_mat[:, :, h1:h2, w1:w2] += 1

        # Verify all regions are covered by sliding windows
        assert torch.min(count_mat).item() > 0, "There are areas not covered by sliding windows"
        # Compute average logits
        seg_logits = preds / count_mat

        # Crop back to original size if padding was applied
        if need_padding:
            assert pad is not None, "Missing padding info, cannot crop back to original size"
            pad_left, pad_right, pad_top, pad_bottom = pad
            seg_logits = seg_logits[:, :, pad_top:h_padded-pad_bottom, pad_left:w_padded-pad_right]

        return seg_logits


class mgam_Seg3D_Lite(mgam_Seg_Lite):
    def loss(self, inputs:Tensor, data_samples:Sequence[BaseDataElement]) -> dict:
        """Calculate losses from a batch of inputs and data samples.
        
        Args:
            inputs (Tensor): The input tensor with shape (N, C, Z, Y, X)
            data_samples (Sequence[BaseDataElement]): The seg data samples
            
        Returns:
            dict[str, Tensor]: A dictionary of loss components
        """
        # Forward pass to get prediction logits
        seg_logits = self._forward(inputs, data_samples)
        
        # Extract ground truth volumes from data_samples
        gt_segs = []
        for data_sample in data_samples:
            gt_segs.append(data_sample.get(self.gt_sem_seg_key).data)
        gt_segs = torch.stack(gt_segs, dim=0).squeeze(1)  # [N, Z, Y, X]
        
        return {'loss_' + cri.__class__.__name__: cri(seg_logits, gt_segs) 
                for cri in self.criterion}

    @torch.inference_mode()
    def predict(self, inputs:Tensor, data_samples:Sequence[BaseDataElement]|None=None) -> Sequence[BaseDataElement]:
        """Predict results from a batch of inputs and data samples.

        Args:
            inputs (Tensor): The input tensor with shape (N, C, Z, Y, X).
            data_samples (Sequence[BaseDataElement], optional): The seg data samples.
                It usually includes information such as `metainfo`.
                
        Returns:
            Sequence[BaseDataElement]: Segmentation results of the input images.
                Each SegDataSample usually contains:
                - pred_sem_seg (VolumeData): Prediction of semantic segmentation.
                - seg_logits (VolumeData): Predicted logits of semantic segmentation.
        """
        
        def _predict(force_cpu:bool=False):
            nonlocal data_samples
            
            seg_logits = self.inference(inputs, data_samples, force_cpu) # [N, C, Z, Y, X]
            
            batch_size = inputs.shape[0]
            out_channels = seg_logits.shape[1]
            
            if out_channels > 1 and self.binary_segment_threshold is not None:
                raise ValueError(
                    f"Multi-class model (out_channels={out_channels}) should not set binary_segment_threshold; "
                    f"current value is {self.binary_segment_threshold}, expected None"
                )
            if out_channels == 1 and self.binary_segment_threshold is None:
                raise ValueError(f"Binary model (out_channels={out_channels}) must set binary_segment_threshold; current value is None")
            
            if data_samples is None:
                data_samples = [BaseDataElement() for _ in range(batch_size)]
            
            for i in range(batch_size):
                # Process each sample
                i_seg_logits = seg_logits[i] # [C, Z, Y, X]
                
                # Generate prediction volume
                if out_channels > 1:  # Multi-class segmentation
                    # Argmax on forward_device with optional chunking
                    argmax_proc = ArgmaxProcessor(self.inference_config)
                    i_seg_pred = argmax_proc.argmax(i_seg_logits, dim=0, keepdim=True)
                else:  # Binary segmentation
                    assert self.binary_segment_threshold is not None, \
                        f"Binary segmentation model (out_channels={out_channels}) must set binary_segment_threshold，" \
                        f"currently it's {self.binary_segment_threshold}"
                    i_seg_logits_sigmoid = i_seg_logits.sigmoid()
                    i_seg_pred = (i_seg_logits_sigmoid > self.binary_segment_threshold).to(i_seg_logits)
                
                # Store results into data_samples
                data_samples[i].seg_logits = VolumeData(**{"data": i_seg_logits})
                data_samples[i].pred_sem_seg = VolumeData(**{"data": i_seg_pred})
            
            return data_samples
        
        try:
            return _predict()
        except torch.OutOfMemoryError as e:
            print_log("OOM during slide inference, trying cpu accumulate.", 'current', logging.WARNING)
            return _predict(force_cpu=True)

    def _forward(self, inputs: Tensor, data_samples:Sequence[BaseDataElement]|None=None) -> Tensor:
        """Network forward process.

        Args:
            inputs (Tensor): The input tensor with shape (N, C, Z, Y, X).
            data_samples (Sequence[BaseDataElement], optional): The seg data samples.
            
        Returns:
            Tensor: Output tensor from backbone
        """
        return self.backbone(inputs)

    @torch.inference_mode()
    def inference(self, inputs: Tensor, data_samples:Sequence[BaseDataElement]|None=None, force_cpu:bool=False) -> Tensor:
        """Perform inference, supporting sliding-window or full-volume.

        Args:
            inputs (Tensor): Input tensor of shape (N, C, Z, Y, X).
            data_samples (Sequence[BaseDataElement], optional): Data samples.
            force_cpu (bool): Whether to force accumulation on CPU to avoid GPU OOM. Default is False.

        Returns:
            Tensor: Segmentation logits.
        """
        if self.inference_config.patch_size is not None and self.inference_config.patch_stride is not None:
            seg_logits = self.slide_inference(inputs, data_samples, force_cpu=force_cpu)

        else:
            seg_logits = self._forward(inputs, data_samples)
            
        return seg_logits

    @torch.inference_mode()
    def slide_inference(self,
                        inputs: Tensor,
                        data_samples: Sequence[BaseDataElement] | None = None,
                        force_cpu: bool = False,
                        forward_batch_windows: int | None = None) -> Tensor:
        """Perform sliding-window inference with overlapping sub-volumes.

        Args:
            inputs (Tensor): Input tensor of shape (N, C, Z, Y, X).
            data_samples (Sequence[BaseDataElement], optional): Data samples.
            force_cpu (bool): Whether to force accumulation on CPU to avoid GPU OOM. Default is False.
            batch_windows (int): Number of sub-volumes to process in a batch. Default is 1.

        Returns:
            Tensor: Segmentation logits.
        """
        # Retrieve sliding-window parameters
        assert self.inference_config.patch_size is not None and self.inference_config.patch_stride is not None, \
            f"When using sliding window, patch_size({self.inference_config.patch_size}) and patch_stride({self.inference_config.patch_stride}) must be set, " \
            f"elsewise, please set both to `None` to disable sliding window."
        z_stride, y_stride, x_stride = self.inference_config.patch_stride
        z_crop, y_crop, x_crop = self.inference_config.patch_size
        batch_windows = forward_batch_windows or self.inference_config.forward_batch_windows
        batch_size, _, z_img, y_img, x_img = inputs.size()
        assert batch_size == 1, "Currently only batch_size=1 is supported for 3D sliding-window inference"
        
        # Convert sizes to Python ints to avoid tensor-to-bool issues
        z_img = int(z_img)
        y_img = int(y_img)
        x_img = int(x_img)
        
        # Check if padding is needed for small volumes
        need_padding = z_img < z_crop or y_img < y_crop or x_img < x_crop
        if need_padding:
            # Compute padding sizes
            pad_z = max(z_crop - z_img, 0)
            pad_y = max(y_crop - y_img, 0)
            pad_x = max(x_crop - x_img, 0)
            # Apply symmetric padding: (left, right, top, bottom, front, back)
            pad = (pad_x // 2, pad_x - pad_x // 2, 
                   pad_y // 2, pad_y - pad_y // 2,
                   pad_z // 2, pad_z - pad_z // 2)
            padded_inputs = torch.nn.functional.pad(inputs, pad, mode='replicate', value=0)
            z_padded, y_padded, x_padded = padded_inputs.shape[2], padded_inputs.shape[3], padded_inputs.shape[4]
        else:
            padded_inputs = inputs
            z_padded, y_padded, x_padded = z_img, y_img, x_img
            pad = None

        # Prepare accumulation and count tensors on target device
        accumulate_device = torch.device('cpu') if force_cpu else torch.device(self.inference_config.accumulate_device)
        if accumulate_device.type == 'cuda':
            # Clear CUDA cache if using GPU accumulation
            torch.cuda.empty_cache()

        # Create accumulation and count matrices on specified device
        preds = torch.zeros(
            size = (batch_size, self.num_classes, z_padded, y_padded, x_padded),
            dtype = torch.float16,
            device = accumulate_device,
            pin_memory = True if accumulate_device.type == 'cpu' else False
        )
        count_mat = torch.zeros(
            size = (batch_size, 1, z_padded, y_padded, x_padded),
            dtype = torch.uint8,
            device = accumulate_device,
            pin_memory = True if accumulate_device.type == 'cpu' else False
        )
        patch_cache = torch.empty(
            size = (batch_windows, self.num_classes, z_crop, y_crop, x_crop),
            dtype = torch.float16,
            device = accumulate_device,
            pin_memory = True if accumulate_device.type == 'cpu' else False
        )
        
        def _device_to_host_pinned_tensor(device_tensor: Tensor, non_blocking: bool = False) -> Tensor:
            """Inplace ops on pinned tensor for efficient transfer."""
            nonlocal patch_cache, preds
            device_tensor = device_tensor.to(preds.dtype) # NOTE Inconsistent dtype can SEVERLY impact tranfer speed.
            if device_tensor.shape == patch_cache.shape:
                # If the shape matches, copy directly to patch_cache
                patch_cache.copy_(device_tensor, non_blocking)
            else:
                # Otherwise, resize patch_cache to match the device tensor shape
                patch_cache.resize_(device_tensor.shape)
                patch_cache.copy_(device_tensor, non_blocking)
            return patch_cache

        # calculate window slices
        window_slices = []
        z_grids = max(z_padded - z_crop + z_stride - 1, 0) // z_stride + 1
        y_grids = max(y_padded - y_crop + y_stride - 1, 0) // y_stride + 1
        x_grids = max(x_padded - x_crop + x_stride - 1, 0) // x_stride + 1
        for z_idx in range(z_grids):
            for y_idx in range(y_grids):
                for x_idx in range(x_grids):
                    z1 = z_idx * z_stride
                    y1 = y_idx * y_stride
                    x1 = x_idx * x_stride
                    z2 = min(z1 + z_crop, z_padded)
                    y2 = min(y1 + y_crop, y_padded)
                    x2 = min(x1 + x_crop, x_padded)
                    z1 = max(z2 - z_crop, 0)
                    y1 = max(y2 - y_crop, 0)
                    x1 = max(x2 - x_crop, 0)
                    window_slices.append((slice(z1, z2), slice(y1, y2), slice(x1, x2)))
        
        for i in tqdm(range(0, len(window_slices), batch_windows),
                      desc="Slide Win. Infer.",
                      disable=not (is_main_process() and self.allow_pbar),
                      dynamic_ncols=True,
                      leave=False):
            batch_slices = window_slices[i:i+batch_windows]
            
            # prepare inference batch
            batch_patches = []
            for (z_slice, y_slice, x_slice) in batch_slices:
                batch_patches.append(padded_inputs[:, :, z_slice, y_slice, x_slice])
            batch_patches = torch.cat(batch_patches, dim=0).to(self.inference_config.forward_device)  # [B, C, z_crop, y_crop, x_crop]
            
            # run forward
            # prevent crop_logits of previous patch inference from being overlapped by next patch copy
            # HACK NOT SURE IF THIS STILL HAPPEN, This is only observed when using `.copy(non_blocking=True)`.
            if torch.cuda.is_available() and torch.device(self.inference_config.forward_device).type == "cuda":
                torch.cuda.synchronize()
            patch_logits_on_device = self._forward(batch_patches)
            patch_cache = _device_to_host_pinned_tensor(patch_logits_on_device)

            # accumulate results
            for j, (z_slice, y_slice, x_slice) in enumerate(batch_slices):
                preds[:, :, z_slice, y_slice, x_slice] += patch_cache[j:j+1]
                count_mat[:, :, z_slice, y_slice, x_slice] += 1
            
        # 使用tensor操作进行断言检查，避免tensor到boolean转换
        min_count = torch.min(count_mat)
        assert min_count.item() > 0, "There are areas not covered by sliding windows"
        # 计算平均值
        seg_logits = (preds / count_mat).to(dtype=torch.float16)
        
        # Crop back to original size if padding was applied
        if need_padding:
            assert pad is not None, "Missing padding info, cannot crop back to original size"
            pad_x_left, pad_x_right, pad_y_top, pad_y_bottom, pad_z_front, pad_z_back = pad
            seg_logits = seg_logits[:, :, 
                                   pad_z_front:z_padded-pad_z_back,
                                   pad_y_top:y_padded-pad_y_bottom,
                                   pad_x_left:x_padded-pad_x_right]
        
        return seg_logits
