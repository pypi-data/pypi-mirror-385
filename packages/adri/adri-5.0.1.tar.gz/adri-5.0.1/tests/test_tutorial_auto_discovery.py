"""Auto-discovery tests for ADRI tutorials.

This module automatically discovers and tests all tutorials in ADRI/tutorials/.
When users add a new tutorial by simply dropping two CSV files into a directory,
these tests automatically:

1. Generate a standard from the training data
2. Verify that training data scores 100% against its own standard (CRITICAL)
3. Test that error data is properly detected
4. Verify CLI/Decorator parity

File Naming Convention Required:
    ADRI/tutorials/<use_case>/
    ├── <use_case>_data.csv          # Clean training data (must score 100%)
    └── test_<use_case>_data.csv     # Test data with quality issues

Benefits:
- Zero manual test creation - tests auto-generate from file discovery
- Guaranteed 100% baseline for all tutorials
- Validated use case library - every tutorial is production-tested
- Consistent testing across all examples
- Easy contribution - just add two CSV files

Example:
    To add a new tutorial for customer support:

    1. Create ADRI/tutorials/customer_support/
    2. Add customer_support_data.csv (clean data)
    3. Add test_customer_support_data.csv (data with errors)
    4. Run pytest - tests auto-generate and validate!
"""

import os
import pandas as pd
import pytest
import yaml
from pathlib import Path

from tests.fixtures.tutorial_discovery import find_tutorial_directories
from tests.fixtures.tutorial_scenarios import TutorialScenarios
from src.adri.decorator import adri_protected
from src.adri.analysis.standard_generator import StandardGenerator


# Discover all tutorials for parametrized testing
def discover_tutorial_ids():
    """Get tutorial IDs for pytest parametrization.

    Returns list of tuples (tutorial_name, metadata) for all discovered tutorials.
    Each tutorial becomes a test case.
    """
    tutorials = find_tutorial_directories()
    return [(meta.use_case_name, meta) for meta in tutorials]


# Mark all tests to skip if no tutorials found
tutorial_list = discover_tutorial_ids()
pytestmark = pytest.mark.skipif(
    len(tutorial_list) == 0,
    reason="No tutorials found in ADRI/tutorials/"
)


@pytest.mark.parametrize("tutorial_name,tutorial_metadata", tutorial_list)
def test_training_data_scores_100_percent(tutorial_name, tutorial_metadata, tutorial_project):
    """CRITICAL TEST: Training data must score 100% against its own standard.

    This is the most important test in the framework. It guarantees that:
    1. The tutorial's training data is truly clean (100% quality)
    2. The generated standard accurately represents the data
    3. Users can trust this tutorial as a validated example

    Every tutorial MUST pass this test. If this fails, the tutorial is invalid.

    This test uses the same approach as test_tutorial_invoice_flow_validation.py
    to ensure consistency.

    Args:
        tutorial_name: Use case name (e.g., "invoice")
        tutorial_metadata: Metadata from discovery
        tutorial_project: Test project fixture
    """
    from src.adri.validator.engine import DataQualityAssessor

    # Setup tutorial scenario
    scenario = TutorialScenarios.setup_tutorial_from_directory(
        tutorial_metadata, tutorial_project
    )

    # Set environment for the test
    os.environ['ADRI_ENV'] = 'development'
    os.environ['ADRI_CONFIG_PATH'] = str(tutorial_project / 'adri-config.yaml')

    # Load training data (should be 100% clean)
    training_data = pd.read_csv(scenario['training_data_path'])

    # Create assessor and assess against the generated standard
    assessor = DataQualityAssessor()
    assessment = assessor.assess(
        data=training_data,
        standard_path=scenario['standard_path']
    )

    # Extract scores
    overall_score = assessment.overall_score
    dimension_scores = {
        dim: dim_score.score if hasattr(dim_score, 'score') else dim_score
        for dim, dim_score in assessment.dimension_scores.items()
    }

    # CRITICAL ASSERTION: Training data must score 100%
    assert overall_score == 100.0, (
        f"Tutorial '{tutorial_name}': Training data should score 100% against its own standard, "
        f"got {overall_score}%. Training data must be perfectly clean."
    )

    # CRITICAL ASSERTION: All dimensions must score 20/20
    expected_dimensions = ['validity', 'completeness', 'consistency', 'freshness', 'plausibility']
    for dim_name in expected_dimensions:
        assert dim_name in dimension_scores, f"Tutorial '{tutorial_name}': Missing dimension: {dim_name}"
        dim_score = dimension_scores[dim_name]
        assert dim_score == 20.0, (
            f"Tutorial '{tutorial_name}': {dim_name} should be 20/20, got {dim_score}. "
            f"All dimensions must be perfect for training data."
        )


@pytest.mark.parametrize("tutorial_name,tutorial_metadata", tutorial_list)
def test_standard_generation_succeeds(tutorial_name, tutorial_metadata, tutorial_project):
    """Test that standard generation completes successfully for tutorial.

    Verifies:
    1. Standard can be generated from training data
    2. Standard file is created with valid YAML structure
    3. Required fields are present in the standard

    Args:
        tutorial_name: Use case name (e.g., "invoice")
        tutorial_metadata: Metadata from discovery
        tutorial_project: Test project fixture
    """
    # Setup tutorial scenario (includes standard generation)
    scenario = TutorialScenarios.setup_tutorial_from_directory(
        tutorial_metadata, tutorial_project
    )

    # Verify standard file was created
    assert scenario['standard_path'].exists(), (
        f"Tutorial '{tutorial_name}': Standard file not created at "
        f"{scenario['standard_path']}"
    )

    # Load and validate standard structure
    with open(scenario['standard_path'], 'r', encoding='utf-8') as f:
        standard = yaml.safe_load(f)

    # Verify required top-level fields (actual ADRI standard structure)
    required_fields = ['metadata', 'requirements']
    for field in required_fields:
        assert field in standard, (
            f"Tutorial '{tutorial_name}': Standard missing required field '{field}'"
        )

    # Verify metadata exists
    assert 'metadata' in standard, (
        f"Tutorial '{tutorial_name}': Standard missing metadata"
    )

    # Verify requirements has field_requirements
    assert 'field_requirements' in standard['requirements'], (
        f"Tutorial '{tutorial_name}': Standard requirements missing 'field_requirements'"
    )
    assert len(standard['requirements']['field_requirements']) > 0, (
        f"Tutorial '{tutorial_name}': Standard has no field requirements"
    )

    # Verify dimension requirements
    assert 'dimension_requirements' in standard['requirements'], (
        f"Tutorial '{tutorial_name}': Standard requirements missing 'dimension_requirements'"
    )
    assert len(standard['requirements']['dimension_requirements']) > 0, (
        f"Tutorial '{tutorial_name}': Standard has no dimension requirements"
    )


@pytest.mark.parametrize("tutorial_name,tutorial_metadata", tutorial_list)
def test_error_detection_works(tutorial_name, tutorial_metadata, tutorial_project):
    """Test that error data is properly detected and scored below 100%.

    Verifies:
    1. Test data (with errors) scores below 100%
    2. Errors are properly detected
    3. Quality issues are flagged by the framework

    Uses the same DataQualityAssessor approach as the 100% scoring test
    to ensure consistency.

    Args:
        tutorial_name: Use case name (e.g., "invoice")
        tutorial_metadata: Metadata from discovery
        tutorial_project: Test project fixture
    """
    from src.adri.validator.engine import DataQualityAssessor

    # Setup tutorial scenario
    scenario = TutorialScenarios.setup_tutorial_from_directory(
        tutorial_metadata, tutorial_project
    )

    # Set environment for the test
    os.environ['ADRI_ENV'] = 'development'
    os.environ['ADRI_CONFIG_PATH'] = str(tutorial_project / 'adri-config.yaml')

    # Load test data (should have quality issues)
    test_data = pd.read_csv(scenario['test_data_path'])

    # Create assessor and assess against the generated standard
    assessor = DataQualityAssessor()
    assessment = assessor.assess(
        data=test_data,
        standard_path=scenario['standard_path']
    )

    # Extract overall score
    overall_score = assessment.overall_score

    # ASSERTION: Test data should score below 100% (has quality issues)
    assert overall_score < 100.0, (
        f"Tutorial '{tutorial_name}': Test data scored {overall_score}%, "
        f"expected < 100%. Test data should contain quality issues to validate error detection."
    )


@pytest.mark.parametrize("tutorial_name,tutorial_metadata", tutorial_list)
def test_data_structure_consistency(tutorial_name, tutorial_metadata):
    """Test that training and test data have consistent structure.

    Verifies:
    1. Both CSV files have the same columns
    2. Column order matches
    3. Both files are readable

    Args:
        tutorial_name: Use case name (e.g., "invoice")
        tutorial_metadata: Metadata from discovery
    """
    # Load both CSV files
    training_data = pd.read_csv(tutorial_metadata.training_data_path)
    test_data = pd.read_csv(tutorial_metadata.test_data_path)

    # Verify column consistency
    assert list(training_data.columns) == list(test_data.columns), (
        f"Tutorial '{tutorial_name}': Column mismatch between training and test data. "
        f"Training columns: {list(training_data.columns)}, "
        f"Test columns: {list(test_data.columns)}"
    )

    # Verify both have data
    assert len(training_data) > 0, (
        f"Tutorial '{tutorial_name}': Training data is empty"
    )
    assert len(test_data) > 0, (
        f"Tutorial '{tutorial_name}': Test data is empty"
    )


@pytest.mark.parametrize("tutorial_name,tutorial_metadata", tutorial_list)
def test_file_naming_convention(tutorial_name, tutorial_metadata):
    """Test that tutorial files follow the required naming convention.

    Verifies:
    1. Training file matches pattern: <use_case>_data.csv
    2. Test file matches pattern: test_<use_case>_data.csv
    3. Use case names are consistent

    Args:
        tutorial_name: Use case name (e.g., "invoice")
        tutorial_metadata: Metadata from discovery
    """
    # Verify training file name
    training_name = tutorial_metadata.training_data_path.name
    assert training_name.endswith('_data.csv'), (
        f"Tutorial '{tutorial_name}': Training file '{training_name}' "
        f"should end with '_data.csv'"
    )
    assert not training_name.startswith('test_'), (
        f"Tutorial '{tutorial_name}': Training file '{training_name}' "
        f"should not start with 'test_'"
    )

    # Verify test file name
    test_name = tutorial_metadata.test_data_path.name
    assert test_name.startswith('test_'), (
        f"Tutorial '{tutorial_name}': Test file '{test_name}' "
        f"should start with 'test_'"
    )
    assert test_name.endswith('_data.csv'), (
        f"Tutorial '{tutorial_name}': Test file '{test_name}' "
        f"should end with '_data.csv'"
    )

    # Verify use case name consistency
    assert tutorial_metadata.use_case_name == tutorial_name, (
        f"Use case name mismatch: metadata has '{tutorial_metadata.use_case_name}', "
        f"parameter has '{tutorial_name}'"
    )


@pytest.mark.parametrize("tutorial_name,tutorial_metadata", tutorial_list)
def test_generated_standard_is_valid(tutorial_name, tutorial_metadata, tutorial_project):
    """Test that auto-generated standard passes ADRI validation requirements.

    This test ensures that standards generated for tutorials meet ADRI's
    quality requirements and structural specifications. It validates that
    the generated standard can be used reliably in production.

    Verifies:
    1. Standard has correct normalized structure
    2. Standard metadata is valid and complete
    3. Field requirements are properly structured
    4. All required sections are present
    5. All 5 quality dimensions are configured

    Args:
        tutorial_name: Use case name (e.g., "invoice")
        tutorial_metadata: Metadata from discovery
        tutorial_project: Test project fixture
    """
    from src.adri.analysis.types import (
        is_valid_standard,
        get_standard_name,
        get_field_requirements
    )

    # Setup tutorial scenario (includes standard generation)
    scenario = TutorialScenarios.setup_tutorial_from_directory(
        tutorial_metadata, tutorial_project
    )

    # Load the generated standard
    with open(scenario['standard_path'], 'r', encoding='utf-8') as f:
        standard = yaml.safe_load(f)

    # === VALIDATION 1: Structure ===
    assert is_valid_standard(standard), (
        f"Tutorial '{tutorial_name}': Generated standard does not have valid structure. "
        f"Expected: {{'standards': {{...}}, 'requirements': {{...}}}}"
    )

    # === VALIDATION 2: Metadata ===
    standard_name = get_standard_name(standard)
    assert standard_name is not None, (
        f"Tutorial '{tutorial_name}': Standard missing name in metadata"
    )
    assert len(standard_name) > 0, (
        f"Tutorial '{tutorial_name}': Standard name is empty"
    )

    # Verify complete metadata
    metadata_section = standard.get('standards', {})
    required_metadata = ['id', 'name', 'version', 'authority', 'description']
    for field in required_metadata:
        assert field in metadata_section, (
            f"Tutorial '{tutorial_name}': Standard metadata missing required field '{field}'"
        )
        assert metadata_section[field], (
            f"Tutorial '{tutorial_name}': Standard metadata field '{field}' is empty"
        )

    # === VALIDATION 3: Field Requirements ===
    field_reqs = get_field_requirements(standard)
    assert len(field_reqs) > 0, (
        f"Tutorial '{tutorial_name}': Standard has no field requirements"
    )

    # Verify each field requirement has required properties
    for field_name, field_req in field_reqs.items():
        assert 'type' in field_req, (
            f"Tutorial '{tutorial_name}': Field '{field_name}' missing 'type' property"
        )
        assert field_req['type'] in ['string', 'integer', 'float', 'date', 'datetime', 'boolean'], (
            f"Tutorial '{tutorial_name}': Field '{field_name}' has invalid type '{field_req['type']}'"
        )
        assert 'nullable' in field_req, (
            f"Tutorial '{tutorial_name}': Field '{field_name}' missing 'nullable' property"
        )

    # === VALIDATION 4: Dimension Requirements ===
    dimension_reqs = standard.get('requirements', {}).get('dimension_requirements', {})
    assert len(dimension_reqs) > 0, (
        f"Tutorial '{tutorial_name}': Standard has no dimension requirements"
    )

    # Verify all 5 ADRI dimensions are present
    expected_dimensions = ['validity', 'completeness', 'consistency', 'freshness', 'plausibility']
    for dimension in expected_dimensions:
        assert dimension in dimension_reqs, (
            f"Tutorial '{tutorial_name}': Standard missing dimension '{dimension}'"
        )

        # Verify dimension has minimum score configured
        dim_config = dimension_reqs[dimension]
        assert 'minimum_score' in dim_config, (
            f"Tutorial '{tutorial_name}': Dimension '{dimension}' missing 'minimum_score'"
        )
        assert isinstance(dim_config['minimum_score'], (int, float)), (
            f"Tutorial '{tutorial_name}': Dimension '{dimension}' minimum_score must be numeric"
        )
        assert 0 <= dim_config['minimum_score'] <= 20, (
            f"Tutorial '{tutorial_name}': Dimension '{dimension}' minimum_score must be 0-20, "
            f"got {dim_config['minimum_score']}"
        )

    # === VALIDATION 5: Overall Minimum ===
    overall_minimum = standard.get('requirements', {}).get('overall_minimum')
    assert overall_minimum is not None, (
        f"Tutorial '{tutorial_name}': Standard missing 'overall_minimum' threshold"
    )
    assert isinstance(overall_minimum, (int, float)), (
        f"Tutorial '{tutorial_name}': 'overall_minimum' must be numeric"
    )
    assert 0 <= overall_minimum <= 100, (
        f"Tutorial '{tutorial_name}': 'overall_minimum' must be 0-100, got {overall_minimum}"
    )


@pytest.mark.parametrize("tutorial_name,tutorial_metadata", tutorial_list)
def test_assessment_and_logs_are_valid(tutorial_name, tutorial_metadata, tutorial_project):
    """Test that ADRI audit log CSV files conform to ADRI standards.

    This test validates that ADRI's own CSV audit outputs meet the standards:
    - ADRI/standards/ADRI_audit_log.yaml (main assessment logs)
    - ADRI/standards/ADRI_dimension_scores.yaml (dimension score details)
    - ADRI/standards/ADRI_failed_validations.yaml (validation failures)

    This is ADRI validating itself - ensuring audit log quality and consistency.

    Verifies:
    1. Main audit log CSV structure and content validity
    2. Dimension scores CSV structure and content validity
    3. Failed validations CSV structure and content validity (when failures exist)

    Args:
        tutorial_name: Use case name (e.g., "invoice")
        tutorial_metadata: Metadata from discovery
        tutorial_project: Test project fixture
    """
    from src.adri.validator.engine import DataQualityAssessor

    # Setup tutorial scenario
    scenario = TutorialScenarios.setup_tutorial_from_directory(
        tutorial_metadata, tutorial_project
    )

    # Set environment for the test
    os.environ['ADRI_ENV'] = 'development'
    os.environ['ADRI_CONFIG_PATH'] = str(tutorial_project / 'adri-config.yaml')

    # Load training data and run assessment to generate logs
    training_data = pd.read_csv(scenario['training_data_path'])

    # Create assessor and perform assessment (generates audit logs)
    assessor = DataQualityAssessor()
    assessment = assessor.assess(
        data=training_data,
        standard_path=scenario['standard_path']
    )

    audit_logs_dir = tutorial_project / 'ADRI' / 'dev' / 'audit-logs'

    # === VALIDATION 1: Main Assessment Log CSV ===
    main_log_path = audit_logs_dir / 'adri_assessment_logs.csv'

    if main_log_path.exists():
        main_log_df = pd.read_csv(main_log_path)
        main_log_standard = Path('ADRI/standards/ADRI_audit_log.yaml')

        if main_log_standard.exists() and len(main_log_df) > 0:
            # Validate most recent entry
            recent_main_log = main_log_df.tail(1)

            validator = DataQualityAssessor()
            validation = validator.assess(
                data=recent_main_log,
                standard_path=str(main_log_standard)
            )

            assert validation.overall_score >= 85.0, (
                f"Tutorial '{tutorial_name}': Main audit log quality score is "
                f"{validation.overall_score}%, expected >= 85%. "
                f"ADRI's main audit log must meet quality standards."
            )

    # === VALIDATION 2: Dimension Scores CSV ===
    dim_scores_path = audit_logs_dir / 'adri_dimension_scores.csv'

    if dim_scores_path.exists():
        dim_scores_df = pd.read_csv(dim_scores_path)
        dim_scores_standard = Path('ADRI/standards/ADRI_dimension_scores.yaml')

        if dim_scores_standard.exists() and len(dim_scores_df) > 0:
            # Get the most recent assessment's dimension scores (last 5 entries)
            recent_dim_scores = dim_scores_df.tail(5)

            validator = DataQualityAssessor()
            validation = validator.assess(
                data=recent_dim_scores,
                standard_path=str(dim_scores_standard)
            )

            assert validation.overall_score >= 85.0, (
                f"Tutorial '{tutorial_name}': Dimension scores CSV quality score is "
                f"{validation.overall_score}%, expected >= 85%. "
                f"ADRI's dimension score logs must meet quality standards."
            )

            # Verify all 5 dimensions are logged
            dimension_names = set(recent_dim_scores['dimension_name'].unique())
            expected_dimensions = {'validity', 'completeness', 'consistency', 'freshness', 'plausibility'}
            assert dimension_names == expected_dimensions, (
                f"Tutorial '{tutorial_name}': Expected all 5 dimensions in logs, "
                f"got {dimension_names}"
            )

    # === VALIDATION 3: Failed Validations CSV (if failures exist) ===
    failed_val_path = audit_logs_dir / 'adri_failed_validations.csv'

    if failed_val_path.exists():
        failed_val_df = pd.read_csv(failed_val_path)
        failed_val_standard = Path('ADRI/standards/ADRI_failed_validations.yaml')

        if failed_val_standard.exists() and len(failed_val_df) > 0:
            # Validate a sample of recent failures
            recent_failures = failed_val_df.tail(min(10, len(failed_val_df)))

            validator = DataQualityAssessor()
            validation = validator.assess(
                data=recent_failures,
                standard_path=str(failed_val_standard)
            )

            # Note: Lower threshold since this file only exists when there are failures
            assert validation.overall_score >= 80.0, (
                f"Tutorial '{tutorial_name}': Failed validations CSV quality score is "
                f"{validation.overall_score}%, expected >= 80%. "
                f"ADRI's validation failure logs must meet quality standards."
            )


@pytest.mark.parametrize("tutorial_name,tutorial_metadata", tutorial_list)
def test_baseline_regression(tutorial_name, tutorial_metadata, tutorial_project):
    """Test for baseline regression detection in tutorial artifacts.

    This test implements a self-initializing baseline regression system that:
    1. First run: Creates baseline_outcome/ folder and captures all artifacts
    2. Subsequent runs: Compares current artifacts against baseline
    3. Auto-heals: Regenerates accidentally deleted baseline files
    4. Fails: When regressions are detected with detailed diff report

    Baseline artifacts tracked (4 files):
    - <use_case>_data_ADRI_standard.yaml: Generated standard
    - adri_assessment_logs.csv: Assessment results
    - adri_dimension_scores.csv: Dimension breakdowns
    - adri_failed_validations.csv: Validation failures

    Update workflow (when changes are intentional):
        rm -rf ADRI/tutorials/<tutorial>/baseline_outcome/
        pytest tests/test_tutorial_auto_discovery.py::test_baseline_regression -k <tutorial>
        git add ADRI/tutorials/<tutorial>/baseline_outcome/
        git commit -m "Update baseline for <tutorial>"

    Args:
        tutorial_name: Use case name (e.g., "invoice")
        tutorial_metadata: Metadata from discovery
        tutorial_project: Test project fixture
    """
    from tests.fixtures.baseline_utils import (
        check_baseline_status,
        get_generated_artifacts,
        capture_baseline_artifacts,
        get_baseline_directory,
        compare_with_baseline,
        format_diff_report
    )
    from src.adri.validator.engine import DataQualityAssessor

    # Setup tutorial scenario to generate artifacts
    scenario = TutorialScenarios.setup_tutorial_from_directory(
        tutorial_metadata, tutorial_project
    )

    # Set environment for the test
    os.environ['ADRI_ENV'] = 'development'
    os.environ['ADRI_CONFIG_PATH'] = str(tutorial_project / 'adri-config.yaml')

    # Run assessment to generate all artifacts
    training_data = pd.read_csv(scenario['training_data_path'])

    # Load config to enable audit logging
    import yaml
    config_path = tutorial_project / 'adri-config.yaml'
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)

    # FIX: Convert relative audit log path to absolute path in test project
    # The parity tests use absolute paths - we need to do the same
    if 'adri' in config and 'audit' in config['adri']:
        audit_config = config['adri']['audit']
        # Create absolute path to audit logs in test project
        audit_logs_dir = tutorial_project / 'ADRI' / 'dev' / 'audit-logs'
        audit_logs_dir.mkdir(parents=True, exist_ok=True)
        # Replace relative path with absolute path
        audit_config['log_dir'] = str(audit_logs_dir)

    # Create assessor with corrected audit logging config
    assessor = DataQualityAssessor(config=config.get('adri', {}))
    assessment = assessor.assess(
        data=training_data,
        standard_path=scenario['standard_path']
    )

    # Collect generated artifacts from test project
    artifacts = get_generated_artifacts(tutorial_project, tutorial_metadata.use_case_name)

    # Also ensure audit logs directory exists and check for CSVs
    audit_logs_dir = tutorial_project / 'ADRI' / 'dev' / 'audit-logs'
    if audit_logs_dir.exists():
        print(f"Audit logs directory exists: {audit_logs_dir}")
        for file in audit_logs_dir.glob('*.csv'):
            print(f"  Found CSV: {file.name}")

    # Check if this is the actual tutorial directory (not test project)
    # We need to use the real tutorial directory for baseline storage
    actual_tutorial_dir = tutorial_metadata.directory

    # Check baseline status
    status = check_baseline_status(tutorial_metadata)

    if status.is_first_run:
        # First run: Capture baseline and skip test
        print(f"\n{'=' * 80}")
        print(f"FIRST RUN: Capturing baseline for '{tutorial_name}'")
        print(f"{'=' * 80}")
        print(f"Tutorial directory: {actual_tutorial_dir}")
        print(f"Baseline will be saved to: {actual_tutorial_dir / 'baseline_outcome'}")
        print(f"\nCapturing {len(artifacts)} artifacts:")
        for artifact_type, artifact_path in artifacts.items():
            print(f"  - {artifact_type}: {artifact_path.name}")
        print(f"\n{'=' * 80}\n")

        capture_baseline_artifacts(actual_tutorial_dir, artifacts)

        print(f"\n{'=' * 80}")
        print(f"✓ Baseline captured successfully for '{tutorial_name}'")
        print(f"✓ Baseline location: {actual_tutorial_dir / 'baseline_outcome'}")
        print(f"\nNext steps:")
        print(f"1. Review captured baseline files")
        print(f"2. Commit to git: git add {actual_tutorial_dir / 'baseline_outcome'}")
        print(f"3. Re-run test to verify baseline comparison")
        print(f"{'=' * 80}\n")

        pytest.skip(f"First run: Baseline captured for '{tutorial_name}'")

    else:
        # Subsequent runs: Compare with baseline
        baseline_dir = get_baseline_directory(tutorial_metadata)

        print(f"\nComparing artifacts against baseline for '{tutorial_name}'...")
        print(f"Baseline directory: {baseline_dir}")
        print(f"Artifacts to compare: {len(artifacts)}")

        results = compare_with_baseline(artifacts, baseline_dir)

        # Check for auto-healed files
        auto_healed = [r for r in results if r.auto_healed]
        if auto_healed:
            print(f"\n⚠ Auto-healed {len(auto_healed)} missing baseline file(s):")
            for result in auto_healed:
                print(f"  ✓ {result.artifact.filename}")
            print(f"\nBaseline files regenerated. Please commit updated baseline to git.")

        # Check for failures
        failures = [r for r in results if not r.matches and not r.auto_healed]

        if failures:
            # Generate detailed diff report
            diff_report = format_diff_report(failures)
            pytest.fail(
                f"\nBaseline regression detected in '{tutorial_name}'!\n\n{diff_report}"
            )

        # All passed
        print(f"✓ All {len(results)} artifacts match baseline")
        for result in results:
            status_icon = "🔄" if result.auto_healed else "✓"
            print(f"  {status_icon} {result.artifact.filename}")


# Additional test to verify discovery mechanism itself
def test_tutorial_discovery_finds_tutorials():
    """Test that the discovery mechanism finds at least one tutorial.

    This test validates the auto-discovery framework itself.
    """
    tutorials = find_tutorial_directories()

    # Should find at least the invoice tutorial
    assert len(tutorials) > 0, (
        "No tutorials found in ADRI/tutorials/. "
        "Expected at least the invoice_processing tutorial."
    )

    # Verify structure of discovered tutorials
    for tutorial in tutorials:
        assert tutorial.use_case_name, "Tutorial missing use_case_name"
        assert tutorial.directory.exists(), f"Tutorial directory doesn't exist: {tutorial.directory}"
        assert tutorial.training_data_path.exists(), f"Training data missing: {tutorial.training_data_path}"
        assert tutorial.test_data_path.exists(), f"Test data missing: {tutorial.test_data_path}"
