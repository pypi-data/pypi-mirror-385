{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69aeaf81-607b-4d55-9e77-be267ac08479",
   "metadata": {},
   "source": [
    "# PELICAN Quickstart\n",
    "# [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/heidelberg-hepml/pelican/blob/main/examples/demo.ipynb)\n",
    "\n",
    "In this tutorial, we give a quick introduction for how to use the PELICAN architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58232c6b-85e1-4e48-857b-24995368ef6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install the pelican-hep package\n",
    "%pip install pelican-hep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad94a506-afb2-47ad-9c44-dbdf7f1081f1",
   "metadata": {},
   "source": [
    "### 0) Generate particle data\n",
    "\n",
    "We start by generating toy particle data, for instance for an amplitude regression task. We describe particles by a four-momentum and one scalar feature, for instance the particle type. Using random numbers, we generate a batch of 128 events with 10 particles each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecd9b4b6-eff1-4609-bdda-44157908e7e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 10, 4]) torch.Size([128, 10, 1])\n"
     ]
    }
   ],
   "source": [
    "# generate particle data\n",
    "import torch\n",
    "\n",
    "num_scalars = 1\n",
    "B, N = 128, 10\n",
    "mass = 1\n",
    "p3 = torch.randn(B, N, 3)\n",
    "fourmomenta = torch.cat(\n",
    "    [(mass**2 + (p3**2).sum(dim=-1, keepdims=True)).sqrt(), p3], dim=-1\n",
    ")\n",
    "scalars = torch.randn(B, N, num_scalars)\n",
    "print(fourmomenta.shape, scalars.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edfa166-ae03-44b2-8dfe-afa0d2ede4bc",
   "metadata": {},
   "source": [
    "### 1) Represent particle data as a sparse tensor\n",
    "\n",
    "At the core of this PELICAN implementation is the representation of sparse tensors. Instead of relying on zero-padding, this approach flattens the batch and node dimensions into a common node-across-batch dimension. The `ptr` or `batch` objects carry the information of which batch a node belongs to. In addition the `edge_index` allows us to operate on arbitrary graphs, instead of only fully connected graphs. Note that no zero-padding is required for the toy data that we are using in this demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "349e272d-ae9c-473f-9699-e4b24027f83b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1280, 1]) torch.Size([1280, 4])\n",
      "torch.Size([2, 12800]) torch.Size([1280])\n"
     ]
    }
   ],
   "source": [
    "from pelican.utils import get_batch_from_ptr, get_edge_index_from_ptr, get_edge_index_from_shape\n",
    "\n",
    "scalars_sparse = scalars.flatten(end_dim=-2)\n",
    "fourmomenta_sparse = fourmomenta.flatten(end_dim=-2)\n",
    "print(scalars_sparse.shape, fourmomenta_sparse.shape)\n",
    "\n",
    "# approach 1 (assumes dense tensors)\n",
    "edge_index, batch = get_edge_index_from_shape(fourmomenta.shape, remove_self_loops=False)\n",
    "print(edge_index.shape, batch.shape)\n",
    "\n",
    "# approach 2 (start from generic ptr)\n",
    "ptr = torch.arange(B+1) * N\n",
    "batch_2 = get_batch_from_ptr(ptr)\n",
    "edge_index_2 = get_edge_index_from_ptr(ptr, fourmomenta_sparse.shape, remove_self_loops=False)\n",
    "\n",
    "# consistency checks\n",
    "assert torch.all(batch == batch_2)\n",
    "assert torch.all(edge_index == edge_index_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92eea1d-50e3-498d-82e6-8d9e712997b2",
   "metadata": {},
   "source": [
    "### 2) Organize PELICAN inputs into ranks\n",
    "\n",
    "PELICAN assumes Lorentz-invariant inputs and processes them with permutation-equivariant operations. To this end, the inputs to the architecture have to be organized by their transformation behaviour under permutations. There is graph-level information (rank 0), node-level information (rank 1), and edge-level information (rank 2). Extending this framework to higher-order representations is straight-forward, however those are expected to be less relevant in high-energy physics applications.\n",
    "\n",
    "In our case, we first have the particle-wise scalar information as rank 1 inputs. To turn the Lorentz vectors $p_i^\\mu$ into Lorentz-invariants, we can take their inner products $p_i^\\mu p_{j,\\mu}$, or equivalently $(p_i+p_j)^\\mu (p_i+p_j)_\\mu$, as rank 2 or edge-level information. Note that the particle masses $m_i^2 = p_i^\\mu p_{i,\\mu}$ are node-level invariants, but emerge from the set of rank 2 objects. There are no rank 0 objects in this example, but they are also supported in the PELICAN architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e20a97f-da8a-4faf-9ac7-d1526b8eb957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1280, 1]) torch.Size([12800, 1])\n"
     ]
    }
   ],
   "source": [
    "from pelican.utils import get_edge_attr\n",
    "\n",
    "in_rank1 = scalars_sparse\n",
    "in_rank2 = get_edge_attr(fourmomenta_sparse, edge_index).unsqueeze(-1)\n",
    "print(in_rank1.shape, in_rank2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce40fbce-9dd7-49a0-b708-2bb96fd9adb9",
   "metadata": {},
   "source": [
    "### 3) Process inputs with PELICAN network\n",
    "\n",
    "We are now ready to process our data with a PELICAN network. It first projects all input data into rank 2 representations, processes them in that representation, and finally maps them back to outputs of rank 0, 1, or 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28b9b311-1cd1-4791-9cf9-245eeead9ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([128, 1])\n",
      "1 torch.Size([1280, 1])\n",
      "2 torch.Size([12800, 1])\n"
     ]
    }
   ],
   "source": [
    "from pelican.nets import PELICAN\n",
    "\n",
    "for out_rank in range(3):\n",
    "    net = PELICAN(\n",
    "        in_channels_rank2=1,\n",
    "        in_channels_rank1=num_scalars,\n",
    "        in_channels_rank0=0,\n",
    "        out_rank=out_rank,\n",
    "        out_channels=1,\n",
    "        num_blocks=2,\n",
    "        hidden_channels=16,\n",
    "    )\n",
    "    out = net(edge_index, batch, in_rank2=in_rank2, in_rank1=in_rank1)\n",
    "    print(out_rank, out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e11b15a-8cd2-425b-87de-d13f0b077889",
   "metadata": {},
   "source": [
    "Thats it, now you're ready to build your own `PELICAN` networks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfcf599-b98c-4098-8a9a-93686968bb8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
