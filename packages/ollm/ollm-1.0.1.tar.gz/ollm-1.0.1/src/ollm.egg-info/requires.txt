numpy
torch>2.6.0
transformers>=4.57.0
accelerate
flash-linear-attention
flash-attn
