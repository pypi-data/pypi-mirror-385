workload: 
  type: dummy

docker:
  env:
    - "LMCACHE_CHUNK_SIZE=256"
    - "LMCACHE_LOCAL_CPU=True"
    - "LMCACHE_MAX_LOCAL_CPU_SIZE=5"

vllm:
  model: "meta-llama/Llama-3.2-1B-Instruct"
  args:
    - "--max-model-len"
    - "1024"
    - "--gpu-memory-utilization"
    - "0.35"
    - "--enforce-eager"
    - "--load-format"
    - "dummy"
    - "--kv-transfer-config"
    - "{\"kv_connector\":\"LMCacheConnectorV1\",\"kv_role\":\"kv_both\"}"
