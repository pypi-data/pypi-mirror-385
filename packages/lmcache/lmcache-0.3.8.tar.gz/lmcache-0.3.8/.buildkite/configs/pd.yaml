workload:
  type: long_doc_qa
  max-inflight-requests: 20
  repeat-count: 1
  expected-latency: 1.625
  completion: true

feature:
  type: pd

docker-prefiller:
  env:
    - "LMCACHE_LOCAL_CPU=False"
    - "LMCACHE_MAX_LOCAL_CPU_SIZE=5"
    - "LMCACHE_ENABLE_PD=True"
    - "LMCACHE_TRANSFER_CHANNEL=nixl"
    - "LMCACHE_PD_ROLE=sender"
    - "LMCACHE_PD_PROXY_HOST=localhost"
    - "LMCACHE_PD_BUFFER_SIZE=1073741824" # 1GB
    - "LMCACHE_PD_BUFFER_DEVICE=cuda"
    - "PYTHONHASHSEED=0"
  proxy-port: 7500

docker-decoder:
  env:
    - "LMCACHE_LOCAL_CPU=False"
    - "LMCACHE_MAX_LOCAL_CPU_SIZE=0"
    - "LMCACHE_ENABLE_PD=True"
    - "LMCACHE_TRANSFER_CHANNEL=nixl"
    - "LMCACHE_PD_ROLE=receiver"
    - "LMCACHE_PD_PEER_HOST=localhost"
    - "LMCACHE_PD_BUFFER_SIZE=2147483648" # 2GB
    - "LMCACHE_PD_BUFFER_DEVICE=cuda"
    - "PYTHONHASHSEED=0"
  init-port: 7300
  alloc-port: 7400

vllm-prefiller:
  model: "meta-llama/Llama-3.2-1B-Instruct"
  args:
    - "--load-format"
    - "dummy"
    - "--no-enable-prefix-caching"
    - "--kv-transfer-config"
    - "{\"kv_connector\":\"LMCacheConnectorV1\",\"kv_role\":\"kv_producer\",\"kv_connector_extra_config\": {\"discard_partial_chunks\": false, \"lmcache_rpc_port\": \"producer1\"}}"

vllm-decoder:
  model: "meta-llama/Llama-3.2-1B-Instruct"
  args:
    - "--load-format"
    - "dummy"
    - "--no-enable-prefix-caching"
    - "--kv-transfer-config"
    - "{\"kv_connector\":\"LMCacheConnectorV1\",\"kv_role\":\"kv_consumer\",\"kv_connector_extra_config\": {\"discard_partial_chunks\": false, \"lmcache_rpc_port\": \"consumer1\", \"skip_last_n_tokens\": 1}}"
