[
  {
    "type": "PackageTool",
    "name": "get_scikit_learn_info",
    "description": "Get comprehensive information about scikit-learn – simple and efficient tools for predictive data analysis",
    "parameter": {
      "type": "object",
      "properties": {
        "include_examples": {
          "type": "boolean",
          "description": "Whether to include usage examples and quick start guide",
          "default": true
        }
      },
      "required": [
        "include_examples"
      ]
    },
    "package_name": "scikit-learn",
    "local_info": {
      "name": "scikit-learn",
      "description": "Simple and efficient tools for predictive data analysis. Built on NumPy, SciPy, and matplotlib. Provides a range of supervised and unsupervised learning algorithms.",
      "category": "Machine Learning",
      "import_name": "sklearn",
      "popularity": 90,
      "keywords": [
        "machine learning",
        "classification",
        "regression",
        "clustering",
        "dimensionality reduction"
      ],
      "documentation": "https://scikit-learn.org/stable/",
      "repository": "https://github.com/scikit-learn/scikit-learn",
      "installation": {
        "pip": "pip install scikit-learn",
        "conda": "conda install scikit-learn"
      },
      "usage_example": "from sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Load dataset\niris = datasets.load_iris()\nX, y = iris.data, iris.target\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n\n# Train model\nclf = RandomForestClassifier()\nclf.fit(X_train, y_train)\n\n# Make predictions\ny_pred = clf.predict(X_test)\nprint(f'Accuracy: {accuracy_score(y_test, y_pred)}')",
      "quick_start": [
        "Install: pip install scikit-learn",
        "Import: from sklearn import datasets, model_selection",
        "Load data: X, y = datasets.load_iris(return_X_y=True)",
        "Split: train_test_split(X, y, test_size=0.3)",
        "Train: model.fit(X_train, y_train); predict: model.predict(X_test)"
      ]
    }
  },
  {
    "type": "PackageTool",
    "name": "get_pytorch_info",
    "description": "Get comprehensive information about PyTorch – an open source machine learning framework",
    "parameter": {
      "type": "object",
      "properties": {
        "include_examples": {
          "type": "boolean",
          "description": "Whether to include usage examples and quick start guide",
          "default": true
        }
      },
      "required": [
        "include_examples"
      ]
    },
    "package_name": "torch",
    "local_info": {
      "name": "PyTorch",
      "description": "An open source machine learning framework that accelerates the path from research prototyping to production deployment. Provides tensor computation with strong GPU acceleration and deep neural networks.",
      "category": "Deep Learning",
      "import_name": "torch",
      "popularity": 88,
      "keywords": [
        "deep learning",
        "neural networks",
        "tensors",
        "GPU",
        "autograd"
      ],
      "documentation": "https://pytorch.org/docs/stable/",
      "repository": "https://github.com/pytorch/pytorch",
      "installation": {
        "pip": "pip install torch torchvision torchaudio",
        "conda": "conda install pytorch torchvision torchaudio -c pytorch"
      },
      "usage_example": "import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n# Create a simple neural network\nclass SimpleNet(nn.Module):\n    def __init__(self):\n        super(SimpleNet, self).__init__()\n        self.fc1 = nn.Linear(784, 128)\n        self.fc2 = nn.Linear(128, 10)\n        self.relu = nn.ReLU()\n    \n    def forward(self, x):\n        x = self.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\n# Create model instance\nmodel = SimpleNet()\nprint(model)",
      "quick_start": [
        "Install: pip install torch torchvision",
        "Import: import torch, import torch.nn as nn",
        "Create tensors: torch.randn(2, 3)",
        "Define model: class MyModel(nn.Module)",
        "Train: optimizer.zero_grad(); loss.backward(); optimizer.step()"
      ]
    }
  },
  {
    "type": "PackageTool",
    "name": "get_torch_geometric_info",
    "description": "Get comprehensive information about PyTorch Geometric – a high-performance library for graph neural networks widely used in molecular and materials science.",
    "parameter": {
      "type": "object",
      "properties": {
        "include_examples": {
          "type": "boolean",
          "description": "Whether to include usage examples and quick start guide",
          "default": true
        }
      },
      "required": [
        "include_examples"
      ]
    },
    "package_name": "torch_geometric",
    "local_info": {
      "name": "PyTorch Geometric",
      "description": "A geometric deep-learning extension library for PyTorch that offers efficient GNN layers, data loaders and utilities – ideal for molecules, crystals and knowledge-graphs.",
      "category": "AI for Science / Graph Neural Networks",
      "import_name": "torch_geometric",
      "popularity": 78,
      "keywords": [
        "graph neural networks",
        "geometric deep learning",
        "molecular data",
        "PyTorch",
        "GNNs"
      ],
      "documentation": "https://pytorch-geometric.readthedocs.io/",
      "repository": "https://github.com/pyg-team/pytorch_geometric",
      "installation": {
        "pip": "pip install torch-geometric",
        "conda": "conda install pyg -c pyg -c conda-forge"
      },
      "usage_example": "from torch_geometric.datasets import QM9\nfrom torch_geometric.nn import GCNConv, global_mean_pool\nimport torch\n\ndataset = QM9(root='data/QM9')\n\nclass GCN(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = GCNConv(dataset.num_node_features, 128)\n        self.conv2 = GCNConv(128, 64)\n        self.lin = torch.nn.Linear(64, 1)\n    def forward(self, x, edge_index, batch):\n        x = torch.relu(self.conv1(x, edge_index))\n        x = torch.relu(self.conv2(x, edge_index))\n        x = global_mean_pool(x, batch)\n        return self.lin(x)\n\nmodel = GCN()",
      "quick_start": [
        "Install: pip install torch-geometric",
        "Load data: from torch_geometric.datasets import TUDataset",
        "Create GNN: from torch_geometric.nn import GCNConv",
        "Prepare graph data (e.g., QM9)",
        "Define a GNN model and loss",
        "Train and evaluate with PyTorch"
      ]
    }
  },
  {
    "type": "PackageTool",
    "name": "get_hyperopt_info",
    "description": "Get comprehensive information about Hyperopt – distributed hyperparameter optimization",
    "parameter": {
      "type": "object",
      "properties": {
        "include_examples": {
          "type": "boolean",
          "description": "Whether to include usage examples and quick start guide",
          "default": true
        }
      },
      "required": [
        "include_examples"
      ]
    },
    "package_name": "hyperopt",
    "local_info": {
      "name": "Hyperopt",
      "description": "Python library for hyperparameter optimization using algorithms like random search, Tree of Parzen Estimators (TPE), and adaptive TPE. Supports distributed optimization across multiple cores/machines.",
      "category": "Machine Learning Optimization",
      "import_name": "hyperopt",
      "popularity": 85,
      "keywords": [
        "hyperparameter optimization",
        "Bayesian optimization",
        "TPE",
        "distributed computing",
        "model tuning"
      ],
      "documentation": "http://hyperopt.github.io/hyperopt/",
      "repository": "https://github.com/hyperopt/hyperopt",
      "installation": {
        "pip": "pip install hyperopt",
        "conda": "conda install -c conda-forge hyperopt"
      },
      "usage_example": "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\n\n# Define search space\nspace = {\n    'n_estimators': hp.choice('n_estimators', [10, 50, 100, 200]),\n    'max_depth': hp.choice('max_depth', [3, 5, 10, None])\n}\n\n# Objective function\ndef objective(params):\n    clf = RandomForestClassifier(**params)\n    score = cross_val_score(clf, X, y, cv=3).mean()\n    return {'loss': -score, 'status': STATUS_OK}\n\n# Optimize\ntrials = Trials()\nbest = fmin(objective, space, algo=tpe.suggest, max_evals=100, trials=trials)",
      "quick_start": [
        "Install: pip install hyperopt",
        "Import: from hyperopt import fmin, tpe, hp",
        "Define space: space = {'param': hp.choice('param', [1, 2, 3])}",
        "Define objective: def objective(params): return loss",
        "Optimize: best = fmin(objective, space, algo=tpe.suggest)"
      ]
    }
  },
  {
    "type": "PackageTool",
    "name": "get_umap_learn_info",
    "description": "Get comprehensive information about UMAP-learn – dimensionality reduction technique",
    "parameter": {
      "type": "object",
      "properties": {
        "include_examples": {
          "type": "boolean",
          "description": "Whether to include usage examples and quick start guide",
          "default": true
        }
      },
      "required": [
        "include_examples"
      ]
    },
    "package_name": "umap-learn",
    "local_info": {
      "name": "UMAP",
      "description": "Uniform Manifold Approximation and Projection for dimensionality reduction. Preserves local and global structure while being computationally efficient for large datasets.",
      "category": "Dimensionality Reduction",
      "import_name": "umap",
      "popularity": 88,
      "keywords": [
        "dimensionality reduction",
        "manifold learning",
        "visualization",
        "clustering",
        "single-cell"
      ],
      "documentation": "https://umap-learn.readthedocs.io/",
      "repository": "https://github.com/lmcinnes/umap",
      "installation": {
        "pip": "pip install umap-learn",
        "conda": "conda install -c conda-forge umap-learn"
      },
      "usage_example": "import umap\nfrom sklearn.datasets import load_iris\nimport matplotlib.pyplot as plt\n\n# Load data\niris = load_iris()\nX, y = iris.data, iris.target\n\n# Apply UMAP\nreducer = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=2)\nembedding = reducer.fit_transform(X)\n\n# Plot results\nplt.scatter(embedding[:, 0], embedding[:, 1], c=y, cmap='viridis')\nplt.title('UMAP projection of Iris dataset')\nplt.show()",
      "quick_start": [
        "Install: pip install umap-learn",
        "Import: import umap",
        "Create: reducer = umap.UMAP(n_components=2)",
        "Fit: embedding = reducer.fit_transform(data)",
        "Plot: Use matplotlib/seaborn for visualization"
      ]
    }
  },
  {
    "type": "PackageTool",
    "name": "get_statsmodels_info",
    "description": "Get comprehensive information about statsmodels – statistical modeling and econometrics",
    "parameter": {
      "type": "object",
      "properties": {
        "include_examples": {
          "type": "boolean",
          "description": "Whether to include usage examples and quick start guide",
          "default": true
        }
      },
      "required": [
        "include_examples"
      ]
    },
    "package_name": "statsmodels",
    "local_info": {
      "name": "statsmodels",
      "description": "Statistical modeling library providing classes and functions for statistical estimation, statistical tests, statistical data exploration, and regression analysis.",
      "category": "Statistical Analysis",
      "import_name": "statsmodels",
      "popularity": 85,
      "keywords": [
        "statistical modeling",
        "regression",
        "time series",
        "econometrics",
        "hypothesis testing"
      ],
      "documentation": "https://www.statsmodels.org/stable/",
      "repository": "https://github.com/statsmodels/statsmodels",
      "installation": {
        "pip": "pip install statsmodels",
        "conda": "conda install -c conda-forge statsmodels"
      },
      "usage_example": "import statsmodels.api as sm\nimport numpy as np\nimport pandas as pd\n\n# Generate sample data\nnp.random.seed(42)\nX = np.random.randn(100, 2)\ny = X[:, 0] + 2*X[:, 1] + np.random.randn(100)\n\n# Add constant for intercept\nX = sm.add_constant(X)\n\n# Fit OLS regression\nmodel = sm.OLS(y, X)\nresults = model.fit()\nprint(results.summary())",
      "quick_start": [
        "Install: pip install statsmodels",
        "Import: import statsmodels.api as sm",
        "Prepare: X = sm.add_constant(X)  # for intercept",
        "Fit: model = sm.OLS(y, X); results = model.fit()",
        "Analyze: results.summary(), results.params"
      ]
    }
  },
  {
    "type": "PackageTool",
    "name": "get_schnetpack_info",
    "description": "Get comprehensive information about SchNetPack – a deep-learning toolbox for molecules and materials built on PyTorch.",
    "parameter": {
      "type": "object",
      "properties": {
        "include_examples": {
          "type": "boolean",
          "description": "Whether to include usage examples and a quick-start guide",
          "default": true
        }
      },
      "required": [
        "include_examples"
      ]
    },
    "package_name": "schnetpack",
    "local_info": {
      "name": "SchNetPack",
      "description": "A modular deep-learning framework implementing SchNet and related architectures for quantum-chemical and atomistic property prediction.",
      "category": "AI for Science / Atomistic ML",
      "import_name": "schnetpack",
      "popularity": 60,
      "keywords": [
        "atomistic ML",
        "quantum chemistry",
        "materials",
        "SchNet",
        "PyTorch"
      ],
      "documentation": "https://schnetpack.readthedocs.io/",
      "repository": "https://github.com/atomistic-machine-learning/schnetpack",
      "installation": {
        "pip": "pip install schnetpack",
        "conda": "conda install -c conda-forge schnetpack"
      },
      "usage_example": "import schnetpack as spk\nimport torch\n\ndataset = spk.data.AtomsData('qm9.db')\ntrain, val, test = spk.train.train_test_split(data=dataset, split=[8,1,1])\nrepresentation = spk.representation.SchNet(n_atom_basis=128)\nprediction = spk.output_modules.Atomwise(property='energy', mean=dataset._property_mean('energy'))\nmodel = spk.AtomisticModel(representation, prediction)\ntrainer = spk.train.Trainer('run', model, loss_fn=torch.nn.MSELoss(),\n                            train_loader=train, validation_loader=val)\ntrainer.train(device=torch.device('cpu'))",
      "quick_start": [
        "1. Install SchNetPack: pip install schnetpack",
        "2. Load or create an AtomsData database",
        "3. Build a SchNet representation layer",
        "4. Add an output module (e.g., Atomwise)",
        "5. Train with spk.train.Trainer"
      ]
    }
  },
  {
    "type": "PackageTool",
    "name": "get_harmony_pytorch_info",
    "description": "Get comprehensive information about harmony-pytorch – single-cell data integration",
    "parameter": {
      "type": "object",
      "properties": {
        "info_type": {
          "type": "string",
          "enum": [
            "overview",
            "installation",
            "usage",
            "documentation"
          ],
          "description": "Type of information to retrieve about harmony-pytorch"
        }
      },
      "required": [
        "info_type"
      ]
    },
    "package_name": "harmony-pytorch",
    "local_info": {
      "name": "harmony-pytorch",
      "description": "PyTorch implementation of Harmony algorithm for single-cell data integration. Removes batch effects and integrates datasets from different experimental conditions, technologies, or laboratories.",
      "category": "Single-Cell Integration",
      "import_name": "harmony",
      "popularity": 68,
      "keywords": [
        "batch correction",
        "data integration",
        "single-cell",
        "harmony",
        "batch effects"
      ],
      "documentation": "https://github.com/lilab-bcb/harmony-pytorch",
      "repository": "https://github.com/lilab-bcb/harmony-pytorch",
      "installation": {
        "pip": "pip install harmony-pytorch",
        "conda": "conda install -c conda-forge harmony-pytorch"
      },
      "usage_example": "import numpy as np\nimport pandas as pd\nfrom harmony import harmonize\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.datasets import make_blobs\n\n# Create sample data with batch effects\nnp.random.seed(42)\n\n# Generate two batches of data\nbatch1_data, _ = make_blobs(n_samples=200, centers=3, n_features=50, \n                           cluster_std=1.0, center_box=(0.0, 5.0))\nbatch2_data, _ = make_blobs(n_samples=200, centers=3, n_features=50, \n                           cluster_std=1.0, center_box=(3.0, 8.0))  # Shifted\n\n# Combine data\nX = np.vstack([batch1_data, batch2_data])\nbatch_labels = ['Batch1'] * 200 + ['Batch2'] * 200\n\nprint(f'Combined data shape: {X.shape}')\nprint(f'Batch distribution: {pd.Series(batch_labels).value_counts().to_dict()}')\n\n# Apply PCA for visualization\npca = PCA(n_components=50)\nX_pca = pca.fit_transform(X)\n\nprint(f'PCA data shape: {X_pca.shape}')\nprint(f'Explained variance ratio (first 5 PCs): {pca.explained_variance_ratio_[:5]}')\n\n# Apply Harmony for batch correction\nprint('\\nApplying Harmony batch correction...')\n\n# Create batch array (required format)\nbatch_array = np.array([0 if b == 'Batch1' else 1 for b in batch_labels])\n\n# Run Harmony\nX_harmony = harmonize(\n    X_pca,  # PCA coordinates\n    batch_array,  # Batch assignments\n    batch_key='batch',\n    max_iter_harmony=20,\n    random_state=42\n)\n\nprint(f'Harmony corrected data shape: {X_harmony.shape}')\n\n# Visualize results\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n\n# Plot before Harmony\ncolors = ['red' if b == 'Batch1' else 'blue' for b in batch_labels]\nax1.scatter(X_pca[:, 0], X_pca[:, 1], c=colors, alpha=0.6)\nax1.set_title('Before Harmony Correction')\nax1.set_xlabel('PC1')\nax1.set_ylabel('PC2')\nax1.legend(['Batch1', 'Batch2'])\n\n# Plot after Harmony\nax2.scatter(X_harmony[:, 0], X_harmony[:, 1], c=colors, alpha=0.6)\nax2.set_title('After Harmony Correction')\nax2.set_xlabel('Harmony PC1')\nax2.set_ylabel('Harmony PC2')\nax2.legend(['Batch1', 'Batch2'])\n\nplt.tight_layout()\nplt.show()\n\nprint('\\nHarmony integration complete!')\nprint('Use the corrected coordinates for downstream analysis')",
      "quick_start": [
        "Install: pip install harmony-pytorch",
        "Import: from harmony import harmonize",
        "Prepare data: PCA coordinates + batch labels",
        "Run Harmony: harmonize(X_pca, batch_array)",
        "Use corrected coordinates for clustering/UMAP",
        "Integrate with scanpy workflows"
      ]
    }
  },
  {
    "type": "PackageTool",
    "name": "get_python_libsbml_info",
    "description": "Get comprehensive information about python-libsbml – SBML (Systems Biology Markup Language) support",
    "parameter": {
      "type": "object",
      "properties": {
        "info_type": {
          "type": "string",
          "enum": [
            "overview",
            "installation",
            "usage",
            "documentation"
          ],
          "description": "Type of information to retrieve about python-libsbml"
        }
      },
      "required": [
        "info_type"
      ]
    },
    "package_name": "python-libsbml",
    "local_info": {
      "name": "python-libsbml",
      "description": "Python bindings for libSBML, the Systems Biology Markup Language library. Enables reading, writing, and manipulation of SBML files for systems biology and metabolic modeling applications.",
      "category": "Systems Biology / SBML",
      "import_name": "libsbml",
      "popularity": 68,
      "keywords": [
        "SBML",
        "systems biology",
        "metabolic models",
        "biochemical networks",
        "model exchange"
      ],
      "documentation": "http://sbml.org/Software/libSBML/docs/python-api/",
      "repository": "https://github.com/sbmlteam/python-libsbml",
      "installation": {
        "pip": "pip install python-libsbml",
        "conda": "conda install -c conda-forge python-libsbml"
      },
      "usage_example": "import libsbml\nimport sys\n\n# Create a new SBML document\ndocument = libsbml.SBMLDocument(3, 1)  # SBML Level 3, Version 1\nmodel = document.createModel()\nmodel.setId('example_model')\nmodel.setName('Example Metabolic Model')\n\nprint(f'Created model: {model.getName()}')\nprint(f'SBML Level: {document.getLevel()}, Version: {document.getVersion()}')\n\n# Create compartment\ncompartment = model.createCompartment()\ncompartment.setId('cytoplasm')\ncompartment.setName('Cytoplasm')\ncompartment.setConstant(True)\ncompartment.setSize(1.0)\n\n# Create species (metabolites)\nglucose = model.createSpecies()\nglucose.setId('glucose')\nglucose.setName('Glucose')\nglucose.setCompartment('cytoplasm')\nglucose.setInitialConcentration(10.0)\nglucose.setConstant(False)\n\ng6p = model.createSpecies()\ng6p.setId('g6p')\ng6p.setName('Glucose-6-phosphate')\ng6p.setCompartment('cytoplasm')\ng6p.setInitialConcentration(0.0)\ng6p.setConstant(False)\n\n# Create reaction\nreaction = model.createReaction()\nreaction.setId('hexokinase')\nreaction.setName('Hexokinase')\nreaction.setReversible(False)\n\n# Add reactants and products\nreactant = reaction.createReactant()\nreactant.setSpecies('glucose')\nreactant.setStoichiometry(1.0)\n\nproduct = reaction.createProduct()\nproduct.setSpecies('g6p')\nproduct.setStoichiometry(1.0)\n\nprint(f'Model components:')\nprint(f'  Compartments: {model.getNumCompartments()}')\nprint(f'  Species: {model.getNumSpecies()}')\nprint(f'  Reactions: {model.getNumReactions()}')\n\n# Validate model\nerrors = document.checkConsistency()\nprint(f'\\nValidation errors: {errors}')\n\n# Write to file\nwriter = libsbml.SBMLWriter()\nsuccess = writer.writeSBMLToFile(document, 'example_model.xml')\nif success:\n    print('Model saved to example_model.xml')\nelse:\n    print('Failed to write model')",
      "quick_start": [
        "Install: pip install python-libsbml",
        "Create document: libsbml.SBMLDocument(level, version)",
        "Create model: document.createModel()",
        "Add compartments: model.createCompartment()",
        "Add species: model.createSpecies()",
        "Add reactions: model.createReaction()"
      ]
    }
  },
  {
    "type": "PackageTool",
    "name": "get_pymzml_info",
    "description": "Get comprehensive information about pymzML – mzML file parser for mass spectrometry",
    "parameter": {
      "type": "object",
      "properties": {
        "info_type": {
          "type": "string",
          "enum": [
            "overview",
            "installation",
            "usage",
            "documentation"
          ],
          "description": "Type of information to retrieve about pymzML"
        }
      },
      "required": [
        "info_type"
      ]
    },
    "package_name": "pymzml",
    "local_info": {
      "name": "pymzML",
      "description": "Python interface for mzML mass spectrometry files. Provides efficient parsing of mzML format with support for random access, spectrum extraction, and metadata handling for proteomics and metabolomics applications.",
      "category": "Mass Spectrometry I/O",
      "import_name": "pymzml",
      "popularity": 70,
      "keywords": [
        "mzML",
        "mass spectrometry",
        "proteomics",
        "metabolomics",
        "file parsing"
      ],
      "documentation": "https://pymzml.readthedocs.io/",
      "repository": "https://github.com/pymzml/pymzML",
      "installation": {
        "pip": "pip install pymzml",
        "conda": "conda install -c conda-forge pymzml"
      },
      "usage_example": "import pymzml\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Open mzML file\nmsrun = pymzml.run.Reader('sample.mzML')\n\n# Basic file information\nprint(f'MS run info: {msrun.info}')\n\n# Iterate through spectra\nms1_spectra = []\nms2_spectra = []\n\nfor spectrum in msrun:\n    if spectrum.ms_level == 1:\n        ms1_spectra.append(spectrum)\n    elif spectrum.ms_level == 2:\n        ms2_spectra.append(spectrum)\n\nprint(f'Found {len(ms1_spectra)} MS1 and {len(ms2_spectra)} MS2 spectra')\n\n# Access spectrum data\nif ms1_spectra:\n    first_spectrum = ms1_spectra[0]\n    print(f'Spectrum ID: {first_spectrum.ID}')\n    print(f'Retention time: {first_spectrum.scan_time_in_minutes():.2f} min')\n    print(f'Number of peaks: {len(first_spectrum.peaks(\"centroided\"))}')\n    \n    # Get m/z and intensity arrays\n    mz_array, intensity_array = first_spectrum.peaks(\"centroided\")\n    \n    # Plot spectrum\n    plt.figure(figsize=(12, 6))\n    plt.plot(mz_array, intensity_array)\n    plt.xlabel('m/z')\n    plt.ylabel('Intensity')\n    plt.title(f'MS1 Spectrum (RT: {first_spectrum.scan_time_in_minutes():.2f} min)')\n    plt.show()\n    \n    # Find peaks above threshold\n    threshold = np.max(intensity_array) * 0.05\n    high_peaks = [(mz, intensity) for mz, intensity in zip(mz_array, intensity_array) \n                  if intensity > threshold]\n    print(f'Peaks above 5% threshold: {len(high_peaks)}')",
      "quick_start": [
        "Install: pip install pymzml",
        "Open file: msrun = pymzml.run.Reader('file.mzML')",
        "Iterate spectra: for spectrum in msrun",
        "Get peaks: spectrum.peaks('centroided')",
        "Access metadata: spectrum.scan_time_in_minutes()",
        "Filter by MS level: spectrum.ms_level"
      ]
    }
  },
  {
    "type": "PackageTool",
    "name": "get_faiss_info",
    "description": "Get comprehensive information about Faiss – efficient similarity search and clustering",
    "parameter": {
      "type": "object",
      "properties": {
        "include_examples": {
          "type": "boolean",
          "description": "Whether to include usage examples and quick start guide",
          "default": true
        }
      },
      "required": [
        "include_examples"
      ]
    },
    "package_name": "faiss-cpu",
    "local_info": {
      "name": "Faiss",
      "description": "Library for efficient similarity search and clustering of dense vectors. Provides GPU-accelerated implementations of nearest neighbor search algorithms for large-scale applications.",
      "category": "Vector Search",
      "import_name": "faiss",
      "popularity": 92,
      "keywords": [
        "similarity search",
        "nearest neighbors",
        "clustering",
        "vector search",
        "embeddings"
      ],
      "documentation": "https://faiss.ai/",
      "repository": "https://github.com/facebookresearch/faiss",
      "installation": {
        "pip": "pip install faiss-cpu",
        "conda": "conda install -c conda-forge faiss-cpu"
      },
      "usage_example": "import faiss\nimport numpy as np\n\n# Generate random data\nnp.random.seed(42)\nd = 64  # dimension\nnb = 100000  # database size\nnq = 10000  # number of queries\nxb = np.random.random((nb, d)).astype('float32')\nxq = np.random.random((nq, d)).astype('float32')\n\n# Build index\nindex = faiss.IndexFlatL2(d)\nindex.add(xb)\n\n# Search\nk = 4  # number of nearest neighbors\nD, I = index.search(xq, k)\nprint(f'Found {len(I)} results')\nprint(f'Distances shape: {D.shape}')",
      "quick_start": [
        "1. Install Faiss: pip install faiss-cpu",
        "2. Import: import faiss",
        "3. Create index: index = faiss.IndexFlatL2(dimension)",
        "4. Add vectors: index.add(vectors)",
        "5. Search: distances, indices = index.search(query, k)"
      ]
    }
  },
  {
    "type": "PackageTool",
    "name": "get_hmmlearn_info",
    "description": "Get comprehensive information about hmmlearn – Hidden Markov Models in Python",
    "parameter": {
      "type": "object",
      "properties": {
        "info_type": {
          "type": "string",
          "enum": [
            "overview",
            "installation",
            "usage",
            "models",
            "documentation"
          ],
          "description": "Type of information to retrieve about hmmlearn"
        }
      },
      "required": [
        "info_type"
      ]
    },
    "package_name": "hmmlearn",
    "local_info": {
      "name": "hmmlearn",
      "description": "Hidden Markov Models in Python with scikit-learn compatible API. Provides implementations of Gaussian HMM, Multinomial HMM, and other variants for sequence modeling tasks.",
      "category": "Machine Learning / Sequence Modeling",
      "import_name": "hmmlearn",
      "popularity": 70,
      "keywords": [
        "hidden markov models",
        "sequence modeling",
        "time series",
        "state estimation",
        "Baum-Welch"
      ],
      "documentation": "https://hmmlearn.readthedocs.io/",
      "repository": "https://github.com/hmmlearn/hmmlearn",
      "installation": {
        "pip": "pip install hmmlearn",
        "conda": "conda install -c conda-forge hmmlearn"
      },
      "usage_example": "from hmmlearn import hmm\nimport numpy as np\n\n# Create sample data\nnp.random.seed(42)\nlengths = [10, 20, 15]\nX = np.concatenate([\n    np.random.normal(0, 1, (10, 2)),\n    np.random.normal(3, 1, (20, 2)),\n    np.random.normal(-2, 1, (15, 2))\n])\n\n# Fit Gaussian HMM\nmodel = hmm.GaussianHMM(n_components=3, covariance_type='full')\nmodel.fit(X, lengths)\n\nprint(f'Converged: {model.monitor_.converged}')\nprint(f'Log likelihood: {model.score(X, lengths):.2f}')\n\n# Predict hidden states\nstates = model.predict(X, lengths)\nprint(f'Predicted states: {states[:10]}')\n\n# Decode most likely state sequence\nlogprob, path = model.decode(X, lengths)\nprint(f'Most likely path: {path[:10]}')",
      "quick_start": [
        "Install: pip install hmmlearn",
        "Import: from hmmlearn import hmm",
        "Create model: model = hmm.GaussianHMM(n_components=3)",
        "Fit data: model.fit(X, lengths)",
        "Predict states: model.predict(X)",
        "Decode sequence: model.decode(X)"
      ]
    }
  },
  {
    "type": "PackageTool",
    "name": "get_cobrapy_info",
    "description": "Get comprehensive information about COBRApy – constraint-based metabolic modeling",
    "parameter": {
      "type": "object",
      "properties": {
        "info_type": {
          "type": "string",
          "enum": [
            "overview",
            "installation",
            "usage",
            "documentation"
          ],
          "description": "Type of information to retrieve about COBRApy"
        }
      },
      "required": [
        "info_type"
      ]
    },
    "package_name": "cobra",
    "local_info": {
      "name": "COBRApy",
      "description": "Constraint-Based Reconstruction and Analysis (COBRA) methods for metabolic modeling. Enables flux balance analysis, gene essentiality prediction, and metabolic network optimization.",
      "category": "Systems Biology / Metabolic Modeling",
      "import_name": "cobra",
      "popularity": 85,
      "keywords": [
        "metabolism",
        "flux balance analysis",
        "systems biology",
        "metabolic networks",
        "SBML"
      ],
      "documentation": "https://cobrapy.readthedocs.io/",
      "repository": "https://github.com/opencobra/cobrapy",
      "installation": {
        "pip": "pip install cobra",
        "conda": "conda install -c conda-forge cobra"
      },
      "usage_example": "import cobra\nimport cobra.test\nfrom cobra.flux_analysis import flux_variability_analysis\nimport pandas as pd\n\n# Load example E. coli model\nprint('Loading E. coli metabolic model...')\nmodel = cobra.test.create_test_model('textbook')\n\nprint(f'Model: {model.id}')\nprint(f'Reactions: {len(model.reactions)}')\nprint(f'Metabolites: {len(model.metabolites)}')\nprint(f'Genes: {len(model.genes)}')\n\n# Perform flux balance analysis\nprint('\\nPerforming flux balance analysis...')\nsolution = model.optimize()\n\nprint(f'Optimization status: {solution.status}')\nprint(f'Objective value: {solution.objective_value:.3f}')\nprint(f'Growth rate: {solution.fluxes[\"BIOMASS_Ecoli_core\"]:.3f} h⁻¹')\n\n# Show top fluxes\nprint('\\nTop active fluxes:')\nactive_fluxes = solution.fluxes[abs(solution.fluxes) > 0.01].sort_values(key=abs, ascending=False)\nfor rxn_id, flux in active_fluxes.head(10).items():\n    rxn = model.reactions.get_by_id(rxn_id)\n    print(f'{rxn_id:15s}: {flux:8.3f} {rxn.name}')\n\n# Gene essentiality analysis\nprint('\\nAnalyzing gene essentiality...')\ngene_results = []\nfor gene in list(model.genes)[:10]:  # Test first 10 genes\n    with model:\n        gene.knock_out()\n        ko_solution = model.optimize()\n        growth_rate = ko_solution.objective_value if ko_solution.status == 'optimal' else 0\n        essential = growth_rate < 0.01\n        gene_results.append({\n            'gene': gene.id,\n            'growth_rate': growth_rate,\n            'essential': essential\n        })\n\nessential_genes = [g for g in gene_results if g['essential']]\nprint(f'Essential genes found: {len(essential_genes)}/{len(gene_results)}')\nfor gene in essential_genes:\n    print(f'  {gene[\"gene\"]}: growth rate {gene[\"growth_rate\"]:.3f}')\n\n# Flux variability analysis\nprint('\\nPerforming flux variability analysis...')\nfva_result = flux_variability_analysis(model, model.reactions[:5])  # First 5 reactions\nprint(fva_result)\n\n# Medium composition analysis\nprint(f'\\nCurrent medium ({len(model.medium)} components):')\nfor metabolite, flux in model.medium.items():\n    met = model.metabolites.get_by_id(metabolite)\n    print(f'  {metabolite:15s}: {flux:8.3f} ({met.name})')\n\n# Test different carbon sources\nprint('\\nTesting different carbon sources...')\ncarbon_sources = ['EX_glc__D_e', 'EX_fru_e', 'EX_xyl__D_e']\nfor carbon in carbon_sources:\n    if carbon in [r.id for r in model.exchanges]:\n        with model:\n            # Close all carbon sources\n            for ex in model.exchanges:\n                if 'EX_' in ex.id and any(c in ex.id for c in ['glc', 'fru', 'xyl', 'ac']):\n                    ex.lower_bound = 0\n            # Open specific carbon source\n            model.reactions.get_by_id(carbon).lower_bound = -10\n            solution = model.optimize()\n            growth = solution.objective_value if solution.status == 'optimal' else 0\n            carbon_name = carbon.replace('EX_', '').replace('_e', '')\n            print(f'  {carbon_name:10s}: growth rate {growth:.3f}')\n\nprint('\\nCOBRApy enables:')\nprint('- Constraint-based metabolic modeling')\nprint('- Flux balance analysis (FBA)')\nprint('- Gene essentiality prediction')\nprint('- Flux variability analysis')\nprint('- SBML model import/export')\nprint('- Integration with experimental data')",
      "quick_start": [
        "Install: pip install cobra",
        "Load model: model = cobra.test.create_test_model()",
        "Optimize: solution = model.optimize()",
        "Gene knockout: gene.knock_out()",
        "FVA: flux_variability_analysis(model, reactions)",
        "Save/load: cobra.io.read_sbml_model(file)"
      ]
    }
  },
  {
    "type": "PackageTool",
    "name": "get_deepxde_info",
    "description": "Get comprehensive information about DeepXDE – a library for physics-informed neural networks (PINNs) solving PDEs and inverse problems.",
    "parameter": {
      "type": "object",
      "properties": {
        "include_examples": {
          "type": "boolean",
          "description": "Whether to include usage examples and a quick-start guide",
          "default": true
        }
      },
      "required": [
        "include_examples"
      ]
    },
    "package_name": "deepxde",
    "local_info": {
      "name": "DeepXDE",
      "description": "A flexible TensorFlow/PyTorch-based framework for building physics-informed neural networks to solve differential equations and scientific inverse problems.",
      "category": "AI for Science / Physics-Informed NNs",
      "import_name": "deepxde",
      "popularity": 65,
      "keywords": [
        "PINN",
        "differential equations",
        "scientific ML",
        "inverse problems"
      ],
      "documentation": "https://deepxde.readthedocs.io/",
      "repository": "https://github.com/lululxvi/deepxde",
      "installation": {
        "pip": "pip install deepxde",
        "conda": "conda install -c conda-forge deepxde"
      },
      "usage_example": "import deepxde as dde\nimport numpy as np\n\ndef pde(x, y):\n    y_xx = dde.grad.hessian(y, x)\n    return y_xx + np.pi**2 * np.sin(np.pi * x)\n\ndomain = dde.geometry.Interval(0, 1)\nbc = dde.icbc.DirichletBC(domain, lambda x: 0, lambda _, on_boundary: on_boundary)\n\ndata = dde.data.PDE(domain, pde, [bc], num_domain=20, num_boundary=10)\nnet = dde.nn.FNN([1, 32, 32, 32, 1], 'tanh', 'Glorot normal')\nmodel = dde.Model(data, net)\nmodel.compile('adam', lr=1e-3)\nmodel.train(epochs=5000)",
      "quick_start": [
        "1. Install DeepXDE: pip install deepxde",
        "2. Define geometry and PDE function",
        "3. Add boundary/initial conditions",
        "4. Build a data object: dde.data.PDE(...)",
        "5. Train the PINN with model.train()"
      ]
    }
  },
  {
    "type": "PackageTool",
    "name": "get_deeppurpose_info",
    "description": "Get comprehensive information about DeepPurpose – deep learning toolkit for drug discovery",
    "parameter": {
      "type": "object",
      "properties": {
        "info_type": {
          "type": "string",
          "enum": [
            "overview",
            "installation",
            "usage",
            "models",
            "documentation"
          ],
          "description": "Type of information to retrieve about DeepPurpose"
        }
      },
      "required": [
        "info_type"
      ]
    },
    "package_name": "deeppurpose",
    "local_info": {
      "name": "DeepPurpose",
      "description": "Deep learning toolkit for drug discovery and drug-target interaction prediction. Provides unified framework for various molecular representations and deep learning architectures for drug discovery tasks.",
      "category": "Drug Discovery / AI",
      "import_name": "DeepPurpose",
      "popularity": 75,
      "keywords": [
        "drug discovery",
        "drug-target interaction",
        "molecular representation",
        "pharmaceutical AI"
      ],
      "documentation": "https://deeppurpose.readthedocs.io/",
      "repository": "https://github.com/kexinhuang12345/DeepPurpose",
      "installation": {
        "pip": "pip install DeepPurpose",
        "conda": "conda install -c conda-forge deeppurpose"
      },
      "usage_example": "from DeepPurpose import utils, DTI\nimport pandas as pd\n\n# Load sample drug-target interaction data\nX_drug, X_target, y = utils.load_process_DAVIS(\n    path='./data', binary=False, convert_to_log=True\n)\n\n# Create train/validation/test splits\ntrain, val, test = utils.data_process(\n    X_drug, X_target, y, \n    drug_encoding='Morgan', \n    target_encoding='CNN',\n    split_method='random',\n    frac=[0.7, 0.1, 0.2]\n)\n\n# Initialize and train model\nconfig = utils.generate_config(\n    drug_encoding='Morgan',\n    target_encoding='CNN', \n    cls_hidden_dims=[1024, 1024, 512]\n)\nmodel = DTI.model_initialize(**config)\nmodel.train(train, val, test)\n\n# Make predictions\npredictions = model.predict(test)",
      "quick_start": [
        "Install: pip install DeepPurpose",
        "Load data: utils.load_process_DAVIS()",
        "Choose encodings: drug_encoding='Morgan', target_encoding='CNN'",
        "Process data: utils.data_process()",
        "Initialize model: DTI.model_initialize()",
        "Train and predict: model.train(), model.predict()"
      ]
    }
  },
  {
    "type": "PackageTool",
    "name": "get_xgboost_info",
    "description": "Get information about the xgboost package. Optimized gradient boosting framework",
    "package_name": "xgboost",
    "parameter": {
      "type": "object",
      "properties": {},
      "required": []
    },
    "required": []
  },
  {
    "type": "PackageTool",
    "name": "get_lightgbm_info",
    "description": "Get information about the lightgbm package. Fast gradient boosting framework",
    "package_name": "lightgbm",
    "parameter": {
      "type": "object",
      "properties": {},
      "required": []
    },
    "required": []
  },
  {
    "type": "PackageTool",
    "name": "get_catboost_info",
    "description": "Get information about the catboost package. High-performance gradient boosting library",
    "package_name": "catboost",
    "parameter": {
      "type": "object",
      "properties": {},
      "required": []
    },
    "required": []
  },
  {
    "type": "PackageTool",
    "name": "get_optuna_info",
    "description": "Get information about the optuna package. Hyperparameter optimization framework",
    "package_name": "optuna",
    "parameter": {
      "type": "object",
      "properties": {},
      "required": []
    },
    "required": []
  },
  {
    "type": "PackageTool",
    "name": "get_skopt_info",
    "description": "Get information about the skopt package. Scikit-Optimize: sequential model-based optimization",
    "package_name": "skopt",
    "parameter": {
      "type": "object",
      "properties": {},
      "required": []
    },
    "required": []
  },
  {
    "type": "PackageTool",
    "name": "get_imbalanced_learn_info",
    "description": "Get information about the imbalanced-learn package. Python toolbox for imbalanced dataset learning",
    "package_name": "imbalanced-learn",
    "parameter": {
      "type": "object",
      "properties": {},
      "required": []
    },
    "required": []
  }
]
