[
  {
    "type": "PackageTool",
    "name": "get_biopython_info",
    "description": "Get comprehensive information about Biopython – powerful tools for computational molecular biology and bioinformatics",
    "parameter": {
      "type": "object",
      "properties": {
        "include_examples": {
          "type": "boolean",
          "description": "Whether to include usage examples and quick start guide",
          "default": true
        }
      },
      "required": [
        "include_examples"
      ]
    },
    "package_name": "biopython",
    "local_info": {
      "name": "Biopython",
      "description": "Freely available Python tools for computational molecular biology. Provides a wide range of bioinformatics functionality including sequence analysis, phylogenetics, structure analysis, and database access.",
      "category": "Bioinformatics",
      "import_name": "Bio",
      "popularity": 92,
      "keywords": [
        "bioinformatics",
        "sequence analysis",
        "phylogenetics",
        "structure analysis",
        "database access"
      ],
      "documentation": "https://biopython.org/docs/latest/",
      "repository": "https://github.com/biopython/biopython",
      "installation": {
        "pip": "pip install biopython",
        "conda": "conda install -c conda-forge biopython"
      },
      "usage_example": "from Bio.Seq import Seq\nfrom Bio import SeqIO\nfrom Bio.SeqUtils import GC\n\n# Create and analyze a DNA sequence\ndna_seq = Seq('AGTACACTGGT')\nprint(f'Length: {len(dna_seq)}')\nprint(f'GC content: {GC(dna_seq):.1f}%')\nprint(f'Translation: {dna_seq.translate()}')\n\n# Read sequences from FASTA\nfor record in SeqIO.parse('example.fasta', 'fasta'):\n    print(f'{record.id}: {len(record.seq)} bp')",
      "quick_start": [
        "Install: pip install biopython",
        "Import: from Bio.Seq import Seq, from Bio import SeqIO",
        "Create sequence: seq = Seq('ATCG')",
        "Analyze: GC content, translation, complement",
        "File I/O: SeqIO.parse() for FASTA/GenBank files"
      ]
    }
  },
  {
    "type": "PackageTool",
    "name": "get_scikit_bio_info",
    "description": "Get comprehensive information about scikit-bio – bioinformatics library built on scientific Python stack",
    "parameter": {
      "type": "object",
      "properties": {
        "include_examples": {
          "type": "boolean",
          "description": "Whether to include usage examples and quick start guide",
          "default": true
        }
      },
      "required": [
        "include_examples"
      ]
    },
    "package_name": "scikit-bio",
    "local_info": {
      "name": "scikit-bio",
      "description": "Open-source Python library providing data structures, algorithms, and educational resources for bioinformatics. Built on the scientific Python stack for efficient computation.",
      "category": "Bioinformatics",
      "import_name": "skbio",
      "popularity": 75,
      "keywords": [
        "bioinformatics",
        "phylogenetics",
        "sequence analysis",
        "diversity analysis",
        "scientific computing"
      ],
      "documentation": "http://scikit-bio.org/",
      "repository": "https://github.com/biocore/scikit-bio",
      "installation": {
        "pip": "pip install scikit-bio",
        "conda": "conda install -c conda-forge scikit-bio"
      },
      "usage_example": "import skbio\nfrom skbio import DNA\nfrom skbio.alignment import local_pairwise_align_ssw\n\n# Create DNA sequences\nseq1 = DNA('ACCGGTGGAACCGGTAACACCCAC')\nseq2 = DNA('ACCGGTAACAC')\n\n# Perform alignment\nalignment = local_pairwise_align_ssw(seq1, seq2)\nprint(alignment)\n\n# Calculate sequence statistics\nprint(f'GC content: {seq1.gc_content():.2f}')\nprint(f'Complement: {seq1.complement()}')",
      "quick_start": [
        "Install: pip install scikit-bio",
        "Import: import skbio, from skbio import DNA",
        "Create sequences: DNA('ATCG')",
        "Alignment: local_pairwise_align_ssw()",
        "Analysis: GC content, diversity metrics"
      ]
    }
  },
  {
    "type": "PackageTool",
    "name": "get_biotite_info",
    "description": "Get comprehensive information about Biotite – comprehensive computational molecular biology library",
    "parameter": {
      "type": "object",
      "properties": {
        "include_examples": {
          "type": "boolean",
          "description": "Whether to include usage examples and quick start guide",
          "default": true
        }
      },
      "required": [
        "include_examples"
      ]
    },
    "package_name": "biotite",
    "local_info": {
      "name": "Biotite",
      "description": "Swiss army knife for bioinformatics. Comprehensive library for computational molecular biology including sequence analysis, structure analysis, database access, and visualization with NumPy-based performance.",
      "category": "Structural Biology",
      "import_name": "biotite",
      "popularity": 70,
      "keywords": [
        "structural biology",
        "sequence analysis",
        "protein structure",
        "molecular biology",
        "bioinformatics"
      ],
      "documentation": "https://www.biotite-python.org/",
      "repository": "https://github.com/biotite-dev/biotite",
      "installation": {
        "pip": "pip install biotite",
        "conda": "conda install -c conda-forge biotite"
      },
      "usage_example": "import biotite.sequence as seq\nimport biotite.sequence.align as align\nimport biotite.database.entrez as entrez\n\n# Create and align sequences\nseq1 = seq.ProteinSequence('MVHLPEWHTDFGKNLMVILQLLLQQL')\nseq2 = seq.ProteinSequence('MVHLPEWHTDFGKGQMVAQVILLQQL')\n\n# Perform alignment\nmatrix = align.SubstitutionMatrix.std_protein_matrix()\nalignments = align.align_optimal(seq1, seq2, matrix)\nprint(alignments[0])",
      "quick_start": [
        "Install: pip install biotite",
        "Import modules: import biotite.sequence as seq",
        "Create sequences: seq.ProteinSequence('MVHL...')",
        "Analyze structures: load PDB, calculate contacts",
        "Access databases: Entrez, PDB downloads"
      ]
    }
  },
  {
    "type": "PackageTool",
    "name": "get_gget_info",
    "description": "Get comprehensive information about gget – genomics command-line tool and Python package",
    "parameter": {
      "type": "object",
      "properties": {
        "include_examples": {
          "type": "boolean",
          "description": "Whether to include usage examples and quick start guide",
          "default": true
        }
      },
      "required": [
        "include_examples"
      ]
    },
    "package_name": "gget",
    "local_info": {
      "name": "gget",
      "description": "Command-line tool and Python package for efficient querying of genomic databases. Enables quick access to gene information, sequences, and annotations from major genomics resources.",
      "category": "Genomics Databases",
      "import_name": "gget",
      "popularity": 70,
      "keywords": [
        "genomics databases",
        "gene information",
        "ENSEMBL",
        "sequence retrieval",
        "annotations"
      ],
      "documentation": "https://github.com/pachterlab/gget",
      "repository": "https://github.com/pachterlab/gget",
      "installation": {
        "pip": "pip install gget",
        "conda": "conda install -c bioconda gget"
      },
      "usage_example": "import gget\n\n# Search for genes\nresults = gget.search('BRCA1')\nprint(results.head())\n\n# Get gene information\ninfo = gget.info('ENSG00000012048')\nprint(info)\n\n# Fetch sequences\nseq = gget.seq('ENSG00000012048', seqtype='transcript')\nprint(seq)",
      "quick_start": [
        "Install: pip install gget",
        "Search genes: gget.search('BRCA1')",
        "Gene info: gget.info('ENSG00000012048')",
        "Get sequences: gget.seq(gene_id, seqtype='transcript')",
        "Access databases: ENSEMBL, UniProt, PDB"
      ]
    }
  },
  {
    "type": "PackageTool",
    "name": "get_trackpy_info",
    "description": "Get comprehensive information about trackpy – particle tracking toolkit for Python",
    "parameter": {
      "type": "object",
      "properties": {
        "info_type": {
          "type": "string",
          "enum": [
            "overview",
            "installation",
            "usage",
            "documentation"
          ],
          "description": "Type of information to retrieve about trackpy"
        }
      },
      "required": [
        "info_type"
      ]
    },
    "package_name": "trackpy",
    "local_info": {
      "name": "trackpy",
      "description": "Python package for particle tracking in 2D, 3D, and higher dimensions. Provides feature detection, linking trajectories over time, and motion analysis for applications in biophysics and materials science.",
      "category": "Image Analysis / Particle Tracking",
      "import_name": "trackpy",
      "popularity": 68,
      "keywords": [
        "particle tracking",
        "trajectory analysis",
        "microscopy",
        "biophysics",
        "motion analysis"
      ],
      "documentation": "http://soft-matter.github.io/trackpy/",
      "repository": "https://github.com/soft-matter/trackpy",
      "installation": {
        "pip": "pip install trackpy",
        "conda": "conda install -c conda-forge trackpy"
      },
      "usage_example": "import trackpy as tp\nimport pandas as pd\nimport numpy as np\nfrom skimage import data\n\n# Load sample images (normally from microscopy)\nframes = [data.coins() for _ in range(5)]  # Mock time series\n\n# Locate particles in each frame\nf = []\nfor i, frame in enumerate(frames):\n    features = tp.locate(frame, diameter=15, minmass=1000)\n    features['frame'] = i\n    f.append(features)\n\nfeatures = pd.concat(f, ignore_index=True)\n\n# Link particles into trajectories\ntrajectories = tp.link(features, search_range=20, memory=3)\n\nprint(f'Found {len(trajectories)} particle detections')\nprint(f'Unique particles: {trajectories[\"particle\"].nunique()}')\n\n# Filter out short trajectories\nfiltered = tp.filter_stubs(trajectories, threshold=3)\nprint(f'After filtering: {len(filtered)} detections')\n\n# Calculate motion statistics\nmotion = tp.motion.msd(filtered, mpp=0.1, fps=10)\nprint(motion.head())",
      "quick_start": [
        "Install: pip install trackpy",
        "Locate particles: tp.locate(image, diameter=15)",
        "Link trajectories: tp.link(features, search_range=20)",
        "Filter trajectories: tp.filter_stubs(tracks, threshold=3)",
        "Analyze motion: tp.motion.msd(tracks)",
        "Visualize: tp.plot_traj(tracks)"
      ]
    }
  },
  {
    "type": "PackageTool",
    "name": "get_numba_info",
    "description": "Get comprehensive information about Numba – JIT compiler for Python",
    "parameter": {
      "type": "object",
      "properties": {
        "info_type": {
          "type": "string",
          "enum": [
            "overview",
            "installation",
            "usage",
            "documentation"
          ],
          "description": "Type of information to retrieve about Numba"
        }
      },
      "required": [
        "info_type"
      ]
    },
    "package_name": "numba",
    "local_info": {
      "name": "Numba",
      "description": "Open source JIT compiler that translates a subset of Python and NumPy code into fast machine code. Provides significant speedups for numerical computations with minimal code changes.",
      "category": "Performance / JIT Compilation",
      "import_name": "numba",
      "popularity": 87,
      "keywords": [
        "JIT compiler",
        "performance",
        "GPU computing",
        "CUDA",
        "numerical acceleration"
      ],
      "documentation": "https://numba.pydata.org/",
      "repository": "https://github.com/numba/numba",
      "installation": {
        "pip": "pip install numba",
        "conda": "conda install numba"
      },
      "usage_example": "import numba\nfrom numba import jit, njit, prange, cuda\nimport numpy as np\nimport time\nimport math\n\nprint('Numba - JIT Compiler for Python')\nprint('=' * 35)\n\n# Basic JIT compilation example\nprint('\\n=== Basic JIT Compilation ===')\n\n# Pure Python function\ndef python_function(x):\n    total = 0\n    for i in range(x):\n        total += i * i\n    return total\n\n# JIT compiled function\n@jit\ndef numba_function(x):\n    total = 0\n    for i in range(x):\n        total += i * i\n    return total\n\n# No-Python mode (faster)\n@njit\ndef numba_nopython(x):\n    total = 0\n    for i in range(x):\n        total += i * i\n    return total\n\n# Performance comparison\nn = 1000000\nprint(f'Computing sum of squares for {n:} numbers')\n\n# Warm up JIT functions\nnumba_function(100)\nnumba_nopython(100)\n\n# Time Python function\nstart = time.time()\nresult_python = python_function(n)\ntime_python = time.time() - start\n\n# Time JIT function\nstart = time.time()\nresult_numba = numba_function(n)\ntime_numba = time.time() - start\n\n# Time no-Python JIT\nstart = time.time()\nresult_nopython = numba_nopython(n)\ntime_nopython = time.time() - start\n\nprint(f'Python result: {result_python}')\nprint(f'Numba result: {result_numba}')\nprint(f'No-Python result: {result_nopython}')\nprint(f'\\nPython time: {time_python:.4f} seconds')\nprint(f'Numba time: {time_numba:.4f} seconds')\nprint(f'No-Python time: {time_nopython:.4f} seconds')\nprint(f'Speedup (Numba): {time_python/time_numba:.1f}x')\nprint(f'Speedup (No-Python): {time_python/time_nopython:.1f}x')\n\n# NumPy array operations\nprint('\\n=== NumPy Array Operations ===')\n\n@njit\ndef matrix_multiply_numba(A, B):\n    return np.dot(A, B)\n\n@njit\ndef element_wise_operation(arr):\n    result = np.zeros_like(arr)\n    for i in range(arr.shape[0]):\n        for j in range(arr.shape[1]):\n            result[i, j] = math.sqrt(arr[i, j]**2 + 1)\n    return result\n\n# Create test arrays\nsize = 500\nA = np.random.random((size, size))\nB = np.random.random((size, size))\n\nprint(f'Matrix operations on {size}x{size} arrays')\n\n# Warm up\nmatrix_multiply_numba(A[:10, :10], B[:10, :10])\nelement_wise_operation(A[:10, :10])\n\n# Time NumPy operations\nstart = time.time()\nnumpy_result = np.dot(A, B)\ntime_numpy = time.time() - start\n\n# Time Numba operations\nstart = time.time()\nnumba_result = matrix_multiply_numba(A, B)\ntime_numba_matrix = time.time() - start\n\nprint(f'NumPy matrix multiply: {time_numpy:.4f} seconds')\nprint(f'Numba matrix multiply: {time_numba_matrix:.4f} seconds')\nprint(f'Results equal: {np.allclose(numpy_result, numba_result)}')\n\n# Parallel execution\nprint('\\n=== Parallel Execution ===')\n\n@njit(parallel=True)\ndef parallel_sum(arr):\n    total = 0.0\n    for i in prange(arr.shape[0]):\n        total += arr[i]\n    return total\n\n@njit\ndef serial_sum(arr):\n    total = 0.0\n    for i in range(arr.shape[0]):\n        total += arr[i]\n    return total\n\nlarge_array = np.random.random(10000000)\n\n# Warm up\nparallel_sum(large_array[:1000])\nserial_sum(large_array[:1000])\n\n# Time serial version\nstart = time.time()\nserial_result = serial_sum(large_array)\ntime_serial = time.time() - start\n\n# Time parallel version\nstart = time.time()\nparallel_result = parallel_sum(large_array)\ntime_parallel = time.time() - start\n\nprint(f'Array size: {len(large_array):} elements')\nprint(f'Serial sum: {serial_result:.6f} ({time_serial:.4f} seconds)')\nprint(f'Parallel sum: {parallel_result:.6f} ({time_parallel:.4f} seconds)')\nprint(f'Parallel speedup: {time_serial/time_parallel:.1f}x')\n\n# Mathematical functions\nprint('\\n=== Mathematical Functions ===')\n\n@njit\ndef monte_carlo_pi(n_samples):\n    count = 0\n    for i in range(n_samples):\n        x = np.random.random()\n        y = np.random.random()\n        if x*x + y*y <= 1.0:\n            count += 1\n    return 4.0 * count / n_samples\n\n@njit\ndef mandelbrot_point(c_real, c_imag, max_iter):\n    z_real = 0.0\n    z_imag = 0.0\n    for i in range(max_iter):\n        z_real_new = z_real*z_real - z_imag*z_imag + c_real\n        z_imag_new = 2*z_real*z_imag + c_imag\n        z_real = z_real_new\n        z_imag = z_imag_new\n        if z_real*z_real + z_imag*z_imag > 4:\n            return i\n    return max_iter\n\n# Monte Carlo Pi estimation\nn_samples = 1000000\nprint(f'Monte Carlo π estimation with {n_samples:} samples')\n\nstart = time.time()\npi_estimate = monte_carlo_pi(n_samples)\ntime_mc = time.time() - start\n\nprint(f'Estimated π: {pi_estimate:.6f}')\nprint(f'Actual π: {math.pi:.6f}')\nprint(f'Error: {abs(pi_estimate - math.pi):.6f}')\nprint(f'Time: {time_mc:.4f} seconds')\n\n# Mandelbrot calculation\nprint(f'\\nMandelbrot set calculation')\nc_values = [-0.5 + 0.5j, -0.8 + 0.2j, 0.3 - 0.6j]\nmax_iterations = 1000\n\nfor c in c_values:\n    iterations = mandelbrot_point(c.real, c.imag, max_iterations)\n    if iterations == max_iterations:\n        print(f'Point {c}: In set (>{max_iterations} iterations)')\n    else:\n        print(f'Point {c}: Escaped after {iterations} iterations')\n\n# Type signatures and compilation info\nprint('\\n=== Compilation Information ===')\nprint(f'Numba version: {numba.__version__}')\nprint(f'NumPy version: {np.__version__}')\n\n# Function signatures\nprint(f'\\nFunction signatures:')\nprint(f'numba_function: {numba_function.signatures}')\nprint(f'numba_nopython: {numba_nopython.signatures}')\nprint(f'parallel_sum: {parallel_sum.signatures}')\n\n# GPU example (if CUDA available)\nprint('\\n=== GPU Computing (CUDA) ===')\ntry:\n    # Simple CUDA kernel example\n    @cuda.jit\n    def cuda_add(a, b, c):\n        idx = cuda.grid(1)\n        if idx < c.size:\n            c[idx] = a[idx] + b[idx]\n    \n    # Check if CUDA is available\n    if cuda.is_available():\n        print('CUDA is available!')\n        print(f'CUDA devices: {cuda.list_devices()}')\n        \n        # Small example\n        n = 1000\n        a = np.random.random(n).astype(np.float32)\n        b = np.random.random(n).astype(np.float32)\n        c = np.zeros(n, dtype=np.float32)\n        \n        # Configure grid and block dimensions\n        threads_per_block = 128\n        blocks_per_grid = (n + threads_per_block - 1) // threads_per_block\n        \n        print(f'Running CUDA kernel with {blocks_per_grid} blocks, {threads_per_block} threads each')\n        cuda_add[blocks_per_grid, threads_per_block](a, b, c)\n        \n        # Verify result\n        expected = a + b\n        print(f'CUDA result matches NumPy: {np.allclose(c, expected)}')\n    else:\n        print('CUDA not available on this system')\nexcept Exception as e:\n    print(f'CUDA example failed: {e}')\n\nprint('\\nNumba provides:')\nprint('• Just-in-time compilation for Python')\nprint('• Automatic parallelization with prange')\nprint('• GPU computing with CUDA support')\nprint('• NumPy array optimization')\nprint('• Minimal code changes for maximum speedup')\nprint('• Support for mathematical functions')\nprint('• Type inference and optimization')",
      "quick_start": [
        "Install: pip install numba",
        "Import: from numba import jit, njit",
        "Decorate: @jit or @njit above functions",
        "Parallel: @njit(parallel=True) with prange",
        "GPU: @cuda.jit for CUDA kernels",
        "Enjoy automatic speed improvements!"
      ]
    }
  },
  {
    "type": "PackageTool",
    "name": "get_flask_info",
    "description": "Get comprehensive information about Flask - a lightweight WSGI web application framework",
    "parameter": {
      "type": "object",
      "properties": {
        "include_examples": {
          "type": "boolean",
          "description": "Whether to include usage examples and quick start guide",
          "default": true
        }
      },
      "required": [
        "include_examples"
      ]
    },
    "package_name": "flask",
    "local_info": {
      "name": "Flask",
      "description": "A lightweight WSGI web application framework. It is designed to make getting started quick and easy, with the ability to scale up to complex applications.",
      "category": "Web Framework",
      "import_name": "flask",
      "popularity": 85,
      "keywords": [
        "web",
        "framework",
        "wsgi",
        "http",
        "server"
      ],
      "documentation": "https://flask.palletsprojects.com/",
      "repository": "https://github.com/pallets/flask",
      "installation": {
        "pip": "pip install Flask",
        "conda": "conda install flask"
      },
      "usage_example": "from flask import Flask\n\napp = Flask(__name__)\n\n@app.route('/')\ndef hello_world():\n    return 'Hello, World!'\n\n@app.route('/user/<name>')\ndef user_profile(name):\n    return f'Hello, {name}!'\n\nif __name__ == '__main__':\n    app.run(debug=True)",
      "quick_start": [
        "1. Install Flask: pip install Flask",
        "2. Create app: from flask import Flask; app = Flask(__name__)",
        "3. Define routes: @app.route('/') def home(): return 'Hello!'",
        "4. Run the app: app.run(debug=True)",
        "5. Visit http://localhost:5000 in your browser"
      ]
    }
  },
  {
    "type": "PackageTool",
    "name": "get_googlesearch_python_info",
    "description": "Get comprehensive information about googlesearch-python – Google search automation",
    "parameter": {
      "type": "object",
      "properties": {
        "info_type": {
          "type": "string",
          "enum": [
            "overview",
            "installation",
            "usage",
            "documentation"
          ],
          "description": "Type of information to retrieve about googlesearch-python"
        }
      },
      "required": [
        "info_type"
      ]
    },
    "package_name": "googlesearch-python",
    "local_info": {
      "name": "googlesearch-python",
      "description": "Python library for performing Google searches programmatically. Provides simple interface to retrieve search results, URLs, and snippets without requiring API keys.",
      "category": "Web Scraping / Search",
      "import_name": "googlesearch",
      "popularity": 70,
      "keywords": [
        "Google search",
        "web scraping",
        "search automation",
        "information retrieval"
      ],
      "documentation": "https://github.com/MarioVilas/googlesearch",
      "repository": "https://github.com/MarioVilas/googlesearch",
      "installation": {
        "pip": "pip install googlesearch-python",
        "conda": "conda install -c conda-forge googlesearch-python"
      },
      "usage_example": "from googlesearch import search\nimport time\nimport requests\nfrom urllib.parse import urlparse\n\n# Basic search example\nquery = 'python data science'\nprint(f'Searching for: \"{query}\"')\n\n# Get top 5 search results\ntry:\n    results = list(search(query, num_results=5, sleep_interval=1))\n    \n    print(f'Found {len(results)} results:')\n    for i, url in enumerate(results, 1):\n        print(f'{i}. {url}')\n        \n        # Extract domain\n        domain = urlparse(url).netloc\n        print(f'   Domain: {domain}')\n        \n        # Add delay to be respectful\n        time.sleep(0.5)\n        \nexcept Exception as e:\n    print(f'Search failed: {e}')\n\n# Advanced search with parameters\nprint('\\nAdvanced search example:')\nadvanced_query = 'machine learning papers site:arxiv.org'\n\ntry:\n    arxiv_results = list(search(\n        advanced_query,\n        num_results=3,\n        lang='en',\n        region='us',\n        sleep_interval=2\n    ))\n    \n    print(f'ArXiv ML papers ({len(arxiv_results)} results):')\n    for url in arxiv_results:\n        print(f'- {url}')\n        \nexcept Exception as e:\n    print(f'Advanced search failed: {e}')\n\n# Search with different domains\nprint('\\nDomain-specific searches:')\ndomains = ['github.com', 'stackoverflow.com', 'medium.com']\n\nfor domain in domains:\n    domain_query = f'pytorch tutorial site:{domain}'\n    try:\n        domain_results = list(search(domain_query, num_results=2, sleep_interval=1))\n        print(f'{domain}: {len(domain_results)} results')\n        for url in domain_results[:1]:  # Show first result\n            print(f'  {url}')\n    except:\n        print(f'{domain}: Search failed')\n    \nprint('\\nNote: Be respectful with search frequency to avoid rate limiting')",
      "quick_start": [
        "Install: pip install googlesearch-python",
        "Import: from googlesearch import search",
        "Basic search: search('query', num_results=10)",
        "Add delays: sleep_interval parameter",
        "Filter by site: 'query site:domain.com'",
        "Use responsibly to avoid rate limits"
      ]
    }
  },
  {
    "type": "PackageTool",
    "name": "get_cryosparc_tools_info",
    "description": "Get comprehensive information about cryosparc-tools – interface to CryoSPARC cryo-EM processing",
    "parameter": {
      "type": "object",
      "properties": {
        "info_type": {
          "type": "string",
          "enum": [
            "overview",
            "installation",
            "usage",
            "documentation"
          ],
          "description": "Type of information to retrieve about cryosparc-tools"
        }
      },
      "required": [
        "info_type"
      ]
    },
    "package_name": "cryosparc-tools",
    "local_info": {
      "name": "cryosparc-tools",
      "description": "Python tools for interacting with CryoSPARC cryo-electron microscopy data processing software. Provides programmatic access to projects, datasets, and results for automated workflows and analysis.",
      "category": "Cryo-EM / Structural Biology",
      "import_name": "cryosparc",
      "popularity": 55,
      "keywords": [
        "cryo-EM",
        "CryoSPARC",
        "structural biology",
        "electron microscopy",
        "image processing"
      ],
      "documentation": "https://cryosparc.com/docs",
      "repository": "https://github.com/cryosparc/cryosparc-tools",
      "installation": {
        "pip": "pip install cryosparc-tools",
        "conda": "conda install -c conda-forge cryosparc-tools"
      },
      "usage_example": "# Note: This example assumes CryoSPARC server access\n# from cryosparc.client import Client\n# import numpy as np\n\n# Connect to CryoSPARC instance\n# client = Client(\n#     license='YOUR_LICENSE',\n#     host='localhost',\n#     base_port=39000,\n#     email='user@example.com',\n#     password='password'\n# )\n\nprint('CryoSPARC Tools Usage Example:')\nprint('\\n1. Connection:')\nprint('   client = Client(license, host, base_port, email, password)')\n\nprint('\\n2. Project Management:')\nprint('   projects = client.get_projects()')\nprint('   project = client.create_project(\"My Project\")')\n\nprint('\\n3. Dataset Operations:')\nprint('   datasets = client.get_datasets(project_uid)')\nprint('   particles = client.get_particles(dataset_uid)')\n\nprint('\\n4. Job Management:')\nprint('   jobs = client.get_jobs(project_uid)')\nprint('   job = client.create_job(project_uid, job_type)')\n\nprint('\\n5. Data Analysis:')\nprint('   # Access particle data')\nprint('   # positions = particles[\"alignments2D/pose\"]')\nprint('   # ctf_params = particles[\"ctf/df1_A\"]')\n\nprint('\\n6. Results Export:')\nprint('   # Export maps, particles, or metadata')\nprint('   # client.download_dataset(dataset_uid, \"output.cs\")')\n\nprint('\\nNote: Requires active CryoSPARC installation and license')\nprint('Used for:')\nprint('- Automated cryo-EM workflows')\nprint('- Batch processing of datasets')\nprint('- Custom analysis pipelines')\nprint('- Integration with other tools')",
      "quick_start": [
        "Install: pip install cryosparc-tools",
        "Connect: Client(license, host, port, email, password)",
        "Access projects: client.get_projects()",
        "Manage datasets: client.get_datasets()",
        "Run jobs: client.create_job()",
        "Requires CryoSPARC server access"
      ]
    }
  },
  {
    "type": "PackageTool",
    "name": "get_tskit_info",
    "description": "Get comprehensive information about tskit – tree sequence toolkit for population genetics",
    "parameter": {
      "type": "object",
      "properties": {
        "info_type": {
          "type": "string",
          "enum": [
            "overview",
            "installation",
            "usage",
            "documentation"
          ],
          "description": "Type of information to retrieve about tskit"
        }
      },
      "required": [
        "info_type"
      ]
    },
    "package_name": "tskit",
    "local_info": {
      "name": "tskit",
      "description": "Tree sequence toolkit for storing and analyzing genetic variation data efficiently. Provides succinct representation of genealogical relationships and genetic variants for population genetic analysis.",
      "category": "Population Genetics",
      "import_name": "tskit",
      "popularity": 75,
      "keywords": [
        "tree sequences",
        "population genetics",
        "genealogy",
        "coalescent",
        "genetic variation"
      ],
      "documentation": "https://tskit.dev/tskit/docs/stable/",
      "repository": "https://github.com/tskit-dev/tskit",
      "installation": {
        "pip": "pip install tskit",
        "conda": "conda install -c conda-forge tskit"
      },
      "usage_example": "import tskit\nimport numpy as np\n\n# Load tree sequence from file\nts = tskit.load('example.trees')\n\nprint(f'Number of samples: {ts.num_samples}')\nprint(f'Number of trees: {ts.num_trees}')\nprint(f'Number of sites: {ts.num_sites}')\n\n# Iterate through trees\nfor tree in ts.trees():\n    print(f'Tree {tree.index}: interval [{tree.interval.left}, {tree.interval.right})')\n    \n    # Calculate tree statistics\n    print(f'  TMRCA: {tree.tmrca(tree.roots[0]):.4f}')\n    print(f'  Total branch length: {tree.total_branch_length:.4f}')\n\n# Calculate diversity statistics\ndiversity = ts.diversity()\nprint(f'Nucleotide diversity: {diversity:.6f}')\n\n# Calculate Tajima's D\ntaj_d = ts.Tajimas_D()\nprint(f\"Tajima's D: {taj_d:.4f}\")",
      "quick_start": [
        "Install: pip install tskit",
        "Load tree sequence: ts = tskit.load('file.trees')",
        "Iterate trees: for tree in ts.trees()",
        "Calculate diversity: ts.diversity()",
        "Compute statistics: ts.Tajimas_D()",
        "Access sample data: ts.samples()"
      ]
    }
  },
  {
    "type": "PackageTool",
    "name": "get_fanc_info",
    "description": "Get comprehensive information about FAN-C – framework for analyzing nuclear contacts",
    "parameter": {
      "type": "object",
      "properties": {
        "info_type": {
          "type": "string",
          "enum": [
            "overview",
            "installation",
            "usage",
            "documentation"
          ],
          "description": "Type of information to retrieve about FAN-C"
        }
      },
      "required": [
        "info_type"
      ]
    },
    "package_name": "fanc",
    "local_info": {
      "name": "FAN-C",
      "description": "Framework for Analyzing Nuclear Contacts - comprehensive toolkit for Hi-C and 3C-seq data analysis. Provides tools for processing, visualization, and analysis of chromosome conformation capture data.",
      "category": "3D Genomics / Hi-C Analysis",
      "import_name": "fanc",
      "popularity": 65,
      "keywords": [
        "Hi-C",
        "3C-seq",
        "chromosome conformation",
        "3D genomics",
        "contact matrices"
      ],
      "documentation": "https://fan-c.readthedocs.io/",
      "repository": "https://github.com/vaquerizaslab/fanc",
      "installation": {
        "pip": "pip install fanc",
        "conda": "conda install -c bioconda fanc"
      },
      "usage_example": "import fanc\nimport fanc.plotting as fancplot\nimport matplotlib.pyplot as plt\n\n# Load Hi-C data (example with built-in test data)\n# hic = fanc.load('sample.hic')  # Load from file\n\n# For demonstration, create synthetic contact matrix\nimport numpy as np\nn_bins = 100\ncontact_matrix = np.random.exponential(1, (n_bins, n_bins))\n# Make symmetric\ncontact_matrix = (contact_matrix + contact_matrix.T) / 2\n# Add diagonal decay\nfor i in range(n_bins):\n    for j in range(n_bins):\n        contact_matrix[i, j] *= np.exp(-abs(i-j)/10)\n\nprint(f'Contact matrix shape: {contact_matrix.shape}')\nprint(f'Matrix sum: {contact_matrix.sum():.2e}')\n\n# Basic analysis\n# Calculate insulation score (measure of TAD boundaries)\ninsulation_score = np.array([contact_matrix[max(0, i-5):i+6, max(0, i-5):i+6].sum() \n                            for i in range(n_bins)])\n\nprint(f'Insulation scores range: {insulation_score.min():.2f} - {insulation_score.max():.2f}')\n\n# Visualization\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8))\n\n# Plot contact matrix\nim = ax1.imshow(contact_matrix, cmap='Reds', interpolation='nearest')\nax1.set_title('Hi-C Contact Matrix')\nax1.set_xlabel('Genomic Bin')\nax1.set_ylabel('Genomic Bin')\nplt.colorbar(im, ax=ax1)\n\n# Plot insulation score\nax2.plot(insulation_score, 'b-', linewidth=2)\nax2.set_title('Insulation Score (TAD Boundaries)')\nax2.set_xlabel('Genomic Bin')\nax2.set_ylabel('Insulation Score')\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint('FAN-C provides comprehensive Hi-C analysis including:')\nprint('- Contact matrix processing and normalization')\nprint('- TAD and loop calling')\nprint('- Comparative analysis between samples')\nprint('- Publication-ready visualizations')",
      "quick_start": [
        "Install: pip install fanc",
        "Load data: hic = fanc.load('file.hic')",
        "Plot matrix: fanc.plotting.TriangularMatrixPlot()",
        "Find TADs: fanc.insulation()",
        "Call loops: fanc.peaks()",
        "Export results: fanc.to_cooler() or fanc.to_bigwig()"
      ]
    }
  },
  {
    "type": "PackageTool",
    "name": "get_pybigwig_info",
    "description": "Get comprehensive information about pyBigWig – BigWig file access in Python",
    "parameter": {
      "type": "object",
      "properties": {
        "info_type": {
          "type": "string",
          "enum": [
            "overview",
            "installation",
            "usage",
            "documentation"
          ],
          "description": "Type of information to retrieve about pyBigWig"
        }
      },
      "required": [
        "info_type"
      ]
    },
    "package_name": "pyBigWig",
    "local_info": {
      "name": "pyBigWig",
      "description": "Python interface for accessing BigWig files, which store dense continuous data such as coverage tracks from sequencing experiments. Provides efficient random access to genomic interval data.",
      "category": "Genomics File I/O",
      "import_name": "pyBigWig",
      "popularity": 82,
      "keywords": [
        "BigWig",
        "genomics",
        "coverage",
        "ChIP-seq",
        "RNA-seq",
        "track data"
      ],
      "documentation": "https://github.com/deeptools/pyBigWig",
      "repository": "https://github.com/deeptools/pyBigWig",
      "installation": {
        "pip": "pip install pyBigWig",
        "conda": "conda install -c conda-forge pybigwig"
      },
      "usage_example": "import pyBigWig\nimport numpy as np\n\n# Open BigWig file\nbw = pyBigWig.open('example.bw')\n\n# Get information about the file\nprint(f'Chromosomes: {bw.chroms()}')\nprint(f'Header: {bw.header()}')\n\n# Get values for a specific region\nvalues = bw.values('chr1', 1000, 2000)\nprint(f'Mean value: {np.nanmean(values):.3f}')\n\n# Get stats for a region\nstats = bw.stats('chr1', 1000, 2000, type='mean')\nprint(f'Region mean: {stats[0]:.3f}')\n\n# Get intervals above threshold\nintervals = bw.intervals('chr1', 1000, 2000)\nfor start, end, value in intervals:\n    if value > 10:\n        print(f'{start}-{end}: {value}')\n\nbw.close()",
      "quick_start": [
        "Install: pip install pyBigWig",
        "Open file: bw = pyBigWig.open('file.bw')",
        "Get values: bw.values('chr1', start, end)",
        "Get statistics: bw.stats('chr1', start, end)",
        "Get intervals: bw.intervals('chr1', start, end)",
        "Always close: bw.close()"
      ]
    }
  },
  {
    "type": "PackageTool",
    "name": "get_arxiv_info",
    "description": "Get comprehensive information about arxiv – access to arXiv preprint repository",
    "parameter": {
      "type": "object",
      "properties": {
        "include_examples": {
          "type": "boolean",
          "description": "Whether to include usage examples and quick start guide",
          "default": true
        }
      },
      "required": [
        "include_examples"
      ]
    },
    "package_name": "arxiv",
    "local_info": {
      "name": "arxiv",
      "description": "Python wrapper for the arXiv API. Allows programmatic access to arXiv preprint repository for searching, downloading, and parsing scientific papers.",
      "category": "Literature Mining",
      "import_name": "arxiv",
      "popularity": 75,
      "keywords": [
        "arXiv",
        "preprints",
        "scientific papers",
        "literature search",
        "academic research"
      ],
      "documentation": "https://github.com/lukasschwab/arxiv.py",
      "repository": "https://github.com/lukasschwab/arxiv.py",
      "installation": {
        "pip": "pip install arxiv",
        "conda": "conda install -c conda-forge arxiv"
      },
      "usage_example": "import arxiv\n\n# Search for papers\nsearch = arxiv.Search(\n    query='machine learning AND biology',\n    max_results=10,\n    sort_by=arxiv.SortCriterion.SubmittedDate\n)\n\n# Iterate through results\nfor result in search.results():\n    print(f'Title: {result.title}')\n    print(f'Authors: {[author.name for author.name in result.authors]}')\n    print(f'Published: {result.published}')\n    print(f'Categories: {result.categories}')\n    print(f'Abstract: {result.summary[:200]}...')\n    print(f'PDF URL: {result.pdf_url}')\n    print('---')",
      "quick_start": [
        "1. Install arxiv: pip install arxiv",
        "2. Import: import arxiv",
        "3. Search: search = arxiv.Search(query='machine learning')",
        "4. Iterate: for result in search.results():",
        "5. Access: result.title, result.authors, result.pdf_url"
      ]
    }
  },
  {
    "type": "PackageTool",
    "name": "get_plip_info",
    "description": "Get comprehensive information about PLIP – protein-ligand interaction profiler",
    "parameter": {
      "type": "object",
      "properties": {
        "info_type": {
          "type": "string",
          "enum": [
            "overview",
            "installation",
            "usage",
            "documentation"
          ],
          "description": "Type of information to retrieve about PLIP"
        }
      },
      "required": [
        "info_type"
      ]
    },
    "package_name": "plip",
    "local_info": {
      "name": "PLIP",
      "description": "Protein-Ligand Interaction Profiler for analyzing and visualizing non-covalent protein-ligand interactions in PDB files. Identifies hydrogen bonds, hydrophobic contacts, π-interactions, and salt bridges.",
      "category": "Structural Biology / Drug Discovery",
      "import_name": "plip",
      "popularity": 70,
      "keywords": [
        "protein-ligand interactions",
        "structural biology",
        "drug discovery",
        "PDB analysis",
        "molecular recognition"
      ],
      "documentation": "https://github.com/pharmai/plip",
      "repository": "https://github.com/pharmai/plip",
      "installation": {
        "pip": "pip install plip",
        "conda": "conda install -c conda-forge plip"
      },
      "usage_example": "# PLIP is primarily a command-line tool, but we can demonstrate concepts\nfrom plip.structure.preparation import PDBComplex\nfrom plip.exchange.report import BindingSiteReport\nimport tempfile\nimport os\nimport numpy as np\n\nprint('PLIP - Protein-Ligand Interaction Profiler')\nprint('=' * 45)\n\n# PLIP analyzes these interaction types:\nprint('PLIP identifies these interaction types:')\nprint('• Hydrogen bonds (conventional and weak)')\nprint('• Hydrophobic contacts')\nprint('• π-π stacking interactions')\nprint('• π-cation interactions')\nprint('• Salt bridges')\nprint('• Water-mediated interactions')\nprint('• Halogen bonds')\nprint('• Metal coordination')\n\n# Create a synthetic protein-ligand complex for demonstration\nprint('\\nCreating synthetic protein-ligand complex...')\n\nwith tempfile.NamedTemporaryFile(mode='w', suffix='.pdb', delete=False) as f:\n    f.write('HEADER    PROTEIN-LIGAND COMPLEX                  01-SEP-25\\n')\n    f.write('TITLE     DEMONSTRATION COMPLEX FOR PLIP\\n')\n    \n    atom_id = 1\n    \n    # Create protein binding pocket (simplified)\n    protein_atoms = [\n        # Binding pocket residues\n        ('ALA', 1, [('N', 10.0, 10.0, 10.0), ('CA', 10.5, 10.5, 10.0), \n                   ('C', 11.0, 10.0, 10.0), ('O', 11.5, 10.0, 9.5)]),\n        ('SER', 2, [('N', 12.0, 10.0, 10.5), ('CA', 12.5, 10.5, 11.0),\n                   ('C', 13.0, 10.0, 11.5), ('O', 13.5, 10.5, 12.0),\n                   ('OG', 12.0, 11.5, 11.5)]),  # Hydroxyl for H-bond\n        ('PHE', 3, [('N', 14.0, 10.0, 12.0), ('CA', 14.5, 10.5, 12.5),\n                   ('C', 15.0, 10.0, 13.0), ('O', 15.5, 10.5, 13.5),\n                   ('CG', 14.0, 11.5, 13.0), ('CD1', 13.5, 12.0, 13.5),\n                   ('CD2', 14.0, 12.0, 12.5), ('CE1', 13.0, 13.0, 13.5),\n                   ('CE2', 13.5, 13.0, 12.5), ('CZ', 13.0, 13.5, 13.0)]),  # Aromatic ring\n        ('LYS', 4, [('N', 16.0, 10.0, 14.0), ('CA', 16.5, 10.5, 14.5),\n                   ('C', 17.0, 10.0, 15.0), ('O', 17.5, 10.5, 15.5),\n                   ('NZ', 16.0, 11.5, 15.0)])  # Positive charge\n    ]\n    \n    for res_name, res_id, atoms in protein_atoms:\n        for atom_name, x, y, z in atoms:\n            f.write(f'ATOM  {atom_id:5d}  {atom_name:4s}{res_name:>3s} A{res_id:4d}    '\n                   f'{x:8.3f}{y:8.3f}{z:8.3f}  1.00 20.00           {atom_name[0]:>2s}\\n')\n            atom_id += 1\n    \n    # Create ligand (small molecule)\n    f.write('HETATM{:5d}  C1  LIG A 100    {:8.3f}{:8.3f}{:8.3f}  1.00 20.00           C\\n'\n           .format(atom_id, 12.0, 12.0, 11.0))\n    atom_id += 1\n    f.write('HETATM{:5d}  C2  LIG A 100    {:8.3f}{:8.3f}{:8.3f}  1.00 20.00           C\\n'\n           .format(atom_id, 13.0, 12.5, 11.5))\n    atom_id += 1\n    f.write('HETATM{:5d}  O1  LIG A 100    {:8.3f}{:8.3f}{:8.3f}  1.00 20.00           O\\n'\n           .format(atom_id, 11.5, 11.5, 11.5))  # H-bond acceptor\n    atom_id += 1\n    f.write('HETATM{:5d}  N1  LIG A 100    {:8.3f}{:8.3f}{:8.3f}  1.00 20.00           N\\n'\n           .format(atom_id, 13.5, 13.0, 12.0))  # H-bond donor\n    atom_id += 1\n    f.write('HETATM{:5d}  C3  LIG A 100    {:8.3f}{:8.3f}{:8.3f}  1.00 20.00           C\\n'\n           .format(atom_id, 14.0, 12.0, 12.5))  # Hydrophobic\n    atom_id += 1\n    \n    # Aromatic ring in ligand for π-π interaction\n    aromatic_center = (13.0, 13.0, 13.0)\n    for i, (dx, dy) in enumerate([(0, 1.4), (1.2, 0.7), (1.2, -0.7), (0, -1.4), (-1.2, -0.7), (-1.2, 0.7)]):\n        x = aromatic_center[0] + dx\n        y = aromatic_center[1] + dy\n        z = aromatic_center[2]\n        f.write('HETATM{:5d}  C{:1d}  LIG A 100    {:8.3f}{:8.3f}{:8.3f}  1.00 20.00           C\\n'\n               .format(atom_id, i+6, x, y, z))\n        atom_id += 1\n    \n    f.write('END\\n')\n    pdb_file = f.name\n\nprint(f'Created synthetic PDB file: {pdb_file}')\n\n# PLIP analysis concepts (simplified since we can't run full PLIP)\nprint('\\n=== PLIP Analysis Concepts ===')\n\n# Load PDB and identify potential interactions\nprint('\\n1. Structure Loading and Preparation:')\nprint(f'   - Loading PDB file: {pdb_file}')\nprint('   - Identifying protein chains and ligands')\nprint('   - Preparing structures (adding hydrogens, etc.)')\n\n# Simulate interaction detection\nprint('\\n2. Interaction Detection:')\n\n# Define interaction criteria (simplified)\ninteraction_criteria = {\n    'hydrogen_bond': {\n        'distance_cutoff': 3.5,  # Angstroms\n        'angle_cutoff': 120  # degrees\n    },\n    'hydrophobic_contact': {\n        'distance_cutoff': 4.0  # Angstroms\n    },\n    'pi_pi_stacking': {\n        'distance_cutoff': 5.0,  # Angstroms\n        'angle_cutoff': 30  # degrees from parallel\n    },\n    'pi_cation': {\n        'distance_cutoff': 6.0  # Angstroms\n    },\n    'salt_bridge': {\n        'distance_cutoff': 4.0  # Angstroms\n    }\n}\n\nprint('Interaction detection criteria:')\nfor interaction_type, criteria in interaction_criteria.items():\n    print(f'   {interaction_type}:')\n    for param, value in criteria.items():\n        unit = 'Å' if 'distance' in param else '°'\n        print(f'     {param}: {value} {unit}')\n\n# Simulate detected interactions\ndetected_interactions = [\n    {\n        'type': 'hydrogen_bond',\n        'protein_residue': 'SER2',\n        'protein_atom': 'OG',\n        'ligand_atom': 'O1',\n        'distance': 2.8,\n        'angle': 165.0,\n        'strength': 'strong'\n    },\n    {\n        'type': 'hydrogen_bond',\n        'protein_residue': 'SER2',\n        'protein_atom': 'N',\n        'ligand_atom': 'N1',\n        'distance': 3.1,\n        'angle': 140.0,\n        'strength': 'medium'\n    },\n    {\n        'type': 'hydrophobic_contact',\n        'protein_residue': 'ALA1',\n        'protein_atom': 'CB',\n        'ligand_atom': 'C3',\n        'distance': 3.7,\n        'strength': 'weak'\n    },\n    {\n        'type': 'pi_pi_stacking',\n        'protein_residue': 'PHE3',\n        'protein_atom': 'CZ',\n        'ligand_atom': 'C6-ring',\n        'distance': 4.2,\n        'angle': 15.0,\n        'strength': 'medium'\n    },\n    {\n        'type': 'pi_cation',\n        'protein_residue': 'LYS4',\n        'protein_atom': 'NZ',\n        'ligand_atom': 'C6-ring',\n        'distance': 5.1,\n        'strength': 'medium'\n    }\n]\n\nprint('\\n3. Detected Interactions:')\nprint(f'   Total interactions found: {len(detected_interactions)}')\n\ninteraction_counts = {}\nfor interaction in detected_interactions:\n    itype = interaction['type']\n    interaction_counts[itype] = interaction_counts.get(itype, 0) + 1\n\nfor itype, count in interaction_counts.items():\n    print(f'   {itype.replace(\"_\", \" \").title()}: {count}')\n\nprint('\\nDetailed interaction analysis:')\nfor i, interaction in enumerate(detected_interactions, 1):\n    print(f'   {i}. {interaction[\"type\"].replace(\"_\", \" \").title()}')\n    print(f'      Protein: {interaction[\"protein_residue\"]} ({interaction[\"protein_atom\"]})')\n    print(f'      Ligand: {interaction[\"ligand_atom\"]}')\n    print(f'      Distance: {interaction[\"distance\"]:.1f} Å')\n    if 'angle' in interaction:\n        print(f'      Angle: {interaction[\"angle\"]:.1f}°')\n    print(f'      Strength: {interaction[\"strength\"]}')\n    print()\n\n# Binding affinity estimation\nprint('4. Binding Affinity Estimation:')\n\n# Simple scoring based on interactions\nscoring_weights = {\n    'hydrogen_bond': {'strong': 2.0, 'medium': 1.5, 'weak': 1.0},\n    'hydrophobic_contact': {'strong': 1.0, 'medium': 0.7, 'weak': 0.5},\n    'pi_pi_stacking': {'strong': 1.5, 'medium': 1.2, 'weak': 0.8},\n    'pi_cation': {'strong': 1.5, 'medium': 1.2, 'weak': 0.8},\n    'salt_bridge': {'strong': 2.5, 'medium': 2.0, 'weak': 1.5}\n}\n\ntotal_score = 0\nfor interaction in detected_interactions:\n    itype = interaction['type']\n    strength = interaction['strength']\n    score = scoring_weights.get(itype, {}).get(strength, 0)\n    total_score += score\n    print(f'   {interaction[\"type\"]} ({strength}): +{score:.1f}')\n\nprint(f'\\n   Total interaction score: {total_score:.1f}')\nprint(f'   Estimated binding strength: {\"Strong\" if total_score > 8 else \"Medium\" if total_score > 4 else \"Weak\"}')\n\n# Pharmacophore analysis\nprint('\\n5. Pharmacophore Analysis:')\n\npharmacophore_features = {\n    'hydrogen_bond_donor': 1,\n    'hydrogen_bond_acceptor': 1, \n    'hydrophobic_center': 1,\n    'aromatic_ring': 1,\n    'positive_ionizable': 0,\n    'negative_ionizable': 0\n}\n\nprint('   Key pharmacophore features in ligand:')\nfor feature, count in pharmacophore_features.items():\n    if count > 0:\n        print(f'     {feature.replace(\"_\", \" \").title()}: {count}')\n\n# Drug-likeness assessment\nprint('\\n6. Drug-likeness Assessment (conceptual):')\n\n# Simplified molecular properties\nmol_properties = {\n    'molecular_weight': 250.3,  # Simulated\n    'logP': 2.1,  # Simulated\n    'hbd': 1,  # Hydrogen bond donors\n    'hba': 2,  # Hydrogen bond acceptors\n    'rotatable_bonds': 3,\n    'aromatic_rings': 1\n}\n\nprint('   Molecular properties:')\nfor prop, value in mol_properties.items():\n    print(f'     {prop.replace(\"_\", \" \").title()}: {value}')\n\n# Lipinski's Rule of Five\nlipinski_violations = 0\nif mol_properties['molecular_weight'] > 500:\n    lipinski_violations += 1\nif mol_properties['logP'] > 5:\n    lipinski_violations += 1\nif mol_properties['hbd'] > 5:\n    lipinski_violations += 1\nif mol_properties['hba'] > 10:\n    lipinski_violations += 1\n\nprint(f'\\n   Lipinski Rule of Five violations: {lipinski_violations}/4')\nprint(f'   Drug-likeness: {\"Good\" if lipinski_violations <= 1 else \"Poor\"}')\n\n# Visualization concepts\nprint('\\n=== Visualization and Output ===')\n\nprint('PLIP generates:')\nprint('• Interaction diagrams (2D and 3D)')\nprint('• Detailed interaction reports')\nprint('• PyMOL visualization scripts')\nprint('• XML/JSON output formats')\nprint('• Statistical summaries')\nprint('• Binding site characterization')\n\n# Create a simple interaction summary\nprint('\\n=== Interaction Summary Report ===')\nprint('=' * 45)\nprint(f'PDB File: {os.path.basename(pdb_file)}')\nprint(f'Ligand: LIG (Chain A, Residue 100)')\nprint(f'Binding Site: Chain A')\nprint(f'\\nInteraction Summary:')\nprint(f'  Total Interactions: {len(detected_interactions)}')\nfor itype, count in interaction_counts.items():\n    print(f'  {itype.replace(\"_\", \" \").title()}: {count}')\nprint(f'\\nBinding Assessment:')\nprint(f'  Interaction Score: {total_score:.1f}')\nprint(f'  Binding Strength: {\"Strong\" if total_score > 8 else \"Medium\" if total_score > 4 else \"Weak\"}')\nprint(f'  Drug-likeness: {\"Good\" if lipinski_violations <= 1 else \"Poor\"}')\nprint(f'\\nKey Interactions:')\nfor interaction in detected_interactions[:3]:  # Top 3\n    print(f'  • {interaction[\"protein_residue\"]} - {interaction[\"ligand_atom\"]} '\n          f'({interaction[\"type\"].replace(\"_\", \" \")}): {interaction[\"distance\"]:.1f} Å')\n\n# Cleanup\nos.unlink(pdb_file)\nprint(f'\\nDemo complete - temporary files cleaned up')\n\nprint('\\nPLIP provides:')\nprint('• Automated protein-ligand interaction detection')\nprint('• Comprehensive interaction classification')\nprint('• 3D visualization and 2D diagrams')\nprint('• Binding site characterization')\nprint('• Drug discovery insights')\nprint('• Integration with molecular viewers')\nprint('• Batch processing capabilities')\nprint('• Command-line and Python API')\n\nprint('\\nCommon PLIP usage:')\nprint('Command line: plip -f complex.pdb -o output_folder')\nprint('Python API: Use PDBComplex and BindingSiteReport classes')",
      "quick_start": [
        "Install: pip install plip",
        "Command line: plip -f protein_ligand.pdb -o results/",
        "Python API: complex = PDBComplex(); complex.load_pdb('file.pdb')",
        "Analyze: complex.analyze()",
        "Report: BindingSiteReport(binding_site).generate_report()",
        "View interactions in PyMOL or VMD"
      ]
    }
  },
  {
    "type": "PackageTool",
    "name": "get_pdbfixer_info",
    "description": "Get comprehensive information about PDBFixer – protein structure preparation tool",
    "parameter": {
      "type": "object",
      "properties": {
        "info_type": {
          "type": "string",
          "enum": [
            "overview",
            "installation",
            "usage",
            "documentation"
          ],
          "description": "Type of information to retrieve about PDBFixer"
        }
      },
      "required": [
        "info_type"
      ]
    },
    "package_name": "pdbfixer",
    "local_info": {
      "name": "PDBFixer",
      "description": "Application for fixing common problems in Protein Data Bank (PDB) files. Automatically adds missing residues, atoms, and hydrogens while removing heterogens and water molecules for molecular dynamics simulations.",
      "category": "Structural Biology / Protein Preparation",
      "import_name": "pdbfixer",
      "popularity": 75,
      "keywords": [
        "PDB",
        "protein structure",
        "missing atoms",
        "structure preparation",
        "molecular dynamics"
      ],
      "documentation": "https://github.com/openmm/pdbfixer",
      "repository": "https://github.com/openmm/pdbfixer",
      "installation": {
        "pip": "pip install pdbfixer",
        "conda": "conda install -c conda-forge pdbfixer"
      },
      "usage_example": "from pdbfixer import PDBFixer\nfrom openmm.app import PDBFile\nimport os\n\n# Load PDB file\nfixer = PDBFixer(filename='input.pdb')\n\nprint(f'Original structure: {fixer.topology.getNumAtoms()} atoms')\nprint(f'Chains: {[chain.id for chain in fixer.topology.chains()]}')\n\n# Find and report problems\nfixer.findMissingResidues()\nfixer.findMissingAtoms()\nfixer.findNonstandardResidues()\n\nprint(f'Missing residues: {len(fixer.missingResidues)}')\nprint(f'Missing atoms: {len(fixer.missingAtoms)}')\nprint(f'Nonstandard residues: {len(fixer.nonstandardResidues)}')\n\n# Fix common problems\n# Remove heterogens (keep water)\nfixer.removeHeterogens(keepWater=True)\n\n# Add missing residues and atoms\nfixer.addMissingAtoms()\nfixer.addMissingHydrogens(7.0)  # pH 7.0\n\nprint(f'Fixed structure: {fixer.topology.getNumAtoms()} atoms')\n\n# Save fixed structure\nPDBFile.writeFile(fixer.topology, fixer.positions, open('fixed.pdb', 'w'))\nprint('Saved fixed structure to fixed.pdb')\n\n# Alternative: Load from PDB ID\n# fixer = PDBFixer(pdbid='1YCR')  # Downloads from PDB",
      "quick_start": [
        "Install: pip install pdbfixer",
        "Load PDB: fixer = PDBFixer(filename='file.pdb')",
        "Find problems: fixer.findMissingResidues/Atoms()",
        "Remove heterogens: fixer.removeHeterogens()",
        "Add missing atoms: fixer.addMissingAtoms()",
        "Save result: PDBFile.writeFile()"
      ]
    }
  },
  {
    "type": "PackageTool",
    "name": "get_prody_info",
    "description": "Get comprehensive information about ProDy – protein dynamics analysis",
    "parameter": {
      "type": "object",
      "properties": {
        "info_type": {
          "type": "string",
          "enum": [
            "overview",
            "installation",
            "usage",
            "documentation"
          ],
          "description": "Type of information to retrieve about ProDy"
        }
      },
      "required": [
        "info_type"
      ]
    },
    "package_name": "prody",
    "local_info": {
      "name": "ProDy",
      "description": "Python package for protein structural dynamics analysis. Provides tools for normal mode analysis, elastic network models, and conformational dynamics studies.",
      "category": "Protein Dynamics / Structural Biology",
      "import_name": "prody",
      "popularity": 75,
      "keywords": [
        "protein dynamics",
        "normal modes",
        "elastic network",
        "conformational analysis",
        "PDB"
      ],
      "documentation": "http://prody.csb.pitt.edu/",
      "repository": "https://github.com/prody/ProDy",
      "installation": {
        "pip": "pip install prody",
        "conda": "conda install -c conda-forge prody"
      },
      "usage_example": "import prody as pr\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tempfile\nimport os\nfrom io import StringIO\n\nprint('ProDy - Protein Structural Dynamics Analysis')\nprint('=' * 50)\n\n# Configure ProDy\npr.confProDy(verbosity='warning')  # Reduce verbosity for demo\n\n# Create a synthetic protein structure for demonstration\nprint('Creating synthetic protein structure for ProDy analysis...')\n\n# Generate a simple beta-sheet structure\nwith tempfile.NamedTemporaryFile(mode='w', suffix='.pdb', delete=False) as f:\n    f.write('HEADER    SYNTHETIC PROTEIN                       01-SEP-25\\n')\n    f.write('TITLE     DEMONSTRATION PROTEIN FOR PRODY\\n')\n    \n    atom_id = 1\n    res_id = 1\n    \n    # Create two beta strands\n    for strand in range(2):\n        for i in range(10):  # 10 residues per strand\n            if strand == 0:\n                # First strand\n                x = i * 3.5  # Extended conformation\n                y = 0\n                z = 0\n            else:\n                # Second strand (antiparallel)\n                x = (9 - i) * 3.5  # Reverse direction\n                y = 8  # Separated by ~8 Å\n                z = 0\n            \n            # Add some variation\n            x += np.random.normal(0, 0.1)\n            y += np.random.normal(0, 0.1)\n            z += np.random.normal(0, 0.1)\n            \n            res_name = 'VAL' if i % 2 == 0 else 'ILE'\n            \n            # Backbone atoms\n            atoms = [\n                ('N', x-0.5, y-0.3, z-0.1),\n                ('CA', x, y, z),\n                ('C', x+0.5, y+0.3, z+0.1),\n                ('O', x+0.8, y+0.5, z+0.2)\n            ]\n            \n            for atom_name, ax, ay, az in atoms:\n                f.write(f'ATOM  {atom_id:5d}  {atom_name:4s}{res_name:>3s} A{res_id:4d}    '\n                       f'{ax:8.3f}{ay:8.3f}{az:8.3f}  1.00 20.00           {atom_name[0]:>2s}\\n')\n                atom_id += 1\n            \n            res_id += 1\n    \n    f.write('END\\n')\n    pdb_file = f.name\n\nprint(f'Created synthetic PDB file: {pdb_file}')\n\n# Load structure with ProDy\nprint('\\n=== Loading Structure with ProDy ===')\n\ntry:\n    # Parse PDB structure\n    structure = pr.parsePDB(pdb_file)\n    print(f'Structure loaded: {structure}')\n    print(f'Number of atoms: {structure.numAtoms()}')\n    print(f'Number of residues: {structure.numResidues()}')\n    print(f'Number of chains: {structure.numChains()}')\n    \n    # Get atom information\n    print(f'Atom names: {set(structure.getNames())}')\n    print(f'Residue names: {set(structure.getResnames())}')\n    \nexcept Exception as e:\n    print(f'Error loading structure: {e}')\n    # Create a simple structure manually for demonstration\n    coords = np.random.random((80, 3)) * 20  # 80 atoms, 3D coordinates\n    structure = pr.AtomGroup('demo')\n    structure.setCoords(coords)\n    structure.setNames(['CA'] * 80)\n    structure.setResnames(['ALA'] * 80)\n    structure.setResnums(np.repeat(range(1, 21), 4))\n    print('Created synthetic AtomGroup for demonstration')\n\n# Basic structural analysis\nprint('\\n=== Basic Structural Analysis ===')\n\nif hasattr(structure, 'getCoords'):\n    coords = structure.getCoords()\n    print(f'Coordinate array shape: {coords.shape}')\n    \n    # Center of mass\n    center_of_mass = coords.mean(axis=0)\n    print(f'Center of mass: ({center_of_mass[0]:.2f}, {center_of_mass[1]:.2f}, {center_of_mass[2]:.2f})')\n    \n    # Calculate radius of gyration\n    centered_coords = coords - center_of_mass\n    rg = np.sqrt(np.mean(np.sum(centered_coords**2, axis=1)))\n    print(f'Radius of gyration: {rg:.2f} Å')\n\n# Select specific atoms\nprint('\\n=== Atom Selection ===')\n\ntry:\n    # Select backbone atoms\n    backbone = structure.select('backbone')\n    if backbone:\n        print(f'Backbone atoms: {backbone.numAtoms()}')\n    \n    # Select CA atoms\n    ca_atoms = structure.select('name CA')\n    if ca_atoms:\n        print(f'CA atoms: {ca_atoms.numAtoms()}')\n        ca_coords = ca_atoms.getCoords()\n        print(f'CA coordinate range:')\n        print(f'  X: {ca_coords[:, 0].min():.1f} to {ca_coords[:, 0].max():.1f} Å')\n        print(f'  Y: {ca_coords[:, 1].min():.1f} to {ca_coords[:, 1].max():.1f} Å')\n        print(f'  Z: {ca_coords[:, 2].min():.1f} to {ca_coords[:, 2].max():.1f} Å')\n    \nexcept Exception as e:\n    print(f'Selection error: {e}')\n    # Use all atoms as CA atoms for demo\n    ca_atoms = structure\n    ca_coords = structure.getCoords()\n\n# Elastic Network Model (ENM)\nprint('\\n=== Elastic Network Model Analysis ===')\n\ntry:\n    # Create Gaussian Network Model\n    if ca_atoms and ca_atoms.numAtoms() > 3:\n        print('Building Gaussian Network Model (GNM)...')\n        gnm = pr.GNM('GNM Analysis')\n        gnm.buildKirchhoff(ca_atoms, cutoff=10.0)  # 10 Å cutoff\n        \n        print(f'GNM Kirchhoff matrix shape: {gnm.getKirchhoff().shape}')\n        \n        # Calculate normal modes\n        print('Calculating normal modes...')\n        gnm.calcModes(n_modes=10)  # Calculate first 10 modes\n        \n        print(f'Number of modes calculated: {gnm.numModes()}')\n        print(f'Eigenvalues (first 5): {gnm.getEigvals()[:5]}')\n        \n        # Get mode information\n        mode_data = []\n        for i in range(min(5, gnm.numModes())):\n            eigenval = gnm.getEigvals()[i]\n            mode_data.append({\n                'mode': i + 1,\n                'eigenvalue': eigenval,\n                'frequency': np.sqrt(eigenval) if eigenval > 0 else 0\n            })\n        \n        print('\\nMode analysis:')\n        for data in mode_data:\n            print(f'  Mode {data[\"mode\"]}: eigenvalue = {data[\"eigenvalue\"]:.6f}, '\n                  f'frequency = {data[\"frequency\"]:.6f}')\n    \nexcept Exception as e:\n    print(f'ENM analysis error: {e}')\n    # Create synthetic mode data\n    mode_data = [\n        {'mode': i+1, 'eigenvalue': np.random.random()*0.1, 'frequency': np.random.random()*0.3}\n        for i in range(5)\n    ]\n    print('Created synthetic mode data for demonstration')\n\n# Anisotropic Network Model (ANM)\nprint('\\n=== Anisotropic Network Model Analysis ===')\n\ntry:\n    if ca_atoms and ca_atoms.numAtoms() > 3:\n        print('Building Anisotropic Network Model (ANM)...')\n        anm = pr.ANM('ANM Analysis')\n        anm.buildHessian(ca_atoms, cutoff=15.0)  # 15 Å cutoff\n        \n        print(f'ANM Hessian matrix shape: {anm.getHessian().shape}')\n        \n        # Calculate normal modes\n        anm.calcModes(n_modes=10)\n        \n        print(f'Number of ANM modes: {anm.numModes()}')\n        \n        # Calculate mean square fluctuations\n        if anm.numModes() > 0:\n            msf = pr.calcSqFlucts(anm)\n            print(f'Mean square fluctuations calculated: {len(msf)} residues')\n            print(f'MSF range: {msf.min():.4f} to {msf.max():.4f} Ų')\n            print(f'Average MSF: {msf.mean():.4f} Ų')\n        \n        # Mode analysis\n        anm_mode_data = []\n        for i in range(min(5, anm.numModes())):\n            eigenval = anm.getEigvals()[i]\n            anm_mode_data.append({\n                'mode': i + 1,\n                'eigenvalue': eigenval,\n                'frequency': np.sqrt(eigenval) if eigenval > 0 else 0\n            })\n        \n        print('\\nANM mode analysis:')\n        for data in anm_mode_data:\n            print(f'  Mode {data[\"mode\"]}: eigenvalue = {data[\"eigenvalue\"]:.6f}')\n    \nexcept Exception as e:\n    print(f'ANM analysis error: {e}')\n    # Create synthetic data\n    msf = np.random.random(20) * 2.0\n    anm_mode_data = mode_data.copy()\n    print('Created synthetic ANM data for demonstration')\n\n# B-factor analysis\nprint('\\n=== B-factor Analysis ===')\n\ntry:\n    # Calculate theoretical B-factors from ENM\n    if 'anm' in locals() and anm.numModes() > 0:\n        bfactors = pr.calcTempFactors(anm, ca_atoms)\n        print(f'Calculated B-factors: {len(bfactors)} atoms')\n        print(f'B-factor range: {bfactors.min():.2f} to {bfactors.max():.2f} Ų')\n        print(f'Average B-factor: {bfactors.mean():.2f} Ų')\n    else:\n        # Synthetic B-factors\n        bfactors = np.random.random(20) * 50 + 10\n        print('Created synthetic B-factors for demonstration')\n    \n    # Compare with experimental (if available)\n    if hasattr(structure, 'getBetas'):\n        exp_bfactors = structure.getBetas()\n        if exp_bfactors is not None and len(exp_bfactors) > 0:\n            correlation = np.corrcoef(bfactors[:len(exp_bfactors)], exp_bfactors)[0, 1]\n            print(f'Correlation with experimental B-factors: {correlation:.3f}')\n        else:\n            print('No experimental B-factors available')\n    \nexcept Exception as e:\n    print(f'B-factor analysis error: {e}')\n    bfactors = np.random.random(20) * 50 + 10\n\n# Cross-correlation analysis\nprint('\\n=== Cross-correlation Analysis ===')\n\ntry:\n    if 'anm' in locals() and anm.numModes() > 0:\n        # Calculate cross-correlations\n        cross_corr = pr.calcCrossCorr(anm)\n        print(f'Cross-correlation matrix shape: {cross_corr.shape}')\n        print(f'Correlation range: {cross_corr.min():.3f} to {cross_corr.max():.3f}')\n        \n        # Average correlation\n        # Exclude diagonal elements\n        off_diagonal = cross_corr[np.triu_indices_from(cross_corr, k=1)]\n        avg_correlation = off_diagonal.mean()\n        print(f'Average off-diagonal correlation: {avg_correlation:.3f}')\n    else:\n        # Synthetic correlation matrix\n        n = 20\n        cross_corr = np.random.random((n, n))\n        cross_corr = (cross_corr + cross_corr.T) / 2  # Make symmetric\n        np.fill_diagonal(cross_corr, 1.0)\n        print('Created synthetic cross-correlation matrix')\n    \nexcept Exception as e:\n    print(f'Cross-correlation analysis error: {e}')\n    n = 20\n    cross_corr = np.eye(n) + np.random.random((n, n)) * 0.5\n\n# Visualization\nprint('\\n=== Visualization ===')\n\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\n\n# 1. Mode eigenvalues\nif 'mode_data' in locals():\n    modes = [d['mode'] for d in mode_data]\n    eigenvals = [d['eigenvalue'] for d in mode_data]\n    axes[0, 0].bar(modes, eigenvals, color='skyblue')\n    axes[0, 0].set_xlabel('Mode Number')\n    axes[0, 0].set_ylabel('Eigenvalue')\n    axes[0, 0].set_title('Normal Mode Eigenvalues')\n    axes[0, 0].grid(True, alpha=0.3)\n\n# 2. Mean square fluctuations\nif 'msf' in locals():\n    residue_indices = np.arange(1, len(msf) + 1)\n    axes[0, 1].plot(residue_indices, msf, 'o-', color='red')\n    axes[0, 1].set_xlabel('Residue Number')\n    axes[0, 1].set_ylabel('Mean Square Fluctuation (Ų)')\n    axes[0, 1].set_title('Residue Flexibility')\n    axes[0, 1].grid(True, alpha=0.3)\n\n# 3. B-factors\nif 'bfactors' in locals():\n    residue_indices = np.arange(1, len(bfactors) + 1)\n    axes[1, 0].plot(residue_indices, bfactors, 's-', color='green')\n    axes[1, 0].set_xlabel('Residue Number')\n    axes[1, 0].set_ylabel('B-factor (Ų)')\n    axes[1, 0].set_title('Theoretical B-factors')\n    axes[1, 0].grid(True, alpha=0.3)\n\n# 4. Cross-correlation matrix\nif 'cross_corr' in locals():\n    im = axes[1, 1].imshow(cross_corr, cmap='RdBu_r', vmin=-1, vmax=1)\n    axes[1, 1].set_xlabel('Residue Number')\n    axes[1, 1].set_ylabel('Residue Number')\n    axes[1, 1].set_title('Cross-correlation Matrix')\n    plt.colorbar(im, ax=axes[1, 1], label='Correlation')\n\nplt.tight_layout()\n\n# Save visualization\nwith tempfile.NamedTemporaryFile(suffix='.png', delete=False) as tmp:\n    plt.savefig(tmp.name, dpi=150, bbox_inches='tight')\n    viz_file = tmp.name\n\nplt.close()\nprint(f'Analysis visualization saved to: {viz_file}')\n\n# Summary report\nprint('\\n' + '=' * 50)\nprint('PRODY PROTEIN DYNAMICS ANALYSIS SUMMARY')\nprint('=' * 50)\nif 'structure' in locals():\n    print(f'Structure: {structure.numAtoms()} atoms, {structure.numResidues()} residues')\nif 'rg' in locals():\n    print(f'Radius of gyration: {rg:.2f} Å')\nif 'mode_data' in locals():\n    print(f'Normal modes calculated: {len(mode_data)}')\n    print(f'Lowest frequency mode eigenvalue: {mode_data[0][\"eigenvalue\"]:.6f}')\nif 'msf' in locals():\n    print(f'Average residue flexibility: {msf.mean():.3f} Ų')\n    print(f'Most flexible residue: {np.argmax(msf)+1} (MSF = {msf.max():.3f} Ų)')\nif 'bfactors' in locals():\n    print(f'Average theoretical B-factor: {bfactors.mean():.2f} Ų')\nif 'cross_corr' in locals():\n    off_diag = cross_corr[np.triu_indices_from(cross_corr, k=1)]\n    print(f'Average residue correlation: {off_diag.mean():.3f}')\n\n# Cleanup\nos.unlink(pdb_file)\nos.unlink(viz_file)\nprint('\\nDemo complete - temporary files cleaned up')\n\nprint('\\nProDy provides:')\nprint('• Elastic network models (GNM, ANM)')\nprint('• Normal mode analysis')\nprint('• Protein dynamics calculations')\nprint('• B-factor prediction')\nprint('• Cross-correlation analysis')\nprint('• Ensemble analysis')\nprint('• PDB file manipulation')\nprint('• Integration with VMD and PyMOL')",
      "quick_start": [
        "Install: pip install prody",
        "Load PDB: structure = prody.parsePDB('1xyz')",
        "Select atoms: ca = structure.select('name CA')",
        "Build ANM: anm = prody.ANM(); anm.buildHessian(ca)",
        "Calculate modes: anm.calcModes()",
        "Get B-factors: bfactors = prody.calcTempFactors(anm)"
      ]
    }
  },
  {
    "type": "PackageTool",
    "name": "get_mageck_info",
    "description": "Get comprehensive information about MAGeCK – CRISPR screen analysis toolkit",
    "parameter": {
      "type": "object",
      "properties": {
        "info_type": {
          "type": "string",
          "enum": [
            "overview",
            "installation",
            "usage",
            "documentation"
          ],
          "description": "Type of information to retrieve about MAGeCK"
        }
      },
      "required": [
        "info_type"
      ]
    },
    "package_name": "mageck",
    "local_info": {
      "name": "MAGeCK",
      "description": "Model-based Analysis of Genome-wide CRISPR-Cas9 Knockout (MAGeCK) for identifying essential and non-essential genes from CRISPR screening experiments. Provides robust statistical methods for hit identification.",
      "category": "CRISPR Analysis",
      "import_name": "mageck",
      "popularity": 80,
      "keywords": [
        "CRISPR",
        "genome editing",
        "genetic screens",
        "gene essentiality",
        "sgRNA"
      ],
      "documentation": "https://sourceforge.net/p/mageck/wiki/Home/",
      "repository": "https://github.com/liulab-dfci/MAGeCK",
      "installation": {
        "pip": "pip install mageck",
        "conda": "conda install -c bioconda mageck"
      },
      "usage_example": "# MAGeCK is primarily used as command-line tool\n# Python interface available for programmatic access\n\nimport subprocess\nimport pandas as pd\n\n# Example MAGeCK workflow (typically run from command line):\n\n# 1. Count sgRNA reads\n# mageck count -l library.txt -n sample --sample-label day0,day23 \n#              --fastq day0.fastq day23.fastq\n\n# 2. Test for essential genes\n# mageck test -k sample.count.txt -t day23 -c day0 -n essential_genes\n\n# 3. Maximum Likelihood Estimation (MLE) for time course\n# mageck mle -k sample.count.txt -d design_matrix.txt -n timecourse\n\n# Read MAGeCK results\nresults = pd.read_csv('essential_genes.gene_summary.txt', sep='\\t')\nprint('Top 10 essential genes:')\nprint(results.head(10)[['id', 'neg|score', 'neg|p-value', 'neg|fdr']])\n\n# Visualize results\nimport matplotlib.pyplot as plt\nplt.scatter(results['neg|score'], -np.log10(results['neg|p-value']))\nplt.xlabel('Gene Score')\nplt.ylabel('-log10(p-value)')\nplt.title('MAGeCK Results: Gene Essentiality')\nplt.show()",
      "quick_start": [
        "Install: pip install mageck",
        "Prepare sgRNA library file",
        "Count reads: mageck count -l library.txt --fastq files",
        "Test essentiality: mageck test -k counts.txt",
        "Use MLE for complex designs: mageck mle",
        "Analyze results with pandas/matplotlib"
      ]
    }
  },
  {
    "type": "PackageTool",
    "name": "get_flowkit_info",
    "description": "Get comprehensive information about FlowKit – flow cytometry analysis toolkit",
    "parameter": {
      "type": "object",
      "properties": {
        "info_type": {
          "type": "string",
          "enum": [
            "overview",
            "installation",
            "usage",
            "documentation"
          ],
          "description": "Type of information to retrieve about FlowKit"
        }
      },
      "required": [
        "info_type"
      ]
    },
    "package_name": "flowkit",
    "local_info": {
      "name": "FlowKit",
      "description": "Comprehensive toolkit for flow cytometry data analysis in Python. Provides functions for data import, compensation, transformation, gating, and visualization of flow cytometry experiments.",
      "category": "Flow Cytometry",
      "import_name": "flowkit",
      "popularity": 65,
      "keywords": [
        "flow cytometry",
        "cell analysis",
        "gating",
        "compensation",
        "immunology",
        "cell sorting"
      ],
      "documentation": "https://flowkit.readthedocs.io/",
      "repository": "https://github.com/whitews/FlowKit",
      "installation": {
        "pip": "pip install flowkit",
        "conda": "conda install -c conda-forge flowkit"
      },
      "usage_example": "import flowkit as fk\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Load FCS file\nsample = fk.Sample('sample.fcs')\nprint(f'Events: {sample.event_count}')\nprint(f'Channels: {len(sample.channels)}')\nprint(f'Channel names: {[ch.pnn_label for ch in sample.channels]}')\n\n# Apply compensation (if available)\nif sample.metadata.get('spill'):\n    sample.apply_compensation()\n    print('Applied compensation matrix')\n\n# Apply transforms (e.g., logicle)\nsample.apply_transform('logicle')\n\n# Create gates\nfrom flowkit import gates\n\n# Polygon gate example\nvertices = [(100, 200), (400, 200), (400, 500), (100, 500)]\npoly_gate = gates.PolygonGate('PolyGate', channels=['FSC-A', 'SSC-A'], vertices=vertices)\n\n# Apply gate\ngated_events = sample.get_events(gate=poly_gate)\nprint(f'Events after gating: {len(gated_events)}')\n\n# Plot data\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n\n# Original data\nax1.scatter(sample.get_events()[:, 0], sample.get_events()[:, 1], alpha=0.3)\nax1.set_title('Original Data')\nax1.set_xlabel('FSC-A')\nax1.set_ylabel('SSC-A')\n\n# Gated data\nax2.scatter(gated_events[:, 0], gated_events[:, 1], alpha=0.3)\nax2.set_title('Gated Data')\nax2.set_xlabel('FSC-A')\nax2.set_ylabel('SSC-A')\n\nplt.tight_layout()\nplt.show()",
      "quick_start": [
        "Install: pip install flowkit",
        "Load FCS file: sample = fk.Sample('file.fcs')",
        "Apply compensation: sample.apply_compensation()",
        "Transform data: sample.apply_transform('logicle')",
        "Create gates: gates.PolygonGate() or gates.RectangleGate()",
        "Analyze gated events: sample.get_events(gate=gate)"
      ]
    }
  },
  {
    "type": "PackageTool",
    "name": "get_poliastro_info",
    "description": "Get comprehensive information about poliastro – astrodynamics library",
    "parameter": {
      "type": "object",
      "properties": {
        "info_type": {
          "type": "string",
          "enum": [
            "overview",
            "installation",
            "usage",
            "documentation"
          ],
          "description": "Type of information to retrieve about poliastro"
        }
      },
      "required": [
        "info_type"
      ]
    },
    "package_name": "poliastro",
    "local_info": {
      "name": "poliastro",
      "description": "Astrodynamics in Python. Pure Python library for orbital mechanics computations including orbit determination, propagation, maneuvers, and space mission analysis.",
      "category": "Scientific Computing / Aerospace",
      "import_name": "poliastro",
      "popularity": 65,
      "keywords": [
        "astrodynamics",
        "orbital mechanics",
        "space",
        "satellites",
        "mission planning"
      ],
      "documentation": "https://docs.poliastro.space/",
      "repository": "https://github.com/poliastro/poliastro",
      "installation": {
        "pip": "pip install poliastro",
        "conda": "conda install -c conda-forge poliastro"
      },
      "usage_example": "# poliastro astrodynamics simulation\n# This demonstrates orbital mechanics concepts\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport tempfile\nimport os\nfrom datetime import datetime, timedelta\n\nprint('poliastro - Astrodynamics in Python')\nprint('=' * 38)\n\nprint('poliastro Features:')\nprint('• Orbital mechanics computations')\nprint('• Orbit propagation and determination')\nprint('• Maneuver planning and optimization')\nprint('• Two-body and n-body problems')\nprint('• Coordinate transformations')\nprint('• Mission analysis tools')\nprint('• Integration with astronomy libraries')\n\nprint('\\nApplications:')\nprint('• Satellite mission planning')\nprint('• Spacecraft trajectory design')\nprint('• Orbit analysis and visualization')\nprint('• Interplanetary mission design')\nprint('• Space situational awareness')\n\n# Constants (approximate values)\nprint('\\n=== Astronomical Constants ===')\n\n# Gravitational parameters (km³/s²)\nGM_earth = 398600.4418  # Earth\nGM_sun = 1.32712442018e11  # Sun\nGM_moon = 4902.7779  # Moon\n\n# Planetary radii (km)\nR_earth = 6378.137\nR_sun = 695700\nR_moon = 1737.1\n\n# Orbital elements for demonstration\nprint(f'Earth gravitational parameter: {GM_earth:.1f} km³/s²')\nprint(f'Earth radius: {R_earth:.1f} km')\nprint(f'Sun gravitational parameter: {GM_sun:.2e} km³/s²')\n\n# Define orbital elements\nprint('\\n=== Orbital Elements ===')\n\n# Example orbit: International Space Station (ISS)\niss_elements = {\n    'semi_major_axis': 6793.0,  # km\n    'eccentricity': 0.0003,\n    'inclination': 51.6,  # degrees\n    'raan': 150.0,  # Right Ascension of Ascending Node (degrees)\n    'arg_periapsis': 90.0,  # Argument of periapsis (degrees)\n    'true_anomaly': 0.0  # degrees\n}\n\nprint('ISS-like orbit:')\nfor element, value in iss_elements.items():\n    print(f'  {element}: {value}')\n\n# Convert angles to radians\niss_elements_rad = iss_elements.copy()\nfor angle in ['inclination', 'raan', 'arg_periapsis', 'true_anomaly']:\n    iss_elements_rad[angle] = np.radians(iss_elements[angle])\n\n# Orbital mechanics functions\ndef orbital_elements_to_cartesian(elements, mu):\n    \"\"\"Convert orbital elements to Cartesian coordinates\"\"\"\n    a = elements['semi_major_axis']\n    e = elements['eccentricity']\n    i = elements['inclination']\n    raan = elements['raan']\n    w = elements['arg_periapsis']\n    nu = elements['true_anomaly']\n    \n    # Semi-latus rectum\n    p = a * (1 - e**2)\n    \n    # Distance\n    r = p / (1 + e * np.cos(nu))\n    \n    # Position in orbital plane\n    x_orb = r * np.cos(nu)\n    y_orb = r * np.sin(nu)\n    z_orb = 0\n    \n    # Velocity in orbital plane\n    h = np.sqrt(mu * p)  # Angular momentum\n    vx_orb = -(mu / h) * np.sin(nu)\n    vy_orb = (mu / h) * (e + np.cos(nu))\n    vz_orb = 0\n    \n    # Rotation matrices\n    cos_raan = np.cos(raan)\n    sin_raan = np.sin(raan)\n    cos_i = np.cos(i)\n    sin_i = np.sin(i)\n    cos_w = np.cos(w)\n    sin_w = np.sin(w)\n    \n    # Rotation matrix from orbital plane to inertial frame\n    R11 = cos_raan * cos_w - sin_raan * sin_w * cos_i\n    R12 = -cos_raan * sin_w - sin_raan * cos_w * cos_i\n    R13 = sin_raan * sin_i\n    \n    R21 = sin_raan * cos_w + cos_raan * sin_w * cos_i\n    R22 = -sin_raan * sin_w + cos_raan * cos_w * cos_i\n    R23 = -cos_raan * sin_i\n    \n    R31 = sin_w * sin_i\n    R32 = cos_w * sin_i\n    R33 = cos_i\n    \n    # Transform to inertial frame\n    x = R11 * x_orb + R12 * y_orb + R13 * z_orb\n    y = R21 * x_orb + R22 * y_orb + R23 * z_orb\n    z = R31 * x_orb + R32 * y_orb + R33 * z_orb\n    \n    vx = R11 * vx_orb + R12 * vy_orb + R13 * vz_orb\n    vy = R21 * vx_orb + R22 * vy_orb + R23 * vz_orb\n    vz = R31 * vx_orb + R32 * vy_orb + R33 * vz_orb\n    \n    return np.array([x, y, z]), np.array([vx, vy, vz])\n\ndef propagate_orbit(r0, v0, mu, dt):\n    \"\"\"Propagate orbit using universal variables (simplified)\"\"\"\n    # This is a simplified version - real implementation would be more complex\n    r0_mag = np.linalg.norm(r0)\n    v0_mag = np.linalg.norm(v0)\n    \n    # Specific energy\n    energy = v0_mag**2 / 2 - mu / r0_mag\n    \n    # Semi-major axis\n    if energy < 0:  # Elliptical orbit\n        a = -mu / (2 * energy)\n        \n        # Mean motion\n        n = np.sqrt(mu / a**3)\n        \n        # Simple Keplerian propagation (circular approximation)\n        theta = n * dt\n        \n        # Rotation matrix for circular orbit approximation\n        cos_theta = np.cos(theta)\n        sin_theta = np.sin(theta)\n        \n        # Simplified rotation (assumes orbit in xy-plane)\n        r_new = np.array([\n            cos_theta * r0[0] + sin_theta * r0[1],\n            -sin_theta * r0[0] + cos_theta * r0[1],\n            r0[2]\n        ])\n        \n        v_new = np.array([\n            cos_theta * v0[0] + sin_theta * v0[1],\n            -sin_theta * v0[0] + cos_theta * v0[1],\n            v0[2]\n        ])\n    else:\n        # Hyperbolic or parabolic - just return original for simplicity\n        r_new = r0\n        v_new = v0\n    \n    return r_new, v_new\n\ndef calculate_orbital_period(a, mu):\n    \"\"\"Calculate orbital period using Kepler's third law\"\"\"\n    return 2 * np.pi * np.sqrt(a**3 / mu)\n\ndef hohmann_transfer(r1, r2, mu):\n    \"\"\"Calculate Hohmann transfer orbit\"\"\"\n    # Semi-major axis of transfer orbit\n    a_transfer = (r1 + r2) / 2\n    \n    # Velocities\n    v1_circular = np.sqrt(mu / r1)\n    v2_circular = np.sqrt(mu / r2)\n    \n    # Transfer orbit velocities\n    v1_transfer = np.sqrt(mu * (2/r1 - 1/a_transfer))\n    v2_transfer = np.sqrt(mu * (2/r2 - 1/a_transfer))\n    \n    # Delta-v requirements\n    dv1 = v1_transfer - v1_circular\n    dv2 = v2_circular - v2_transfer\n    \n    # Transfer time\n    t_transfer = np.pi * np.sqrt(a_transfer**3 / mu)\n    \n    return {\n        'delta_v1': dv1,\n        'delta_v2': dv2,\n        'total_delta_v': abs(dv1) + abs(dv2),\n        'transfer_time': t_transfer,\n        'a_transfer': a_transfer\n    }\n\n# Orbital calculations\nprint('\\n=== Orbital Calculations ===')\n\n# Convert orbital elements to Cartesian\nr0, v0 = orbital_elements_to_cartesian(iss_elements_rad, GM_earth)\nprint(f'Initial position: [{r0[0]:.1f}, {r0[1]:.1f}, {r0[2]:.1f}] km')\nprint(f'Initial velocity: [{v0[0]:.3f}, {v0[1]:.3f}, {v0[2]:.3f}] km/s')\n\n# Calculate orbital properties\na = iss_elements['semi_major_axis']\ne = iss_elements['eccentricity']\n\nperiod = calculate_orbital_period(a, GM_earth)\nperiapsis = a * (1 - e) - R_earth\napoapsis = a * (1 + e) - R_earth\n\nprint(f'\\nOrbital properties:')\nprint(f'  Period: {period/3600:.2f} hours')\nprint(f'  Periapsis altitude: {periapsis:.1f} km')\nprint(f'  Apoapsis altitude: {apoapsis:.1f} km')\nprint(f'  Orbital velocity: {np.linalg.norm(v0):.3f} km/s')\n\n# Orbit propagation\nprint('\\n=== Orbit Propagation ===')\n\n# Propagate orbit for one complete period\nn_points = 100\ntime_steps = np.linspace(0, period, n_points)\n\norbit_positions = []\norbit_velocities = []\n\nfor dt in time_steps:\n    r, v = propagate_orbit(r0, v0, GM_earth, dt)\n    orbit_positions.append(r)\n    orbit_velocities.append(v)\n\norbit_positions = np.array(orbit_positions)\norbit_velocities = np.array(orbit_velocities)\n\nprint(f'Propagated orbit for {period/3600:.2f} hours')\nprint(f'Generated {n_points} orbital positions')\n\n# Calculate ground track (simplified)\nprint('\\n=== Ground Track Calculation ===')\n\n# Earth rotation rate (rad/s)\nearth_rotation_rate = 7.2921159e-5\n\n# Calculate longitude drift due to Earth rotation\nlongitudes = []\nlatitudes = []\n\nfor i, (r, t) in enumerate(zip(orbit_positions, time_steps)):\n    # Convert to latitude/longitude (simplified)\n    x, y, z = r\n    \n    # Latitude\n    lat = np.degrees(np.arcsin(z / np.linalg.norm(r)))\n    \n    # Longitude (accounting for Earth rotation)\n    lon = np.degrees(np.arctan2(y, x)) - np.degrees(earth_rotation_rate * t)\n    \n    # Normalize longitude to [-180, 180]\n    while lon > 180:\n        lon -= 360\n    while lon < -180:\n        lon += 360\n    \n    latitudes.append(lat)\n    longitudes.append(lon)\n\nprint(f'Ground track covers latitudes: {min(latitudes):.1f}° to {max(latitudes):.1f}°')\nprint(f'Longitude range: {min(longitudes):.1f}° to {max(longitudes):.1f}°')\n\n# Mission planning: Hohmann transfer\nprint('\\n=== Mission Planning: Hohmann Transfer ===')\n\n# Transfer from ISS orbit to geostationary orbit\nr_iss = a  # ISS semi-major axis\nr_geo = 42164  # Geostationary orbit radius (km)\n\ntransfer = hohmann_transfer(r_iss, r_geo, GM_earth)\n\nprint(f'Hohmann transfer from ISS to GEO:')\nprint(f'  First burn (ΔV₁): {transfer[\"delta_v1\"]:.3f} km/s')\nprint(f'  Second burn (ΔV₂): {transfer[\"delta_v2\"]:.3f} km/s')\nprint(f'  Total ΔV: {transfer[\"total_delta_v\"]:.3f} km/s')\nprint(f'  Transfer time: {transfer[\"transfer_time\"]/3600:.2f} hours')\n\n# Interplanetary mission example\nprint('\\n=== Interplanetary Mission: Earth to Mars ===')\n\n# Simplified Earth-Mars transfer\n# Average orbital radii (AU converted to km)\nAU = 149597870.7  # km\nr_earth_orbit = 1.0 * AU\nr_mars_orbit = 1.52 * AU\n\n# Hohmann transfer to Mars\nmars_transfer = hohmann_transfer(r_earth_orbit, r_mars_orbit, GM_sun)\n\nprint(f'Earth to Mars Hohmann transfer:')\nprint(f'  ΔV at Earth departure: {mars_transfer[\"delta_v1\"]:.2f} km/s')\nprint(f'  ΔV at Mars arrival: {mars_transfer[\"delta_v2\"]:.2f} km/s')\nprint(f'  Total ΔV: {mars_transfer[\"total_delta_v\"]:.2f} km/s')\nprint(f'  Transfer time: {mars_transfer[\"transfer_time\"]/(24*3600):.0f} days')\n\n# Orbital perturbations analysis\nprint('\\n=== Orbital Perturbations ===')\n\n# J2 perturbation (Earth oblateness)\nJ2 = 1.08262668e-3  # Earth's J2 coefficient\n\ndef j2_perturbation_rates(a, e, i):\n    \"\"\"Calculate secular rates due to J2 perturbation\"\"\"\n    n = np.sqrt(GM_earth / a**3)  # Mean motion\n    \n    # Rate of change of RAAN (rad/s)\n    raan_dot = -1.5 * n * J2 * (R_earth/a)**2 * np.cos(i) / (1 - e**2)**2\n    \n    # Rate of change of argument of periapsis (rad/s)\n    w_dot = 0.75 * n * J2 * (R_earth/a)**2 * (5*np.cos(i)**2 - 1) / (1 - e**2)**2\n    \n    return raan_dot, w_dot\n\ni_rad = np.radians(iss_elements['inclination'])\nraan_dot, w_dot = j2_perturbation_rates(a, e, i_rad)\n\nprint(f'J2 perturbation effects (ISS orbit):')\nprint(f'  RAAN precession: {np.degrees(raan_dot)*86400:.3f} deg/day')\nprint(f'  Periapsis rotation: {np.degrees(w_dot)*86400:.3f} deg/day')\n\n# Solar radiation pressure (simplified)\narea_to_mass = 0.01  # m²/kg (typical for satellites)\nsolar_flux = 1361  # W/m² (solar constant)\nspeed_of_light = 299792458  # m/s\n\nsolar_pressure = solar_flux / speed_of_light  # N/m²\naccel_srp = solar_pressure * area_to_mass * 1e-3  # km/s² (very small)\n\nprint(f'\\nSolar radiation pressure:')\nprint(f'  Pressure: {solar_pressure*1e6:.2f} μN/m²')\nprint(f'  Acceleration (A/m=0.01): {accel_srp*1e9:.2f} nm/s²')\n\n# Atmospheric drag (simplified)\naltitude = periapsis  # km\n\n# Simplified atmospheric density model\nif altitude < 200:\n    density = 2.8e-11  # kg/m³\nelif altitude < 300:\n    density = 1.7e-12\nelif altitude < 400:\n    density = 2.8e-13\nelif altitude < 500:\n    density = 7.2e-14\nelse:\n    density = 2.4e-14\n\ncd = 2.2  # Drag coefficient\nvelocity = np.linalg.norm(v0) * 1000  # m/s\naccel_drag = 0.5 * density * cd * area_to_mass * velocity**2 * 1e-3  # km/s²\n\nprint(f'\\nAtmospheric drag:')\nprint(f'  Altitude: {altitude:.1f} km')\nprint(f'  Density: {density:.2e} kg/m³')\nprint(f'  Drag acceleration: {accel_drag*1e9:.2f} nm/s²')\n\n# Visualization\nprint('\\n=== Visualization ===')\n\nfig = plt.figure(figsize=(16, 12))\n\n# 3D orbit visualization\nax1 = fig.add_subplot(221, projection='3d')\n\n# Plot Earth\nu = np.linspace(0, 2 * np.pi, 50)\nv = np.linspace(0, np.pi, 50)\nearth_x = R_earth * np.outer(np.cos(u), np.sin(v))\nearth_y = R_earth * np.outer(np.sin(u), np.sin(v))\nearth_z = R_earth * np.outer(np.ones(np.size(u)), np.cos(v))\nax1.plot_surface(earth_x, earth_y, earth_z, alpha=0.3, color='blue')\n\n# Plot orbit\nax1.plot(orbit_positions[:, 0], orbit_positions[:, 1], orbit_positions[:, 2], \n         'r-', linewidth=2, label='Orbit')\n\n# Plot initial position\nax1.scatter(*r0, color='green', s=100, label='Initial position')\n\nax1.set_xlabel('X (km)')\nax1.set_ylabel('Y (km)')\nax1.set_zlabel('Z (km)')\nax1.set_title('3D Orbital Trajectory')\nax1.legend()\n\n# Set equal aspect ratio\nmax_range = np.max(orbit_positions) * 1.1\nax1.set_xlim(-max_range, max_range)\nax1.set_ylim(-max_range, max_range)\nax1.set_zlim(-max_range, max_range)\n\n# Ground track\nax2 = fig.add_subplot(222)\nax2.plot(longitudes, latitudes, 'b-', linewidth=2, alpha=0.7)\nax2.scatter(longitudes[0], latitudes[0], color='green', s=100, label='Start')\nax2.scatter(longitudes[-1], latitudes[-1], color='red', s=100, label='End')\nax2.set_xlabel('Longitude (degrees)')\nax2.set_ylabel('Latitude (degrees)')\nax2.set_title('Ground Track')\nax2.grid(True, alpha=0.3)\nax2.legend()\n\n# Add world map outline (simplified)\nax2.axhline(y=0, color='k', linestyle='-', alpha=0.3)  # Equator\nax2.axvline(x=0, color='k', linestyle='-', alpha=0.3)  # Prime meridian\nax2.set_xlim(-180, 180)\nax2.set_ylim(-90, 90)\n\n# Orbital elements evolution (with perturbations)\nax3 = fig.add_subplot(223)\n\n# Simulate perturbation effects over time\ndays = np.linspace(0, 30, 100)  # 30 days\nraan_evolution = iss_elements['raan'] + np.degrees(raan_dot) * days * 86400\nw_evolution = iss_elements['arg_periapsis'] + np.degrees(w_dot) * days * 86400\n\nax3.plot(days, raan_evolution, 'b-', label='RAAN', linewidth=2)\nax3.plot(days, w_evolution, 'r-', label='Arg. of Periapsis', linewidth=2)\nax3.set_xlabel('Time (days)')\nax3.set_ylabel('Angle (degrees)')\nax3.set_title('Orbital Element Evolution (J2 Perturbation)')\nax3.legend()\nax3.grid(True, alpha=0.3)\n\n# Transfer orbit comparison\nax4 = fig.add_subplot(224)\n\n# Plot circular orbits\ntheta = np.linspace(0, 2*np.pi, 100)\nr_iss_circle = r_iss * np.ones_like(theta)\nr_geo_circle = r_geo * np.ones_like(theta)\nr_transfer_peri = r_iss * np.ones_like(theta)\nr_transfer_apo = r_geo * np.ones_like(theta)\n\n# Convert to Cartesian for plotting\niss_x = r_iss_circle * np.cos(theta)\niss_y = r_iss_circle * np.sin(theta)\ngeo_x = r_geo_circle * np.cos(theta)\ngeo_y = r_geo_circle * np.sin(theta)\n\n# Hohmann transfer ellipse\na_transfer = transfer['a_transfer']\ne_transfer = (r_geo - r_iss) / (r_geo + r_iss)\ntheta_transfer = np.linspace(0, np.pi, 50)  # Half ellipse\nr_transfer = a_transfer * (1 - e_transfer**2) / (1 + e_transfer * np.cos(theta_transfer))\ntransfer_x = r_transfer * np.cos(theta_transfer)\ntransfer_y = r_transfer * np.sin(theta_transfer)\n\nax4.plot(iss_x, iss_y, 'b-', label='ISS Orbit', linewidth=2)\nax4.plot(geo_x, geo_y, 'g-', label='GEO Orbit', linewidth=2)\nax4.plot(transfer_x, transfer_y, 'r--', label='Hohmann Transfer', linewidth=2)\n\n# Plot Earth\nearth_circle = plt.Circle((0, 0), R_earth, color='blue', alpha=0.3)\nax4.add_patch(earth_circle)\n\nax4.set_xlabel('X (km)')\nax4.set_ylabel('Y (km)')\nax4.set_title('Hohmann Transfer to GEO')\nax4.legend()\nax4.grid(True, alpha=0.3)\nax4.set_aspect('equal')\nax4.set_xlim(-50000, 50000)\nax4.set_ylim(-50000, 50000)\n\nplt.tight_layout()\n\n# Save visualization\nwith tempfile.NamedTemporaryFile(suffix='.png', delete=False) as tmp:\n    plt.savefig(tmp.name, dpi=150, bbox_inches='tight')\n    viz_file = tmp.name\n\nplt.close()\nprint(f'Astrodynamics visualization saved to: {viz_file}')\n\n# Summary report\nprint('\\n' + '=' * 38)\nprint('POLIASTRO ASTRODYNAMICS SUMMARY')\nprint('=' * 38)\nprint(f'Analyzed orbit: {iss_elements[\"semi_major_axis\"]:.1f} km semi-major axis')\nprint(f'Orbital period: {period/3600:.2f} hours')\nprint(f'Inclination: {iss_elements[\"inclination\"]:.1f}°')\nprint(f'\\nPerturbation effects (per day):')\nprint(f'  RAAN precession: {np.degrees(raan_dot)*86400:.3f}°')\nprint(f'  Periapsis rotation: {np.degrees(w_dot)*86400:.3f}°')\nprint(f'\\nHohmann transfer to GEO:')\nprint(f'  Total ΔV required: {transfer[\"total_delta_v\"]:.3f} km/s')\nprint(f'  Transfer time: {transfer[\"transfer_time\"]/3600:.2f} hours')\nprint(f'\\nInterplanetary mission (Earth-Mars):')\nprint(f'  Transfer time: {mars_transfer[\"transfer_time\"]/(24*3600):.0f} days')\nprint(f'  Total ΔV: {mars_transfer[\"total_delta_v\"]:.2f} km/s')\n\n# Cleanup\nos.unlink(viz_file)\nprint('\\nDemo complete - temporary files cleaned up')\n\nprint('\\npoliastro provides:')\nprint('• Pure Python astrodynamics library')\nprint('• Orbit propagation and determination')\nprint('• Maneuver planning and optimization')\nprint('• Coordinate system transformations')\nprint('• Mission analysis tools')\nprint('• Integration with scientific Python ecosystem')\nprint('• Interactive plotting and visualization')\n\nprint('\\nTypical poliastro usage:')\nprint('from poliastro.bodies import Earth')\nprint('from poliastro.twobody import Orbit')\nprint('orbit = Orbit.from_classical(Earth, a, ecc, inc, raan, argp, nu)')\nprint('orbit.plot()')",
      "quick_start": [
        "Install: pip install poliastro",
        "Import: from poliastro.bodies import Earth",
        "Create: from poliastro.twobody import Orbit",
        "Define: orbit = Orbit.from_classical(...)",
        "Plot: orbit.plot()",
        "Analyze orbital mechanics and mission planning"
      ]
    }
  },
  {
    "type": "PackageTool",
    "name": "get_scholarly_info",
    "description": "Get comprehensive information about scholarly – Google Scholar data retrieval",
    "parameter": {
      "type": "object",
      "properties": {
        "info_type": {
          "type": "string",
          "enum": [
            "overview",
            "installation",
            "usage",
            "documentation"
          ],
          "description": "Type of information to retrieve about scholarly"
        }
      },
      "required": [
        "info_type"
      ]
    },
    "package_name": "scholarly",
    "local_info": {
      "name": "scholarly",
      "description": "Python library for retrieving scholarly publications from Google Scholar. Provides programmatic access to publication data, author profiles, and citation information for bibliometric analysis.",
      "category": "Academic Research / Bibliometrics",
      "import_name": "scholarly",
      "popularity": 75,
      "keywords": [
        "Google Scholar",
        "publications",
        "citations",
        "bibliometrics",
        "academic research"
      ],
      "documentation": "https://scholarly.readthedocs.io/",
      "repository": "https://github.com/scholarly-python-package/scholarly",
      "installation": {
        "pip": "pip install scholarly",
        "conda": "conda install -c conda-forge scholarly"
      },
      "usage_example": "from scholarly import scholarly\nimport time\n\n# Search for publications\nprint('Searching for publications on \"machine learning\"...')\n\ntry:\n    # Search for papers\n    search_query = scholarly.search_pubs('machine learning')\n    \n    # Get first 3 results\n    papers = []\n    for i in range(3):\n        try:\n            paper = next(search_query)\n            papers.append(paper)\n            time.sleep(1)  # Be respectful\n        except StopIteration:\n            break\n    \n    print(f'Found {len(papers)} papers:')\n    for i, paper in enumerate(papers, 1):\n        print(f'\\n{i}. {paper.get(\"title\", \"Unknown title\")}')\n        print(f'   Authors: {paper.get(\"author\", \"Unknown authors\")}')\n        print(f'   Year: {paper.get(\"year\", \"Unknown year\")}')\n        print(f'   Citations: {paper.get(\"num_citations\", 0)}')\n        if 'pub_url' in paper:\n            print(f'   URL: {paper[\"pub_url\"]}')\n            \nexcept Exception as e:\n    print(f'Publication search failed: {e}')\n\n# Search for author\nprint('\\n' + '='*50)\nprint('Searching for author profile...')\n\ntry:\n    # Search for an author (example)\n    search_query = scholarly.search_author('Andrew Ng')\n    author = next(search_query)\n    \n    # Fill in author details\n    author_filled = scholarly.fill(author)\n    \n    print(f'Author: {author_filled.get(\"name\", \"Unknown\")}')\n    print(f'Affiliation: {author_filled.get(\"affiliation\", \"Unknown\")}')\n    print(f'Total citations: {author_filled.get(\"citedby\", 0)}')\n    print(f'h-index: {author_filled.get(\"hindex\", 0)}')\n    print(f'i10-index: {author_filled.get(\"i10index\", 0)}')\n    \n    # List recent publications\n    publications = author_filled.get('publications', [])\n    print(f'\\nRecent publications ({min(3, len(publications))}):')\n    for i, pub in enumerate(publications[:3], 1):\n        print(f'{i}. {pub.get(\"title\", \"Unknown title\")}')\n        print(f'   Citations: {pub.get(\"num_citations\", 0)}')\n        \nexcept Exception as e:\n    print(f'Author search failed: {e}')\n\nprint('\\nNote: Be mindful of rate limits when using Google Scholar')",
      "quick_start": [
        "Install: pip install scholarly",
        "Search papers: scholarly.search_pubs('query')",
        "Search authors: scholarly.search_author('name')",
        "Get details: scholarly.fill(result)",
        "Access citations: result['num_citations']",
        "Use delays to avoid rate limits"
      ]
    }
  },
  {
    "type": "PackageTool",
    "name": "get_flowio_info",
    "description": "Get comprehensive information about FlowIO – FCS file I/O for flow cytometry",
    "parameter": {
      "type": "object",
      "properties": {
        "info_type": {
          "type": "string",
          "enum": [
            "overview",
            "installation",
            "usage",
            "documentation"
          ],
          "description": "Type of information to retrieve about FlowIO"
        }
      },
      "required": [
        "info_type"
      ]
    },
    "package_name": "FlowIO",
    "local_info": {
      "name": "FlowIO",
      "description": "Python library for reading and writing Flow Cytometry Standard (FCS) files. Provides efficient parsing of FCS 2.0, 3.0, and 3.1 formats with full metadata access and event data extraction.",
      "category": "Flow Cytometry I/O",
      "import_name": "flowio",
      "popularity": 60,
      "keywords": [
        "FCS files",
        "flow cytometry",
        "file format",
        "data import",
        "metadata"
      ],
      "documentation": "https://github.com/whitews/FlowIO",
      "repository": "https://github.com/whitews/FlowIO",
      "installation": {
        "pip": "pip install FlowIO",
        "conda": "conda install -c conda-forge flowio"
      },
      "usage_example": "import flowio\nimport numpy as np\nimport pandas as pd\n\n# Read FCS file\nfcs_file = flowio.FlowData('sample.fcs')\n\n# Get basic information\nprint(f'FCS version: {fcs_file.header[\"FCS\"]}')\nprint(f'Event count: {fcs_file.event_count}')\nprint(f'Parameter count: {fcs_file.channel_count}')\n\n# Access metadata\nprint('\\nChannel information:')\nfor i, channel in enumerate(fcs_file.channels):\n    print(f'Channel {i+1}: {channel[\"PnN\"]} ({channel[\"PnS\"]})')\n\n# Get event data as numpy array\nevents = np.reshape(fcs_file.events, (-1, fcs_file.channel_count))\nprint(f'Events shape: {events.shape}')\n\n# Convert to pandas DataFrame\nchannel_names = [ch['PnN'] for ch in fcs_file.channels]\ndf = pd.DataFrame(events, columns=channel_names)\nprint('\\nFirst 5 events:')\nprint(df.head())\n\n# Access specific parameters\nprint(f'\\nAcquisition date: {fcs_file.header.get(\"$DATE\", \"Not available\")}')\nprint(f'Instrument: {fcs_file.header.get(\"$CYT\", \"Not available\")}')\n\n# Check for compensation matrix\nif '$SPILLOVER' in fcs_file.header:\n    print('Compensation matrix available')\nelse:\n    print('No compensation matrix found')",
      "quick_start": [
        "Install: pip install FlowIO",
        "Read FCS: fcs = flowio.FlowData('file.fcs')",
        "Get events: events = fcs.events",
        "Get metadata: header = fcs.header",
        "Channel info: fcs.channels",
        "Convert to pandas: pd.DataFrame(events)"
      ]
    }
  },
  {
    "type": "PackageTool",
    "name": "get_ruptures_info",
    "description": "Get comprehensive information about ruptures – change point detection library",
    "parameter": {
      "type": "object",
      "properties": {
        "info_type": {
          "type": "string",
          "enum": [
            "overview",
            "installation",
            "usage",
            "documentation"
          ],
          "description": "Type of information to retrieve about ruptures"
        }
      },
      "required": [
        "info_type"
      ]
    },
    "package_name": "ruptures",
    "local_info": {
      "name": "ruptures",
      "description": "Change point detection library. Provides algorithms for detecting abrupt changes in time series data, with applications in signal processing, genomics, finance, and scientific data analysis.",
      "category": "Scientific Computing / Signal Processing",
      "import_name": "ruptures",
      "popularity": 75,
      "keywords": [
        "change point detection",
        "time series",
        "signal processing",
        "segmentation",
        "anomaly detection"
      ],
      "documentation": "https://centre-borelli.github.io/ruptures-docs/",
      "repository": "https://github.com/deepcharles/ruptures",
      "installation": {
        "pip": "pip install ruptures",
        "conda": "conda install -c conda-forge ruptures"
      },
      "usage_example": "# ruptures change point detection demonstration\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy import signal\nfrom sklearn.preprocessing import StandardScaler\nimport tempfile\nimport os\n\n# Simulate ruptures functionality\ndef pelt_algorithm(data, penalty=10):\n    \"\"\"Simplified PELT algorithm for change point detection\"\"\"\n    n = len(data)\n    F = np.full(n + 1, np.inf)\n    F[0] = -penalty\n    cp_candidates = [0]\n    \n    for t in range(1, n + 1):\n        for s in cp_candidates:\n            if s < t:\n                segment_data = data[s:t]\n                if len(segment_data) > 0:\n                    cost = np.var(segment_data) * len(segment_data)\n                    total_cost = F[s] + cost + penalty\n                    \n                    if total_cost < F[t]:\n                        F[t] = total_cost\n        \n        # Pruning step\n        cp_candidates = [s for s in cp_candidates if F[s] <= F[t] - penalty]\n        cp_candidates.append(t)\n    \n    # Backtrack to find change points\n    change_points = []\n    t = n\n    while t > 0:\n        for s in range(t):\n            if s in cp_candidates:\n                segment_data = data[s:t]\n                if len(segment_data) > 0:\n                    cost = np.var(segment_data) * len(segment_data)\n                    if abs(F[t] - (F[s] + cost + penalty)) < 1e-10:\n                        if s > 0:\n                            change_points.append(s)\n                        t = s\n                        break\n        else:\n            break\n    \n    return sorted(change_points)\n\ndef binary_segmentation(data, max_changepoints=10):\n    \"\"\"Simplified binary segmentation algorithm\"\"\"\n    def find_best_split(segment_data, start_idx):\n        n = len(segment_data)\n        if n < 4:  # Minimum segment size\n            return None, -np.inf\n        \n        best_score = -np.inf\n        best_split = None\n        \n        for split in range(2, n - 1):\n            left = segment_data[:split]\n            right = segment_data[split:]\n            \n            # Calculate score based on variance reduction\n            total_var = np.var(segment_data) * n\n            left_var = np.var(left) * len(left)\n            right_var = np.var(right) * len(right)\n            \n            score = total_var - (left_var + right_var)\n            \n            if score > best_score:\n                best_score = score\n                best_split = start_idx + split\n        \n        return best_split, best_score\n    \n    change_points = []\n    segments = [(data, 0)]  # (segment_data, start_index)\n    \n    for _ in range(max_changepoints):\n        if not segments:\n            break\n        \n        best_segment = None\n        best_split = None\n        best_score = -np.inf\n        \n        # Find the best split among all segments\n        for i, (segment_data, start_idx) in enumerate(segments):\n            split, score = find_best_split(segment_data, start_idx)\n            if split is not None and score > best_score:\n                best_score = score\n                best_split = split\n                best_segment = i\n        \n        if best_split is None or best_score <= 0:\n            break\n        \n        # Apply the best split\n        segment_data, start_idx = segments.pop(best_segment)\n        split_point = best_split - start_idx\n        \n        left_segment = segment_data[:split_point]\n        right_segment = segment_data[split_point:]\n        \n        if len(left_segment) > 0:\n            segments.append((left_segment, start_idx))\n        if len(right_segment) > 0:\n            segments.append((right_segment, best_split))\n        \n        change_points.append(best_split)\n    \n    return sorted(change_points)\n\nprint('ruptures - Change Point Detection Library')\nprint('=' * 45)\n\nprint('ruptures Features:')\nprint('• Multiple change point detection algorithms')\nprint('• PELT, Binary Segmentation, Window-based methods')\nprint('• Support for various cost functions')\nprint('• Multivariate time series analysis')\nprint('• Model selection and validation')\nprint('• Efficient implementations')\n\nprint('\\nApplications:')\nprint('• Signal processing and anomaly detection')\nprint('• Financial time series analysis')\nprint('• Genomic segmentation')\nprint('• Climate data analysis')\nprint('• Quality control in manufacturing')\n\n# Generate synthetic time series with change points\nprint('\\n=== Synthetic Time Series Generation ===')\n\nnp.random.seed(42)\n\n# Time series parameters\ntotal_length = 1000\ntrue_change_points = [200, 400, 650, 800]\nsegment_means = [1.0, 3.0, 0.5, 2.5, 1.8]\nsegment_stds = [0.5, 0.8, 0.3, 0.6, 0.4]\n\nprint(f'Generating time series with {len(true_change_points)} change points')\nprint(f'True change points: {true_change_points}')\nprint(f'Total length: {total_length} points')\n\n# Generate segments\ntime_series = []\ncurrent_pos = 0\n\nfor i, cp in enumerate(true_change_points + [total_length]):\n    segment_length = cp - current_pos\n    segment = np.random.normal(\n        segment_means[i], \n        segment_stds[i], \n        segment_length\n    )\n    time_series.extend(segment)\n    current_pos = cp\n\ntime_series = np.array(time_series)\ntime_points = np.arange(len(time_series))\n\nprint(f'Generated time series shape: {time_series.shape}')\nprint(f'Value range: {time_series.min():.2f} to {time_series.max():.2f}')\n\n# Add some noise and trends\nprint('\\nAdding noise and trends...')\n\n# Add noise\nnoise_level = 0.1\nnoise = np.random.normal(0, noise_level, len(time_series))\ntime_series_noisy = time_series + noise\n\n# Add slight trend\ntrend = 0.0005 * time_points\ntime_series_with_trend = time_series_noisy + trend\n\nprint(f'Noise level: {noise_level}')\nprint(f'Trend coefficient: 0.0005 per time unit')\n\n# Apply change point detection algorithms\nprint('\\n=== Change Point Detection ===')\n\n# Test different algorithms\nalgorithms = {\n    'PELT (penalty=5)': lambda x: pelt_algorithm(x, penalty=5),\n    'PELT (penalty=10)': lambda x: pelt_algorithm(x, penalty=10),\n    'PELT (penalty=20)': lambda x: pelt_algorithm(x, penalty=20),\n    'Binary Segmentation': lambda x: binary_segmentation(x, max_changepoints=8)\n}\n\nresults = {}\n\nfor algo_name, algo_func in algorithms.items():\n    print(f'\\nRunning {algo_name}...')\n    \n    detected_cps = algo_func(time_series_with_trend)\n    \n    # Calculate performance metrics\n    def calculate_metrics(true_cps, detected_cps, tolerance=50):\n        \"\"\"Calculate precision, recall, and F1 score\"\"\"\n        true_positives = 0\n        \n        for true_cp in true_cps:\n            if any(abs(det_cp - true_cp) <= tolerance for det_cp in detected_cps):\n                true_positives += 1\n        \n        precision = true_positives / len(detected_cps) if detected_cps else 0\n        recall = true_positives / len(true_cps) if true_cps else 0\n        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n        \n        return precision, recall, f1\n    \n    precision, recall, f1 = calculate_metrics(true_change_points, detected_cps)\n    \n    results[algo_name] = {\n        'detected_cps': detected_cps,\n        'precision': precision,\n        'recall': recall,\n        'f1': f1\n    }\n    \n    print(f'  Detected change points: {detected_cps}')\n    print(f'  Precision: {precision:.3f}')\n    print(f'  Recall: {recall:.3f}')\n    print(f'  F1 Score: {f1:.3f}')\n\n# Compare algorithms\nprint('\\n=== Algorithm Comparison ===')\n\nperformance_df = pd.DataFrame({\n    'Algorithm': list(results.keys()),\n    'Precision': [results[algo]['precision'] for algo in results],\n    'Recall': [results[algo]['recall'] for algo in results],\n    'F1 Score': [results[algo]['f1'] for algo in results],\n    'Num Detected': [len(results[algo]['detected_cps']) for algo in results]\n})\n\nprint(performance_df.round(3))\n\n# Best algorithm\nbest_algo = performance_df.loc[performance_df['F1 Score'].idxmax(), 'Algorithm']\nprint(f'\\nBest performing algorithm: {best_algo}')\nprint(f'F1 Score: {performance_df.loc[performance_df[\"F1 Score\"].idxmax(), \"F1 Score\"]:.3f}')\n\n# Multivariate change point detection simulation\nprint('\\n=== Multivariate Change Point Detection ===')\n\n# Generate multivariate time series\nn_dims = 3\nmv_length = 500\nmv_change_points = [150, 300, 400]\n\nprint(f'Generating {n_dims}D time series with change points at {mv_change_points}')\n\nmv_time_series = []\ncurrent_pos = 0\n\n# Different correlation structures for each segment\ncorr_matrices = [\n    np.array([[1.0, 0.2, 0.1], [0.2, 1.0, 0.3], [0.1, 0.3, 1.0]]),  # Low correlation\n    np.array([[1.0, 0.8, 0.6], [0.8, 1.0, 0.7], [0.6, 0.7, 1.0]]),  # High correlation\n    np.array([[1.0, -0.5, 0.2], [-0.5, 1.0, -0.3], [0.2, -0.3, 1.0]]),  # Mixed correlation\n    np.array([[1.0, 0.1, 0.9], [0.1, 1.0, 0.2], [0.9, 0.2, 1.0]])   # Selective correlation\n]\n\nfor i, cp in enumerate(mv_change_points + [mv_length]):\n    segment_length = cp - current_pos\n    \n    # Generate correlated multivariate normal data\n    mean = np.random.normal(0, 2, n_dims)\n    cov = corr_matrices[i]\n    \n    segment = np.random.multivariate_normal(mean, cov, segment_length)\n    mv_time_series.append(segment)\n    \n    current_pos = cp\n\nmv_time_series = np.vstack(mv_time_series)\nprint(f'Multivariate time series shape: {mv_time_series.shape}')\n\n# Detect change points in each dimension\nprint('\\nDetecting change points in each dimension:')\nmv_results = {}\n\nfor dim in range(n_dims):\n    dim_data = mv_time_series[:, dim]\n    detected_cps = binary_segmentation(dim_data, max_changepoints=5)\n    \n    precision, recall, f1 = calculate_metrics(mv_change_points, detected_cps, tolerance=25)\n    \n    mv_results[f'Dimension {dim}'] = {\n        'detected_cps': detected_cps,\n        'precision': precision,\n        'recall': recall,\n        'f1': f1\n    }\n    \n    print(f'  Dim {dim}: CPs = {detected_cps}, F1 = {f1:.3f}')\n\n# Aggregate multivariate detection (simple approach)\nprint('\\nAggregate multivariate detection:')\n\n# Sum of squared differences approach\nsum_sq_diff = np.sum(np.diff(mv_time_series, axis=0)**2, axis=1)\ndetected_cps_mv = binary_segmentation(sum_sq_diff, max_changepoints=5)\n\nprecision_mv, recall_mv, f1_mv = calculate_metrics(mv_change_points, detected_cps_mv, tolerance=25)\nprint(f'  Aggregate CPs: {detected_cps_mv}')\nprint(f'  Precision: {precision_mv:.3f}, Recall: {recall_mv:.3f}, F1: {f1_mv:.3f}')\n\n# Model selection simulation\nprint('\\n=== Model Selection ===')\n\n# Test different penalty values for PELT\npenalty_values = [1, 2, 5, 10, 15, 20, 30, 50]\nmodel_selection_results = []\n\nfor penalty in penalty_values:\n    detected_cps = pelt_algorithm(time_series_with_trend, penalty=penalty)\n    \n    # Calculate BIC-like criterion\n    n_segments = len(detected_cps) + 1\n    n_params = n_segments * 2  # mean and variance for each segment\n    \n    # Calculate likelihood (simplified)\n    log_likelihood = 0\n    current_pos = 0\n    \n    for cp in detected_cps + [len(time_series_with_trend)]:\n        segment_data = time_series_with_trend[current_pos:cp]\n        if len(segment_data) > 0:\n            segment_var = np.var(segment_data)\n            if segment_var > 0:\n                log_likelihood -= 0.5 * len(segment_data) * np.log(2 * np.pi * segment_var)\n                log_likelihood -= 0.5 * len(segment_data)\n        current_pos = cp\n    \n    bic = -2 * log_likelihood + n_params * np.log(len(time_series_with_trend))\n    \n    precision, recall, f1 = calculate_metrics(true_change_points, detected_cps)\n    \n    model_selection_results.append({\n        'penalty': penalty,\n        'n_changepoints': len(detected_cps),\n        'bic': bic,\n        'precision': precision,\n        'recall': recall,\n        'f1': f1\n    })\n\nmodel_df = pd.DataFrame(model_selection_results)\n\nprint('Model selection results:')\nprint(model_df.round(3))\n\n# Best model by BIC\nbest_bic_idx = model_df['bic'].idxmin()\nbest_penalty = model_df.loc[best_bic_idx, 'penalty']\nprint(f'\\nBest penalty by BIC: {best_penalty}')\nprint(f'Corresponding F1 score: {model_df.loc[best_bic_idx, \"f1\"]:.3f}')\n\n# Visualization\nprint('\\n=== Visualization ===')\n\nfig, axes = plt.subplots(2, 2, figsize=(15, 10))\n\n# 1. Original time series with change points\nax1 = axes[0, 0]\nax1.plot(time_points, time_series_with_trend, 'b-', alpha=0.7, linewidth=1)\n\n# True change points\nfor cp in true_change_points:\n    ax1.axvline(x=cp, color='red', linestyle='--', alpha=0.8, label='True CP' if cp == true_change_points[0] else '')\n\n# Best detected change points\nbest_detected = results[best_algo]['detected_cps']\nfor cp in best_detected:\n    ax1.axvline(x=cp, color='green', linestyle=':', alpha=0.8, label='Detected CP' if cp == best_detected[0] else '')\n\nax1.set_xlabel('Time')\nax1.set_ylabel('Value')\nax1.set_title('Time Series with Change Points')\nax1.legend()\nax1.grid(True, alpha=0.3)\n\n# 2. Algorithm performance comparison\nax2 = axes[0, 1]\nmetrics = ['Precision', 'Recall', 'F1 Score']\nbar_width = 0.2\nx_pos = np.arange(len(metrics))\n\nfor i, algo in enumerate(results.keys()):\n    values = [results[algo]['precision'], results[algo]['recall'], results[algo]['f1']]\n    ax2.bar(x_pos + i*bar_width, values, bar_width, label=algo, alpha=0.8)\n\nax2.set_xlabel('Metrics')\nax2.set_ylabel('Score')\nax2.set_title('Algorithm Performance Comparison')\nax2.set_xticks(x_pos + bar_width * 1.5)\nax2.set_xticklabels(metrics)\nax2.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\nax2.grid(True, alpha=0.3)\nax2.set_ylim(0, 1.1)\n\n# 3. Multivariate time series\nax3 = axes[1, 0]\nfor dim in range(min(n_dims, 3)):\n    ax3.plot(mv_time_series[:, dim], label=f'Dimension {dim}', alpha=0.7)\n\nfor cp in mv_change_points:\n    ax3.axvline(x=cp, color='red', linestyle='--', alpha=0.6)\n\nax3.set_xlabel('Time')\nax3.set_ylabel('Value')\nax3.set_title('Multivariate Time Series')\nax3.legend()\nax3.grid(True, alpha=0.3)\n\n# 4. Model selection (BIC vs penalty)\nax4 = axes[1, 1]\nax4.plot(model_df['penalty'], model_df['bic'], 'bo-', label='BIC')\nax4.axvline(x=best_penalty, color='red', linestyle='--', alpha=0.8, label=f'Best penalty ({best_penalty})')\n\n# Secondary y-axis for F1 score\nax4_twin = ax4.twinx()\nax4_twin.plot(model_df['penalty'], model_df['f1'], 'ro-', alpha=0.7, label='F1 Score')\n\nax4.set_xlabel('Penalty Value')\nax4.set_ylabel('BIC', color='blue')\nax4_twin.set_ylabel('F1 Score', color='red')\nax4.set_title('Model Selection: BIC vs Penalty')\nax4.legend(loc='upper left')\nax4_twin.legend(loc='upper right')\nax4.grid(True, alpha=0.3)\n\nplt.tight_layout()\n\n# Save visualization\nwith tempfile.NamedTemporaryFile(suffix='.png', delete=False) as tmp:\n    plt.savefig(tmp.name, dpi=150, bbox_inches='tight')\n    viz_file = tmp.name\n\nplt.close()\nprint(f'Change point detection visualization saved to: {viz_file}')\n\n# Summary report\nprint('\\n' + '=' * 45)\nprint('RUPTURES CHANGE POINT DETECTION SUMMARY')\nprint('=' * 45)\nprint(f'Time series length: {len(time_series_with_trend):} points')\nprint(f'True change points: {len(true_change_points)}')\nprint(f'Best algorithm: {best_algo}')\nprint(f'Best F1 score: {max(results[algo][\"f1\"] for algo in results):.3f}')\nprint(f'\\nAlgorithm rankings by F1 score:')\nfor i, (algo, metrics) in enumerate(sorted(results.items(), key=lambda x: x[1]['f1'], reverse=True), 1):\n    print(f'  {i}. {algo}: {metrics[\"f1\"]:.3f}')\nprint(f'\\nMultivariate detection F1 score: {f1_mv:.3f}')\nprint(f'Optimal penalty (BIC): {best_penalty}')\n\n# Cleanup\nos.unlink(viz_file)\nprint('\\nDemo complete - temporary files cleaned up')\n\nprint('\\nruptures provides:')\nprint('• Multiple change point detection algorithms')\nprint('• PELT, Binary Segmentation, Window methods')\nprint('• Multivariate time series support')\nprint('• Model selection and validation')\nprint('• Custom cost functions')\nprint('• Efficient implementations')\nprint('• Extensive documentation and examples')\n\nprint('\\nTypical ruptures usage:')\nprint('import ruptures as rpt')\nprint('algo = rpt.Pelt(model=\"rbf\").fit(signal)')\nprint('result = algo.predict(pen=10)')",
      "quick_start": [
        "Install: pip install ruptures",
        "Import: import ruptures as rpt",
        "Fit: algo = rpt.Pelt(model='rbf').fit(signal)",
        "Predict: result = algo.predict(pen=10)",
        "Display: rpt.display(signal, result)",
        "Check change points in result list"
      ]
    }
  },
  {
    "type": "PackageTool",
    "name": "get_loompy_info",
    "description": "Get comprehensive information about loompy – efficient storage for large omics datasets",
    "parameter": {
      "type": "object",
      "properties": {
        "info_type": {
          "type": "string",
          "enum": [
            "overview",
            "installation",
            "usage",
            "documentation"
          ],
          "description": "Type of information to retrieve about loompy"
        }
      },
      "required": [
        "info_type"
      ]
    },
    "package_name": "loompy",
    "local_info": {
      "name": "loompy",
      "description": "Python API for the Loom file format, designed for efficient storage and access of large omics datasets. Provides sparse matrix storage with rich metadata for single-cell and spatial genomics data.",
      "category": "Genomics Data Storage",
      "import_name": "loompy",
      "popularity": 70,
      "keywords": [
        "loom format",
        "sparse matrices",
        "single-cell",
        "genomics storage",
        "HDF5"
      ],
      "documentation": "https://loompy.org/",
      "repository": "https://github.com/linnarsson-lab/loompy",
      "installation": {
        "pip": "pip install loompy",
        "conda": "conda install -c conda-forge loompy"
      },
      "usage_example": "import loompy\nimport numpy as np\nimport pandas as pd\n\n# Create sample data\nn_cells = 1000\nn_genes = 2000\nexpression_matrix = np.random.negative_binomial(10, 0.3, (n_genes, n_cells))\n\n# Create metadata\ngene_names = [f'Gene_{i}' for i in range(n_genes)]\ncell_types = np.random.choice(['TypeA', 'TypeB', 'TypeC'], n_cells)\ncell_ids = [f'Cell_{i}' for i in range(n_cells)]\n\n# Row attributes (genes)\nrow_attrs = {\n    'Gene': np.array(gene_names, dtype='S'),\n    'Chromosome': np.random.choice(['chr1', 'chr2', 'chr3'], n_genes)\n}\n\n# Column attributes (cells)\ncol_attrs = {\n    'CellID': np.array(cell_ids, dtype='S'),\n    'CellType': np.array(cell_types, dtype='S'),\n    'nUMI': np.random.poisson(5000, n_cells)\n}\n\n# Create Loom file\nwith loompy.new('dataset.loom') as ds:\n    ds.add_columns(expression_matrix, col_attrs)\n    for key, value in row_attrs.items():\n        ds.ra[key] = value\n    \n    print(f'Created Loom file with {ds.shape[0]} genes and {ds.shape[1]} cells')\n\n# Read from Loom file\nwith loompy.connect('dataset.loom') as ds:\n    print(f'Dataset shape: {ds.shape}')\n    print(f'Gene attributes: {list(ds.ra.keys())}')\n    print(f'Cell attributes: {list(ds.ca.keys())}')\n    \n    # Query specific genes\n    mask = ds.ra.Gene == b'Gene_0'\n    gene_expression = ds[mask, :]\n    print(f'Expression for Gene_0: {gene_expression[0, :5]}')\n    \n    # Query specific cell types\n    type_a_cells = ds.ca.CellType == b'TypeA'\n    type_a_data = ds[:, type_a_cells]\n    print(f'TypeA cells: {type_a_data.shape[1]}')",
      "quick_start": [
        "Install: pip install loompy",
        "Create file: loompy.new('file.loom')",
        "Add data: ds.add_columns(matrix, col_attrs)",
        "Set attributes: ds.ra['gene_name'] = names",
        "Read file: loompy.connect('file.loom')",
        "Query data: ds[gene_mask, cell_mask]"
      ]
    }
  },
  {
    "type": "PackageTool",
    "name": "get_requests_info",
    "description": "Get comprehensive information about Requests - Python HTTP library for humans",
    "parameter": {
      "type": "object",
      "properties": {
        "include_examples": {
          "type": "boolean",
          "description": "Whether to include usage examples and quick start guide",
          "default": true
        }
      },
      "required": [
        "include_examples"
      ]
    },
    "package_name": "requests",
    "local_info": {
      "name": "Requests",
      "description": "Python HTTP library for humans. Allows you to send HTTP/1.1 requests extremely easily. There's no need to manually add query strings to your URLs, or to form-encode your POST data.",
      "category": "HTTP/Web",
      "import_name": "requests",
      "popularity": 88,
      "keywords": [
        "http",
        "web",
        "api",
        "rest",
        "client"
      ],
      "documentation": "https://requests.readthedocs.io/",
      "repository": "https://github.com/psf/requests",
      "installation": {
        "pip": "pip install requests",
        "conda": "conda install requests"
      },
      "usage_example": "import requests\n\n# GET request\nresponse = requests.get('https://api.github.com/users/octocat')\nprint(response.json())\n\n# POST request\ndata = {'key': 'value'}\nresponse = requests.post('https://httpbin.org/post', json=data)\nprint(response.status_code)",
      "quick_start": [
        "1. Install requests: pip install requests",
        "2. Import the library: import requests",
        "3. Make a GET request: response = requests.get(url)",
        "4. Check response: response.status_code, response.json()",
        "5. Handle errors with response.raise_for_status()"
      ]
    }
  },
  {
    "type": "PackageTool",
    "name": "get_pyscreener_info",
    "description": "Get comprehensive information about PyScreener – high-throughput virtual screening in Python",
    "parameter": {
      "type": "object",
      "properties": {
        "info_type": {
          "type": "string",
          "enum": [
            "overview",
            "installation",
            "usage",
            "documentation"
          ],
          "description": "Type of information to retrieve about PyScreener"
        }
      },
      "required": [
        "info_type"
      ]
    },
    "package_name": "pyscreener",
    "local_info": {
      "name": "PyScreener",
      "description": "High-throughput virtual screening toolkit for drug discovery. Provides Python interface to popular docking software and enables large-scale molecular docking workflows with parallel processing.",
      "category": "Drug Discovery / Virtual Screening",
      "import_name": "pyscreener",
      "popularity": 60,
      "keywords": [
        "virtual screening",
        "molecular docking",
        "drug discovery",
        "high-throughput",
        "AutoDock"
      ],
      "documentation": "https://pyscreener.readthedocs.io/",
      "repository": "https://github.com/coleygroup/pyscreener",
      "installation": {
        "pip": "pip install pyscreener",
        "conda": "conda install -c conda-forge pyscreener"
      },
      "usage_example": "import pyscreener as ps\nfrom pyscreener.docking import VinaDocking\n\n# Set up docking configuration\nreceptor_file = 'protein.pdbqt'\nligand_files = ['ligand1.sdf', 'ligand2.sdf', 'ligand3.sdf']\n\n# Initialize docking software (AutoDock Vina)\ndocking_runner = VinaDocking(\n    receptor_file=receptor_file,\n    center=(10.0, 15.0, 20.0),  # Binding site center\n    size=(20.0, 20.0, 20.0),   # Search box size\n    num_modes=5,\n    exhaustiveness=8\n)\n\n# Run virtual screening\nscores = docking_runner.dock(ligand_files, \n                           ncpu=4,  # Parallel processing\n                           score_mode='best')\n\nprint('Docking scores:')\nfor ligand, score in scores.items():\n    print(f'{ligand}: {score:.2f}')\n\n# Rank compounds by docking score\nsorted_scores = sorted(scores.items(), key=lambda x: x[1])\nprint(f'Best compound: {sorted_scores[0][0]} (score: {sorted_scores[0][1]:.2f})')",
      "quick_start": [
        "Install: pip install pyscreener",
        "Prepare receptor: convert protein to PDBQT format",
        "Initialize docking: VinaDocking(receptor_file, center, size)",
        "Run screening: docking_runner.dock(ligand_files)",
        "Analyze results: sort by docking scores",
        "Requires AutoDock Vina installation"
      ]
    }
  },
  {
    "type": "PackageTool",
    "name": "get_pykalman_info",
    "description": "Get comprehensive information about PyKalman – Kalman filtering and smoothing",
    "parameter": {
      "type": "object",
      "properties": {
        "info_type": {
          "type": "string",
          "enum": [
            "overview",
            "installation",
            "usage",
            "documentation"
          ],
          "description": "Type of information to retrieve about PyKalman"
        }
      },
      "required": [
        "info_type"
      ]
    },
    "package_name": "pykalman",
    "local_info": {
      "name": "PyKalman",
      "description": "Implementation of Kalman Filter, Extended Kalman Filter, and Unscented Kalman Filter for state estimation in time series and dynamic systems. Useful for tracking, smoothing, and parameter estimation.",
      "category": "Time Series / State Estimation",
      "import_name": "pykalman",
      "popularity": 65,
      "keywords": [
        "Kalman filter",
        "state estimation",
        "time series",
        "tracking",
        "smoothing",
        "dynamic systems"
      ],
      "documentation": "https://pykalman.github.io/",
      "repository": "https://github.com/pykalman/pykalman",
      "installation": {
        "pip": "pip install pykalman",
        "conda": "conda install -c conda-forge pykalman"
      },
      "usage_example": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom pykalman import KalmanFilter\n\n# Generate noisy observations of a sine wave\nnp.random.seed(42)\nn_timesteps = 100\ntrue_states = np.sin(np.linspace(0, 4*np.pi, n_timesteps))\nobservations = true_states + 0.3 * np.random.normal(size=n_timesteps)\n\n# Set up Kalman filter for 1D system\n# State: [position, velocity]\ntransition_matrices = np.array([[1, 1], [0, 1]])  # Position = position + velocity\nobservation_matrices = np.array([[1, 0]])  # Observe position only\n\nkf = KalmanFilter(\n    transition_matrices=transition_matrices,\n    observation_matrices=observation_matrices,\n    initial_state_mean=[0, 0],\n    n_dim_state=2\n)\n\n# Fit parameters and smooth\nstate_means, state_covariances = kf.em(\n    observations.reshape(-1, 1),\n    n_iter=10\n).smooth()[0:2]\n\n# Extract smoothed positions\nsmoothed_positions = state_means[:, 0]\n\n# Plot results\nplt.figure(figsize=(12, 6))\nplt.plot(true_states, 'g-', label='True signal', linewidth=2)\nplt.plot(observations, 'r.', alpha=0.5, label='Noisy observations')\nplt.plot(smoothed_positions, 'b-', label='Kalman smoothed', linewidth=2)\nplt.legend()\nplt.title('Kalman Filter Smoothing')\nplt.xlabel('Time')\nplt.ylabel('Value')\nplt.show()\n\nprint(f'Original noise level: {np.std(observations - true_states):.3f}')\nprint(f'After smoothing: {np.std(smoothed_positions - true_states):.3f}')",
      "quick_start": [
        "Install: pip install pykalman",
        "Import: from pykalman import KalmanFilter",
        "Define matrices: transition, observation matrices",
        "Create filter: KalmanFilter(transition_matrices, ...)",
        "Fit and smooth: kf.em().smooth()",
        "Extract results: state_means, state_covariances"
      ]
    }
  },
  {
    "type": "PackageTool",
    "name": "get_khmer_info",
    "description": "Get comprehensive information about khmer – nucleotide sequence k-mer analysis",
    "parameter": {
      "type": "object",
      "properties": {
        "info_type": {
          "type": "string",
          "enum": [
            "overview",
            "installation",
            "usage",
            "documentation"
          ],
          "description": "Type of information to retrieve about khmer"
        }
      },
      "required": [
        "info_type"
      ]
    },
    "package_name": "khmer",
    "local_info": {
      "name": "khmer",
      "description": "In-memory nucleotide sequence k-mer counting, filtering, graph traversal and more. Provides efficient data structures for genomic sequence analysis and assembly preprocessing.",
      "category": "Genomics / Sequence Analysis",
      "import_name": "khmer",
      "popularity": 70,
      "keywords": [
        "k-mer",
        "sequence analysis",
        "bloom filter",
        "DNA",
        "graph traversal",
        "assembly"
      ],
      "documentation": "https://khmer.readthedocs.io/",
      "repository": "https://github.com/dib-lab/khmer",
      "installation": {
        "pip": "pip install khmer",
        "conda": "conda install -c conda-forge khmer"
      },
      "usage_example": "import khmer\nimport tempfile\nimport os\n\n# Create temporary files for demo\nwith tempfile.NamedTemporaryFile(mode='w', suffix='.fa', delete=False) as f:\n    # Write sample sequences\n    sequences = [\n        '>seq1\\nACGTACGTACGTACGTACGTACGT',\n        '>seq2\\nTGCATGCATGCATGCATGCATGCA',\n        '>seq3\\nACGTTGCATGCATGCAAGCTACGT',\n        '>seq4\\nGGGGAAAATTTTCCCCGGGGAAAA',\n        '>seq5\\nACGTACGTTGCATGCATGCAACGT'\n    ]\n    for seq in sequences:\n        f.write(seq + '\\n')\n    fasta_file = f.name\n\nprint(f'Created sample FASTA file: {fasta_file}')\nprint('Sample sequences for k-mer analysis\\n')\n\n# K-mer counting with Countgraph\nprint('=== K-mer Counting ===')\nksize = 15  # k-mer size\ntablesize = 1e5  # hash table size\nn_tables = 4  # number of hash tables\n\n# Create counting hash\nprint(f'Creating Countgraph: k={ksize}, tablesize={int(tablesize)}, tables={n_tables}')\ncountgraph = khmer.Countgraph(ksize, tablesize, n_tables)\n\n# Count k-mers from sequences\nprint('Counting k-mers from sequences...')\ncountgraph.consume_seqfile(fasta_file)\n\nprint(f'Total k-mers counted: {countgraph.n_occupied()}')\nprint(f'Unique k-mers: {countgraph.n_unique_kmers()}')\n\n# Test specific k-mers\ntest_kmers = ['ACGTACGTACGTACG', 'TGCATGCATGCATGC', 'GGGGAAAAATTTTCC']\nprint('\\nK-mer abundance check:')\nfor kmer in test_kmers:\n    if len(kmer) == ksize:\n        count = countgraph.get(kmer)\n        print(f'  {kmer}: {count} occurrences')\n\n# K-mer filtering with Bloom Filter\nprint('\\n=== K-mer Filtering with Bloom Filter ===')\nbloom = khmer.Nodegraph(ksize, tablesize, n_tables)\n\n# Add k-mers to bloom filter\nbloom.consume_seqfile(fasta_file)\nprint(f'Bloom filter populated with k-mers')\nprint(f'False positive rate: {bloom.false_positive_rate():.6f}')\n\n# Test k-mer presence\nprint('\\nK-mer presence check:')\nfor kmer in test_kmers:\n    if len(kmer) == ksize:\n        present = bloom.get(kmer)\n        print(f'  {kmer}: {\"present\" if present else \"absent\"}')\n\n# Sequence filtering\nprint('\\n=== Sequence Filtering ===')\nwith tempfile.NamedTemporaryFile(mode='w', suffix='.fa', delete=False) as f:\n    # Add some low-complexity sequences\n    filtered_seqs = sequences + [\n        '>low_complex1\\nAAAAAAAAAAAAAAAAAAAAA',\n        '>low_complex2\\nATATATATATATATATATATAT',\n        '>good_seq\\nACGTGCATTGCAAACGTGCATT'\n    ]\n    for seq in filtered_seqs:\n        f.write(seq + '\\n')\n    filtered_file = f.name\n\nprint(f'Testing sequence filtering on {len(filtered_seqs)} sequences')\n\n# Create abundance filter\nabundance_filter = khmer.Countgraph(ksize, tablesize, n_tables)\nabundance_filter.consume_seqfile(filtered_file)\n\n# Filter sequences by k-mer abundance\nfiltered_count = 0\nwith open(filtered_file, 'r') as f:\n    lines = f.readlines()\n    \nfor i in range(0, len(lines), 2):\n    if i+1 < len(lines):\n        header = lines[i].strip()\n        sequence = lines[i+1].strip()\n        \n        # Calculate median k-mer abundance\n        if len(sequence) >= ksize:\n            abundances = []\n            for j in range(len(sequence) - ksize + 1):\n                kmer = sequence[j:j+ksize]\n                abundances.append(abundance_filter.get(kmer))\n            \n            if abundances:\n                median_abundance = sorted(abundances)[len(abundances)//2]\n                high_abundance = median_abundance > 1\n                \n                seq_name = header.replace('>', '')\n                print(f'  {seq_name:12s}: median abundance {median_abundance}, '\n                      f'{\"keep\" if high_abundance else \"filter\"}')\n                \n                if high_abundance:\n                    filtered_count += 1\n\nprint(f'\\nFiltered sequences: {filtered_count}/{len(filtered_seqs)}')\n\n# Graph traversal example\nprint('\\n=== Graph Traversal ===')\ngraph = khmer.Nodegraph(ksize, tablesize, n_tables)\ngraph.consume_seqfile(fasta_file)\n\n# Try to traverse from a starting k-mer\nstarting_kmer = 'ACGTACGTACGTACG'\nif len(starting_kmer) == ksize and graph.get(starting_kmer):\n    print(f'Starting graph traversal from: {starting_kmer}')\n    \n    # Get neighbors (this is a simplified example)\n    neighbors = []\n    bases = ['A', 'T', 'G', 'C']\n    \n    # Check right extensions\n    for base in bases:\n        extended_kmer = starting_kmer[1:] + base\n        if graph.get(extended_kmer):\n            neighbors.append(extended_kmer)\n    \n    print(f'Right neighbors found: {len(neighbors)}')\n    for neighbor in neighbors[:3]:  # Show first 3\n        print(f'  -> {neighbor}')\n\n# Cleanup\nos.unlink(fasta_file)\nos.unlink(filtered_file)\nprint('\\nDemo complete - temporary files cleaned up')\n\nprint('\\nkhmer provides:')\nprint('- Efficient k-mer counting and filtering')\nprint('- Bloom filters for sequence data')\nprint('- Graph traversal algorithms')\nprint('- Memory-efficient data structures')\nprint('- Preprocessing for genome assembly')",
      "quick_start": [
        "Install: pip install khmer",
        "Count k-mers: cg = khmer.Countgraph(k, size, tables)",
        "Load sequences: cg.consume_seqfile(file)",
        "Check abundance: cg.get(kmer)",
        "Bloom filter: ng = khmer.Nodegraph(k, size, tables)",
        "Filter sequences by k-mer abundance"
      ]
    }
  },
  {
    "type": "PackageTool",
    "name": "get_pymed_info",
    "description": "Get comprehensive information about PyMed – PubMed access in Python",
    "parameter": {
      "type": "object",
      "properties": {
        "include_examples": {
          "type": "boolean",
          "description": "Whether to include usage examples and quick start guide",
          "default": true
        }
      },
      "required": [
        "include_examples"
      ]
    },
    "package_name": "pymed",
    "local_info": {
      "name": "PyMed",
      "description": "Python library providing access to PubMed database for searching and retrieving biomedical literature. Simplifies querying and parsing of PubMed records.",
      "category": "Literature Mining",
      "import_name": "pymed",
      "popularity": 70,
      "keywords": [
        "PubMed",
        "literature search",
        "biomedical research",
        "scientific papers",
        "NCBI"
      ],
      "documentation": "https://github.com/gijswobben/pymed",
      "repository": "https://github.com/gijswobben/pymed",
      "installation": {
        "pip": "pip install pymed",
        "conda": "conda install -c conda-forge pymed"
      },
      "usage_example": "from pymed import PubMed\n\n# Create PubMed object\npubmed = PubMed(tool='MyTool', email='my_email@example.com')\n\n# Search for articles\nquery = 'COVID-19 AND vaccine'\nresults = pubmed.query(query, max_results=10)\n\n# Process results\nfor article in results:\n    print(f'Title: {article.title}')\n    print(f'Authors: {article.authors}')\n    print(f'Journal: {article.journal}')\n    print(f'Publication date: {article.publication_date}')\n    print(f'Abstract: {article.abstract[:200]}...')\n    print('---')",
      "quick_start": [
        "1. Install PyMed: pip install pymed",
        "2. Import: from pymed import PubMed",
        "3. Initialize: pubmed = PubMed(tool='MyTool', email='email')",
        "4. Search: results = pubmed.query('COVID-19', max_results=10)",
        "5. Process: iterate through results"
      ]
    }
  },
  {
    "type": "PackageTool",
    "name": "get_networkx_info",
    "description": "Get comprehensive information about NetworkX – network analysis library",
    "parameter": {
      "type": "object",
      "properties": {
        "info_type": {
          "type": "string",
          "enum": [
            "overview",
            "installation",
            "usage",
            "documentation"
          ],
          "description": "Type of information to retrieve about NetworkX"
        }
      },
      "required": [
        "info_type"
      ]
    },
    "package_name": "networkx",
    "local_info": {
      "name": "NetworkX",
      "description": "Python package for creation, manipulation, and study of complex networks. Provides data structures for graphs, digraphs, and multigraphs with many standard graph algorithms.",
      "category": "Network Analysis / Graph Theory",
      "import_name": "networkx",
      "popularity": 85,
      "keywords": [
        "graph theory",
        "network analysis",
        "social networks",
        "algorithms",
        "complex networks"
      ],
      "documentation": "https://networkx.org/documentation/stable/",
      "repository": "https://github.com/networkx/networkx",
      "installation": {
        "pip": "pip install networkx",
        "conda": "conda install networkx"
      },
      "usage_example": "import networkx as nx\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom collections import Counter\nimport random\nimport tempfile\nimport os\n\nprint('NetworkX - Network Analysis in Python')\nprint('=' * 40)\n\n# Basic graph creation\nprint('\\n=== Basic Graph Creation ===')\n\n# Create different types of graphs\nG = nx.Graph()  # Undirected graph\nDG = nx.DiGraph()  # Directed graph\nMG = nx.MultiGraph()  # Multi-edge graph\n\n# Add nodes and edges\nG.add_node(1)\nG.add_nodes_from([2, 3, 4, 5])\nG.add_edge(1, 2)\nG.add_edges_from([(1, 3), (2, 4), (3, 4), (4, 5)])\n\nprint(f'Graph G: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges')\nprint(f'Nodes: {list(G.nodes())}')\nprint(f'Edges: {list(G.edges())}')\nprint(f'Neighbors of node 1: {list(G.neighbors(1))}')\n\n# Graph from edge list\nedge_list = [(1, 2), (2, 3), (3, 4), (4, 1), (1, 3)]\nG_from_edges = nx.Graph(edge_list)\nprint(f'\\nGraph from edge list: {G_from_edges.number_of_nodes()} nodes, {G_from_edges.number_of_edges()} edges')\n\n# Famous graphs\nprint('\\n=== Famous Graphs ===')\nkarate = nx.karate_club_graph()\nprint(f'Karate Club: {karate.number_of_nodes()} nodes, {karate.number_of_edges()} edges')\n\npetersen = nx.petersen_graph()\nprint(f'Petersen Graph: {petersen.number_of_nodes()} nodes, {petersen.number_of_edges()} edges')\n\ncomplete = nx.complete_graph(6)\nprint(f'Complete K6: {complete.number_of_nodes()} nodes, {complete.number_of_edges()} edges')\n\n# Random graphs\nrandom_graph = nx.erdos_renyi_graph(50, 0.1)\nprint(f'Random ER(50, 0.1): {random_graph.number_of_nodes()} nodes, {random_graph.number_of_edges()} edges')\n\nscale_free = nx.barabasi_albert_graph(50, 3)\nprint(f'Scale-free BA(50, 3): {scale_free.number_of_nodes()} nodes, {scale_free.number_of_edges()} edges')\n\n# Basic graph properties\nprint('\\n=== Graph Properties ===')\nG_analysis = karate\n\n# Degree statistics\ndegrees = dict(G_analysis.degree())\nprint(f'Degree sequence: {sorted(degrees.values(), reverse=True)[:10]}...')\nprint(f'Average degree: {np.mean(list(degrees.values())):.2f}')\nprint(f'Max degree: {max(degrees.values())}')\nprint(f'Min degree: {min(degrees.values())}')\n\n# Connectivity\nprint(f'\\nIs connected: {nx.is_connected(G_analysis)}')\nprint(f'Number of connected components: {nx.number_connected_components(G_analysis)}')\nif nx.is_connected(G_analysis):\n    print(f'Diameter: {nx.diameter(G_analysis)}')\n    print(f'Radius: {nx.radius(G_analysis)}')\n    print(f'Average shortest path length: {nx.average_shortest_path_length(G_analysis):.3f}')\n\n# Clustering\nprint(f'\\nAverage clustering coefficient: {nx.average_clustering(G_analysis):.3f}')\nprint(f'Global clustering coefficient: {nx.transitivity(G_analysis):.3f}')\n\n# Centrality measures\nprint('\\n=== Centrality Measures ===')\n\n# Different centrality measures\nbetweenness = nx.betweenness_centrality(G_analysis)\ncloseness = nx.closeness_centrality(G_analysis)\neigenvector = nx.eigenvector_centrality(G_analysis)\npagerank = nx.pagerank(G_analysis)\n\n# Top nodes by different centralities\nprint('Top 5 nodes by betweenness centrality:')\nfor node, centrality in sorted(betweenness.items(), key=lambda x: x[1], reverse=True)[:5]:\n    print(f'  Node {node}: {centrality:.3f}')\n\nprint('\\nTop 5 nodes by closeness centrality:')\nfor node, centrality in sorted(closeness.items(), key=lambda x: x[1], reverse=True)[:5]:\n    print(f'  Node {node}: {centrality:.3f}')\n\nprint('\\nTop 5 nodes by PageRank:')\nfor node, centrality in sorted(pagerank.items(), key=lambda x: x[1], reverse=True)[:5]:\n    print(f'  Node {node}: {centrality:.3f}')\n\n# Community detection\nprint('\\n=== Community Detection ===')\ntry:\n    # Greedy modularity communities\n    communities = nx.community.greedy_modularity_communities(G_analysis)\n    print(f'Number of communities (greedy modularity): {len(communities)}')\n    for i, community in enumerate(communities):\n        print(f'  Community {i}: {len(community)} nodes')\n    \n    # Modularity\n    modularity = nx.community.modularity(G_analysis, communities)\n    print(f'Modularity: {modularity:.3f}')\nexcept:\n    print('Community detection requires additional packages')\n\n# Shortest paths\nprint('\\n=== Shortest Paths ===')\nsource = 0\ntarget = 33  # Assuming karate club graph\n\nif G_analysis.has_node(source) and G_analysis.has_node(target):\n    try:\n        shortest_path = nx.shortest_path(G_analysis, source, target)\n        path_length = nx.shortest_path_length(G_analysis, source, target)\n        print(f'Shortest path from {source} to {target}: {shortest_path}')\n        print(f'Path length: {path_length}')\n    except nx.NetworkXNoPath:\n        print(f'No path between {source} and {target}')\n\n# All shortest paths lengths\npath_lengths = dict(nx.all_pairs_shortest_path_length(G_analysis))\nprint(f'Average shortest path in network: {np.mean([np.mean(list(lengths.values())) for lengths in path_lengths.values()]):.3f}')\n\n# Graph algorithms\nprint('\\n=== Graph Algorithms ===')\n\n# Minimum spanning tree\nif nx.is_connected(G_analysis):\n    mst = nx.minimum_spanning_tree(G_analysis)\n    print(f'Minimum spanning tree: {mst.number_of_edges()} edges')\n    print(f'MST weight: {mst.size(weight=\"weight\", default=1)}')\n\n# Maximal independent set\nindependent_set = nx.maximal_independent_set(G_analysis)\nprint(f'Maximal independent set size: {len(independent_set)}')\n\n# Graph coloring\ncoloring = nx.greedy_color(G_analysis, strategy='largest_first')\nnum_colors = len(set(coloring.values()))\nprint(f'Graph coloring: {num_colors} colors needed')\n\n# Clique analysis\ncliques = list(nx.find_cliques(G_analysis))\nmax_clique_size = max(len(clique) for clique in cliques)\nprint(f'Maximum clique size: {max_clique_size}')\nprint(f'Number of maximal cliques: {len(cliques)}')\n\n# Directed graph analysis\nprint('\\n=== Directed Graph Analysis ===')\nDG_test = nx.DiGraph()\nDG_test.add_edges_from([(1, 2), (2, 3), (3, 1), (3, 4), (4, 5)])\n\nprint(f'Directed graph: {DG_test.number_of_nodes()} nodes, {DG_test.number_of_edges()} edges')\nprint(f'Is strongly connected: {nx.is_strongly_connected(DG_test)}')\nprint(f'Number of strongly connected components: {nx.number_strongly_connected_components(DG_test)}')\n\n# Strongly connected components\nscc = list(nx.strongly_connected_components(DG_test))\nprint('Strongly connected components:')\nfor i, component in enumerate(scc):\n    print(f'  Component {i}: {component}')\n\n# Topological sort (for DAGs)\nif nx.is_directed_acyclic_graph(DG_test):\n    topo_sort = list(nx.topological_sort(DG_test))\n    print(f'Topological sort: {topo_sort}')\nelse:\n    print('Graph contains cycles - not a DAG')\n\n# Network flow (create a flow network)\nprint('\\n=== Network Flow ===')\nflow_graph = nx.DiGraph()\nflow_graph.add_edge('s', 'a', capacity=10)\nflow_graph.add_edge('s', 'b', capacity=8)\nflow_graph.add_edge('a', 't', capacity=5)\nflow_graph.add_edge('a', 'b', capacity=3)\nflow_graph.add_edge('b', 't', capacity=12)\n\nmax_flow_value, flow_dict = nx.maximum_flow(flow_graph, 's', 't')\nprint(f'Maximum flow from s to t: {max_flow_value}')\nprint('Flow on edges:')\nfor source in flow_dict:\n    for target in flow_dict[source]:\n        if flow_dict[source][target] > 0:\n            print(f'  {source} -> {target}: {flow_dict[source][target]}')\n\n# Graph generators and models\nprint('\\n=== Graph Generators ===')\n\n# Small world networks\nsmall_world = nx.watts_strogatz_graph(30, 4, 0.3)\nprint(f'Small world (30, 4, 0.3): clustering = {nx.average_clustering(small_world):.3f}')\n\n# Scale-free networks\nscale_free_large = nx.barabasi_albert_graph(100, 2)\ndegree_sequence = [d for n, d in scale_free_large.degree()]\nprint(f'Scale-free network: max degree = {max(degree_sequence)}, power law fit')\n\n# Regular graphs\nregular = nx.random_regular_graph(4, 20)  # 4-regular graph with 20 nodes\nprint(f'4-regular graph: all nodes have degree {list(regular.degree())[0][1]}')\n\n# Graph I/O\nprint('\\n=== Graph Input/Output ===')\n\n# Save graph in different formats\nwith tempfile.NamedTemporaryFile(mode='w', suffix='.gml', delete=False) as f:\n    gml_file = f.name\n    nx.write_gml(G_analysis, gml_file)\n\nwith tempfile.NamedTemporaryFile(mode='w', suffix='.graphml', delete=False) as f:\n    graphml_file = f.name\n    nx.write_graphml(G_analysis, graphml_file)\n\nprint(f'Saved graph to GML format: {gml_file}')\nprint(f'Saved graph to GraphML format: {graphml_file}')\n\n# Read graph back\nG_loaded = nx.read_gml(gml_file)\nprint(f'Loaded graph: {G_loaded.number_of_nodes()} nodes, {G_loaded.number_of_edges()} edges')\n\n# Convert to/from other formats\nadjacency_matrix = nx.to_numpy_array(G_analysis)\nprint(f'Adjacency matrix shape: {adjacency_matrix.shape}')\n\nedge_list = nx.to_edgelist(G_analysis)\nprint(f'Edge list length: {len(edge_list)}')\n\n# Graph visualization preparation\nprint('\\n=== Visualization Setup ===')\npos = nx.spring_layout(G_analysis, seed=42)\nprint(f'Generated layout positions for {len(pos)} nodes')\nprint('Use matplotlib to visualize:')\nprint('plt.figure(figsize=(12, 8))')\nprint('nx.draw(G, pos, with_labels=True, node_color=\"lightblue\")')\nprint('plt.show()')\n\n# Graph statistics summary\nprint('\\n=== Graph Statistics Summary ===')\nstats = {\n    'Nodes': G_analysis.number_of_nodes(),\n    'Edges': G_analysis.number_of_edges(),\n    'Density': nx.density(G_analysis),\n    'Average degree': np.mean([d for n, d in G_analysis.degree()]),\n    'Clustering': nx.average_clustering(G_analysis),\n    'Connected': nx.is_connected(G_analysis)\n}\n\nfor stat, value in stats.items():\n    if isinstance(value, float):\n        print(f'{stat}: {value:.3f}')\n    else:\n        print(f'{stat}: {value}')\n\n# Cleanup temporary files\nos.unlink(gml_file)\nos.unlink(graphml_file)\n\nprint('\\nNetworkX provides:')\nprint('• Graph creation and manipulation')\nprint('• Network algorithms and analysis')\nprint('• Centrality and community detection')\nprint('• Shortest paths and flows')\nprint('• Graph generators and models')\nprint('• Import/export various formats')\nprint('• Integration with NumPy and SciPy')\nprint('• Visualization with matplotlib')",
      "quick_start": [
        "Install: pip install networkx",
        "Import: import networkx as nx",
        "Create: G = nx.Graph()",
        "Add: G.add_edge(1, 2)",
        "Analyze: nx.centrality_measures(G)",
        "Visualize: nx.draw(G)"
      ]
    }
  },
  {
    "type": "PackageTool",
    "name": "get_msprime_info",
    "description": "Get comprehensive information about msprime – coalescent simulation framework",
    "parameter": {
      "type": "object",
      "properties": {
        "include_examples": {
          "type": "boolean",
          "description": "Whether to include usage examples and quick start guide",
          "default": true
        }
      },
      "required": [
        "include_examples"
      ]
    },
    "package_name": "msprime",
    "local_info": {
      "name": "msprime",
      "description": "Efficient coalescent simulation framework for population genetics. Simulates genetic ancestry and generates tree sequences representing evolutionary relationships.",
      "category": "Population Genetics",
      "import_name": "msprime",
      "popularity": 78,
      "keywords": [
        "coalescent simulation",
        "population genetics",
        "tree sequences",
        "evolutionary genetics",
        "ancestry"
      ],
      "documentation": "https://msprime.readthedocs.io/",
      "repository": "https://github.com/tskit-dev/msprime",
      "installation": {
        "pip": "pip install msprime",
        "conda": "conda install -c conda-forge msprime"
      },
      "usage_example": "import msprime\nimport tskit\n\n# Simulate tree sequence\nts = msprime.sim_ancestry(\n    samples=10,\n    population_size=1e4,\n    sequence_length=1e6,\n    recombination_rate=1e-8,\n    random_seed=42\n)\n\n# Add mutations\nmts = msprime.sim_mutations(ts, rate=1e-8, random_seed=42)\n\n# Basic statistics\nprint(f'Number of trees: {ts.num_trees}')\nprint(f'Number of samples: {ts.num_samples}')\nprint(f'Number of mutations: {mts.num_mutations}')\nprint(f'Sequence length: {ts.sequence_length}')\n\n# Calculate diversity\ndiversity = mts.diversity()\nprint(f'Diversity: {diversity:.6f}')",
      "quick_start": [
        "1. Install msprime: pip install msprime",
        "2. Import: import msprime",
        "3. Simulate: ts = msprime.sim_ancestry(samples=10)",
        "4. Add mutations: mts = msprime.sim_mutations(ts)",
        "5. Analyze: diversity, statistics"
      ]
    }
  },
  {
    "type": "PackageTool",
    "name": "get_pymassspec_info",
    "description": "Get comprehensive information about PyMassSpec – mass spectrometry data analysis",
    "parameter": {
      "type": "object",
      "properties": {
        "info_type": {
          "type": "string",
          "enum": [
            "overview",
            "installation",
            "usage",
            "documentation"
          ],
          "description": "Type of information to retrieve about PyMassSpec"
        }
      },
      "required": [
        "info_type"
      ]
    },
    "package_name": "PyMassSpec",
    "local_info": {
      "name": "PyMassSpec",
      "description": "Python toolkit for mass spectrometry data analysis. Provides functions for reading mass spectra files, peak detection, alignment, and metabolomics/proteomics data processing workflows.",
      "category": "Mass Spectrometry",
      "import_name": "pyms",
      "popularity": 68,
      "keywords": [
        "mass spectrometry",
        "metabolomics",
        "proteomics",
        "peak detection",
        "chromatography"
      ],
      "documentation": "https://pymassspec.readthedocs.io/",
      "repository": "https://github.com/PyMassSpec/PyMassSpec",
      "installation": {
        "pip": "pip install PyMassSpec",
        "conda": "conda install -c conda-forge pymassspec"
      },
      "usage_example": "from pyms.GCMS.IO.JCAMP import JCAMP_reader\nfrom pyms.GCMS.Function import build_intensity_matrix\nfrom pyms.Noise.SavitzkyGolay import savitzky_golay\nfrom pyms.Peak.Function import peak_sum_area\nimport matplotlib.pyplot as plt\n\n# Read mass spectrometry data\n# data = JCAMP_reader('sample.jdx')  # Example file format\n# im = build_intensity_matrix(data)\n\n# For demonstration with synthetic data\nimport numpy as np\ntime_points = np.linspace(0, 1000, 1000)\nintensity = np.exp(-(time_points-500)**2/10000) + 0.1*np.random.normal(size=1000)\n\n# Apply smoothing\nsmoothed = savitzky_golay(intensity, window_size=11, order=3)\n\n# Peak detection (simplified example)\nfrom scipy.signal import find_peaks\npeaks, _ = find_peaks(smoothed, height=0.3, distance=20)\n\nprint(f'Found {len(peaks)} peaks')\nprint(f'Peak positions: {time_points[peaks]}')\n\n# Plot results\nplt.figure(figsize=(12, 6))\nplt.plot(time_points, intensity, 'b-', alpha=0.5, label='Raw')\nplt.plot(time_points, smoothed, 'r-', label='Smoothed')\nplt.plot(time_points[peaks], smoothed[peaks], 'go', label='Peaks')\nplt.xlabel('Retention Time')\nplt.ylabel('Intensity')\nplt.legend()\nplt.title('Mass Spectrometry Peak Detection')\nplt.show()",
      "quick_start": [
        "Install: pip install PyMassSpec",
        "Read data: JCAMP_reader() or ANDI_reader()",
        "Build intensity matrix: build_intensity_matrix()",
        "Smooth data: savitzky_golay()",
        "Detect peaks: use built-in peak detection",
        "Analyze metabolomics/proteomics workflows"
      ]
    }
  },
  {
    "type": "PackageTool",
    "name": "get_pypdf2_info",
    "description": "Get comprehensive information about PyPDF2 – PDF manipulation library",
    "parameter": {
      "type": "object",
      "properties": {
        "info_type": {
          "type": "string",
          "enum": [
            "overview",
            "installation",
            "usage",
            "documentation"
          ],
          "description": "Type of information to retrieve about PyPDF2"
        }
      },
      "required": [
        "info_type"
      ]
    },
    "package_name": "PyPDF2",
    "local_info": {
      "name": "PyPDF2",
      "description": "Pure-Python library for reading, merging, splitting, and transforming PDF files. Provides functionality for extracting text, metadata, and pages from PDF documents without external dependencies.",
      "category": "Document Processing / PDF",
      "import_name": "PyPDF2",
      "popularity": 82,
      "keywords": [
        "PDF",
        "document processing",
        "text extraction",
        "merge",
        "split",
        "metadata"
      ],
      "documentation": "https://pypdf2.readthedocs.io/",
      "repository": "https://github.com/py-pdf/PyPDF2",
      "installation": {
        "pip": "pip install PyPDF2",
        "conda": "conda install -c conda-forge pypdf2"
      },
      "usage_example": "import PyPDF2\nimport io\nfrom reportlab.pdfgen import canvas\nfrom reportlab.lib.pagesizes import letter\n\n# Create a sample PDF for demonstration\nbuffer = io.BytesIO()\nc = canvas.Canvas(buffer, pagesize=letter)\nc.drawString(100, 750, 'This is page 1')\nc.showPage()\nc.drawString(100, 750, 'This is page 2')\nc.showPage()\nc.drawString(100, 750, 'This is page 3')\nc.save()\n\n# Reset buffer position\nbuffer.seek(0)\n\n# Read PDF\nreader = PyPDF2.PdfReader(buffer)\n\nprint(f'Number of pages: {len(reader.pages)}')\nprint(f'PDF metadata: {reader.metadata}')\n\n# Extract text from first page\nif len(reader.pages) > 0:\n    first_page = reader.pages[0]\n    text = first_page.extract_text()\n    print(f'Text from first page: \"{text.strip()}\"')\n\n# Create a new PDF with selected pages\nwriter = PyPDF2.PdfWriter()\n\n# Add pages 1 and 3 (skip page 2)\nwriter.add_page(reader.pages[0])  # Page 1\nif len(reader.pages) > 2:\n    writer.add_page(reader.pages[2])  # Page 3\n\n# Save new PDF\noutput_buffer = io.BytesIO()\nwriter.write(output_buffer)\n\nprint(f'Created new PDF with {len(writer.pages)} pages')\n\n# Example: Merge multiple PDFs\nprint('\\nMerging multiple PDFs:')\nmerger = PyPDF2.PdfMerger()\n\n# Add the same PDF multiple times for demo\nbuffer.seek(0)\nmerger.append(buffer)\nbuffer.seek(0)\nmerger.append(buffer)\n\nmerged_buffer = io.BytesIO()\nmerger.write(merged_buffer)\nmerger.close()\n\nprint('PDFs merged successfully')\n\n# Read the merged PDF\nmerged_buffer.seek(0)\nmerged_reader = PyPDF2.PdfReader(merged_buffer)\nprint(f'Merged PDF has {len(merged_reader.pages)} pages')",
      "quick_start": [
        "Install: pip install PyPDF2",
        "Read PDF: reader = PyPDF2.PdfReader('file.pdf')",
        "Extract text: page.extract_text()",
        "Create writer: writer = PyPDF2.PdfWriter()",
        "Add pages: writer.add_page(page)",
        "Merge PDFs: PyPDF2.PdfMerger()"
      ]
    }
  },
  {
    "type": "PackageTool",
    "name": "get_pytdc_info",
    "description": "Get comprehensive information about PyTDC – Therapeutics Data Commons in Python",
    "parameter": {
      "type": "object",
      "properties": {
        "info_type": {
          "type": "string",
          "enum": [
            "overview",
            "installation",
            "usage",
            "datasets",
            "documentation"
          ],
          "description": "Type of information to retrieve about PyTDC"
        }
      },
      "required": [
        "info_type"
      ]
    },
    "package_name": "pytdc",
    "local_info": {
      "name": "PyTDC",
      "description": "Python interface to Therapeutics Data Commons (TDC), a comprehensive collection of curated datasets and benchmarks for drug discovery and development. Provides easy access to ADMET, toxicity, and bioactivity data.",
      "category": "Drug Discovery / Datasets",
      "import_name": "tdc",
      "popularity": 72,
      "keywords": [
        "therapeutics",
        "drug discovery",
        "ADMET",
        "toxicity",
        "bioactivity",
        "benchmarks"
      ],
      "documentation": "https://tdc.readthedocs.io/",
      "repository": "https://github.com/mims-harvard/TDC",
      "installation": {
        "pip": "pip install PyTDC",
        "conda": "conda install -c conda-forge pytdc"
      },
      "usage_example": "from tdc import Evaluator\nfrom tdc.single_pred import ADME, Tox\nfrom tdc.multi_pred import DTI\nimport pandas as pd\n\n# Load ADME dataset (e.g., Caco-2 permeability)\ndata = ADME(name='Caco2_Wang')\nsplit = data.get_split()\nprint(f'Train set size: {len(split[\"train\"])}')\nprint(f'Test set size: {len(split[\"test\"])}')\n\n# Load toxicity dataset\ntox_data = Tox(name='hERG')\ntox_split = tox_data.get_split()\nprint(f'Toxicity data shape: {tox_split[\"train\"].shape}')\n\n# Load drug-target interaction dataset\ndti_data = DTI(name='Davis')\ndti_split = dti_data.get_split()\nprint(f'DTI train size: {len(dti_split[\"train\"])}')\n\n# Use evaluator for model assessment\nevaluator = Evaluator(name='ROC-AUC')\n# predictions = model.predict(test_data)  # Your model predictions\n# score = evaluator(test_labels, predictions)\nprint('Available evaluators:', Evaluator.list_evaluator())",
      "quick_start": [
        "Install: pip install PyTDC",
        "Load ADME data: ADME(name='Caco2_Wang')",
        "Load toxicity data: Tox(name='hERG')",
        "Load DTI data: DTI(name='Davis')",
        "Get data splits: data.get_split()",
        "Evaluate models: Evaluator(name='ROC-AUC')"
      ]
    }
  },
  {
    "type": "PackageTool",
    "name": "get_lifelines_info",
    "description": "Get comprehensive information about lifelines – survival analysis in Python",
    "parameter": {
      "type": "object",
      "properties": {
        "include_examples": {
          "type": "boolean",
          "description": "Whether to include usage examples and quick start guide",
          "default": true
        }
      },
      "required": [
        "include_examples"
      ]
    },
    "package_name": "lifelines",
    "local_info": {
      "name": "lifelines",
      "description": "Complete survival analysis library for Python. Includes implementations of popular survival models like Cox proportional hazards, accelerated failure time models, and non-parametric estimators.",
      "category": "Survival Analysis",
      "import_name": "lifelines",
      "popularity": 80,
      "keywords": [
        "survival analysis",
        "kaplan-meier",
        "cox regression",
        "hazard models",
        "time-to-event"
      ],
      "documentation": "https://lifelines.readthedocs.io/",
      "repository": "https://github.com/CamDavidsonPilon/lifelines",
      "installation": {
        "pip": "pip install lifelines",
        "conda": "conda install -c conda-forge lifelines"
      },
      "usage_example": "from lifelines import KaplanMeierFitter, CoxPHFitter\nimport pandas as pd\n\n# Kaplan-Meier survival analysis\nkmf = KaplanMeierFitter()\nT = [1, 2, 3, 4, 5, 6]  # duration\nE = [1, 0, 1, 1, 0, 1]  # event occurred\nkmf.fit(T, E)\nkmf.plot_survival_function()\n\n# Cox proportional hazards model\ncph = CoxPHFitter()\ndf = pd.DataFrame({'T': T, 'E': E, 'age': [25, 30, 35, 40, 45, 50]})\ncph.fit(df, duration_col='T', event_col='E')\nprint(cph.summary)",
      "quick_start": [
        "1. Install lifelines: pip install lifelines",
        "2. Import: from lifelines import KaplanMeierFitter",
        "3. Fit model: kmf = KaplanMeierFitter(); kmf.fit(T, E)",
        "4. Plot curves: kmf.plot_survival_function()",
        "5. Cox regression: CoxPHFitter().fit(data)"
      ]
    }
  },
  {
    "type": "PackageTool",
    "name": "get_bioservices_info",
    "description": "Get information about the bioservices package. Python package: bioservices",
    "package_name": "bioservices",
    "parameter": {
      "type": "object",
      "properties": {},
      "required": []
    },
    "required": []
  },
  {
    "type": "PackageTool",
    "name": "get_ete3_info",
    "description": "Get information about the ete3 package. Python package: ete3",
    "package_name": "ete3",
    "parameter": {
      "type": "object",
      "properties": {},
      "required": []
    },
    "required": []
  },
  {
    "type": "PackageTool",
    "name": "get_dendropy_info",
    "description": "Get information about the dendropy package. Python package: dendropy",
    "package_name": "dendropy",
    "parameter": {
      "type": "object",
      "properties": {},
      "required": []
    },
    "required": []
  }
]
