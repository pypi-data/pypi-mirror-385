"""Integration tests for ADRI Validator Engine.

This module consolidates integration tests for the validator engine, focusing on:
- Audit logging integration with CSVAuditLogger and VerodatLogger
- Real-world validation scenarios (financial, healthcare, e-commerce, IoT, social media)
- Edge cases and error handling
- Comprehensive method coverage

Consolidates tests from:
- test_validator_engine_integration.py (20 tests)

Organization:
- TestAuditLoggingIntegration: Audit logging and enterprise integration
- TestComprehensiveValidationScenarios: Real-world data validation scenarios
- TestValidationEngineEdgeCases: Edge cases and error scenarios
- TestValidationEngineMethodCoverage: Comprehensive method coverage
"""

import unittest
import pandas as pd
import tempfile
import os
import shutil
from unittest.mock import Mock, patch, MagicMock
from datetime import datetime

from src.adri.validator.engine import (
    ValidationEngine,
    DataQualityAssessor,
    AssessmentResult,
    DimensionScore,
    FieldAnalysis,
    RuleExecutionResult,
    BundledStandardWrapper
)


class TestAuditLoggingIntegration(unittest.TestCase):
    """Integration tests for audit logging functionality.

    Tests audit logging integration with CSVAuditLogger and VerodatLogger,
    including execution context tracking, performance metrics, and failed checks.
    """

    def setUp(self):
        """Set up test environment with audit logging."""
        self.test_dir = tempfile.mkdtemp()
        self.log_dir = os.path.join(self.test_dir, "audit_logs")
        os.makedirs(self.log_dir, exist_ok=True)

        # Configuration with audit logging enabled
        self.audit_config = {
            "audit": {
                "enabled": True,
                "log_dir": self.log_dir,
                "log_assessment": True,
                "log_level": "INFO"
            }
        }

        # High quality test data
        self.sample_data = pd.DataFrame({
            "customer_id": [1001, 1002, 1003, 1004, 1005],
            "name": ["Alice Johnson", "Bob Smith", "Charlie Brown", "Diana Prince", "Eva Martinez"],
            "email": ["alice@company.com", "bob@enterprise.org", "charlie@startup.io", "diana@corp.net", "eva@business.com"],
            "age": [28, 34, 29, 31, 26],
            "balance": [15000.50, 23500.75, 12750.00, 18900.25, 21200.80],
            "status": ["active", "active", "active", "pending", "active"]
        })

    def tearDown(self):
        """Clean up test environment."""
        shutil.rmtree(self.test_dir)

    @patch('src.adri.validator.engine.CSVAuditLogger')
    def test_audit_logging_assessment_workflow(self, mock_csv_logger_class):
        """Test complete audit logging workflow during assessment."""
        # Set up mock audit logger
        mock_audit_logger = Mock()
        mock_csv_logger_class.return_value = mock_audit_logger

        # Create assessor with audit logging
        assessor = DataQualityAssessor(self.audit_config)

        # Verify audit logger was initialized
        self.assertIsNotNone(assessor.audit_logger)
        mock_csv_logger_class.assert_called_once_with(self.audit_config["audit"])

        # Perform assessment
        result = assessor.assess(self.sample_data)

        # Verify assessment succeeded
        self.assertIsInstance(result, AssessmentResult)
        self.assertGreater(result.overall_score, 0)

        # Verify audit logging was called
        mock_audit_logger.log_assessment.assert_called_once()

        # Verify audit call parameters
        call_args = mock_audit_logger.log_assessment.call_args
        self.assertIsInstance(call_args[1]["assessment_result"], AssessmentResult)
        self.assertIn("execution_context", call_args[1])
        self.assertIn("data_info", call_args[1])
        self.assertIn("performance_metrics", call_args[1])

    @patch('src.adri.validator.engine.CSVAuditLogger')
    @patch('src.adri.validator.engine.VerodatLogger')
    def test_verodat_enterprise_logging_integration(self, mock_verodat_class, mock_csv_logger_class):
        """Test integration with enterprise Verodat logging."""
        # Set up mock loggers
        mock_audit_logger = Mock()
        mock_verodat_logger = Mock()
        mock_csv_logger_class.return_value = mock_audit_logger
        mock_verodat_class.return_value = mock_verodat_logger

        # Configuration with Verodat enabled
        verodat_config = {
            "audit": {
                "enabled": True,
                "log_dir": self.log_dir
            },
            "verodat": {
                "enabled": True,
                "batch_size": 10,
                "auto_flush": True
            }
        }

        # Create assessor
        assessor = DataQualityAssessor(verodat_config)

        # Verify Verodat logger was attached
        mock_verodat_class.assert_called_once_with(verodat_config["verodat"])
        self.assertEqual(assessor.audit_logger.verodat_logger, mock_verodat_logger)

        # Perform assessment
        result = assessor.assess(self.sample_data)

        # Verify assessment succeeded
        self.assertIsInstance(result, AssessmentResult)

        # Verify Verodat integration was called
        mock_audit_logger.log_assessment.assert_called_once()
        mock_verodat_logger.add_to_batch.assert_called_once()

    def test_audit_logging_execution_context_details(self):
        """Test detailed execution context logging."""
        # Mock the audit logger to capture call details
        with patch('src.adri.validator.engine.CSVAuditLogger') as mock_csv_logger_class:
            mock_audit_logger = Mock()
            mock_csv_logger_class.return_value = mock_audit_logger

            # Create assessor and perform assessment
            assessor = DataQualityAssessor(self.audit_config)
            result = assessor.assess(self.sample_data)

            # Extract audit call arguments
            call_args = mock_audit_logger.log_assessment.call_args[1]

            # Verify execution context details
            execution_context = call_args["execution_context"]
            self.assertEqual(execution_context["function_name"], "assess")
            self.assertEqual(execution_context["module_path"], "adri.validator.engine")
            self.assertIn("environment", execution_context)

            # Verify data info
            data_info = call_args["data_info"]
            self.assertEqual(data_info["row_count"], 5)
            self.assertEqual(data_info["column_count"], 6)
            self.assertEqual(data_info["columns"], list(self.sample_data.columns))

            # Verify performance metrics
            performance_metrics = call_args["performance_metrics"]
            self.assertIn("duration_ms", performance_metrics)
            self.assertIn("rows_per_second", performance_metrics)
            self.assertGreaterEqual(performance_metrics["duration_ms"], 0)

    def test_audit_logging_failed_checks_tracking(self):
        """Test audit logging captures failed quality checks."""
        # Create data with quality issues to trigger failed checks
        poor_quality_data = pd.DataFrame({
            "name": ["Alice", None, "Charlie", "", "Eva"],
            "email": ["alice@test.com", "invalid-email", "charlie@test.com", "bad@", "eva@test.com"],
            "age": [25, -5, 30, 200, 28],
            "score": [85.5, None, 92.0, 78.5, None]
        })

        with patch('src.adri.validator.engine.CSVAuditLogger') as mock_csv_logger_class:
            mock_audit_logger = Mock()
            mock_csv_logger_class.return_value = mock_audit_logger

            assessor = DataQualityAssessor(self.audit_config)
            result = assessor.assess(poor_quality_data)

            # Extract audit call arguments
            call_args = mock_audit_logger.log_assessment.call_args[1]

            # Should have failed checks due to poor data quality
            failed_checks = call_args.get("failed_checks")
            if failed_checks:
                self.assertIsInstance(failed_checks, list)
                for check in failed_checks:
                    self.assertIn("dimension", check)
                    self.assertIn("issue", check)
                    self.assertIn("affected_percentage", check)

    def test_audit_logging_performance_metrics_calculation(self):
        """Test performance metrics calculation in audit logging."""
        with patch('src.adri.validator.engine.CSVAuditLogger') as mock_csv_logger_class:
            mock_audit_logger = Mock()
            mock_csv_logger_class.return_value = mock_audit_logger

            # Use larger dataset to ensure measurable execution time
            large_data = pd.DataFrame({
                "id": range(1000),
                "name": [f"Customer {i}" for i in range(1000)],
                "email": [f"customer{i}@test.com" for i in range(1000)],
                "value": [100.0 + i for i in range(1000)]
            })

            assessor = DataQualityAssessor(self.audit_config)
            result = assessor.assess(large_data)

            # Extract performance metrics
            call_args = mock_audit_logger.log_assessment.call_args[1]
            performance_metrics = call_args["performance_metrics"]

        # Verify performance calculations
        duration_ms = performance_metrics["duration_ms"]
        rows_per_second = performance_metrics["rows_per_second"]

        # Duration can be 0ms for sub-millisecond operations on fast machines (int truncates < 1ms to 0)
        # This is valid - the test validates metrics calculation, not minimum execution time
        self.assertGreaterEqual(duration_ms, 0)

        # When duration is 0ms, rows_per_second will also be 0 (or very large, depending on implementation)
        # Only verify > 0 if we have measurable duration
        if duration_ms > 0:
            self.assertGreater(rows_per_second, 0)
            # Verify calculation accuracy
            expected_rps = 1000 / (duration_ms / 1000.0)
            self.assertAlmostEqual(rows_per_second, expected_rps, delta=1.0)
        else:
            # For 0ms duration, just verify the metric exists and is non-negative
            self.assertGreaterEqual(rows_per_second, 0)


class TestComprehensiveValidationScenarios(unittest.TestCase):
    """Test comprehensive real-world validation scenarios.

    Tests validator behavior with realistic datasets from different domains:
    - Financial data (banking, credit, investments)
    - Healthcare data (patient records, medical data)
    - E-commerce data (products, orders, customers)
    - IoT sensor data (time series, device readings)
    - Social media data (user profiles, engagement metrics)
    """

    def setUp(self):
        """Set up test environment."""
        self.engine = ValidationEngine()
        self.assessor = DataQualityAssessor()

    def test_financial_data_validation_scenario(self):
        """Test comprehensive financial data validation."""
        financial_data = pd.DataFrame({
            "account_id": ["ACC001", "ACC002", "ACC003", "ACC004", "ACC005"],
            "customer_email": ["john@bank.com", "mary@credit.org", "david@finance.net", "sarah@invest.com", "mike@trading.io"],
            "balance": [15000.50, 250000.75, 5500.00, 1000000.25, 75000.80],
            "age": [35, 42, 28, 55, 31],
            "credit_score": [720, 810, 650, 780, 695],
            "account_type": ["checking", "savings", "investment", "premium", "business"]
        })

        result = self.assessor.assess(financial_data)

        # Financial data should have high quality scores
        self.assertIsInstance(result, AssessmentResult)
        self.assertGreater(result.overall_score, 80)

        # All key dimensions should be measured
        expected_dimensions = ["validity", "completeness", "consistency", "freshness", "plausibility"]
        for dim in expected_dimensions:
            self.assertIn(dim, result.dimension_scores)
            self.assertIsInstance(result.dimension_scores[dim], DimensionScore)

    def test_healthcare_data_validation_scenario(self):
        """Test healthcare data validation with sensitive information."""
        healthcare_data = pd.DataFrame({
            "patient_id": ["PAT001", "PAT002", "PAT003", "PAT004"],
            "email": ["patient1@hospital.com", "patient2@clinic.org", "patient3@health.net", "patient4@medical.com"],
            "age": [45, 67, 32, 78],
            "diagnosis_date": ["2024-01-15", "2024-02-20", "2024-03-10", "2024-01-30"],
            "vital_score": [85.5, 72.3, 94.1, 68.7],
            "status": ["stable", "monitoring", "recovered", "critical"]
        })

        result = self.assessor.assess(healthcare_data)

        # Healthcare data should meet quality standards
        self.assertIsInstance(result, AssessmentResult)
        self.assertGreater(result.overall_score, 70)

        # Completeness should be high (no missing critical data)
        completeness_score = result.dimension_scores["completeness"].score
        self.assertGreater(completeness_score, 18)

    def test_ecommerce_data_validation_scenario(self):
        """Test e-commerce data validation with mixed quality."""
        ecommerce_data = pd.DataFrame({
            "product_id": ["PROD001", "PROD002", None, "PROD004", "PROD005"],
            "customer_email": ["buyer1@shop.com", "invalid-email", "buyer3@store.org", "buyer4@market.net", ""],
            "price": [29.99, -10.50, 45.00, 1000000.00, 15.75],
            "age": [25, 150, 30, 22, -5],
            "rating": [4.5, 3.8, 5.0, 2.1, 4.2],
            "category": ["electronics", "clothing", "books", "home", "sports"]
        })

        result = self.assessor.assess(ecommerce_data)

        # E-commerce data with issues - score improved due to auto-activated consistency rules
        self.assertIsInstance(result, AssessmentResult)
        # Score improved from <85 to ~93 due to format_consistency and cross_field_logic auto-activation
        self.assertLess(result.overall_score, 95)

        # Validity should be impacted by invalid emails and values
        validity_score = result.dimension_scores["validity"].score
        self.assertLess(validity_score, 18)

        # Completeness should be impacted by missing values
        completeness_score = result.dimension_scores["completeness"].score
        self.assertLess(completeness_score, 20)

    def test_iot_sensor_data_validation_scenario(self):
        """Test IoT sensor data validation with time series characteristics."""
        iot_data = pd.DataFrame({
            "sensor_id": [f"SENSOR_{i:03d}" for i in range(1, 21)],
            "timestamp": [f"2024-03-{i:02d}T10:00:00Z" for i in range(1, 21)],
            "temperature": [20.5 + (i * 0.5) for i in range(20)],
            "humidity": [45.0 + (i * 1.2) for i in range(20)],
            "pressure": [1013.25 + (i * 0.1) for i in range(20)],
            "status": ["active"] * 18 + ["maintenance", "error"]
        })

        result = self.assessor.assess(iot_data)

        # IoT data should have high quality (structured, complete)
        self.assertIsInstance(result, AssessmentResult)
        self.assertGreater(result.overall_score, 85)

        # Should have perfect completeness (no missing sensor readings)
        completeness_score = result.dimension_scores["completeness"].score
        self.assertEqual(completeness_score, 20.0)

    def test_social_media_data_validation_scenario(self):
        """Test social media data validation with text content."""
        social_data = pd.DataFrame({
            "user_id": ["USER001", "USER002", "USER003", "USER004", "USER005"],
            "email": ["user1@social.com", "user2@platform.org", "user3@network.net", "user4@media.io", "user5@connect.com"],
            "age": [22, 28, 35, 19, 31],
            "followers": [1500, 25000, 500, 10000, 3200],
            "engagement_rate": [3.5, 8.2, 1.9, 6.4, 4.1],
            "content_type": ["video", "image", "text", "mixed", "video"]
        })

        result = self.assessor.assess(social_data)

        # Social media data should have good quality
        self.assertIsInstance(result, AssessmentResult)
        self.assertGreater(result.overall_score, 75)

        # Verify all dimension scores are reasonable
        for dim_name, dim_score in result.dimension_scores.items():
            self.assertIsInstance(dim_score, DimensionScore)
            self.assertGreaterEqual(dim_score.score, 0)
            self.assertLessEqual(dim_score.score, 20)


class TestValidationEngineEdgeCases(unittest.TestCase):
    """Test edge cases and error scenarios for comprehensive coverage.

    Tests validator behavior in unusual or error conditions:
    - Malformed standards
    - Empty data
    - Extreme values
    - All-null columns
    - Comprehensive standard structures
    - BundledStandardWrapper edge cases
    """

    def setUp(self):
        """Set up test environment."""
        self.engine = ValidationEngine()

    def test_validation_with_malformed_standard_file(self):
        """Test validation behavior with malformed standard files."""
        # Create temporary malformed standard file
        with tempfile.NamedTemporaryFile(mode='w', suffix='.yaml', delete=False) as f:
            f.write("invalid: yaml: content: [[[")
            malformed_standard = f.name

        try:
            test_data = pd.DataFrame({"test": [1, 2, 3]})

            # Should fallback to basic assessment when standard can't be loaded
            result = self.engine.assess(test_data, malformed_standard)

            self.assertIsInstance(result, AssessmentResult)
            self.assertGreater(result.overall_score, 0)

        finally:
            os.unlink(malformed_standard)

    def test_validation_with_empty_dataframe(self):
        """Test validation with various empty data scenarios."""
        # Completely empty DataFrame
        empty_df = pd.DataFrame()
        result = self.engine._basic_assessment(empty_df)
        self.assertIsInstance(result, AssessmentResult)
        self.assertEqual(result.dimension_scores["completeness"].score, 0.0)

        # DataFrame with columns but no rows
        empty_with_columns = pd.DataFrame(columns=["name", "age", "email"])
        result = self.engine._basic_assessment(empty_with_columns)
        self.assertIsInstance(result, AssessmentResult)
        self.assertEqual(result.dimension_scores["completeness"].score, 0.0)

    def test_validation_with_extreme_data_values(self):
        """Test validation with extreme or unusual data values."""
        extreme_data = pd.DataFrame({
            "large_numbers": [1e15, 2e15, 3e15],
            "small_decimals": [1e-10, 2e-10, 3e-10],
            "unicode_text": ["🚀 Rocket", "🌟 Star", "🎯 Target"],
            "mixed_types": [1, "text", 3.14],
            "email": ["test@domain.com", "unicode🚀@domain.com", "normal@test.org"]
        })

        result = self.engine._basic_assessment(extreme_data)

        self.assertIsInstance(result, AssessmentResult)
        self.assertGreater(result.overall_score, 0)

        # Should handle extreme values without crashing
        for dim_score in result.dimension_scores.values():
            self.assertIsInstance(dim_score.score, (int, float))
            self.assertGreaterEqual(dim_score.score, 0)
            self.assertLessEqual(dim_score.score, 20)

    def test_validation_with_all_null_columns(self):
        """Test validation with columns that are entirely null."""
        null_data = pd.DataFrame({
            "all_null_1": [None, None, None],
            "all_null_2": [None, None, None],
            "mixed": ["value", None, "another"]
        })

        result = self.engine._basic_assessment(null_data)

        self.assertIsInstance(result, AssessmentResult)
        # Completeness should be significantly reduced
        completeness_score = result.dimension_scores["completeness"].score
        self.assertLess(completeness_score, 15)

    def test_assess_with_standard_dict_comprehensive(self):
        """Test assess_with_standard_dict with comprehensive standard."""
        comprehensive_standard = {
            "standards": {
                "id": "comprehensive_test",
                "name": "Comprehensive Test Standard",
                "version": "1.0.0"
            },
            "requirements": {
                "overall_minimum": 80.0,
                "field_requirements": {
                    "name": {
                        "type": "string",
                        "nullable": False,
                        "pattern": "^[A-Za-z ]+$",
                        "min_length": 2,
                        "max_length": 50
                    },
                    "age": {
                        "type": "integer",
                        "nullable": False,
                        "min_value": 0,
                        "max_value": 120
                    },
                    "email": {
                        "type": "string",
                        "nullable": False,
                        "pattern": "^[^@]+@[^@]+\\.[^@]+$"
                    },
                    "score": {
                        "type": "float",
                        "nullable": True,
                        "min_value": 0.0,
                        "max_value": 100.0
                    }
                }
            }
        }

        test_data = pd.DataFrame({
            "name": ["Alice Johnson", "Bob Smith", "Charlie Brown"],
            "age": [25, 30, 35],
            "email": ["alice@test.com", "bob@test.com", "charlie@test.com"],
            "score": [85.5, 92.0, 78.5]
        })

        result = self.engine.assess_with_standard_dict(test_data, comprehensive_standard)

        self.assertIsInstance(result, AssessmentResult)
        self.assertGreater(result.overall_score, 75)

        # Should pass the 80% minimum from the standard
        self.assertTrue(result.passed or result.overall_score >= 80.0)

    def test_bundled_standard_wrapper_all_methods(self):
        """Test all methods of BundledStandardWrapper thoroughly."""
        # Test with comprehensive standard structure
        standard_dict = {
            "requirements": {
                "overall_minimum": 90.0,
                "field_requirements": {
                    "required_field": {"type": "string", "nullable": False},
                    "optional_field": {"type": "integer", "nullable": True}
                }
            }
        }

        wrapper = BundledStandardWrapper(standard_dict)

        # Test field requirements
        field_reqs = wrapper.get_field_requirements()
        self.assertIsInstance(field_reqs, dict)
        self.assertIn("required_field", field_reqs)
        self.assertIn("optional_field", field_reqs)

        # Test overall minimum
        minimum = wrapper.get_overall_minimum()
        self.assertEqual(minimum, 90.0)

        # Test with missing requirements
        empty_wrapper = BundledStandardWrapper({})
        self.assertEqual(empty_wrapper.get_field_requirements(), {})
        self.assertEqual(empty_wrapper.get_overall_minimum(), 75.0)

        # Test with invalid structure
        invalid_wrapper = BundledStandardWrapper({"requirements": "not_a_dict"})
        self.assertEqual(invalid_wrapper.get_field_requirements(), {})
        self.assertEqual(invalid_wrapper.get_overall_minimum(), 75.0)


class TestValidationEngineMethodCoverage(unittest.TestCase):
    """Test methods to achieve comprehensive coverage of all public methods.

    Tests all public assessment methods with various configurations:
    - assess_validity with field requirements
    - assess_completeness with mandatory fields
    - assess_consistency with format rules
    - assess_freshness with date fields
    - assess_plausibility with business rules
    - Email validation edge cases
    - AssessmentResult comprehensive methods
    """

    def setUp(self):
        """Set up test environment."""
        self.engine = ValidationEngine()

    def test_public_assessment_methods_comprehensive(self):
        """Test all public assessment methods comprehensively."""
        test_data = pd.DataFrame({
            "name": ["Alice", "Bob", "Charlie"],
            "age": [25, 30, 35],
            "email": ["alice@test.com", "bob@test.com", "charlie@test.com"],
            "created_at": ["2024-01-01", "2024-01-02", "2024-01-03"],
            "score": [85, 92, 78]
        })

        # Test assess_validity with field requirements
        validity_requirements = {
            "email": {"type": "string", "pattern": r"^[^@]+@[^@]+\.[^@]+$"},
            "age": {"type": "integer", "min_value": 0, "max_value": 120}
        }
        validity_score = self.engine.assess_validity(test_data, validity_requirements)
        self.assertIsInstance(validity_score, float)
        self.assertGreaterEqual(validity_score, 0)
        self.assertLessEqual(validity_score, 20)

        # Test assess_completeness with requirements
        completeness_requirements = {
            "mandatory_fields": ["name", "email", "age"]
        }
        completeness_score = self.engine.assess_completeness(test_data, completeness_requirements)
        self.assertIsInstance(completeness_score, float)
        self.assertGreaterEqual(completeness_score, 0)
        self.assertLessEqual(completeness_score, 20)

        # Test assess_consistency with format rules
        consistency_rules = {
            "format_rules": {
                "name": "title_case"
            }
        }
        consistency_score = self.engine.assess_consistency(test_data, consistency_rules)
        self.assertIsInstance(consistency_score, float)
        self.assertGreaterEqual(consistency_score, 0)
        self.assertLessEqual(consistency_score, 20)

        # Test assess_freshness with date fields
        freshness_config = {
            "date_fields": ["created_at"]
        }
        freshness_score = self.engine.assess_freshness(test_data, freshness_config)
        self.assertIsInstance(freshness_score, float)
        # Score changed from 18.0 to 20.0 due to weight normalization in dynamic weights
        self.assertEqual(freshness_score, 20.0)

        # Test assess_plausibility with business rules
        plausibility_config = {
            "business_rules": {
                "age": {"min": 0, "max": 120},
                "score": {"min": 0, "max": 100}
            },
            "outlier_detection": {
                "score": {"method": "range", "min": 0, "max": 100}
            }
        }
        plausibility_score = self.engine.assess_plausibility(test_data, plausibility_config)
        self.assertIsInstance(plausibility_score, float)
        self.assertGreaterEqual(plausibility_score, 0)
        self.assertLessEqual(plausibility_score, 20)

    def test_email_validation_comprehensive(self):
        """Test email validation with comprehensive test cases."""
        engine = self.engine

        # Valid email formats
        valid_emails = [
            "user@domain.com",
            "test.email@example.org",
            "user+tag@domain.co.uk",
            "firstname.lastname@company.com",
            "user123@test-domain.net",
            "admin@subdomain.domain.com"
        ]

        for email in valid_emails:
            with self.subTest(email=email):
                self.assertTrue(engine._is_valid_email(email), f"Should validate: {email}")

        # Invalid email formats
        invalid_emails = [
            "invalid",
            "user@",
            "@domain.com",
            "user@@domain.com",
            "user@domain",
            "user@.com",
            "user@domain.",
            "",
            "user space@domain.com",
            "user@domain@extra.com"
        ]

        for email in invalid_emails:
            with self.subTest(email=email):
                self.assertFalse(engine._is_valid_email(email), f"Should not validate: {email}")

    def test_assessment_result_comprehensive_methods(self):
        """Test all AssessmentResult methods comprehensively."""
        # Create comprehensive assessment result
        dimension_scores = {
            "validity": DimensionScore(16.5, issues=["email format"], details={"check_count": 10}),
            "completeness": DimensionScore(18.0, issues=[], details={"null_count": 2}),
            "consistency": DimensionScore(16.0, issues=["format inconsistency"], details={"format_errors": 1}),
            "freshness": DimensionScore(17.0, issues=[], details={"last_updated": "2024-03-01"}),
            "plausibility": DimensionScore(15.0, issues=["outlier detected"], details={"outlier_count": 1})
        }

        result = AssessmentResult(
            overall_score=82.5,
            passed=True,
            dimension_scores=dimension_scores,
            standard_id="comprehensive_standard",
            assessment_date=datetime.now()
        )

        # Test all methods
        rule_result = RuleExecutionResult(rule_id="test_rule", passed=90, failed=10, total_records=100)
        result.add_rule_execution(rule_result)

        field_analysis = FieldAnalysis("test_field", total_failures=2, ml_readiness="high")
        result.add_field_analysis("test_field", field_analysis)

        result.set_dataset_info(100, 5, 2.5)
        result.set_execution_stats(total_execution_time_ms=500, rules_executed=10)

        # Test dictionary conversion
        result_dict = result.to_dict()
        self.assertIn("adri_assessment_report", result_dict)

        # Test v2 format
        v2_dict = result.to_v2_standard_dict("test_dataset")
        self.assertIn("adri_assessment_report", v2_dict)
        self.assertEqual(v2_dict["adri_assessment_report"]["metadata"]["dataset_name"], "test_dataset")


if __name__ == '__main__':
    unittest.main()
