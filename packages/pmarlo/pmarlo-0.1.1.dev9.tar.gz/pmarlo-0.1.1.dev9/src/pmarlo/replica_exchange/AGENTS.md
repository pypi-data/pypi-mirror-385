Replica exchange module:
- unnegotiable
- explanation
- key values
- best practices
- wanted outcomes
- examples
- files description
- maintenance and evolution

# Unnegotiable
- Deterministic Outputs: All simulation outputs (trajectories, logs, metadata) must be bit-for-bit identical on repeated runs given the same configuration, random seed, and software stack. Every source of randomness (exchange proposals, velocity seeding, etc.) must be controlled by a fixed seed. The MD integration and exchange scheduling should yield the exact same results regardless of runtime timing or hardware differences – use deterministic algorithms and set random_seed (or random_state) for any stochastic component. No dynamic content like timestamps, UUIDs, or run dates may appear in file names or file contents.
-Strict Naming & Layout: Follow a canonical naming scheme and directory structure for all output artifacts (DCD files, JSON metadata, diagnostics). Do not include timestamps, “latest” symlinks, or other non-deterministic labels in file names. For example, replica trajectory files should be named predictably (e.g. replica_00.dcd, replica_01.dcd, …) rather than run1.dcd or latest.dcd. All outputs must live under the specified output_dir (which defaults to a known path like output/replica_exchange). The module itself must create any needed subdirectories (e.g. a bias/ folder for metadynamics outputs) so that files are neatly organized. Never write files to ad-hoc or implicit locations – no scattering of outputs outside the designated folder structure.
-Temperature Ladder Provenance: The temperature ladder (the list of replica temperatures) must be deterministic whether provided by the user or auto-generated. If the ladder is auto-generated (e.g. via an exponential spacing or a power-of-two scheme), it should be fully determined by the input parameters (min/max temp, number of replicas) with no random variation. The chosen ladder (sorted temperatures) must be logged and saved to an artifact for provenance. For example, the module writes the initial temperatures to a text or JSON file (e.g. temps.txt or provenance.json) so that downstream analysis and users know exactly which temperatures were used. There is no hidden “random ladder” – given the same inputs, the ladder will always be identical.
- Versioned Ladder Updates: The function or method that retunes the temperature ladder (e.g. tune_temperature_ladder()) must emit a new ladder configuration as a separate output, rather than silently altering the original. Tuning the ladder uses the recorded exchange statistics to propose a revised set of temperatures, which should be saved (for example, in temperatures_suggested.json). The new ladder is treated as a versioned update – the original ladder remains recorded in the run’s provenance, and the suggested ladder is written out with a clear version or timestamp in the filename if multiple tuning rounds are done (but no generic “latest” name). The key point is that the original temperature list is never mutated in-place or overwritten in the original config; any changes produce a new artifact. Callers must explicitly decide to adopt the new ladder (typically by re-initializing replicas with it).
- Config-Gated Biasing: Metadynamics biasing is strictly optional and controlled by the configuration. If use_metadynamics is false (or not set), the simulation runs unbiased. If true, bias variables (collective variables) and parameters (bias factor, hill height, deposition stride, etc.) must be explicitly defined by the code or config — no implicit or random bias settings. All bias parameters are fixed for the run and must be recorded in the output metadata. For example, if dihedral angles are used as bias CVs with a 1.0 kJ/mol hill height every 500 steps, these values should be documented (in logs or a metadata JSON) and remain constant throughout the run. There must be no on-the-fly adjustment of bias parameters that isn’t reflected in the config. Moreover, bias outputs (such as any HILLS files or bias state data) should be written to the output directory in a deterministic fashion. If metadynamics is not enabled, the module should produce no bias-related files or side-effects.
- Single Responsibility (No Demuxing): This module’s sole focus is running the replica-exchange simulation and recording its raw results. It does not perform trajectory demultiplexing or further analysis of the exchanges – those tasks are handled by the dedicated pmarlo.demultiplexing module (and subsequent analysis modules). The ReplicaExchange module should output the multi-replica trajectories and an exchange history log, but it should not attempt to reorder frames by temperature or produce “demuxed” trajectories on its own. Even though a convenience method ReplicaExchange.demux_trajectories() may be provided, internally it just delegates to the separate demux implementation. There is a clear separation of concerns: this module produces data for all replicas and records which swaps occurred; the demux module (downstream) uses that information to generate per-temperature trajectories.
- Explicit Demux Target: The reference or target temperature for trajectory demultiplexing must be explicitly specified by the user in the configuration or function call – it should never be implicitly assumed. In other words, if the user intends to extract a trajectory at a particular temperature (often the lowest temperature, e.g. 300 K), that temperature must be set in the config or provided as an argument (e.g. target_temperature=300.0 in the demux call). The module should not default to a particular temperature without the user’s consent (the default of 300 K in some functions is for convenience, but policy requires that the choice be confirmed in config). This ensures that downstream free energy calculations or analyses are always tied to a clearly defined reference temperature, and prevents accidental use of the wrong temperature data.
- Resumability and Checkpointing: The replica exchange simulation must support pause-and-resume without any divergence in results. Checkpointing is a first-class feature: the simulation state (including positions, velocities, random number generator state, current replica assignments, and accumulated bias state if metadynamics is on) should be saved to persistent checkpoint files at safe intervals. Resuming from a checkpoint with the same config and seed should produce outputs that exactly match an uninterrupted run. That means no duplicated or skipped frames and no reset of exchange statistics – the continuation should pick up as if the run had never stopped. Checkpoint files themselves should be written in a consistent, deterministic format (e.g. a JSON or pickle of state data, possibly with XML for OpenMM states) and stored in the output directory. Do not treat checkpoints as temporary; they are part of the run’s artifacts and should have stable names (e.g. including the simulation step or segment index rather than a date). The workflow must verify that when a run is split into two parts via a checkpoint, the combined outputs (trajectory DCDs, exchange logs, etc.) are bit-for-bit identical to a single continuous run of the same total length.

# Small explanation
The pmarlo.replica_exchange module implements Replica Exchange Molecular Dynamics (REMD) to enhance sampling across multiple temperatures. In a typical REMD setup, several replicas of the same system run in parallel at different temperatures, and periodic exchange attempts are made between neighboring temperature replicas. This module manages the initialization of multiple MD simulations (using OpenMM) at specified temperatures, handles the timing and criteria of exchange moves, and records the outcomes (which replicas swapped). By running high-temperature replicas, the system can cross energy barriers more easily, while low-temperature replicas provide high-detail sampling; exchanging configurations among them helps each replica explore a broader portion of phase space than it would in isolation.

Users interact with this module primarily through the ReplicaExchange class. A user provides the necessary inputs – a prepared PDB file (structure/topology of the system), force field files, and either a list of temperatures or parameters to generate one. Additional settings like exchange frequency (how often exchanges are attempted in MD steps), the output directory, the random seed, and optional features (such as whether to use metadynamics biases) are also supplied, typically via a RemdConfig dataclass or direct constructor arguments. On initialization, the ReplicaExchange class defines the temperature ladder (validates it’s strictly increasing and positive) and seeds the random number generator for reproducibility. It then prepares the simulation environment: building the system (forces, constraints, integrator), and if configured, setting up bias variables for metadynamics.

Once set up, the module runs the MD simulations in parallel and orchestrates exchanges. Every N steps (e.g. every 50 steps, as set by exchange_frequency), it attempts to swap configurations between certain pairs of replicas (usually adjacent in the temperature ladder) based on the Metropolis criterion. These exchanges are accepted or rejected according to the Boltzmann probability using each replica’s instantaneous potential energy and the temperature difference. The module keeps an internal log of how many attempts were made and which swaps succeeded, as well as a history of replica-state assignments over time (essentially which temperature each replica is at after each exchange). Throughout the simulation, each replica’s trajectory coordinates are being saved to disk (e.g. in DCD files), so at the end we have one trajectory file per replica covering the entire run (or each segment of the run if it was broken up by checkpoints).

The output of the ReplicaExchange module is a set of raw simulation data ready for post-processing. This includes the multi-replica trajectory files, basic metadata about the run, and exchange statistics. For example, the module writes out a provenance.json (containing information like the list of temperatures, the exchange frequency, the random seed, etc.) and a simple temps.txt listing all temperatures in order. If the simulation was stopped and resumed, the same files are updated or appended such that the final collection is consistent. The module does not itself analyze the exchange efficiency deeply or produce final “single-temperature” trajectories – instead, one would call demux_trajectories() (delegating to the demultiplexing module) to create a trajectory at a desired temperature, and then feed that into analysis steps (clustering, MSM construction, free energy calculation, etc.). In summary, pmarlo.replica_exchange is responsible for running the parallel tempered simulation in a reproducible manner, and producing standardized outputs that downstream modules can reliably consume.


# Key values
- Reproducibility: Every run of replica exchange should be completely reproducible. Identical inputs (same PDB, same temperature list, same random seed, same number of steps, and same code version/hardware configuration) must yield identical trajectories and exchange outcomes. This value underpins the entire design: the random number usage (for deciding exchange order or initial velocities) is explicitly seeded, the OpenMM integrator is used in a way that produces consistent results across platforms (e.g. by choosing a deterministic integrator and possibly the reference/CPU platform if needed), and even floating-point variations are minimized. If any aspect of the simulation could introduce nondeterminism (for example, using a GPU with slight differences in arithmetic), the module should document it and, if possible, provide an option to force deterministic behavior (as the code’s platform selector does by preferring a deterministic mode when a seed is set). Reproducibility extends to resumes: given a checkpoint, continuing the run later or on another machine should not alter the final combined output.

- Transparency & Provenance: The module must make the simulation setup and progression as transparent as possible to the users and to downstream processes. All crucial parameters and decisions are recorded. For instance, the exact temperature ladder is saved to a file, the target exchange acceptance probability (target_accept) is noted, and the random seed is stored in the provenance metadata. If the ladder was auto-generated, the method (e.g. exponential spacing from Tmin to Tmax with N replicas) should be evident from the code and ideally mentioned in logs. Any retuning of the ladder produces an artifact that shows the new temperatures and possibly the reason (acceptance stats) for the change. Similarly, if metadynamics biases are used, the bias configuration (which CVs, bias factor, deposition frequency, etc.) should be captured in logs or an output JSON (for example, including a note in provenance.json that bias was on and what the settings were). Nothing is done “behind the curtain” – every feature that affects the physics or output files should leave a trace in the metadata. This transparency ensures that later, one can audit how a given set of trajectories was produced (e.g. “these runs were at 300K–350K with biases on two dihedrals, bias height 1.0 kJ/mol every 500 steps, using seed 12345”).

- Determinism in Algorithms: Beyond just setting seeds, the algorithms and workflows chosen in this module prioritize determinism. That means the order of operations is fixed and not dependent on unpredictable factors. For example, the exchange attempt pattern between replicas is predetermined (e.g. always attempt exchanges between replica pairs (0↔1, 2↔3, etc.) in a fixed sequence or using a fixed pairing scheme each time step, rather than random pair selection). The integration time step and number of steps per segment are constant. If any stochastic thermostat or random velocity assignment is used, it is done with the controlled RNG. Even the checkpointing and writing of files happen at consistent intervals so that two runs with the same inputs will produce the same number of frames per file and the same breakpoints. This determinism also means avoiding any reliance on system time or order of dictionary iteration or other non-fixed ordering in Python – the code should explicitly sort or fix ordering when iterating over replicas, writing files, or aggregating statistics.

- Consistency & Naming Conventions: The output artifacts should follow a consistent naming convention that aligns with the rest of the PMARLO pipeline (the “canonical shard/export style”). Consistency here means that tools or scripts downstream can easily recognize files by name and parse needed information. For example, in reporting and analysis modules, output files encode things like the reference temperature and a short hash of the data/config. In the replica exchange module, while the outputs are more raw, they still should include identifying details either in the directory structure or file names. A consistent scheme might be: all files for a given run reside in a directory named for that run or config digest, and inside, replica_00.dcd … replica_NN.dcd store trajectories for replicas 0..N, a provenance.json provides run-level info, and possibly an exchanges.log or .json could hold exchange events or summary. We avoid ad-hoc or human-written filenames; everything follows a documented pattern (e.g. zero-padded indices for replicas, clear extension and prefix for metadata files). This consistency not only prevents confusion, but also ensures that if the module is integrated into larger automated workflows, those workflows can infer what files to expect and where.

- Single-Source of Truth for Data: The replica exchange module should treat its outputs as the authoritative data for the simulation and never retroactively modify them without record. Once a trajectory frame is written or a temperature list is recorded, that is the source of truth. If adjustments are needed (like changing the ladder), new outputs are created rather than silently updating old ones. Immutability of important records (like the original ladder or the original starting positions) is respected. For instance, the original temperature ladder and simulation parameters remain intact in the initial metadata; a tuned ladder is provided separately rather than “magically” replacing the old one in all contexts. This principle ensures that at any point, one can reconstruct what happened when by looking at the outputs in sequence. Checkpoints similarly capture the exact state at a given time; resuming from them uses that state rather than reinitializing anything unpredictably.

- Fail-Fast and Data Integrity: The module should aggressively validate inputs and internal state to avoid producing invalid data. If something is wrong – e.g., the user provides fewer than 2 temperatures, or a temperature is non-positive, or the PDB file can’t be loaded – the module must raise an error and halt rather than attempting to continue with defaults. This extends to runtime checks: if at any exchange step the data is inconsistent (say a replica’s context is missing or an energy calculation fails), the code should log an error and stop cleanly. Under no circumstances should it output partial or corrupted trajectory files without the user knowing. Each output file that is produced should be complete and usable. For example, if an exchange is attempted but some trajectories can’t be written due to an I/O error, that should cause an exception; we do not allow silent skipping of frames. By failing fast on anomalies, we maintain the integrity of the dataset – either a run produces a full valid dataset, or it stops and alerts the user to fix the issue.

- Scientific Correctness: The physical correctness of the REMD implementation is non-negotiable. Exchanges must use the proper Metropolis criteria based on the potential energy difference and the inverse temperatures (β values) of the two replicas. The module must compute acceptance probabilities accurately and apply them without bias. Likewise, if metadynamics is enabled, the bias forces added to the simulations must follow the defined method (OpenMM’s Metadynamics is used in a standard way with the chosen bias variables). The outcome is that the ensemble of states collected (especially after using demux to isolate a temperature) truly represents a canonical ensemble at that temperature (assuming sufficient equilibration and exchange). No shortcuts should undermine this: e.g., do not force acceptance to meet the target probability – the target acceptance is achieved by adjusting the ladder, not by fudging the Monte Carlo criterion. This key value ensures that while we enforce determinism and reproducibility, we do so in a way that stays true to the statistical mechanics underpinning REMD.


# Best practices
- Always set a random seed: When launching a replica exchange run, provide a specific random_seed in the configuration. This guarantees reproducibility across runs and even across different machines. Depending on the environment, also consider fixing the computational platform for consistency (for example, if two machines have different GPUs, minor floating-point differences could arise; running on a CPU or using OpenMM’s deterministic double precision mode can mitigate this). By explicitly setting the seed and platform, you make the run deterministic and portable.

- Choose a unique output directory per run: Do not reuse the same output_dir for different simulation runs unless they are part of a planned continuation (checkpoint resume). It’s best practice to incorporate a meaningful name or hash in the output path (for instance, include an experiment ID or a short digest of the config). This prevents files from different runs clobbering each other. The module will create the directory if it doesn’t exist; if it does exist and contains old data, consider cleaning it or pointing to a new directory to avoid confusion. Keeping outputs separate per run makes it easy to compare runs and ensures canonical naming stays collision-free.

- Validate the temperature ladder before running: Ensure the list of temperatures is correct and sorted ascending. The module already checks for positivity and strict increase (and will error out if the ladder is invalid), but as a user or developer, you should double-check that the temperature range and spacing are appropriate for the desired target acceptance. If you’re generating the ladder via config (e.g., specifying min, max, and count with a certain mode), verify the resulting list (the module logs it). An ill-chosen ladder (too few replicas or too wide a gap) can lead to very low exchange rates or wasted replicas. It’s easier to adjust this upfront than to realize after a long run that the acceptance was, say, 2% or 90%.

- Monitor exchange statistics and retune early: During test runs or initial segments, use the provided tune_temperature_ladder() (or the underlying retune_temperature_ladder function) to check if the current ladder meets the target acceptance rate. It’s a good practice to run a short preliminary simulation (with the same settings) to gather exchange stats, then invoke the tuning function to get a suggested ladder, and adopt that for the full production run. When using tune_temperature_ladder(), remember to re-run setup_replicas() afterwards as required – the module will inform you that the replicas should be reinitialized with the new temperatures. By tuning in a controlled manner (and saving the new ladder JSON), you avoid mid-run changes and ensure the main production run starts with an optimal ladder. Always keep the original ladder’s record; if you do multiple tuning iterations, document each iteration’s suggested list (perhaps by versioning the output file names or storing them separately).

- Enable or disable metadynamics deliberately: If you plan to use biasing to accelerate sampling (metadynamics), explicitly set use_metadynamics=True in the config and define the bias variables of interest. Use the helper functions (if available) to prepare bias variables (for example, a provided setup_bias_variables(pdb_file) can select certain torsions). Understand the default bias parameters – by default, the module uses a biasFactor of 10, hill height of 1.0 kJ/mol, added every 500 steps. If those defaults are not suitable, you’ll need to adjust the code or future config options (currently not exposed, so it might require code changes). The key practice is: don’t accidentally run biases – if you don’t want them, turn the flag off (to avoid any overhead and file outputs in bias/). Conversely, if you do want them, ensure the bias directory is being populated and the biasing is actually happening (check logs for messages about bias variables being added). And note that biasing changes the ensemble; use it only if you have a plan to incorporate it into analysis (e.g. well-tempered metadynamics to later reweight free energies).

- Use checkpointing for long runs: Plan checkpoint intervals in your runs, especially for long simulations, to guard against data loss and to enable exact restarts. For example, if you intend to run 50 million steps, you might checkpoint every 5 or 10 million. If the CheckpointManager or similar utility is available, configure it to save state periodically. When resuming, always verify that the module loaded the checkpoint correctly: it should report something like “Restoring replica exchange from checkpoint…” and show that it restored the random state and exchange counts. After resume, double-check that the new output files continue from where the last left off (frame counts align, no overlap or gap). Following this practice ensures that an interrupted run (due to time limits or crashes) can be continued seamlessly, and you won’t have to restart from scratch. It’s also a good idea to archive the checkpoint files as part of your experiment data, in case you need to reproduce or extend the simulation later.

- Never manually alter output files: Avoid the temptation to manually edit or concatenate the trajectory or log files outside of the provided tools. For example, if you have two halves of a run, prefer using the module’s own mechanisms (or demux’s stitching logic) to join them, rather than editing the binary DCD files or JSON by hand – such actions could break checksums or the expected format. Similarly, do not remove or rename files in the output directory during a run; the module expects its known files (like replica_00.dcd or checkpoint files) to be present. If cleanup is needed, do it after the run or before a fresh run, not in the middle. This best practice maintains consistency and avoids confusing the pipeline.

- Inspect outputs for completeness: After the run finishes, verify that all expected files are present and complete. There should be one DCD per replica (each file roughly the same size if run length per replica is equal), a provenance JSON, a temps list, and possibly a suggested temps file if tuning was invoked. If metadynamics was on, there should be a bias/ directory with bias state files (e.g. bias.txt or hills files updated periodically). Check the provenance.json for the run parameters and ensure they match what you intended (e.g. correct number of replicas, correct random seed, etc.). Also, consider calling get_exchange_statistics() on the ReplicaExchange object or examining any exchange summary output to see the achieved acceptance rates and round-trip times – this can inform future improvements. Being diligent in inspecting outputs helps catch any anomalies early (for instance, if one replica’s DCD is significantly shorter, that could indicate a problem in writing or an early termination for that replica).


# Wanted outcomes
- Exact Reproducibility: The foremost outcome is that a replica exchange simulation can be reproduced exactly. If two scientists run the same REMD configuration, or if the same run is repeated at a later date, the output trajectories, exchange log, and all metadata will match byte-for-byte. This includes after a pause: a run resumed from a checkpoint yields a final trajectory that is indistinguishable from a single continuous run. Achieving this outcome means users can have complete confidence in comparing results across runs and in sharing data, knowing it isn’t tainted by hidden randomness or machine differences.
- Complete and Organized Outputs: Each run produces a well-defined set of output files, and nothing more. For example, suppose a run is configured with 4 replicas at temperatures [300,330,360,390] K. A successful outcome would be an output directory containing exactly these files: replica_00.dcd through replica_03.dcd (each containing the coordinates for one replica’s frames from start to finish), a provenance.json capturing the key run settings (including the temperature list, which should list 300,330,360,390), a temps.txt or similar simple list of temperatures, and perhaps an exchange_stats.json (if implemented) or console log summarizing acceptance. If any retuning was done mid-run or between segments, there should be a temperatures_suggested.json (or with a version number) recording the new ladder. If metadynamics was active, there will be a bias/ folder with files like biasstate.xml or HILLS.csv as generated by OpenMM. Importantly, there will be no extraneous files like temporary logs or “latest.dcd” links – just the canonical set. This outcome means that anyone looking at the directory can immediately recognize and use the contents without guesswork.
- Efficient Sampling (Target Acceptance Met): Although the spec focuses on engineering requirements, a desired scientific outcome of this module is that the exchange scheme performs as intended. That means the temperature ladder either initially or after tuning yields an exchange acceptance rate close to the target (e.g. ~30% per neighbor pair). When this outcome is met, replicas effectively diffuse in temperature space (which one can verify via the exchange history or round-trip times). In practical terms, a wanted outcome is that the compute_exchange_statistics() function shows an overall acceptance rate near the target and reasonable round-trip times for replicas to shuttle from low to high temperature and back. Achieving this indicates that the ladder spacing was well-chosen and the simulation is making good use of the replica exchange mechanism. If this outcome isn’t met, the module at least makes it obvious (through logged acceptance rates) so the user can adjust and re-run.
- Seamless Integration with Demux and Analysis: The outputs of the replica exchange module should feed directly and smoothly into the demultiplexing and analysis stages of PMARLO. A successful outcome is that right after a run, a user can call the demux function with the ReplicaExchange object (or via command-line pipeline) and obtain a demux_Txxx.dcd and corresponding .meta.json for the chosen temperature. That demuxed trajectory should accurately represent what one replica would have experienced had it stayed at that temperature throughout – and since we insisted on determinism, even the demux process is deterministic given the exchange history. Then, subsequent modules (MSM construction, featurization, clustering, etc.) consume those outputs without any special-case handling for REMD. In essence, the wanted outcome is that REMD becomes a drop-in part of the workflow that doesn’t complicate the later analysis: the data format and naming align with what downstream tools expect (e.g. a list of shard files or a combined trajectory plus metadata). This outcome validates the module’s adherence to the “canonical shard” style and single-responsibility principle.
- Robust Restart and Extension: Another desired outcome is that users can extend simulations or recover from crashes effortlessly. If a user wants to double the simulation length, they should be able to take the final checkpoint of an initial run, start a new run from that checkpoint, and end up with a longer trajectory that is exactly as if the original run had been twice as long from the start. The outputs in this case might be split across runs (two sets of replica DCDs), but when concatenated or read in sequence, they form a continuous trajectory per replica. The module’s design (through checkpointing and output naming) should facilitate this. For instance, we expect that the second run would either append to the same DCD files or produce new files that can be concatenated without overlap or loss. The outcome is that there is no penalty or complexity in restarting – scientists can break their simulations into chunks and still get the same scientific results as one monolithic run, which is crucial for utilizing limited computing resources effectively.
- Extensible Design for Future Features: While not immediately visible in outputs, a wanted outcome from a design perspective is that this module can evolve without breaking the established conventions. For example, if in the future the project introduces Hamiltonian replica exchange (where differences are not just temperature but force field parameters), or multiple dimensions of exchange (temperature + pH, etc.), the module’s structure should accommodate it. The current outcome we want is that the code is organized (with clear subcomponents like system building, exchange logic, diagnostics, etc.) and documented (via this spec) such that new developers can add features while preserving determinism and reproducibility. Concretely, if a new ladder generation mode or an alternative integrator is needed, they can be added behind the scenes and logged, without altering the external behavior. This forward-compatibility is an outcome that ensures the longevity of the module – today’s strict rules remain applicable even as REMD methods diversify.

# Examples
- Deterministic run replay: Suppose you run a replica exchange simulation with 4 replicas at [300, 310, 320, 330] K, using random_seed = 42. The module produces outputs in output/replica_exchange/run123/ including four DCD files and a provenance JSON with seed 42 and the temperature list. If you re-run the same setup (same PDB, same config) with random_seed = 42, you will find that each pair of corresponding output files is identical. replica_00.dcd from the second run matches byte-for-byte to replica_00.dcd from the first run, and likewise for the others; the JSON metadata is also identical (except perhaps a timestamp if it had one, but per policy it should not). This confirms that the simulation is fully deterministic. Any divergence (e.g. if the total energies in the trajectories differed) would indicate a bug. The example demonstrates confidence that results can be reproduced exactly.

- Ladder retuning without mutation: Imagine you start a run with a temperature ladder [300, 350, 400, 450] K aiming for 30% acceptance, but partway through, you notice the acceptance is very low between 300 and 350 K. You call tune_temperature_ladder() on the ReplicaExchange object. The function calculates a new set of temperatures, perhaps [300, 330, 360, 450] K, and writes them to temperatures_suggested.json. According to policy, it does not override the original ladder in the provenance of the current run – that original remains in temps.txt (and in memory it’s still accessible until you apply the new one). You then stop the simulation after the current segment, update the config to use the suggested ladder, and resume (or start a new run) with [300,330,360,450]. The new run’s outputs reside in a new directory or carry a new version tag, and its provenance JSON clearly shows the updated ladder. In this process, the original run’s data is still valid and traceable (with the old ladder), and the new run is a clean extension. The example highlights that ladder changes are explicit and versioned, never implicit.

- Metadynamics off vs on: By default, your config might have use_metadynamics=True, but if you set it to false, the module should run a plain REMD with no bias. For example, with use_metadynamics=False, when you call setup_replicas(), you’ll notice in the logs that it does not mention adding bias variables and the bias/ directory is either not created or remains empty. The output files will just be the DCDs and provenance, with no reference to bias. If instead use_metadynamics=True and your system has definable bias CVs (say two dihedral angles), the log will show “Added phi dihedral 1 as bias variable…” etc., and a bias/ directory will appear containing files like biasfactor10_hills.txt (for example). Both runs are deterministic, but the one with biases will have a different ensemble. Crucially, if you compare the outputs, you’ll see that the biased run’s provenance JSON might include a note or flag indicating bias was applied, whereas the unbiased run’s does not. This example ensures that the presence or absence of metadynamics is completely governed by the config and leaves an obvious footprint in the output (no “sneaky” biases).

- Demultiplexing with explicit target: After running a simulation with replicas at 300–450 K, you want to analyze the 300 K behavior. You call remd.demux_trajectories(target_temperature=300.0, ...). Because you explicitly provided 300.0, the module goes ahead and produces demux_T300.0K.dcd (and its metadata JSON). If you had not provided that parameter, by policy the code would either default to 300.0 with a warning or error out prompting you to specify it (depending on how the interface is implemented). In practice, let’s say it defaults but logs: “No target temperature specified, defaulting to 300.0 K.” Our policy dictates that this should be made explicit. The outcome in the example is that you get a demultiplexed trajectory for 300 K with a clear file name indicating 300K. There is no ambiguity which temperature was chosen – it’s in the file name and metadata. If you wanted a different temperature, you re-run demux with that specific value. This example ensures that analysis choices are deliberate and traceable, not left to guesswork.

- Resume vs uninterrupted comparison: You run a 10 ns REMD simulation in two segments of 5 ns each. In the first segment, the module writes out replica_00.dcd … replica_03.dcd each containing 5 ns worth of frames (say 5000 frames each for example). A checkpoint at 5 ns is saved. You then resume from that checkpoint for another 5 ns. The module, instead of creating new files, appends to the existing DCD files (or opens them in append mode via the ClosableDCDReporter). At the end, you still have one DCD per replica, now with 10000 frames each. You compare this to a single 10 ns run (if you had done one in one go) – the files are identical. The exchange log in provenance might show a continuous count of exchanges from 0 to N without reset. This example demonstrates the seamlessness of checkpointing: splitting the job had zero effect on the outputs aside from maybe some log messages about checkpoint restore. If the module instead had created separate files (like a second set of DCDs), then an extra step of concatenation would be needed, and one would verify that concatenating yields the same result. Either way, the frames and their order are exactly the same. This gives users confidence that they can break runs into chunks for practical reasons and not worry about data consistency.

- Consistent naming convention in practice: In an experiment, you run two different REMD setups – one with a ligand bound and one without. You give them different output directories, say output/replica_exchange/bound/ and output/replica_exchange/apo/. Inside each, the file naming is consistent: each has replica_00.dcd, replica_01.dcd, ..., etc., as well as provenance.json. To differentiate the datasets, you rely on the directory names or you could have included an identifier in the file names, but per policy you did it via directory. Now, an analysis script that expects this structure can easily load “all .dcd files in the given output dir” for processing. It doesn’t need to know the details of how many replicas or which naming scheme, because it’s standardized (replica indices). Moreover, if you open provenance.json in each directory, you find entries like "temperatures": [300.0, 310.0, ...], "random_seed": 12345, "exchange_frequency_steps": 100, etc., giving you immediate context about those runs. This consistency is exactly the outcome we want – any PMARLO developer or user can navigate the outputs without a custom guide for each case.

# Files description

## replica_exchange.py
Main simulation class and logic. This file defines the ReplicaExchange class, which encapsulates the REMD simulation. Key methods include:

__init__: sets up initial parameters (PDB path, list of temperatures or generates one, output directory, frequencies, seeds, etc.), creates the output directory, and seeds the RNG. It also initializes internal structures for tracking replicas and exchange statistics (like exchange_attempts, exchanges_accepted, and dictionaries for pair attempt/accept counts). No actual MD is started here unless auto_setup is True, but it prepares everything.

_generate_temperature_ladder: an internal helper that constructs a default temperature list if none is provided (currently using an exponential/geometric spacing between a default min and max). This uses utilities from pmarlo.utils.replica_utils and is deterministic given its inputs.

setup_replicas(bias_variables=None): sets up the OpenMM Simulation objects for each replica. It loads the system (via load_pdb_and_forcefield and create_system), possibly resumes positions if a restart PDB is given, applies metadynamics biases if bias_variables are provided (via setup_metadynamics), selects a computing platform (with a preference for deterministic behavior if seeded), and then for each temperature it creates an integrator and simulation context. It attaches a DCD reporter to each simulation by calling _add_dcd_reporter, which opens a trajectory file (naming it replica_<index>.dcd) and sets the frame stride. It then minimizes and equilibrates each replica (there are internal helper steps for a two-stage minimization). After this, all replicas are ready to run and their initial states are recorded.

auto_setup_if_needed(): a convenience that calls setup_replicas if it hasn’t been done yet. This allows a user to delay setup until just before running.

plan_reporter_stride (not explicitly described above, but implied by the check in setup_replicas): likely a method (or part of another routine) that determines reporter_stride (how often to save frames) such that a target number of frames per replica might be achieved. This is influenced by config’s target_frames_per_replica. The stride is computed to balance file size and resolution. This stride is then used when adding DCD reporters.

save_checkpoint_state(): gathers all necessary information to represent the current state of the simulation. It returns a dictionary containing the number of replicas, current temperatures, the mapping of replica indices to temperature states (replica_states) and vice versa (state_replicas), the counts of attempts and accepts, the full exchange history (list of state assignments after each exchange), the random seed and current RNG state, and crucially the serialized states of each replica’s context (positions, velocities, energies) in XML form. It also stores the reporter stride info. This dictionary can then be saved (by an external CheckpointManager or by the user) to disk (e.g. as JSON or pickle).

restore_from_checkpoint(checkpoint_state, bias_variables=None): the counterpart to above, which takes a saved state dict and restores the ReplicaExchange object’s state. It resets the exchange counts, history, and RNG to what was saved. It reinitializes replicas if needed (calling setup_replicas if the checkpoint indicates setup wasn’t done yet), then applies stored reporter strides, and restores each replica’s Simulation context from the serialized XML states. After calling this, the simulation is ready to continue as if it had never stopped. This method ensures that continuity is maintained (it even logs that exchange stats were restored). If bias was in use, the caller should pass in the same bias variables list so that metadynamics can be reconnected appropriately (the checkpoint includes bias state via the XML context, but reinstantiating the bias forces may require providing the variables again).

tune_temperature_ladder(target_acceptance=None): uses the statistics accumulated so far to suggest a new temperature ladder. Internally it calls diagnostics.retune_temperature_ladder with the current temperatures and pair attempt/accept counts, writing the suggestion to a JSON file (in the output directory, named e.g. temperatures_suggested.json). It then updates self.temperatures to the suggested list and self.n_replicas accordingly, but does not automatically reconfigure the simulations. The docstring notes that you should call setup_replicas again after this if you intend to continue the run with the new ladder. Essentially, this function is a helper to compute a new ladder on the fly. According to our policy, this is where we ensure it doesn’t overwrite original logs of temperatures.

demux_trajectories(target_temperature=..., equilibration_steps=..., progress_callback=None): a thin wrapper that calls out to the demultiplexing module’s demux_trajectories function, passing itself (which contains all needed data). This returns the path to the newly written demuxed trajectory or None if demux failed. This method is provided for convenience so that a user can simply do remd.demux_trajectories(target_temperature=300.0) after a run, but it doesn’t implement demux logic itself.

get_exchange_statistics(): compiles exchange outcomes into a dictionary of metrics. It uses the compute_exchange_statistics function (described below) to get things like per-pair acceptance rates, round-trip times, etc., and combines that with overall totals (total attempts, total accepted, overall acceptance). This provides a quick summary that can be logged or saved. It returns an empty dict if no exchanges have been recorded (e.g. if called before any exchanges happened).

There may also be a run() method or a run_remd_simulation() function (seen at the bottom of the file) which orchestrates the execution: taking total steps, splitting into equilibration and production, handling checkpoint manager callbacks, etc. The run_remd_simulation function likely demonstrates how to use the class: it might call auto_setup_if_needed(), then perform an equilibration phase (perhaps gradually heating replicas), then a production phase with periodic exchanges and checkpointing. In any case, replica_exchange.py is the core of the module, implementing all high-level behaviors.

## diagnostics.py
Exchange statistics and ladder tuning utilities. This file provides standalone functions to analyze exchange performance and to calculate new temperature suggestions. Key contents:

compute_exchange_statistics(exchange_history, n_replicas, pair_attempt_counts, pair_accept_counts): This function computes derived metrics from the raw exchange log. It calculates how long each replica took to return to its starting state at least once (round-trip time), constructs a matrix of probabilities that replica i is found in state j (which could indicate mixing), and compiles acceptance rates for each pair of neighboring replicas. It returns a dictionary containing entries like "replica_state_probabilities" (an n_replicas × n_replicas matrix of visit fractions), "average_round_trip_time", a list of sample "round_trip_times", and a "per_pair_acceptance" mapping (e.g. "(0, 1)": 0.25 meaning 25% acceptance between replica 0 and 1). This is used to assess if the simulation is well-equilibrated across replicas.

compute_diffusion_metrics(exchange_history, exchange_frequency_steps, spark_max_points=200): Computes how quickly replicas diffuse in index space. It looks at the exchange history and calculates the mean absolute displacement in “temperature index” per exchange attempt, and scales it per 10k MD steps for a normalized metric. It also produces a down-sampled sparkline (list of average displacements over time) that can be used for plotting. These metrics can hint at how well replicas are swapping around (higher means more movement). The function returns a dict with keys like "mean_abs_disp_per_sweep" and "mean_abs_disp_per_10k_steps", and a "sparkline" array.

retune_temperature_ladder(temperatures, pair_attempt_counts, pair_accept_counts, target_acceptance=0.30, output_json="output/temperatures_suggested.json", dry_run=False): This is the algorithm for suggesting a new set of temperatures to reach a desired acceptance. It uses an analytic formula based on Kofke’s method: using the inverse error function (erfcinv) to adjust the beta (1/T) spacing. It iterates through each neighbor pair’s current acceptance rate, computes the needed β difference to achieve the target, and from that derives an average optimal Δβ. Then it calculates how many replicas would be needed to cover the full β range from β(min) to β(max) with that Δβ, and generates that many points linearly in β (which corresponds to a geometric spacing in T). The result is a new ladder (which could have more or fewer points than the original). The function prints a summary to stdout (including each pair’s acceptance and suggested Δβ), writes the suggested temperatures list to the specified JSON file (creating the directory if needed), and returns a dictionary with "global_acceptance" (the overall acceptance across all pairs), "suggested_temperatures" (the list of new T values), and "pair_statistics" (the per-pair acceptance and suggested Δβ that were calculated). If dry_run=True, it does not actually remove or add replicas in the suggestion (it still writes the file and returns the dict, but it might print a “predicted speedup” instead of enforcing an exact count – in practice, the code uses dry_run to possibly just report stats without altering the replica count logic). In the context of the module, this function is called with dry_run=True to avoid automatically changing n_replicas in the running simulation. The expectation is that the user will review the suggestion and then apply it. This function is crucial for adjusting the ladder deterministically based on measured data; given the same acceptance stats, it will always output the same suggestion, ensuring consistency across runs.

## system_builder.py
System preparation and bias setup. This file contains helper functions for preparing the OpenMM System and adding metadynamics biases:

load_pdb_and_forcefield(pdb_file, forcefield_files): Loads the PDB file (containing coordinates and topology) and the specified force field XML files, returning an OpenMM PDBFile and ForceField object. This abstracts the OpenMM calls to create the system (so that the main class doesn’t have to deal with file I/O directly).

create_system(pdb, forcefield): Using the given PDB topology and ForceField, it creates an OpenMM System object with certain standardized settings (PME for long-range electrostatics, hydrogen mass repartitioning, SHAKE constraints on bonds involving hydrogen, a 0.9 nm cutoff, etc.). It also ensures only one center-of-mass motion remover is present. This yields the system on which forces and integrators will act.

log_system_info(system, logger): Logs the basic details of the system (number of particles, number of forces, and each force’s type) to the logger. This is helpful to record what kind of simulation we’re running (e.g. if it has a barostat or custom forces, it would show up here).

setup_metadynamics(system, bias_variables, reference_temperature_k, output_dir): If bias variables are provided (a list of OpenMM BiasVariable objects), this sets up an OpenMM Metadynamics object on the given system. It creates a sub-directory bias/ in the output directory to store bias files. Then it instantiates the Metadynamics class with the system and variables, using the reference temperature (usually the lowest temperature replica’s temperature) and fixed parameters: biasFactor=10.0, height=1.0 kJ/mol, frequency=500 steps, and saveFrequency=1000 steps. The returned object (meta) is the handle to the ongoing metadynamics, which automatically adds bias forces during the simulation. If bias_variables is empty or None, this function returns None (meaning no bias will be applied). This design expects that biasVariables are chosen before calling; typically one would call a function to create bias variables from a PDB (like targeting certain torsion angles) – such a function exists in replica_exchange.py (see setup_bias_variables(pdb_file) at the bottom of that file). The bias settings here are currently hard-coded, reflecting policy that they remain fixed. If in the future these need to be configurable, this function would be the place to pass through those parameters from config. For now, it ensures the bias is applied consistently. The output of Metadynamics (like a HILLS file) will be written in bias/ automatically by OpenMM at the specified saveFrequency; those files will follow OpenMM’s naming scheme (which is deterministic given the run) and be contained in the output directory.

## platform_selector.py
Hardware platform selection for OpenMM. This module likely contains logic to choose the best OpenMM computing platform (CPU, CUDA, OpenCL, etc.) and set any platform-specific properties. It probably has a function like select_platform_and_properties(logger, prefer_deterministic) that returns a tuple of (Platform, properties_dict). If prefer_deterministic=True, it might select the CPU reference platform or set the CUDA platform to deterministic modes (e.g. using double precision, disabling stochastic hardware optimizations). The idea is that if a seed is provided, we lean towards bitwise identical results (which often means using CPU or determinism flags), whereas if performance is more important and nondeterminism is acceptable, we might use fast GPU settings. This file encapsulates those decisions. For example, it might check if CUDA is available and, if so, pick CUDA with a specific precision or random seed control, else fall back to CPU. It logs the choice for transparency. By centralizing this, the ReplicaExchange class simply asks for a platform with or without the deterministic preference, rather than hardcoding platform logic in multiple places.

## trajectory.py
Trajectory output helpers. Here we have the definition of ClosableDCDReporter (imported and used in the main class). This likely is a subclass or wrapper around OpenMM’s DCDReporter. Its purpose is to allow explicit closing of the file handle, which the base class might not expose until deletion. The ClosableDCDReporter probably inherits from openmm.app.DCDReporter and adds a close() method or ensures the file is properly closed when the simulation ends or when we call a certain function. The reason this exists is to support appending to DCD files on resume and to ensure file handles are released (preventing corruption or locks on Windows, for example). In usage, after running, the code calls _close_dcd_files() (as seen referenced in the run sequence) which likely finds all ClosableDCDReporter instances and calls their close method, flushing the data to disk. This guarantees that at the end of a run (or segment), the DCD files are properly written and can be reopened for appending if needed. The trajectory.py might also contain any other trajectory-handling utilities, but primarily it’s about the DCD reporter.

There might also be a trajectory_writer.py or similar in pmarlo.io (we saw references to MDTrajReader in the code for counting frames). However, those are part of the I/O utilities outside this module. Within replica_exchange, the main concern is writing the DCDs via the reporter and reading them for frame counts if needed.

## config.py
Configuration data structure. This file defines the RemdConfig dataclass which captures all user-configurable options for a replica exchange run. Fields include: pdb_file (or alias input_pdb), forcefield_files, temperatures (list of float or None if to be generated), output_dir, exchange_frequency, dcd_stride, use_metadynamics, auto_setup, target_frames_per_replica, target_accept (target acceptance probability for exchanges), random_seed, and a set of resume options (start_from_checkpoint, start_from_pdb, jitter_sigma_A, reseed_velocities, temperature_schedule_mode). The dataclass is frozen (immutable once created), aligning with our policy that configs shouldn’t be changed mid-run. In __post_init__, it resolves the PDB file path (preferring input_pdb if given, but that field is deprecated in favor of pdb_file). The various fields map to the parameters of the ReplicaExchange class. For example, if temperatures is None, the code will call _generate_temperature_ladder() possibly using temperature_schedule_mode to decide linear vs geometric scheme (though currently _generate_temperature_ladder uses a fixed exponential approach; future versions might use the mode field). use_metadynamics tells the user (and developers) whether to set up bias variables; however, the ReplicaExchange code as written doesn’t directly consult this flag except to possibly decide on bias variables. In practice, the pipeline might check config.use_metadynamics and if True, prepare bias_variables via setup_bias_variables before calling setup_replicas. All these config options ensure that nothing is hardcoded in the script – everything is controlled via a single config object, making runs reproducible and easily adjustable.


## init.py
Likely exports the main public API symbols, such as ReplicaExchange and RemdConfig, and maybe run_remd_simulation. It might also set a module version (for example, a __version__ or similar) if needed. The package docstring might briefly describe the module (“Tools for running Replica Exchange Molecular Dynamics simulations”). This is mostly boilerplate to make imports convenient (e.g. from pmarlo.replica_exchange import ReplicaExchange).


## Unit tests
The tests for this module (located under tests/unit/replica_exchange/) provide good examples and guardrails. For instance, test_temperature_ladder.py verifies that the ladder generators produce expected outputs (ensuring determinism and correctness of spacing). test_ladder_retuning.py checks that retune_temperature_ladder writes an output file and returns consistent global acceptance calculations. test_acceptance_logger.py checks the aggregation of acceptance stats (global acceptance computation). These tests serve as regression checks: if someone modifies the exchange criteria or ladder logic, the tests will catch changes to the outcomes. It’s important for maintenance that these tests remain passing; they effectively enforce parts of the spec (like deterministic ladder math and file output on tuning).

# Maintenance and evolution
Updating the Spec: Whenever this module is modified in a way that affects its behavior or outputs, this AGENTS.md document must be updated to reflect the change. For example, if a new file output is introduced (say an exchange log CSV), or if the naming scheme is adjusted (perhaps to include an index or hash), document it here under Unnegotiables and File descriptions. This spec is the contract that the module abides by. Developers should treat any deviation in code as a potential bug unless the spec is simultaneously revised. Regularly review this document especially after merging significant changes to ensure it stays accurate.

Versioning and Compatibility: The REMD module should carry a version (either the overall PMARLO version or a sub-module version) that gets bumped when breaking changes occur. If the format of outputs changes (for instance, if we decide to include a digest in DCD filenames, or change the structure of provenance.json), that should correspond to a version increment (e.g. PMARLO v0.3 -> v0.4) and be noted in the file names or metadata. Downstream tools that parse these outputs can use version info to adapt. Always aim for backward compatibility of critical pieces: for example, if adding a new field in provenance.json, don’t remove or reorder existing ones in ways that break old readers. If absolutely necessary to change something incompatible (like rename a key or output file), do it in a major version change and clearly communicate it (and consider providing a migration script for old data).

Extending Ladder Logic: If new temperature ladder strategies are added (e.g. a smarter algorithm that iteratively finds optimal temps, or user-defined ladders), ensure they remain deterministic. Any randomness in ladder design (for instance, Monte Carlo selection of temperatures) is not allowed unless it’s entirely seed-driven and reproducible – even then, it should be avoided in favor of analytic methods. Document any new strategy in the explanation section and log the chosen method in provenance. Additionally, if the ladder can now be generated in different ways, include the method name or parameters in the provenance (for example, "temperature_schedule_mode": "power_of_two" should appear if that mode was used).

Bias & Checkpoint Evolution: In the future, we may allow customization of metadynamics parameters via config (e.g. making biasFactor or hill height configurable). If so, those parameters must be added to RemdConfig and of course recorded in outputs. When implementing such changes, maintain the spirit of determinism (no adaptive changes at runtime unless they are purely formulaic). Also, checkpointing might be expanded to cover bias states more explicitly – if OpenMM’s metadynamics can dump its state, incorporate that into the checkpoint. Any improvements to checkpoint (like more frequent, or incremental checkpointing) should still guarantee that resume = continuous run. Always test after changes to checkpoint logic that starting and stopping at various points yields identical concatenated trajectories. If differences are found, that’s a bug to fix before release.

Demultiplexing Interface: As this module delegates demuxing, any changes in the demux module’s expectations need to be reflected here. For instance, if the demux module in a future version expects a slightly different metadata format (maybe it needs an updated DemuxMetadata schema), then ReplicaExchange should be updated to provide that (perhaps by writing additional info to demux_Txxx.meta.json). Maintaining alignment with the demux layer is crucial – they should evolve in lockstep or with clear interface contracts. If demux adds support for multiple target temperatures at once, this module might gain a method to produce all demuxed trajectories in one go; such additions should be documented and should not violate any existing rules (they’d just be new capabilities built on the same data).

Performance vs Determinism: Over time, we might find opportunities to speed up the REMD simulation (for example, by parallelizing exchanges or using advanced libraries). Any such changes must be carefully evaluated for their impact on determinism. For instance, multi-threading exchange attempts could introduce race conditions – that would be unacceptable unless strictly controlled. The maintenance principle is: do not sacrifice reproducibility for speed without a deliberate opt-in. If a non-deterministic performance mode is ever offered, it should be clearly off by default and outside the scope of normal usage. In general, prefer approaches that maintain the guarantee that results won’t change run-to-run. Always run the full test suite (including integration tests on different platforms if possible) after performance tweaks to ensure nothing subtle has changed in outputs.

Continual Testing and Verification: Use the provided unit tests and consider adding integration tests (longer running scenarios) whenever a new feature is introduced. For example, if we add a new exchange scheme or support for Hamiltonian REMD, write tests that verify the output structure and determinism of that mode. Keep a set of reference outputs (small ones) to compare against when refactoring. If an updated dependency (like a new OpenMM release) changes binary outputs (say different floating-point rounding in DCD), evaluate whether that affects our determinism claims and update documentation or expectations accordingly. The goal is to catch any deviation early. Maintaining this module means being vigilant that “deterministic and canonical” remains true with each code change. If a future change cannot avoid introducing nondeterminism (which should be extremely rare), it must be discussed openly and the spec updated to either forbid that change or to carve out a very specific exception (and even then, find a way to mitigate it).
