structural_alignment_types:
  - name: "CE-align"
    description: "CE-align, or Combinatorial Extension, is a method for pairwise protein structure alignment. It focuses on aligning backbone atoms by identifying and extending aligned fragment pairs, offering insights into protein function and evolutionary history."
    task_name: combinatorial_extension

  - name: "US-align"
    description: "US-align (Universal Structural alignment) is a unified protocol to compare 3D structures of different macromolecules (proteins, RNAs and DNAs) in different forms (monomers, oligomers and heterocomplexes) for both pairwise and multiple structure alignments. The core alogrithm of US-align is extended from TM-align and generates optimal structural alignments by maximizing TM-score of compared strucures through heuristic dynamic programming iterations. Large-scale benchmark tests showed that US-align can generate more accurate structural alignments with significantly reduced CPU time, compared to the state-of-the-art methods developed for specific structural alignment tasks. TM-score has values in (0,1] with 1 indicating an identical structure match, where a TM-score â‰¥0.5 (or 0.45) means the structures share the same global topology for proteins (or RNAs)."
    task_name: universal

  - name: "FATCAT"
    description: "FATCAT (Flexible structure AlignmenT by Chaining Aligned fragment pairs allowing Twists), Protein structures are flexible and undergo structural rearrangements as part of their function. FATCAT (Flexible structure AlignmenT by Chaining Aligned fragment pairs allowing Twists) is an approach for flexible protein structure comparison. It simultaneously addresses the two major goals of flexible structure alignment; optimizing the alignment and minimizing the number of rigid-body movements (twists) around pivot points (hinges) introduced in the reference structure."
    task_name: fatcat

structural_complexity_levels:
  - name: "Protein Chains"
    description: A protein chain refers to a single, linear sequence of amino acids that are covalently bonded together through peptide bonds. This chain, also known as a polypeptide, represents the primary structure of a protein. The sequence in which the amino acids are arranged dictates the protein's unique characteristics and functions. After synthesis, the protein chain undergoes folding, where it adopts a specific three-dimensional structure essential for its biological activity. Protein chains can function independently or combine with other chains (which can be identical or different) to form a functional protein complex. The term "protein chains" emphasizes the linear aspect of proteins before they fold into their functional conformations.

  - name: "Secondary Structures"
    description: Secondary structures refer to the local spatial arrangement of the protein's backbone, excluding the side chains of the amino acids. The two most common types of secondary structures are alpha-helices and beta-sheets. Alpha-helices are right-handed coils stabilized by hydrogen bonds between the backbone atoms, while beta-sheets consist of two or more strands aligned next to each other, forming a sheet-like structure also stabilized by hydrogen bonding. These structures are fundamental components of a protein's overall three-dimensional conformation and play critical roles in defining its function. Secondary structures are formed as a result of hydrogen bonds between the amide hydrogen and carbonyl oxygen atoms in the peptide backbone, and their formation is driven by the protein's primary sequence.

sequence_embedding_types:
  - name: "ESM"
    description: >
      ESM-2 (650M parameters) is a transformer-based protein language model trained with masked language modeling on UniRef50. It learns evolutionary and functional features directly from raw sequences without requiring multiple sequence alignments. With 33 layers and 650M parameters, this model is optimized for accurate structure and function prediction and is widely used for embedding extraction, contact map prediction, and unsupervised learning of protein features.
    task_name: esm
    model_name: facebook/esm2_t33_650M_UR50D

  - name: "Prost-T5"
    description: >
      ProstT5 is a multi-modal protein language model built on top of ProtT5 (encoder-decoder, T5-style). It is designed to encode protein sequences and jointly predict 3Di structural representations. The model leverages sequence-to-sequence translation between amino acid sequences and coarse-grained structural states, learning from both structure and sequence. ProstT5 enables improved representation for downstream tasks such as contact prediction, functional annotation, and classification, and has approximately 1.2 billion parameters.
    task_name: t5
    model_name: Rostlab/ProstT5

  - name: "Prot-T5"
    description: >
      ProtT5-XL-UniRef50 (approx. 1.2B parameters) is a large-scale transformer encoder-decoder model trained on UniRef50 using the T5 architecture. It generates high-dimensional representations of protein sequences by leveraging both local and global sequence dependencies. The model was trained in a self-supervised fashion using masked span prediction and is known for its high transferability across a range of structure and function prediction tasks.
    task_name: t5
    model_name: Rostlab/prot_t5_xl_uniref50

  - name: "Ankh3-Large"
    description: >
      Ankh3-Large is a 620M parameter encoder-only transformer model pre-trained using the T5 architecture with the [NLU] prefix. It is optimized for protein understanding tasks via masked span prediction. Trained on a diverse protein sequence corpus, Ankh3 encodes both semantic and structural protein information. Its architecture supports efficient inference and it serves as a drop-in replacement for ProtT5 in many protein ML pipelines.
    task_name: ankh3
    model_name: ElnaggarLab/ankh3-large

  - name: "ESM3c"
    description: >
      ESM Cambrian 600M is part of the ESM C model family, a new generation of protein language models trained at scale on UniRef, MGnify, and JGI datasets. With 600M parameters, 36 layers, and rotary embeddings, ESM C is optimized for masked language modeling over protein sequences. It delivers improved contact precision, scalability, and efficiency over ESM-2, enabling high-quality structural and functional embeddings without alignments. ESM C embeddings capture biological priors learned directly from sequence data through unsupervised learning, setting a new state of the art in protein representation learning.
    task_name: esm3c
    model_name: esmc_600m



structure_embedding_types:
  - name: "3di"
    description: "Foldseek's 3di embeddings encode the 3D structure into sequences over a structural alphabet, capturing tertiary amino acid interactions."
    task_name: "3di"
    model_name: "mini3di"


prediction_methods:
  - name: "Cosine Similarity"
    description: "Cosine similarity measures the cosine of the angle between two non-zero vectors of an inner product space. It is used to measure the similarity between two embedding spaces."

