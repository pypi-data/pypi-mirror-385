import logging
from typing import List, TypedDict, Any, Literal, Dict
from langchain_core.language_models.chat_models import BaseChatModel
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langgraph.graph import END, StateGraph
from eagle.memory.semantic.base import SemanticMemory

# --- Logging Setup ---
# Configures basic logging to display timestamps, log levels, and messages.
# This helps in tracing the execution flow of the graph.
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')


def get_rfg_chain(llm: BaseChatModel) -> Any:
    """
    Creates a LangChain Expression Language (LCEL) chain for generating a hypothetical document.

    This chain takes a user query and a set of reference documents and generates a new,
    comprehensive passage that aims to answer the query in the style of the references.

    Args:
        llm (BaseChatModel): The language model to use for generating the document.

    Returns:
        A runnable chain that takes a dictionary with "query" and "reference_docs"
        and outputs the generated document as a string.
    """
    BASE_DOC_PROMPT = """
        Task: Write a single, new, and comprehensive passage that provides a definitive answer to the user's query.
        - Use the 'Query' provided below as inspiration or incorporate their ideas if they are relevant and helpful.
        - **Closely match the style of the 'Reference Passages'. This includes their approximate length (e.g., a single concise sentence), tone, and overall structure.**
        - The Generated Passage should be logical and maintain a respectful tone.
        - Your response must ONLY be the text of the final Generated Passage, with no introduction or explanation.

        Query: {query}

        Reference Passages:
        {reference_docs}

        Generated Passage:

        """
    prompt_template = ChatPromptTemplate.from_template(BASE_DOC_PROMPT)
    return prompt_template | llm | StrOutputParser()

# --- Data Models and State ---

class RFGGraphState(TypedDict):
    """
    Represents the state of our Retrieval-Feedback Generation (RFG) graph.

    This dictionary-like object is passed between the nodes of the graph. Each node
    can read from and write to this state, allowing information to flow through the graph.

    Attributes:
        question (str): The user's original question.
        hypothetical_document (str): The document generated by the LLM to guide the final search.
        reference_docs (List[Dict[str, Any]]): The list of initial documents for context generation.
        documents (List[Dict[str, Any]]): The list of final documents retrieved from the knowledge base.
    """
    question: str
    hypothetical_document: str
    reference_docs: List[Dict[str, Any]]
    documents: List[Dict[str, Any]]

# --- Main Graph Class ---

class RFGRetrievalGraph:
    """
    Encapsulates the logic for a Retrieval-Feedback Generation (RFG) graph.

    This graph improves search accuracy by first retrieving a small set of initial documents,
    using them as context to generate an ideal or "hypothetical" answer, and then using
    this generated answer to perform a more precise semantic search for the final, relevant documents.
    """

    def __init__(self, set_id: str, semantic_memory: SemanticMemory,
                 generator_llm: BaseChatModel, prompt_language: Literal["en", "pt", "es"] = "en",
                 top_k_rfg: int = 4, top_k: int = 10):
        """
        Initializes the RFG Retrieval Graph instance.

        Args:
            set_id (str): Identifier for the semantic memory set.
            semantic_memory (SemanticMemory): The retriever for document search.
            generator_llm (BaseChatModel): The language model for generating the hypothetical document.
            prompt_language (Literal["en", "pt", "es"]): The language for prompts (currently unused in this implementation).
            top_k_rfg (int): The number of initial documents to fetch for context.
            top_k (int): The number of final documents to retrieve.
        """
        self.set_id = set_id
        self.semantic_memory = semantic_memory
        self.generator_llm = generator_llm
        self.prompt_language = prompt_language
        self.top_k_rfg = top_k_rfg
        self.top_k = top_k
        self._initialize_components()
        self.app = self._build_and_compile_graph()

    def _initialize_components(self):
        """Creates the necessary generation chain for the graph."""
        self.hyde_chain = get_rfg_chain(self.generator_llm)

    # --- Graph Nodes ---

    def initial_retrieve(self, state: RFGGraphState) -> dict:
        """
        Node: Retrieves the initial set of documents based on the user's question.

        Args:
            state (RFGGraphState): The current state of the graph.

        Returns:
            dict: A dictionary with the 'reference_docs' key to update the state.
        """
        logging.info("---NODE: INITIAL RETRIEVAL---")
        question = state["question"]
        
        # Retrieve initial documents to ground the generation process.
        reference_docs = self.semantic_memory.search_memories(self.set_id, question, self.top_k_rfg)
        logging.info(f"--- Found {len(reference_docs)} initial documents ---")
        return {"reference_docs": reference_docs}

    def generate_hypothetical_document(self, state: RFGGraphState) -> dict:
        """
        Node: Generates a hypothetical document based on the user's question and initial retrieved docs.

        Args:
            state (RFGGraphState): The current state of the graph.

        Returns:
            dict: A dictionary with the 'hypothetical_document' key to update the state.
        """
        logging.info("---NODE: GENERATE HYPOTHETICAL DOCUMENT---")
        
        # Format the reference documents fetched in the previous step.
        reference_docs = state["reference_docs"]
        formatted_candidates = "\n".join(
            [f"{i+1}. {arg['page_content']}" for i, arg in enumerate(reference_docs)]
        )
        
        # Invoke the chain to generate the new document.
        hypothetical_document = self.hyde_chain.invoke({"query": state["question"], "reference_docs": formatted_candidates})
        logging.info(f"--- Hypothetical Document Generated ---")
        return {"hypothetical_document": hypothetical_document}

    def retrieve(self, state: RFGGraphState) -> dict:
        """
        Node: Retrieves final documents using the hypothetical document as the search query.

        Args:
            state (RFGGraphState): The current state of the graph.

        Returns:
            dict: A dictionary with the 'documents' key to update the state.
        """
        logging.info("---NODE: RETRIEVE USING HYPOTHETICAL DOCUMENT---")
        hypothetical_document = state["hypothetical_document"]
        documents = self.semantic_memory.search_memories(self.set_id, hypothetical_document, self.top_k)
        logging.info(f"--- Found {len(documents)} final documents ---")
        return {"documents": documents}

    def _build_and_compile_graph(self) -> Any:
        """
        Builds the StateGraph, adds all nodes and edges, and compiles it into a runnable app.
        """
        workflow = StateGraph(RFGGraphState)

        # Add nodes to the graph
        workflow.add_node("initial_retrieve", self.initial_retrieve)
        workflow.add_node("generate_hypothetical_document", self.generate_hypothetical_document)
        workflow.add_node("retrieve", self.retrieve)

        # Define the graph's structure
        workflow.set_entry_point("initial_retrieve")
        workflow.add_edge("initial_retrieve", "generate_hypothetical_document")
        workflow.add_edge("generate_hypothetical_document", "retrieve")
        workflow.add_edge("retrieve", END)

        logging.info("RFG Retrieval Graph built. Compiling...")
        return workflow.compile()

    def get_compiled_graph(self) -> Any:
        """
        Returns the compiled, runnable graph application.

        This allows other parts of the system (like a LangChain Tool) to use this graph.
        """
        return self.app