from typing import Optional, Dict, Any, Iterator
from eagle.tools.memory_access.semantic.self_rag_graph import SelfRagGraphState
from pydantic import Field, PrivateAttr
from langchain_core.tools import BaseTool
from langchain_core.language_models.chat_models import BaseChatModel
from eagle.memory.semantic.base import SemanticMemory
from eagle.tools.memory_access.semantic.self_rag_graph import SelfRagGraph
import logging
logging.basicConfig(level=logging.INFO)

class SelfRagTool(BaseTool):
    """
    A tool that utilizes a self-correcting RAG (Retrieval-Augmented Generation) 
    graph to answer questions based on a semantic memory, citing the sources used.
    """
    name: str = "self_rag_tool"
    description: str = (
        "A tool that retrieves the most relevant documents from semantic memory and uses a QA chain "
        "to generate a response to a given question, citing the sources used."
    )

    # --- Configuration Attributes ---
    memory: SemanticMemory
    prompt_language: str
    grader_llm: Optional[BaseChatModel] = Field(default=None, description="Language model for grading documents")
    generate_llm: Optional[BaseChatModel] = Field(default=None, description="Language model for generating the answer")
    set_id: str
    top_k:int

    # --- Private attribute to store the compiled graph ---
    # The `_app` attribute is used to indicate it's a private, internally managed attribute.
    _app: Any = PrivateAttr()

    def model_post_init(self, __context: Any) -> None:
        """
        Initializes and compiles the internal RAG graph when the tool is created.

        This method is automatically called by Pydantic after the tool's fields are
        validated. It's the ideal place to perform complex initializations, such as
        building and compiling the LangGraph application, ensuring it's done only once.
        """
        super().model_post_init(__context)
        logging.info("--- Initializing and compiling SelfRagGraph for SelfRagTool ---")
        rag_graph_builder = SelfRagGraph(
            prompt_language= self.prompt_language,
            generate_llm=self.generate_llm,
            grader_llm=self.grader_llm,
            semantic_memory=self.memory,
            set_id=self.set_id,
            top_k=self.top_k
        )
        # The compiled graph application is stored as a private instance attribute.
        self._app = rag_graph_builder.get_compiled_graph()
        logging.info("--- Graph compiled and ready to use. ---")


    def _run(self, question: str, **kwargs: Any) -> SelfRagGraphState:
        """
        Executes the tool synchronously to answer a given question.

        This method provides a direct, blocking way to get the final answer from the RAG graph.
        It uses the `invoke` method on the compiled graph, which runs the entire process
        from start to finish and returns only the final state.

        Args:
            question (str): The user's question to be answered.
            **kwargs (Any): Additional keyword arguments, ignored by this implementation.

        Returns:
            str: The final generated answer as a string.
        """
        logging.info(f"\n--- Running SelfRagTool with question: '{question}' ---")
        
        initial_state = {"question": question}
        
        # We use `invoke()` to run the graph and get only the final state.
        # This is more direct and efficient than `stream()` if intermediate steps are not needed.
        final_state = self._app.invoke(initial_state, {"recursion_limit": 10})
        
        # The final answer is extracted from the graph's state.
        # It's good practice for a tool to return a clean string as its final output.
        final_answer = final_state.get("generation", "Could not generate a final answer.")
        
        logging.info(f"--- Final answer generated by the graph: ---\n{final_answer}")
        
        return final_state
        
    async def _arun(self, question: str, **kwargs: Any) -> SelfRagGraphState:
        """
        Executes the tool asynchronously to answer a given question.

        This method is the asynchronous counterpart to `_run`. It uses the `ainvoke`
        method to run the RAG graph without blocking, making it suitable for
        concurrent applications.

        Args:
            question (str): The user's question to be answered.
            **kwargs (Any): Additional keyword arguments, ignored by this implementation.

        Returns:
            str: The final generated answer as a string.
        """
        logging.info(f"\n--- Running SelfRagTool (async) with question: '{question}' ---")
        
        initial_state = {"question": question}

        # Use `ainvoke` for asynchronous execution.
        final_state = await self._app.ainvoke(initial_state, {"recursion_limit": 10})
        
        final_answer = final_state.get("generation", "Could not generate a final answer.")

        logging.info(f"--- Final answer generated by the graph (async): ---\n{final_answer}")

        return final_state
    
    # --- NEW METHOD FOR STREAMING! ---
    def stream_graph(self, question: str) -> Iterator[Dict[str, Any]]:
        """
        Executes the underlying RAG graph in streaming mode to observe each step.

        This method is ideal for debugging or for building interactive user interfaces,
        as it yields the state of the graph after each computational step. This allows
        callers to inspect the intermediate results, such as retrieved documents,
        relevance grades, and the final generated answer.

        Args:
            question (str): The user's question.

        Yields:
            Iterator[Dict[str, Any]]: A generator that produces the state dictionary
                                      at each step of the graph's execution.
        """
        initial_state = {"question": question}
        
        # Calls `.stream()` on the internal graph and yields each output state.
        # `yield from` is a concise way to loop over the generator and yield each item.
        yield from self._app.stream(initial_state, {"recursion_limit": 10})