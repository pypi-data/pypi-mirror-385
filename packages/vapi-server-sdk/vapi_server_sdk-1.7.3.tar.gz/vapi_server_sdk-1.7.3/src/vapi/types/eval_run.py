# This file was auto-generated by Fern from our API Definition.

from __future__ import annotations

import datetime as dt
import typing

import pydantic
import typing_extensions
from ..core.pydantic_utilities import IS_PYDANTIC_V2, update_forward_refs
from ..core.serialization import FieldMetadata
from ..core.unchecked_base_model import UncheckedBaseModel
from .create_eval_dto import CreateEvalDto
from .eval_run_ended_reason import EvalRunEndedReason
from .eval_run_result import EvalRunResult
from .eval_run_status import EvalRunStatus
from .eval_run_target import EvalRunTarget


class EvalRun(UncheckedBaseModel):
    status: EvalRunStatus = pydantic.Field()
    """
    This is the status of the eval run. When an eval run is created, the status is 'running'.
    When the eval run is completed, the status is 'ended'.
    """

    ended_reason: typing_extensions.Annotated[EvalRunEndedReason, FieldMetadata(alias="endedReason")] = pydantic.Field()
    """
    This is the reason for the eval run to end.
    When the eval run is completed normally i.e end of mock conversation, the status is 'mockConversation.done'.
    When the eval fails due to an error like Chat error or incorrect configuration, the status is 'error'.
    When the eval runs for too long, due to model issues or tool call issues, the status is 'timeout'.
    When the eval run is cancelled by the user, the status is 'cancelled'.
    When the eval run is cancelled by Vapi for any reason, the status is 'aborted'.
    """

    eval: typing.Optional[CreateEvalDto] = pydantic.Field(default=None)
    """
    This is the transient eval that will be run
    """

    target: EvalRunTarget = pydantic.Field()
    """
    This is the target that will be run against the eval
    """

    id: str
    org_id: typing_extensions.Annotated[str, FieldMetadata(alias="orgId")]
    created_at: typing_extensions.Annotated[dt.datetime, FieldMetadata(alias="createdAt")]
    started_at: typing_extensions.Annotated[dt.datetime, FieldMetadata(alias="startedAt")]
    ended_at: typing_extensions.Annotated[dt.datetime, FieldMetadata(alias="endedAt")]
    ended_message: typing_extensions.Annotated[typing.Optional[str], FieldMetadata(alias="endedMessage")] = (
        pydantic.Field(default=None)
    )
    """
    This is the ended message when the eval run ended for any reason apart from mockConversation.done
    """

    results: typing.List[EvalRunResult] = pydantic.Field()
    """
    This is the results of the eval or suite run.
    The array will have a single item for an eval run, and multiple items each corresponding to the an eval in a suite run in the same order as the evals in the suite.
    """

    type: typing.Literal["eval"] = pydantic.Field(default="eval")
    """
    This is the type of the run.
    Currently it is fixed to `eval`.
    """

    eval_id: typing_extensions.Annotated[typing.Optional[str], FieldMetadata(alias="evalId")] = pydantic.Field(
        default=None
    )
    """
    This is the id of the eval that will be run.
    """

    if IS_PYDANTIC_V2:
        model_config: typing.ClassVar[pydantic.ConfigDict] = pydantic.ConfigDict(extra="allow", frozen=True)  # type: ignore # Pydantic v2
    else:

        class Config:
            frozen = True
            smart_union = True
            extra = pydantic.Extra.allow


from .anthropic_model import AnthropicModel  # noqa: E402, F401, I001
from .anyscale_model import AnyscaleModel  # noqa: E402, F401, I001
from .call_hook_assistant_speech_interrupted import CallHookAssistantSpeechInterrupted  # noqa: E402, F401, I001
from .call_hook_call_ending import CallHookCallEnding  # noqa: E402, F401, I001
from .call_hook_customer_speech_interrupted import CallHookCustomerSpeechInterrupted  # noqa: E402, F401, I001
from .call_hook_customer_speech_timeout import CallHookCustomerSpeechTimeout  # noqa: E402, F401, I001
from .cerebras_model import CerebrasModel  # noqa: E402, F401, I001
from .create_assistant_dto import CreateAssistantDto  # noqa: E402, F401, I001
from .create_handoff_tool_dto import CreateHandoffToolDto  # noqa: E402, F401, I001
from .custom_llm_model import CustomLlmModel  # noqa: E402, F401, I001
from .deep_infra_model import DeepInfraModel  # noqa: E402, F401, I001
from .deep_seek_model import DeepSeekModel  # noqa: E402, F401, I001
from .google_model import GoogleModel  # noqa: E402, F401, I001
from .groq_model import GroqModel  # noqa: E402, F401, I001
from .group_condition import GroupCondition  # noqa: E402, F401, I001
from .handoff_destination_assistant import HandoffDestinationAssistant  # noqa: E402, F401, I001
from .inflection_ai_model import InflectionAiModel  # noqa: E402, F401, I001
from .open_ai_model import OpenAiModel  # noqa: E402, F401, I001
from .open_router_model import OpenRouterModel  # noqa: E402, F401, I001
from .perplexity_ai_model import PerplexityAiModel  # noqa: E402, F401, I001
from .together_ai_model import TogetherAiModel  # noqa: E402, F401, I001
from .tool_call_hook_action import ToolCallHookAction  # noqa: E402, F401, I001
from .xai_model import XaiModel  # noqa: E402, F401, I001

update_forward_refs(EvalRun)
