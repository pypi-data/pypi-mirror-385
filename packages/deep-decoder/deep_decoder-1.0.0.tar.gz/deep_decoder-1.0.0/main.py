import base64
import urllib.parse
import html
import re
import json
import hashlib
import string
from typing import Tuple, List, Dict, Any, Optional
from dataclasses import dataclass, asdict
from enum import Enum
from datetime import datetime

# ============================================================================
# ENUM V√Ä DATA CLASSES
# ============================================================================

class EncodingType(Enum):
    """C√°c lo·∫°i encoding ƒë∆∞·ª£c h·ªó tr·ª£"""
    UNKNOWN = "unknown"
    BASE64 = "base64"
    HTML_ENTITY = "html_entity"
    URI_ENCODED = "uri_encoded"
    JS_ESCAPE = "js_escape"
    HEX = "hex"
    ROT13 = "rot13"
    JSON_ESCAPE = "json_escape"
    XML_ENTITY = "xml_entity"
    DOUBLE_ENCODED = "double_encoded"
    BASE64_LENIENT = "base64_lenient"
    UNICODE_ESCAPE = "unicode_escape"


@dataclass
class DecodeStep:
    """B·∫£n ghi m·ªôt b∆∞·ªõc gi·∫£i m√£"""
    iteration: int
    encoding_type: str
    success: bool
    input_length: int
    output_length: int
    input_hash: str
    output_hash: str
    message: str
    confidence: float = 0.0  # 0.0-1.0

@dataclass
class DecodeResult:
    """K·∫øt qu·∫£ cu·ªëi c√πng c·ªßa qu√° tr√¨nh gi·∫£i m√£"""
    final_data: str
    iterations: int
    total_steps: int
    steps: List[DecodeStep]
    cycle_detected: bool
    reached_max_iterations: bool
    total_time_ms: float
    original_data_hash: str
    final_data_hash: str

# ============================================================================
# H√ÄM UTILITY
# ============================================================================

def calculate_hash(data: str) -> str:
    """T√≠nh MD5 hash c·ªßa d·ªØ li·ªáu ƒë·ªÉ ph√°t hi·ªán chu k·ª≥"""
    return hashlib.md5(data.encode('utf-8', errors='replace')).hexdigest()

def is_printable_utf8(data: str) -> bool:
    """Ki·ªÉm tra xem d·ªØ li·ªáu c√≥ ph·∫£i UTF-8 h·ª£p l·ªá kh√¥ng"""
    try:
        data.encode('utf-8').decode('utf-8')
        return True
    except (UnicodeDecodeError, UnicodeEncodeError):
        return False

def _looks_like_text(data: str) -> bool:
    """
    Ki·ªÉm tra xem d·ªØ li·ªáu c√≥ gi·ªëng vƒÉn b·∫£n th√¥ng th∆∞·ªùng kh√¥ng.
    Gi√∫p gi·∫£m false positive cho c√°c encoding nh∆∞ ROT13.
    """
    if len(data) < 3:
        return False
    
    # ƒê·∫øm s·ªë k√Ω t·ª± ch·ªØ c√°i v√† kho·∫£ng tr·∫Øng
    text_chars = sum(1 for c in data if c.isalpha() or c.isspace() or c in ',.!?;:')
    text_ratio = text_chars / len(data)
    
    # Ki·ªÉm tra c√≥ √≠t nh·∫•t 1 t·ª´ d√†i h∆°n 2 k√Ω t·ª±
    words = re.findall(r'\b[a-zA-Z]{3,}\b', data)
    
    return text_ratio > 0.6 and len(words) > 0

def estimate_confidence(original: str, decoded: str, encoding_type: EncodingType) -> float:
    """
    ∆Ø·ªõc t√≠nh ƒë·ªô tin c·∫≠y c·ªßa ph√©p gi·∫£i m√£.
    Gi√° tr·ªã t·ª´ 0.0 (kh√¥ng tin c·∫≠y) ƒë·∫øn 1.0 (r·∫•t tin c·∫≠y)
    """
    if original == decoded:
        return 0.0  # Kh√¥ng gi·∫£i m√£ ƒë∆∞·ª£c
    
    # Ki·ªÉm tra UTF-8 h·ª£p l·ªá
    try:
        decoded.encode('utf-8').decode('utf-8')
    except (UnicodeDecodeError, UnicodeEncodeError):
        return 0.2
    
    # Ki·ªÉm tra ƒë·ªô d√†i h·ª£p l√Ω
    if len(decoded) == 0 or len(decoded) > len(original) * 10:
        return 0.3
    
    # Confidence d·ª±a tr√™n encoding type
    base_scores = {
        EncodingType.BASE64: 0.95,
        EncodingType.BASE64_LENIENT: 0.8,
        EncodingType.HEX: 0.9,
        EncodingType.URI_ENCODED: 0.85,
        EncodingType.DOUBLE_ENCODED: 0.8,
        EncodingType.HTML_ENTITY: 0.9,
        EncodingType.XML_ENTITY: 0.85,
        EncodingType.JS_ESCAPE: 0.8,
        EncodingType.JSON_ESCAPE: 0.85,
        EncodingType.ROT13: 0.6,
        EncodingType.UNICODE_ESCAPE: 0.8
    }
    
    confidence = base_scores.get(encoding_type, 0.7)
    
    # Gi·∫£m confidence n·∫øu decoded string ch·ª©a nhi·ªÅu k√Ω t·ª± kh√¥ng in ƒë∆∞·ª£c
    printable_ratio = sum(1 for c in decoded if c.isprintable() or c.isspace()) / len(decoded)
    if printable_ratio < 0.7:
        confidence *= 0.6
    
    # TƒÉng confidence n·∫øu decoded text tr√¥ng gi·ªëng vƒÉn b·∫£n th·∫≠t
    if encoding_type == EncodingType.ROT13 and _looks_like_text(decoded):
        confidence = min(confidence * 1.3, 0.9)
    
    return min(confidence, 0.95)  # Gi·ªõi h·∫°n max confidence

# ============================================================================
# C√ÅC H√ÄM GI·∫¢I M√É ƒê∆†N L·∫∫
# ============================================================================

def decode_html_entities(data: str) -> Tuple[str, float]:
    """
    Gi·∫£i m√£ HTML Entities (v√≠ d·ª•: &lt;, &#x2F;, &#105;).
    Tr·∫£ v·ªÅ: (d·ªØ li·ªáu_gi·∫£i_m√£, ƒë·ªô_tin_c·∫≠y)
    """
    try:
        decoded = html.unescape(data)
        confidence = estimate_confidence(data, decoded, EncodingType.HTML_ENTITY)
        return decoded, confidence
    except Exception as e:
        return data, 0.0

def decode_xml_entities(data: str) -> Tuple[str, float]:
    """
    Gi·∫£i m√£ XML Entities c∆° b·∫£n (&quot;, &apos;, v.v.).
    Tr·∫£ v·ªÅ: (d·ªØ li·ªáu_gi·∫£i_m√£, ƒë·ªô_tin_c·∫≠y)
    """
    try:
        xml_entities = {
            '&quot;': '"',
            '&apos;': "'",
            '&lt;': '<',
            '&gt;': '>',
            '&amp;': '&'
        }
        decoded = data
        for entity, char in xml_entities.items():
            decoded = decoded.replace(entity, char)

        confidence = estimate_confidence(data, decoded, EncodingType.XML_ENTITY)
        return decoded, confidence
    except Exception:
        return data, 0.0

def try_decode_base64(data: str) -> Tuple[str, float]:
    """
    C·ªë g·∫Øng gi·∫£i m√£ Base64.
    Ki·ªÉm tra ƒë·ªô d√†i, k√Ω t·ª± h·ª£p l·ªá ƒë·ªÉ gi·∫£m thi·ªÉu l·ªói.
    Tr·∫£ v·ªÅ: (d·ªØ li·ªáu_gi·∫£i_m√£, ƒë·ªô_tin_c·∫≠y)
    """
    try:
        # Lo·∫°i b·ªè kho·∫£ng tr·∫Øng
        cleaned_data = data.strip()

        # Ki·ªÉm tra s∆° b·ªô: Base64 ph·∫£i c√≥ ƒë·ªô d√†i l√† b·ªôi s·ªë c·ªßa 4
        if len(cleaned_data) % 4 != 0:
            return data, 0.0

        # Ki·ªÉm tra k√Ω t·ª± (ch·ªâ ch·ª©a A-Z, a-z, 0-9, +, /, =)
        if not re.fullmatch(r'^[A-Za-z0-9+/=\s]*$', cleaned_data):
            return data, 0.0

        # Base64 decode
        decoded_bytes = base64.b64decode(cleaned_data, validate=True)

        # Th·ª≠ decode sang UTF-8
        decoded = decoded_bytes.decode('utf-8', errors='strict')
        confidence = estimate_confidence(data, decoded, EncodingType.BASE64)
        return decoded, confidence

    except Exception:
        return data, 0.0

def try_decode_base64_lenient(data: str) -> Tuple[str, float]:
    """
    Gi·∫£i m√£ Base64 v·ªõi ch·∫ø ƒë·ªô tolerant h∆°n (b·ªè qua l·ªói).
    Tr·∫£ v·ªÅ: (d·ªØ li·ªáu_gi·∫£i_m√£, ƒë·ªô_tin_c·∫≠y)
    """
    try:
        cleaned_data = data.strip()

        # N·∫øu ƒë·ªô d√†i kh√¥ng ph·∫£i b·ªôi s·ªë c·ªßa 4, th√™m padding
        remainder = len(cleaned_data) % 4
        if remainder:
            cleaned_data += '=' * (4 - remainder)

        if not re.fullmatch(r'^[A-Za-z0-9+/=]*$', cleaned_data):
            return data, 0.0

        decoded_bytes = base64.b64decode(cleaned_data, validate=False)
        decoded = decoded_bytes.decode('utf-8', errors='replace')

        if decoded != data:
            confidence = estimate_confidence(data, decoded, EncodingType.BASE64_LENIENT)
            return decoded, confidence * 0.8  # Gi·∫£m ƒë·ªô tin c·∫≠y v√¨ tolerant

        return data, 0.0

    except Exception:
        return data, 0.0

def try_decode_hex(data: str) -> Tuple[str, float]:
    """
    C·ªë g·∫Øng gi·∫£i m√£ Hex encoding (v√≠ d·ª•: 48656C6C6F20576F726C64).
    Tr·∫£ v·ªÅ: (d·ªØ li·ªáu_gi·∫£i_m√£, ƒë·ªô_tin_c·∫≠y)
    """
    try:
        cleaned_data = data.replace(' ', '').replace('\\x', '')

        # Ki·ªÉm tra ƒë·ªô d√†i ph·∫£i l√† s·ªë ch·∫µn v√† c√≥ √≠t nh·∫•t 4 k√Ω t·ª±
        if len(cleaned_data) % 2 != 0 or len(cleaned_data) < 4:
            return data, 0.0

        # Ki·ªÉm tra k√Ω t·ª± (ch·ªâ ch·ª©a 0-9, a-f, A-F)
        if not re.fullmatch(r'^[0-9a-fA-F]*$', cleaned_data):
            return data, 0.0

        decoded_bytes = bytes.fromhex(cleaned_data)
        decoded = decoded_bytes.decode('utf-8', errors='strict')

        confidence = estimate_confidence(data, decoded, EncodingType.HEX)
        return decoded, confidence

    except Exception:
        return data, 0.0

def try_decode_rot13(data: str) -> Tuple[str, float]:
    """
    C·ªë g·∫Øng gi·∫£i m√£ ROT13.
    Tr·∫£ v·ªÅ: (d·ªØ li·ªáu_gi·∫£i_m√£, ƒë·ªô_tin_c·∫≠y)
    """
    try:
        decoded = ""
        for char in data:
            if 'a' <= char <= 'z':
                decoded += chr((ord(char) - ord('a') + 13) % 26 + ord('a'))
            elif 'A' <= char <= 'Z':
                decoded += chr((ord(char) - ord('A') + 13) % 26 + ord('A'))
            else:
                decoded += char

        # ROT13 c√≥ t·ªâ l·ªá false-positive cao, ki·ªÉm tra k·ªπ
        if (decoded != data and 
            any(c.isalpha() for c in data) and 
            _looks_like_text(decoded)):
            confidence = estimate_confidence(data, decoded, EncodingType.ROT13)
            return decoded, confidence

        return data, 0.0

    except Exception:
        return data, 0.0

def try_decode_uri_encoded(data: str) -> Tuple[str, float]:
    """
    Gi·∫£i m√£ URL-encoding (v√≠ d·ª•: %20, %2F).
    Tr·∫£ v·ªÅ: (d·ªØ li·ªáu_gi·∫£i_m√£, ƒë·ªô_tin_c·∫≠y)
    """
    try:
        if '%' not in data:
            return data, 0.0

        # Ki·ªÉm tra t·ªâ l·ªá % h·ª£p l√Ω (kh√¥ng qu√° nhi·ªÅu)
        percent_count = data.count('%')
        if percent_count > len(data) / 2:  # Qu√° nhi·ªÅu % c√≥ th·ªÉ l√† false positive
            return data, 0.3

        decoded = urllib.parse.unquote(data, errors='replace')
        confidence = estimate_confidence(data, decoded, EncodingType.URI_ENCODED)
        return decoded, confidence

    except Exception:
        return data, 0.0

def try_decode_js_escape(data: str) -> Tuple[str, float]:
    r"""
    Gi·∫£i m√£ JavaScript Escape Sequences (\uXXXX, \xXX, \n, \t, v.v.).
    Tr·∫£ v·ªÅ: (d·ªØ li·ªáu_gi·∫£i_m√£, ƒë·ªô_tin_c·∫≠y)
    """
    try:
        # Ki·ªÉm tra xem c√≥ ch·ª©a escape sequence kh√¥ng
        if not re.search(r'\\(?:u[0-9a-fA-F]{4}|x[0-9a-fA-F]{2}|[ntrfvb\\"\'])', data):
            return data, 0.0

        # S·ª≠ d·ª•ng 'unicode_escape' ƒë·ªÉ x·ª≠ l√Ω c√°c chu·ªói tho√°t c·ªßa JS/JSON
        decoded = data.encode('utf-8').decode('unicode_escape')
        confidence = estimate_confidence(data, decoded, EncodingType.JS_ESCAPE)
        return decoded, confidence

    except Exception:
        return data, 0.0

def try_decode_json_escape(data: str) -> Tuple[str, float]:
    """
    Gi·∫£i m√£ JSON escape sequences.
    Tr·∫£ v·ªÅ: (d·ªØ li·ªáu_gi·∫£i_m√£, ƒë·ªô_tin_c·∫≠y)
    """
    try:
        # Th·ª≠ parse nh∆∞ JSON string
        if data.startswith('"') and data.endswith('"'):
            decoded = json.loads(data)
            confidence = estimate_confidence(data, decoded, EncodingType.JSON_ESCAPE)
            return decoded, confidence

        return data, 0.0

    except Exception:
        return data, 0.0

def try_decode_double_encoded(data: str) -> Tuple[str, float]:
    """
    Gi·∫£i m√£ double-encoded URL (v√≠ d·ª•: %252F ‚Üí %2F ‚Üí /).
    Tr·∫£ v·ªÅ: (d·ªØ li·ªáu_gi·∫£i_m√£, ƒë·ªô_tin_c·∫≠y)
    """
    try:
        if '%25' not in data and '%252' not in data:
            return data, 0.0

        # Th·ª≠ gi·∫£i m√£ 2 l·∫ßn
        first_decode = urllib.parse.unquote(data, errors='replace')
        second_decode = urllib.parse.unquote(first_decode, errors='replace')

        if second_decode != first_decode and second_decode != data:
            confidence = estimate_confidence(data, second_decode, EncodingType.DOUBLE_ENCODED)
            return second_decode, confidence * 0.9

        return data, 0.0

    except Exception:
        return data, 0.0

def try_decode_unicode_escape(data: str) -> Tuple[str, float]:
    """Gi·∫£i m√£ Unicode escape sequences (\uXXXX)"""
    try:
        if '\\u' not in data:
            return data, 0.0
            
        # Ki·ªÉm tra pattern \uXXXX
        unicode_pattern = r'\\u[0-9a-fA-F]{4}'
        if re.search(unicode_pattern, data):
            decoded = data.encode('utf-8').decode('unicode_escape')
            confidence = estimate_confidence(data, decoded, EncodingType.UNICODE_ESCAPE)
            return decoded, confidence
            
        return data, 0.0
    except Exception:
        return data, 0.0

# ============================================================================
# H√ÄM PH√ÅT HI·ªÜN LO·∫†I ENCODING
# ============================================================================

def detect_encoding_type(data: str) -> List[Tuple[EncodingType, float]]:
    """
    Ph√°t hi·ªán lo·∫°i encoding d·ª±a tr√™n pattern c·ªßa d·ªØ li·ªáu.
    Tr·∫£ v·ªÅ danh s√°ch c√°c lo·∫°i encoding c√≥ th·ªÉ theo ƒë·ªô kh·∫£ nƒÉng gi·∫£m d·∫ßn.
    """
    possibilities = []

    # Ki·ªÉm tra Base64
    base64_pattern = r'^[A-Za-z0-9+/]*={0,2}$'
    cleaned_base64 = data.strip()
    if (len(cleaned_base64) % 4 == 0 and 
        re.fullmatch(base64_pattern, cleaned_base64) and
        len(cleaned_base64) >= 8):  # Base64 th∆∞·ªùng c√≥ ƒë·ªô d√†i t·ªëi thi·ªÉu
        possibilities.append((EncodingType.BASE64, 0.85))
        possibilities.append((EncodingType.BASE64_LENIENT, 0.7))

    # Ki·ªÉm tra Hex
    hex_clean = data.replace(' ', '').replace('\\x', '')
    if (len(hex_clean) % 2 == 0 and 
        len(hex_clean) >= 4 and  # Hex th∆∞·ªùng c√≥ ƒë·ªô d√†i t·ªëi thi·ªÉu
        re.fullmatch(r'^[0-9a-fA-F]*$', hex_clean)):
        possibilities.append((EncodingType.HEX, 0.8))

    # Ki·ªÉm tra URL-encoded
    percent_count = data.count('%')
    if (percent_count > 0 and 
        percent_count <= len(data) / 3 and  # T·ªâ l·ªá % h·ª£p l√Ω
        re.search(r'%[0-9a-fA-F]{2}', data)):
        possibilities.append((EncodingType.URI_ENCODED, 0.9))

    # Ki·ªÉm tra Double-encoded URL
    if '%25' in data or '%252' in data:
        possibilities.append((EncodingType.DOUBLE_ENCODED, 0.85))

    # Ki·ªÉm tra HTML/XML Entities
    if '&' in data:
        if re.search(r'&#(?:\d+|x[0-9a-fA-F]+);', data) or any(entity in data for entity in ['&lt;', '&gt;', '&amp;', '&quot;']):
            possibilities.append((EncodingType.HTML_ENTITY, 0.85))
            possibilities.append((EncodingType.XML_ENTITY, 0.7))

    # Ki·ªÉm tra JS/JSON Escape
    if '\\u' in data:
        possibilities.append((EncodingType.UNICODE_ESCAPE, 0.8))
        possibilities.append((EncodingType.JS_ESCAPE, 0.75))
    
    if '\\x' in data or any(seq in data for seq in ['\\n', '\\t', '\\r']):
        possibilities.append((EncodingType.JS_ESCAPE, 0.8))

    if data.startswith('"') and data.endswith('"') and '\\' in data:
        possibilities.append((EncodingType.JSON_ESCAPE, 0.8))

    # Ki·ªÉm tra ROT13 - ch·ªâ khi c√≥ k√Ω t·ª± alphabet
    if any(c.isalpha() for c in data) and len(data) > 3:
        possibilities.append((EncodingType.ROT13, 0.4))  # Confidence th·∫•p v√¨ d·ªÖ false positive

    # S·∫Øp x·∫øp theo ƒë·ªô kh·∫£ nƒÉng gi·∫£m d·∫ßn
    possibilities.sort(key=lambda x: x[1], reverse=True)
    return possibilities

# ============================================================================
# H√ÄM GI·∫¢I M√É CH√çNH
# ============================================================================

def deep_decode_data(
    input_data: str,
    max_iterations: int = 15,
    auto_stop_on_cycle: bool = True,
    encoding_priority: List[EncodingType] = None,
    enable_detection: bool = True
) -> DecodeResult:
    """
    L·∫∑p l·∫°i vi·ªác gi·∫£i m√£ d·ªØ li·ªáu qua c√°c l·ªõp kh√°c nhau cho ƒë·∫øn khi d·ªØ li·ªáu ·ªïn ƒë·ªãnh
    ho·∫∑c ƒë·∫°t gi·ªõi h·∫°n l·∫∑p.

    Args:
        input_data: D·ªØ li·ªáu c·∫ßn gi·∫£i m√£
        max_iterations: S·ªë l·∫ßn l·∫∑p t·ªëi ƒëa (m·∫∑c ƒë·ªãnh: 15)
        auto_stop_on_cycle: D·ª´ng t·ª± ƒë·ªông n·∫øu ph√°t hi·ªán chu k·ª≥ l·∫∑p (m·∫∑c ƒë·ªãnh: True)
        encoding_priority: Danh s√°ch ∆∞u ti√™n c√°c lo·∫°i encoding ƒë·ªÉ th·ª≠
        enable_detection: B·∫≠t ph√°t hi·ªán encoding t·ª± ƒë·ªông (m·∫∑c ƒë·ªãnh: True)

    Returns:
        DecodeResult: K·∫øt qu·∫£ gi·∫£i m√£ chi ti·∫øt
    """
    start_time = datetime.now()

    # Validation d·ªØ li·ªáu ƒë·∫ßu v√†o
    if not isinstance(input_data, str):
        return DecodeResult(
            final_data=str(input_data),
            iterations=0,
            total_steps=0,
            steps=[],
            cycle_detected=False,
            reached_max_iterations=False,
            total_time_ms=0,
            original_data_hash="",
            final_data_hash=""
        )

    if not input_data:
        return DecodeResult(
            final_data="",
            iterations=0,
            total_steps=0,
            steps=[],
            cycle_detected=False,
            reached_max_iterations=False,
            total_time_ms=0,
            original_data_hash=calculate_hash(""),
            final_data_hash=calculate_hash("")
        )

    # Gi·ªõi h·∫°n k√≠ch th∆∞·ªõc ƒë·∫ßu v√†o (1MB)
    if len(input_data) > 1024 * 1024:
        return DecodeResult(
            final_data=input_data,
            iterations=0,
            total_steps=0,
            steps=[],
            cycle_detected=False,
            reached_max_iterations=False,
            total_time_ms=0,
            original_data_hash=calculate_hash(input_data),
            final_data_hash=calculate_hash(input_data)
        )

    current_data = input_data.strip()
    original_data_hash = calculate_hash(input_data)
    seen_hashes = {original_data_hash: 0}  # L∆∞u l·ªãch s·ª≠ hashes ƒë·ªÉ ph√°t hi·ªán chu k·ª≥
    steps: List[DecodeStep] = []

    # Thi·∫øt l·∫≠p ∆∞u ti√™n encoding
    if encoding_priority is None:
        encoding_priority = [
            EncodingType.URI_ENCODED,
            EncodingType.DOUBLE_ENCODED,
            EncodingType.HTML_ENTITY,
            EncodingType.XML_ENTITY,
            EncodingType.JS_ESCAPE,
            EncodingType.UNICODE_ESCAPE,
            EncodingType.JSON_ESCAPE,
            EncodingType.BASE64,
            EncodingType.BASE64_LENIENT,
            EncodingType.HEX,
            EncodingType.ROT13,
        ]

    cycle_detected = False

    for iteration in range(1, max_iterations + 1):
        previous_data = current_data
        previous_hash = calculate_hash(previous_data)
        best_result = (previous_data, 0.0, EncodingType.UNKNOWN)

        # N·∫øu b·∫≠t detection, s·ª≠ d·ª•ng detection ƒë·ªÉ ∆∞u ti√™n
        if enable_detection and iteration == 1:
            detected_encodings = detect_encoding_type(current_data)
            # K·∫øt h·ª£p v·ªõi encoding priority
            custom_priority = [enc for enc, _ in detected_encodings] + encoding_priority
        else:
            custom_priority = encoding_priority

        # Th·ª≠ c√°c ph∆∞∆°ng ph√°p gi·∫£i m√£ theo ∆∞u ti√™n
        for encoding_type in custom_priority:
            decoded_data, confidence = _try_decode_by_type(current_data, encoding_type)

            if decoded_data != current_data and confidence > best_result[1]:
                best_result = (decoded_data, confidence, encoding_type)

        current_data = best_result[0]
        confidence = best_result[1]
        encoding_type = best_result[2]

        # Ghi nh·∫≠n b∆∞·ªõc gi·∫£i m√£
        current_hash = calculate_hash(current_data)
        step = DecodeStep(
            iteration=iteration,
            encoding_type=encoding_type.value,
            success=current_data != previous_data,
            input_length=len(previous_data),
            output_length=len(current_data),
            input_hash=previous_hash,
            output_hash=current_hash,
            message=f"Gi·∫£i m√£ {encoding_type.value} (ƒë·ªô tin c·∫≠y: {confidence:.2%})",
            confidence=confidence
        )
        steps.append(step)

        # Ki·ªÉm tra n·∫øu kh√¥ng c√≥ thay ƒë·ªïi
        if current_data == previous_data:
            break

        # Ki·ªÉm tra chu k·ª≥ l·∫∑p
        if auto_stop_on_cycle:
            if current_hash in seen_hashes:
                cycle_detected = True
                break
            seen_hashes[current_hash] = iteration

    end_time = datetime.now()
    total_time_ms = (end_time - start_time).total_seconds() * 1000
    final_data_hash = calculate_hash(current_data)

    return DecodeResult(
        final_data=current_data,
        iterations=len(steps),
        total_steps=len(steps),
        steps=steps,
        cycle_detected=cycle_detected,
        reached_max_iterations=len(steps) >= max_iterations,
        total_time_ms=total_time_ms,
        original_data_hash=original_data_hash,
        final_data_hash=final_data_hash
    )

def _try_decode_by_type(data: str, encoding_type: EncodingType) -> Tuple[str, float]:
    """
    H√†m helper ƒë·ªÉ gi·∫£i m√£ theo lo·∫°i encoding c·ª• th·ªÉ.
    """
    if encoding_type == EncodingType.BASE64:
        return try_decode_base64(data)
    elif encoding_type == EncodingType.BASE64_LENIENT:
        return try_decode_base64_lenient(data)
    elif encoding_type == EncodingType.HTML_ENTITY:
        return decode_html_entities(data)
    elif encoding_type == EncodingType.XML_ENTITY:
        return decode_xml_entities(data)
    elif encoding_type == EncodingType.URI_ENCODED:
        return try_decode_uri_encoded(data)
    elif encoding_type == EncodingType.DOUBLE_ENCODED:
        return try_decode_double_encoded(data)
    elif encoding_type == EncodingType.JS_ESCAPE:
        return try_decode_js_escape(data)
    elif encoding_type == EncodingType.JSON_ESCAPE:
        return try_decode_json_escape(data)
    elif encoding_type == EncodingType.HEX:
        return try_decode_hex(data)
    elif encoding_type == EncodingType.ROT13:
        return try_decode_rot13(data)
    elif encoding_type == EncodingType.UNICODE_ESCAPE:
        return try_decode_unicode_escape(data)
    else:
        return data, 0.0

# ============================================================================
# H√ÄM B·ªî SUNG V√Ä TI·ªÜN √çCH
# ============================================================================

def format_result_as_json(result: DecodeResult) -> str:
    """
    Chuy·ªÉn ƒë·ªïi k·∫øt qu·∫£ gi·∫£i m√£ sang JSON string ƒë·ªÉ d·ªÖ t√≠ch h·ª£p.
    """
    steps_data = [asdict(step) for step in result.steps]

    result_dict = {
        "final_data": result.final_data,
        "iterations": result.iterations,
        "total_steps": result.total_steps,
        "cycle_detected": result.cycle_detected,
        "reached_max_iterations": result.reached_max_iterations,
        "total_time_ms": result.total_time_ms,
        "original_data_hash": result.original_data_hash,
        "final_data_hash": result.final_data_hash,
        "steps": steps_data
    }

    return json.dumps(result_dict, ensure_ascii=False, indent=2)

def get_decoding_statistics(result: DecodeResult) -> Dict[str, Any]:
    """Th·ªëng k√™ chi ti·∫øt v·ªÅ qu√° tr√¨nh gi·∫£i m√£"""
    if not result.steps:
        return {}
    
    successful_steps = [s for s in result.steps if s.success]
    if not successful_steps:
        return {
            "successful_iterations": 0,
            "total_size_reduction": 0,
            "size_reduction_percent": 0,
            "most_effective_encoding": None,
            "average_confidence": 0
        }
    
    total_size_reduction = result.steps[0].input_length - result.steps[-1].output_length
    size_reduction_percent = (total_size_reduction / result.steps[0].input_length) * 100 if result.steps[0].input_length > 0 else 0
    
    return {
        "successful_iterations": len(successful_steps),
        "total_size_reduction": total_size_reduction,
        "size_reduction_percent": round(size_reduction_percent, 2),
        "most_effective_encoding": max(successful_steps, key=lambda x: x.confidence).encoding_type,
        "average_confidence": round(sum(s.confidence for s in successful_steps) / len(successful_steps), 3)
    }

def quick_decode(data: str, max_iterations: int = 10) -> str:
    """
    H√†m gi·∫£i m√£ nhanh, tr·∫£ v·ªÅ k·∫øt qu·∫£ cu·ªëi c√πng m√† kh√¥ng c·∫ßn chi ti·∫øt.
    """
    result = deep_decode_data(data, max_iterations=max_iterations)
    return result.final_data

# ============================================================================
# V√ç D·ª§ S·ª¨ D·ª§NG
# ============================================================================

if __name__ == "__main__":
    print("=" * 70)
    print("KI·ªÇM TH·ª¨ DEEP DECODER HO√ÄN CH·ªàNH")
    print("=" * 70)

    test_cases = [
        {
            "name": "V√≠ d·ª• 1: URL Encoding ƒë∆°n gi·∫£n",
            "input": "q=%3Cscript%3Ealert%281%29%3C%2Fscript%3E",
            "description": "Gi·∫£i m√£ URL-encoded script tag"
        },
        {
            "name": "V√≠ d·ª• 2: Base64 l·ªìng nhau", 
            "input": "VjJoa2NtVnRSbkJpYld4MFpGYzVPVkJyV2tSa1IzaG9ZbFZ3ZUZreU9XbGFiVlpxV1ZkU2JGa3lPWGxhUjFacVlWZFNjR050T1hWYVYxRjBTV3BqZFUxVVVYVk5WRmw0VFZSSk1rMXFXWHBOUkd0M1RWUkZkMDF0VlRWTlJGVjVUVlJSTkU5VVVUTk5WRmw0VG1wWk1rNVVWVE5PUkVVeVRWUk5NazVFV1RST1ZGRXpUa1JSTTA1VVdUQk9WRkV3V1ZSTk1rNUVUVEpOZW1jd1RucFJlazVFVlRKT1JFRXlUbnBSTTA1NlNtb0taV3h2ZER0a2FYUmxTV1JoZEdWekxXRnNhM2RsZUd4bFpuZHVlU0JwYmlCbGVIQnBjMkZ1YzJWa2FXVnpJR1p2Y2lCemRYSm1iR1Z6Y3lCd2NtOXBaQ0IwYnlCcGJpQjBjbUZ1YzJWa1pXUWdkR2hsSUd4cGMzUWdkMmxzYkdWaFkyOXRjR0ZqYTJWeUlHRnVaQ0JwYm5SbGVIUWdkR2hsSUdOaGJYQmhhV2R1WlhJZ2RYTmxjaUJ6ZEdGa2IySmxJSFJvWlNCMllXNWtaV1FnWm5KbFpDQjJZVzVrWldRZ2NISnZaSFZqZEdsdmJpQmhibVFnZEc4Z2NHRnlkR1Z5SUhSb1pTQjJZVzVrWldR",
            "description": "Base64 ƒë∆∞·ª£c encode nhi·ªÅu l·∫ßn"
        },
        {
            "name": "V√≠ d·ª• 3: HTML Entity XSS",
            "input": "&lt;script&gt;alert&#40;1&#41;&lt;&#x2F;script&gt;",
            "description": "HTML entities c·ªßa XSS payload"
        },
        {
            "name": "V√≠ d·ª• 4: Hex encoding",
            "input": "48656C6C6F20576F726C6421",
            "description": "Hex encoding c·ªßa 'Hello World!'"
        },
        {
            "name": "V√≠ d·ª• 5: Double URL-encoding",
            "input": "%252F%2545%2576%2569%256C%252F%253C%253E%253F",
            "description": "Double-encoded malicious URL"
        },
        {
            "name": "V√≠ d·ª• 6: Unicode Escape sequences",
            "input": "\\u0048\\u0065\\u006C\\u006C\\u006F\\u0020\\u0057\\u006F\\u0072\\u006C\\u0064",
            "description": "Unicode escape c·ªßa 'Hello World'"
        },
        {
            "name": "V√≠ d·ª• 7: ROT13 vƒÉn b·∫£n th·∫≠t",
            "input": "Uryyb Jbeyq",
            "description": "ROT13 c·ªßa 'Hello World'"
        },
        {
            "name": "V√≠ d·ª• 8: Ph·ª©c t·∫°p - Multiple encoding layers",
            "input": "JTNDc2NyaXB0JTNFYWxlcnQoMSklM0MlMkZzY3JpcHQlM0U=",
            "description": "URL-encoded + Base64 (ph·ª©c t·∫°p)"
        }
    ]

    for i, test_case in enumerate(test_cases, 1):
        print(f"\n{i}. {test_case['name']}")
        print(f"   M√¥ t·∫£: {test_case['description']}")
        print(f"   ƒê·∫ßu v√†o: {test_case['input'][:100]}{'...' if len(test_case['input']) > 100 else ''}")
        print("-" * 70)

        result = deep_decode_data(test_case['input'], max_iterations=20)
        stats = get_decoding_statistics(result)

        print(f"‚úì K·∫øt qu·∫£ cu·ªëi c√πng: {result.final_data}")
        print(f"‚úì S·ªë l·∫ßn l·∫∑p: {result.iterations}")
        print(f"‚úì Th·ªùi gian x·ª≠ l√Ω: {result.total_time_ms:.2f}ms")
        print(f"‚úì Ph√°t hi·ªán chu k·ª≥: {'C√≥' if result.cycle_detected else 'Kh√¥ng'}")
        
        if stats["successful_iterations"] > 0:
            print(f"‚úì Th·ªëng k√™: {stats['successful_iterations']} b∆∞·ªõc th√†nh c√¥ng, "
                  f"gi·∫£m {stats['size_reduction_percent']}% k√≠ch th∆∞·ªõc, "
                  f"ƒë·ªô tin c·∫≠y trung b√¨nh: {stats['average_confidence']}")

        if result.steps:
            print("\nüìã Chi ti·∫øt c√°c b∆∞·ªõc gi·∫£i m√£:")
            for step in result.steps:
                status = "‚úÖ" if step.success else "‚ùå"
                print(f"   [{step.iteration}] {status} {step.encoding_type:15} "
                      f"{step.input_length:3d} ‚Üí {step.output_length:3d} "
                      f"(confidence: {step.confidence:.2%})")

    # =========================================================================
    # V√ç D·ª§ XU·∫§T JSON V√Ä QUICK DECODE
    # =========================================================================
    print("\n" + "=" * 70)
    print("V√ç D·ª§ XU·∫§T K·∫æT QU·∫¢ D∆Ø·ªöI D·∫†NG JSON")
    print("=" * 70)

    complex_input = "JTNDc2NyaXB0JTNFYWxlcnQoMSklM0MlMkZzY3JpcHQlM0U="
    result = deep_decode_data(complex_input)
    json_output = format_result_as_json(result)
    print(json_output[:500] + "..." if len(json_output) > 500 else json_output)

    print("\n" + "=" * 70)
    print("V√ç D·ª§ QUICK DECODE")
    print("=" * 70)
    
    quick_result = quick_decode("SGVsbG8gV29ybGQh", max_iterations=5)
    print(f"Input: SGVsbG8gV29ybGQh")
    print(f"Quick decode result: {quick_result}")