#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
XMind AIæ‰©å±•åŠŸèƒ½æ¨¡å—
æä¾›AIé©±åŠ¨çš„æ€ç»´å¯¼å›¾å¢å¼ºåŠŸèƒ½
"""

import json
import re
import random
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
from enum import Enum

# å‡è®¾OpenAI APIå¯ç”¨
try:
    import openai
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False


class AIFunction(Enum):
    """AIåŠŸèƒ½æšä¸¾"""
    GENERATE_TOPICS = "generate_topics"
    OPTIMIZE_STRUCTURE = "optimize_structure"
    SUGGEST_IMPROVEMENTS = "suggest_improvements"
    CATEGORIZE_CONTENT = "categorize_content"
    EXTRACT_KEYWORDS = "extract_keywords"
    GENERATE_SUMMARY = "generate_summary"


@dataclass
class AITopic:
    """AIç”Ÿæˆçš„ä¸»é¢˜"""
    title: str
    description: Optional[str] = None
    priority: int = 1
    relevance_score: float = 0.8
    subtopics: List['AITopic'] = None
    
    def __post_init__(self):
        if self.subtopics is None:
            self.subtopics = []


@dataclass
class AIMindMapAnalysis:
    """AIæ€ç»´å¯¼å›¾åˆ†æç»“æœ"""
    complexity_score: float
    balance_score: float
    completeness_score: float
    overall_quality: str
    suggestions: List[str]
    optimization_opportunities: List[str]
    structural_issues: List[str]


class XMindAIExtensions:
    """XMind AIæ‰©å±•åŠŸèƒ½ç±»"""
    
    def __init__(self, api_key: Optional[str] = None, model: str = "gpt-4"):
        self.api_key = api_key
        self.model = model
        self.openai_client = None
        
        if OPENAI_AVAILABLE and api_key:
            self.openai_client = openai.OpenAI(api_key=api_key)
    
    def is_ai_available(self) -> bool:
        """æ£€æŸ¥AIåŠŸèƒ½æ˜¯å¦å¯ç”¨"""
        return OPENAI_AVAILABLE and self.openai_client is not None
    
    async def generate_topics(
        self, 
        context: str, 
        existing_topics: List[str] = None,
        max_topics: int = 10,
        creativity_level: float = 0.7
    ) -> List[AITopic]:
        """åŸºäºä¸Šä¸‹æ–‡ç”Ÿæˆç›¸å…³ä¸»é¢˜"""
        if not self.is_ai_available():
            return self._generate_fallback_topics(context, max_topics)
        
        try:
            prompt = self._build_topic_generation_prompt(
                context, existing_topics, max_topics, creativity_level
            )
            
            response = await self._call_openai_api(prompt)
            return self._parse_generated_topics(response)
            
        except Exception as e:
            print(f"AIä¸»é¢˜ç”Ÿæˆå¤±è´¥: {e}")
            return self._generate_fallback_topics(context, max_topics)
    
    async def optimize_structure(
        self, 
        mind_map_data: Dict[str, Any],
        optimization_goals: List[str] = None
    ) -> Dict[str, Any]:
        """ä¼˜åŒ–æ€ç»´å¯¼å›¾ç»“æ„"""
        if not self.is_ai_available():
            return self._basic_structure_optimization(mind_map_data)
        
        try:
            prompt = self._build_structure_optimization_prompt(
                mind_map_data, optimization_goals
            )
            
            response = await self._call_openai_api(prompt)
            return self._parse_optimized_structure(response)
            
        except Exception as e:
            print(f"AIç»“æ„ä¼˜åŒ–å¤±è´¥: {e}")
            return self._basic_structure_optimization(mind_map_data)
    
    async def analyze_mind_map_quality(
        self, 
        mind_map_data: Dict[str, Any]
    ) -> AIMindMapAnalysis:
        """åˆ†ææ€ç»´å¯¼å›¾è´¨é‡"""
        if not self.is_ai_available():
            return self._basic_quality_analysis(mind_map_data)
        
        try:
            prompt = self._build_quality_analysis_prompt(mind_map_data)
            response = await self._call_openai_api(prompt)
            return self._parse_quality_analysis(response)
            
        except Exception as e:
            print(f"AIè´¨é‡åˆ†æå¤±è´¥: {e}")
            return self._basic_quality_analysis(mind_map_data)
    
    async def suggest_improvements(
        self, 
        mind_map_data: Dict[str, Any],
        focus_areas: List[str] = None
    ) -> List[str]:
        """æä¾›æ”¹è¿›å»ºè®®"""
        if not self.is_ai_available():
            return self._generate_basic_suggestions(mind_map_data)
        
        try:
            prompt = self._build_improvement_suggestions_prompt(
                mind_map_data, focus_areas
            )
            
            response = await self._call_openai_api(prompt)
            return self._parse_improvement_suggestions(response)
            
        except Exception as e:
            print(f"AIæ”¹è¿›å»ºè®®ç”Ÿæˆå¤±è´¥: {e}")
            return self._generate_basic_suggestions(mind_map_data)
    
    async def categorize_content(
        self, 
        content: str,
        category_definitions: Dict[str, str] = None
    ) -> Dict[str, List[str]]:
        """å¯¹å†…å®¹è¿›è¡Œæ™ºèƒ½åˆ†ç±»"""
        if not self.is_ai_available():
            return self._basic_content_categorization(content)
        
        try:
            prompt = self._build_content_categorization_prompt(
                content, category_definitions
            )
            
            response = await self._call_openai_api(prompt)
            return self._parse_content_categories(response)
            
        except Exception as e:
            print(f"AIå†…å®¹åˆ†ç±»å¤±è´¥: {e}")
            return self._basic_content_categorization(content)
    
    async def extract_keywords(
        self, 
        content: str,
        max_keywords: int = 10
    ) -> List[str]:
        """æå–å…³é”®è¯"""
        if not self.is_ai_available():
            return self._basic_keyword_extraction(content, max_keywords)
        
        try:
            prompt = self._build_keyword_extraction_prompt(content, max_keywords)
            response = await self._call_openai_api(prompt)
            return self._parse_extracted_keywords(response)
            
        except Exception as e:
            print(f"AIå…³é”®è¯æå–å¤±è´¥: {e}")
            return self._basic_keyword_extraction(content, max_keywords)
    
    def get_ai_tools(self) -> List[Dict[str, Any]]:
        """è·å–AIå·¥å…·åˆ—è¡¨"""
        return [
            {
                "name": "ai_generate_topics",
                "description": "AIç”Ÿæˆæ€ç»´å¯¼å›¾ä¸»é¢˜",
                "endpoint": "/ai-generate-topics",
                "method": "POST",
                "parameters": {
                    "topic": "ä¸»é¢˜",
                    "count": "ç”Ÿæˆæ•°é‡",
                    "style": "é£æ ¼"
                }
            },
            {
                "name": "ai_optimize_structure",
                "description": "AIä¼˜åŒ–æ€ç»´å¯¼å›¾ç»“æ„",
                "endpoint": "/ai-optimize-structure",
                "method": "POST",
                "parameters": {
                    "file": "XMindæ–‡ä»¶",
                    "optimization_type": "ä¼˜åŒ–ç±»å‹"
                }
            },
            {
                "name": "ai_analyze_quality",
                "description": "AIåˆ†ææ€ç»´å¯¼å›¾è´¨é‡",
                "endpoint": "/ai-analyze-quality",
                "method": "POST",
                "parameters": {
                    "file": "XMindæ–‡ä»¶"
                }
            }
        ]
    
    async def generate_summary(
        self, 
        mind_map_data: Dict[str, Any],
        max_length: int = 500
    ) -> str:
        """ç”Ÿæˆæ€ç»´å¯¼å›¾æ‘˜è¦"""
        if not self.is_ai_available():
            return self._basic_summary_generation(mind_map_data, max_length)
        
        try:
            prompt = self._build_summary_generation_prompt(mind_map_data, max_length)
            response = await self._call_openai_api(prompt)
            return response.strip()
            
        except Exception as e:
            print(f"AIæ‘˜è¦ç”Ÿæˆå¤±è´¥: {e}")
            return self._basic_summary_generation(mind_map_data, max_length)
    
    # ç§æœ‰æ–¹æ³•
    
    def _build_topic_generation_prompt(
        self, context: str, existing_topics: List[str], max_topics: int, creativity: float
    ) -> str:
        """æ„å»ºä¸»é¢˜ç”Ÿæˆæç¤º"""
        existing_str = ", ".join(existing_topics) if existing_topics else "æ— "
        
        return f"""åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡ç”Ÿæˆç›¸å…³çš„æ€ç»´å¯¼å›¾ä¸»é¢˜ï¼š

ä¸Šä¸‹æ–‡ï¼š{context}

ç°æœ‰ä¸»é¢˜ï¼š{existing_str}

è¯·ç”Ÿæˆ {max_topics} ä¸ªç›¸å…³ä¸»é¢˜ï¼Œè¦æ±‚ï¼š
1. ä¸»é¢˜åº”è¯¥ä¸ä¸Šä¸‹æ–‡é«˜åº¦ç›¸å…³
2. é¿å…ä¸ç°æœ‰ä¸»é¢˜é‡å¤
3. è€ƒè™‘ä¸åŒå±‚æ¬¡å’Œè§’åº¦
4. åˆ›é€ åŠ›æ°´å¹³ï¼š{creativity}

è¯·æŒ‰ä»¥ä¸‹JSONæ ¼å¼è¿”å›ï¼š
[
  {{
    "title": "ä¸»é¢˜æ ‡é¢˜",
    "description": "ä¸»é¢˜æè¿°",
    "priority": 1,
    "relevance_score": 0.9,
    "subtopics": []
  }}
]
"""
    
    def _build_structure_optimization_prompt(
        self, mind_map_data: Dict[str, Any], goals: List[str]
    ) -> str:
        """æ„å»ºç»“æ„ä¼˜åŒ–æç¤º"""
        goals_str = ", ".join(goals) if goals else "æé«˜å¯è¯»æ€§å’Œé€»è¾‘æ€§"
        
        return f"""è¯·ä¼˜åŒ–ä»¥ä¸‹æ€ç»´å¯¼å›¾çš„ç»“æ„ï¼š

å½“å‰ç»“æ„ï¼š
{json.dumps(mind_map_data, ensure_ascii=False, indent=2)}

ä¼˜åŒ–ç›®æ ‡ï¼š{goals_str}

è¯·æä¾›ä¼˜åŒ–åçš„ç»“æ„ï¼Œè¦æ±‚ï¼š
1. ä¿æŒåŸæœ‰å†…å®¹å®Œæ•´æ€§
2. æ”¹å–„é€»è¾‘å±‚æ¬¡å…³ç³»
3. æé«˜å¯è¯»æ€§å’Œç¾è§‚åº¦
4. ç¡®ä¿ç»“æ„å¹³è¡¡

æŒ‰ä»¥ä¸‹JSONæ ¼å¼è¿”å›ä¼˜åŒ–åçš„ç»“æ„ï¼š
{{
  "title": "ä¼˜åŒ–åçš„æ ‡é¢˜",
  "children": [
    {{
      "title": "å­ä¸»é¢˜1",
      "children": []
    }}
  ]
}}
"""
    
    def _build_quality_analysis_prompt(self, mind_map_data: Dict[str, Any]) -> str:
        """æ„å»ºè´¨é‡åˆ†ææç¤º"""
        return f"""è¯·åˆ†æä»¥ä¸‹æ€ç»´å¯¼å›¾çš„è´¨é‡ï¼š

ç»“æ„æ•°æ®ï¼š
{json.dumps(mind_map_data, ensure_ascii=False, indent=2)}

è¯·ä»ä»¥ä¸‹ç»´åº¦è¿›è¡Œåˆ†æï¼š
1. å¤æ‚åº¦ï¼ˆæ˜¯å¦åˆç†ï¼Œæ˜¯å¦è¿‡äºå¤æ‚æˆ–ç®€å•ï¼‰
2. å¹³è¡¡æ€§ï¼ˆå„åˆ†æ”¯æ˜¯å¦å‡è¡¡å‘å±•ï¼‰
3. å®Œæ•´æ€§ï¼ˆå†…å®¹æ˜¯å¦å…¨é¢ï¼‰
4. ç»“æ„é—®é¢˜ï¼ˆå±‚æ¬¡æ˜¯å¦æ¸…æ™°ï¼‰
5. ä¼˜åŒ–æœºä¼š

æŒ‰ä»¥ä¸‹JSONæ ¼å¼è¿”å›åˆ†æç»“æœï¼š
{{
  "complexity_score": 0.8,
  "balance_score": 0.7,
  "completeness_score": 0.9,
  "overall_quality": "è‰¯å¥½",
  "suggestions": ["å»ºè®®1", "å»ºè®®2"],
  "optimization_opportunities": ["æœºä¼š1", "æœºä¼š2"],
  "structural_issues": ["é—®é¢˜1", "é—®é¢˜2"]
}}
"""
    
    async def _call_openai_api(self, prompt: str) -> str:
        """è°ƒç”¨OpenAI API"""
        if not self.openai_client:
            raise Exception("OpenAIå®¢æˆ·ç«¯æœªåˆå§‹åŒ–")
        
        response = await self.openai_client.chat.completions.create(
            model=self.model,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7,
            max_tokens=2000
        )
        
        return response.choices[0].message.content
    
    def _parse_generated_topics(self, response: str) -> List[AITopic]:
        """è§£æç”Ÿæˆçš„ä¸»é¢˜"""
        try:
            # å°è¯•ä»å“åº”ä¸­æå–JSON
            json_match = re.search(r'\[[\s\S]*\]', response)
            if json_match:
                topics_data = json.loads(json_match.group())
            else:
                topics_data = json.loads(response)
            
            topics = []
            for topic_data in topics_data:
                topic = AITopic(
                    title=topic_data.get("title", ""),
                    description=topic_data.get("description"),
                    priority=topic_data.get("priority", 1),
                    relevance_score=topic_data.get("relevance_score", 0.8)
                )
                
                # å¤„ç†å­ä¸»é¢˜
                if "subtopics" in topic_data:
                    for subtopic_data in topic_data["subtopics"]:
                        subtopic = AITopic(
                            title=subtopic_data.get("title", ""),
                            description=subtopic_data.get("description"),
                            priority=subtopic_data.get("priority", 1),
                            relevance_score=subtopic_data.get("relevance_score", 0.8)
                        )
                        topic.subtopics.append(subtopic)
                
                topics.append(topic)
            
            return topics
            
        except Exception as e:
            print(f"è§£æç”Ÿæˆçš„ä¸»é¢˜å¤±è´¥: {e}")
            return []
    
    def _parse_optimized_structure(self, response: str) -> Dict[str, Any]:
        """è§£æä¼˜åŒ–çš„ç»“æ„"""
        try:
            json_match = re.search(r'\{[\s\S]*\}', response)
            if json_match:
                return json.loads(json_match.group())
            else:
                return json.loads(response)
        except Exception as e:
            print(f"è§£æä¼˜åŒ–çš„ç»“æ„å¤±è´¥: {e}")
            return {}
    
    def _parse_quality_analysis(self, response: str) -> AIMindMapAnalysis:
        """è§£æè´¨é‡åˆ†æç»“æœ"""
        try:
            json_match = re.search(r'\{[\s\S]*\}', response)
            if json_match:
                analysis_data = json.loads(json_match.group())
            else:
                analysis_data = json.loads(response)
            
            return AIMindMapAnalysis(
                complexity_score=analysis_data.get("complexity_score", 0.5),
                balance_score=analysis_data.get("balance_score", 0.5),
                completeness_score=analysis_data.get("completeness_score", 0.5),
                overall_quality=analysis_data.get("overall_quality", "æœªçŸ¥"),
                suggestions=analysis_data.get("suggestions", []),
                optimization_opportunities=analysis_data.get("optimization_opportunities", []),
                structural_issues=analysis_data.get("structural_issues", [])
            )
        except Exception as e:
            print(f"è§£æè´¨é‡åˆ†æå¤±è´¥: {e}")
            return AIMindMapAnalysis(0.5, 0.5, 0.5, "æœªçŸ¥", [], [], [])
    
    # å›é€€æ–¹æ³•ï¼ˆå½“AIä¸å¯ç”¨æ—¶ä½¿ç”¨ï¼‰
    
    def _generate_fallback_topics(self, context: str, max_topics: int) -> List[AITopic]:
        """ç”Ÿæˆå›é€€ä¸»é¢˜"""
        # åŸºäºç®€å•å…³é”®è¯åŒ¹é…ç”Ÿæˆä¸»é¢˜
        keywords = self._extract_basic_keywords(context)
        topics = []
        
        for i, keyword in enumerate(keywords[:max_topics]):
            topic = AITopic(
                title=f"{keyword} ç›¸å…³ä¸»é¢˜",
                description=f"å…³äº {keyword} çš„è¯¦ç»†å†…å®¹",
                priority=i + 1,
                relevance_score=0.7
            )
            topics.append(topic)
        
        return topics
    
    def _basic_structure_optimization(self, mind_map_data: Dict[str, Any]) -> Dict[str, Any]:
        """åŸºæœ¬ç»“æ„ä¼˜åŒ–"""
        # ç®€å•çš„ç»“æ„å¹³è¡¡è°ƒæ•´
        if "children" in mind_map_data:
            children = mind_map_data["children"]
            # å¹³è¡¡å„åˆ†æ”¯çš„å­èŠ‚ç‚¹æ•°é‡
            avg_children = len(children) // 2 if children else 0
            
            for child in children:
                if "children" in child and len(child["children"]) > avg_children + 3:
                    # é‡æ–°åˆ†é…å­èŠ‚ç‚¹
                    excess_children = child["children"][avg_children:]
                    child["children"] = child["children"][:avg_children]
                    
                    # åˆ›å»ºæ–°åˆ†æ”¯
                    new_branch = {
                        "title": f"{child['title']} è¡¥å……",
                        "children": excess_children
                    }
                    children.append(new_branch)
        
        return mind_map_data
    
    def _basic_quality_analysis(self, mind_map_data: Dict[str, Any]) -> AIMindMapAnalysis:
        """åŸºæœ¬è´¨é‡åˆ†æ"""
        # ç®€å•çš„å¯å‘å¼åˆ†æ
        total_nodes = self._count_nodes(mind_map_data)
        max_depth = self._calculate_max_depth(mind_map_data)
        
        complexity_score = min(1.0, total_nodes / 50)  # åŸºäºèŠ‚ç‚¹æ•°é‡
        balance_score = self._calculate_balance_score(mind_map_data)
        completeness_score = 0.8 if total_nodes > 5 else 0.5
        
        suggestions = []
        if max_depth > 4:
            suggestions.append("å»ºè®®å‡å°‘å±‚çº§æ·±åº¦")
        if total_nodes < 3:
            suggestions.append("å»ºè®®å¢åŠ æ›´å¤šå†…å®¹")
        if balance_score < 0.6:
            suggestions.append("å»ºè®®å¹³è¡¡å„åˆ†æ”¯å†…å®¹")
        
        return AIMindMapAnalysis(
            complexity_score=complexity_score,
            balance_score=balance_score,
            completeness_score=completeness_score,
            overall_quality="è‰¯å¥½" if complexity_score > 0.6 else "éœ€è¦æ”¹è¿›",
            suggestions=suggestions,
            optimization_opportunities=[],
            structural_issues=[]
        )
    
    def _generate_basic_suggestions(self, mind_map_data: Dict[str, Any]) -> List[str]:
        """ç”ŸæˆåŸºæœ¬å»ºè®®"""
        suggestions = []
        
        total_nodes = self._count_nodes(mind_map_data)
        max_depth = self._calculate_max_depth(mind_map_data)
        
        if total_nodes < 5:
            suggestions.append("å»ºè®®å¢åŠ æ›´å¤šä¸»é¢˜å’Œå­ä¸»é¢˜")
        if max_depth < 2:
            suggestions.append("å»ºè®®å¢åŠ å±‚çº§æ·±åº¦")
        if max_depth > 5:
            suggestions.append("å»ºè®®å‡å°‘å±‚çº§æ·±åº¦ï¼Œä¿æŒç»“æ„æ¸…æ™°")
        
        return suggestions
    
    def _basic_content_categorization(self, content: str) -> Dict[str, List[str]]:
        """åŸºæœ¬å†…å®¹åˆ†ç±»"""
        # ç®€å•çš„å…³é”®è¯åˆ†ç±»
        categories = {
            "æŠ€æœ¯": ["ç¼–ç¨‹", "ä»£ç ", "å¼€å‘", "ç®—æ³•", "ç³»ç»Ÿ"],
            "ä¸šåŠ¡": ["å¸‚åœº", "é”€å”®", "å®¢æˆ·", "äº§å“", "ç­–ç•¥"],
            "å­¦ä¹ ": ["çŸ¥è¯†", "æŠ€èƒ½", "åŸ¹è®­", "æ•™è‚²", "ç ”ç©¶"]
        }
        
        result = {category: [] for category in categories}
        
        for category, keywords in categories.items():
            for keyword in keywords:
                if keyword in content:
                    result[category].append(keyword)
        
        return result
    
    def _basic_keyword_extraction(self, content: str, max_keywords: int) -> List[str]:
        """åŸºæœ¬å…³é”®è¯æå–"""
        # ç®€å•çš„è¯é¢‘ç»Ÿè®¡
        words = re.findall(r'\b\w+\b', content.lower())
        word_freq = {}
        
        for word in words:
            if len(word) > 2:  # è¿‡æ»¤çŸ­è¯
                word_freq[word] = word_freq.get(word, 0) + 1
        
        # æŒ‰é¢‘ç‡æ’åº
        sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)
        return [word for word, freq in sorted_words[:max_keywords]]
    
    def _basic_summary_generation(self, mind_map_data: Dict[str, Any], max_length: int) -> str:
        """åŸºæœ¬æ‘˜è¦ç”Ÿæˆ"""
        title = mind_map_data.get("title", "æ€ç»´å¯¼å›¾")
        total_nodes = self._count_nodes(mind_map_data)
        
        summary = f"æ€ç»´å¯¼å›¾ã€Š{title}ã€‹åŒ…å«{total_nodes}ä¸ªèŠ‚ç‚¹ã€‚"
        
        if "children" in mind_map_data and mind_map_data["children"]:
            main_topics = [child.get("title", "") for child in mind_map_data["children"][:3]]
            if main_topics:
                summary += f"ä¸»è¦æ¶µç›–ï¼š{', '.join(main_topics)}ç­‰ä¸»é¢˜ã€‚"
        
        return summary[:max_length]
    
    def _extract_basic_keywords(self, content: str) -> List[str]:
        """æå–åŸºæœ¬å…³é”®è¯"""
        words = re.findall(r'\b\w+\b', content.lower())
        # ç®€å•çš„åœç”¨è¯è¿‡æ»¤
        stop_words = {"çš„", "äº†", "åœ¨", "æ˜¯", "æœ‰", "å’Œ", "ä¸", "æˆ–", "ä½†", "è€Œ", "åŠ", "ç­‰", "ç­‰ç­‰"}
        keywords = [word for word in words if len(word) > 2 and word not in stop_words]
        return list(set(keywords))[:10]  # è¿”å›å»é‡åçš„å‰10ä¸ª
    
    def _count_nodes(self, data: Dict[str, Any]) -> int:
        """è®¡ç®—èŠ‚ç‚¹æ•°é‡"""
        count = 1  # å½“å‰èŠ‚ç‚¹
        if "children" in data:
            for child in data["children"]:
                count += self._count_nodes(child)
        return count
    
    def _calculate_max_depth(self, data: Dict[str, Any], current_depth: int = 0) -> int:
        """è®¡ç®—æœ€å¤§æ·±åº¦"""
        max_depth = current_depth
        if "children" in data:
            for child in data["children"]:
                child_depth = self._calculate_max_depth(child, current_depth + 1)
                max_depth = max(max_depth, child_depth)
        return max_depth
    
    def _calculate_balance_score(self, data: Dict[str, Any]) -> float:
        """è®¡ç®—å¹³è¡¡æ€§è¯„åˆ†"""
        if "children" not in data or not data["children"]:
            return 1.0
        
        child_counts = []
        for child in data["children"]:
            count = self._count_nodes(child)
            child_counts.append(count)
        
        if not child_counts:
            return 1.0
        
        # è®¡ç®—æ ‡å‡†å·®ï¼Œè¯„ä¼°å¹³è¡¡æ€§
        avg = sum(child_counts) / len(child_counts)
        variance = sum((x - avg) ** 2 for x in child_counts) / len(child_counts)
        std_dev = variance ** 0.5
        
        # è½¬æ¢ä¸º0-1è¯„åˆ†ï¼Œæ ‡å‡†å·®è¶Šå°è¯„åˆ†è¶Šé«˜
        balance_score = max(0, 1 - (std_dev / (avg + 1)))
        return balance_score


# ä½¿ç”¨ç¤ºä¾‹å’Œæµ‹è¯•
if __name__ == "__main__":
    import asyncio
    
    async def test_ai_extensions():
        """æµ‹è¯•AIæ‰©å±•åŠŸèƒ½"""
        print("ğŸ§ª æµ‹è¯•XMind AIæ‰©å±•åŠŸèƒ½")
        print("=" * 50)
        
        # åˆ›å»ºAIæ‰©å±•å®ä¾‹ï¼ˆä¸ä½¿ç”¨çœŸå®APIï¼‰
        ai_ext = XMindAIExtensions()
        
        # æµ‹è¯•ä¸»é¢˜ç”Ÿæˆ
        print("\nğŸ“ æµ‹è¯•ä¸»é¢˜ç”Ÿæˆ...")
        context = "äººå·¥æ™ºèƒ½å’Œæœºå™¨å­¦ä¹ "
        topics = await ai_ext.generate_topics(context, max_topics=5)
        print(f"ç”Ÿæˆçš„ä¸»é¢˜: {[topic.title for topic in topics]}")
        
        # æµ‹è¯•è´¨é‡åˆ†æ
        print("\nğŸ” æµ‹è¯•è´¨é‡åˆ†æ...")
        sample_mind_map = {
            "title": "AIå­¦ä¹ è·¯å¾„",
            "children": [
                {"title": "åŸºç¡€çŸ¥è¯†", "children": [
                    {"title": "æ•°å­¦åŸºç¡€"},
                    {"title": "ç¼–ç¨‹åŸºç¡€"}
                ]},
                {"title": "æœºå™¨å­¦ä¹ ", "children": [
                    {"title": "ç›‘ç£å­¦ä¹ "},
                    {"title": "æ— ç›‘ç£å­¦ä¹ "}
                ]}
            ]
        }
        
        analysis = await ai_ext.analyze_mind_map_quality(sample_mind_map)
        print(f"å¤æ‚åº¦è¯„åˆ†: {analysis.complexity_score}")
        print(f"å¹³è¡¡æ€§è¯„åˆ†: {analysis.balance_score}")
        print(f"æ•´ä½“è´¨é‡: {analysis.overall_quality}")
        print(f"å»ºè®®: {analysis.suggestions}")
        
        # æµ‹è¯•ç»“æ„ä¼˜åŒ–
        print("\nğŸ”„ æµ‹è¯•ç»“æ„ä¼˜åŒ–...")
        optimized = await ai_ext.optimize_structure(sample_mind_map)
        print(f"ä¼˜åŒ–åæ ‡é¢˜: {optimized.get('title', 'N/A')}")
        
        # æµ‹è¯•å…³é”®è¯æå–
        print("\nğŸ”‘ æµ‹è¯•å…³é”®è¯æå–...")
        content = "äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒä¼å›¾äº†è§£æ™ºèƒ½çš„å®è´¨ï¼Œå¹¶ç”Ÿäº§å‡ºä¸€ç§æ–°çš„èƒ½ä»¥äººç±»æ™ºèƒ½ç›¸ä¼¼çš„æ–¹å¼åšå‡ºååº”çš„æ™ºèƒ½æœºå™¨"
        keywords = await ai_ext.extract_keywords(content, max_keywords=5)
        print(f"æå–çš„å…³é”®è¯: {keywords}")
        
        # æµ‹è¯•å†…å®¹åˆ†ç±»
        print("\nğŸ“Š æµ‹è¯•å†…å®¹åˆ†ç±»...")
        categories = await ai_ext.categorize_content(content)
        print(f"åˆ†ç±»ç»“æœ: {categories}")
        
        print("\n" + "=" * 50)
        print("âœ… AIæ‰©å±•åŠŸèƒ½æµ‹è¯•å®Œæˆ!")
    
    asyncio.run(test_ai_extensions())