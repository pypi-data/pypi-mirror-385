"""
Implementation of DataFrame attributes and methods using overload.
"""

import operator
import re
import warnings
from collections import namedtuple

import numba
import numpy as np
import pandas as pd
import pandas.core.computation.expr
import pandas.core.computation.ops
import pandas.core.computation.parsing
import pandas.core.computation.scope
import pandas.io.formats.printing
from numba.core import cgutils, ir, types
from numba.core.imputils import (
    RefType,
    impl_ret_borrowed,
    impl_ret_new_ref,
    iternext_impl,
    lower_builtin,
)
from numba.core.ir_utils import mk_unique_var, next_label
from numba.core.typing import signature
from numba.core.typing.templates import AbstractTemplate, infer_global
from numba.extending import (
    lower_getattr,
    models,
    overload,
    overload_attribute,
    overload_method,
    register_model,
    type_callable,
)

import bodo
import bodo.pandas as bd
from bodo.hiframes.datetime_date_ext import datetime_date_array_type
from bodo.hiframes.datetime_timedelta_ext import (
    _no_input,
    timedelta_array_type,
)
from bodo.hiframes.pd_categorical_ext import CategoricalArrayType
from bodo.hiframes.pd_dataframe_ext import (
    DataFrameType,
    check_runtime_cols_unsupported,
    handle_inplace_df_type_change,
)
from bodo.hiframes.pd_index_ext import (
    DatetimeIndexType,
    RangeIndexType,
    StringIndexType,
    is_pd_index_type,
)
from bodo.hiframes.pd_multi_index_ext import MultiIndexType
from bodo.hiframes.pd_series_ext import SeriesType, if_series_to_array_type
from bodo.hiframes.pd_timestamp_ext import pd_timestamp_tz_naive_type
from bodo.hiframes.rolling import is_supported_shift_array_type
from bodo.hiframes.split_impl import string_array_split_view_type
from bodo.hiframes.time_ext import TimeArrayType
from bodo.hiframes.timestamptz_ext import timestamptz_array_type
from bodo.libs.array_item_arr_ext import ArrayItemArrayType
from bodo.libs.binary_arr_ext import binary_array_type
from bodo.libs.bool_arr_ext import BooleanArrayType, boolean_array_type
from bodo.libs.decimal_arr_ext import DecimalArrayType
from bodo.libs.dict_arr_ext import dict_str_arr_type
from bodo.libs.float_arr_ext import FloatingArrayType
from bodo.libs.int_arr_ext import IntegerArrayType
from bodo.libs.interval_arr_ext import IntervalArrayType
from bodo.libs.map_arr_ext import MapArrayType
from bodo.libs.str_arr_ext import string_array_type
from bodo.libs.str_ext import string_type
from bodo.libs.struct_arr_ext import StructArrayType
from bodo.utils import tracing
from bodo.utils.transform import (
    bodo_types_with_params,
    gen_const_tup,
    no_side_effect_call_tuples,
)
from bodo.utils.typing import (
    BodoError,
    BodoWarning,
    ColNamesMetaType,
    check_unsupported_args,
    dtype_to_array_type,
    ensure_constant_arg,
    ensure_constant_values,
    get_castable_arr_dtype,
    get_index_data_arr_types,
    get_index_names,
    get_literal_value,
    get_nullable_and_non_nullable_types,
    get_overload_const_bool,
    get_overload_const_int,
    get_overload_const_list,
    get_overload_const_str,
    get_overload_const_tuple,
    get_overload_constant_dict,
    get_overload_constant_series,
    is_common_scalar_dtype,
    is_literal_type,
    is_overload_bool,
    is_overload_bool_list,
    is_overload_constant_bool,
    is_overload_constant_dict,
    is_overload_constant_int,
    is_overload_constant_list,
    is_overload_constant_series,
    is_overload_constant_str,
    is_overload_constant_tuple,
    is_overload_false,
    is_overload_int,
    is_overload_none,
    is_overload_true,
    is_overload_zero,
    is_scalar_type,
    parse_dtype,
    raise_bodo_error,
    unliteral_val,
)
from bodo.utils.utils import (
    bodo_exec,
    is_array_typ,
)


@overload_attribute(DataFrameType, "index", inline="always")
def overload_dataframe_index(df):
    return lambda df: bodo.hiframes.pd_dataframe_ext.get_dataframe_index(
        df
    )  # pragma: no cover


def generate_col_to_index_func_text(col_names: tuple):
    """
    Takes a tuple of column names and generates the necessary func_text
    to replace a constant version of the column names with the appropriate
    index that would be generated by df.columns.
    """

    if all(isinstance(a, str) for a in col_names) or all(
        isinstance(a, bytes) for a in col_names
    ):
        bin_str_arr = f"bodo.utils.conversion.coerce_to_array({col_names})"
        return f"bodo.hiframes.pd_index_ext.init_binary_str_index({bin_str_arr})\n"
    elif all(isinstance(a, (int, float)) for a in col_names):  # pragma: no cover
        # TODO(ehsan): test
        arr = f"bodo.utils.conversion.coerce_to_array({col_names})"
        return f"bodo.hiframes.pd_index_ext.init_numeric_index({arr})\n"
    else:
        return f"bodo.hiframes.pd_index_ext.init_heter_index({col_names})\n"


@overload_attribute(DataFrameType, "columns", inline="always")
def overload_dataframe_columns(df):
    func_text = "def impl(df):\n"
    if df.has_runtime_cols:
        func_text += (
            "  return bodo.hiframes.pd_dataframe_ext.get_dataframe_column_names(df)\n"
        )
    else:
        col_index = bodo.hiframes.dataframe_impl.generate_col_to_index_func_text(
            df.columns
        )
        func_text += f"  return {col_index}"
    loc_vars = {}
    exec(func_text, {"bodo": bodo}, loc_vars)
    impl = loc_vars["impl"]
    return impl


@overload_attribute(DataFrameType, "values")
def overload_dataframe_values(df):
    check_runtime_cols_unsupported(df, "DataFrame.values")

    if not is_df_values_numpy_supported_dftyp(df):
        raise_bodo_error(
            "DataFrame.values: only supported for dataframes containing numeric values"
        )

    n_cols = len(df.columns)
    # convert nullable data columns to float to match Pandas behavior
    nullable_arr_cols = {
        i
        for i in range(n_cols)
        if isinstance(df.data[i], (IntegerArrayType, FloatingArrayType))
    }
    data_args = ", ".join(
        "bodo.hiframes.pd_dataframe_ext.get_dataframe_data(df, {}){}".format(
            i, ".astype(float)" if i in nullable_arr_cols else ""
        )
        for i in range(n_cols)
    )
    func_text = "def f(df):\n".format()
    func_text += f"    return np.stack(({data_args},), 1)\n"

    loc_vars = {}
    exec(func_text, {"bodo": bodo, "np": np}, loc_vars)
    f = loc_vars["f"]
    return f


@overload_method(DataFrameType, "to_numpy", inline="always", no_unliteral=True)
def overload_dataframe_to_numpy(df, dtype=None, copy=False, na_value=_no_input):
    # The copy argument can be ignored here since we always copy the data
    # (our underlying structures are fully columnar which should be copied to get a
    # matrix). This is consistent with Pandas since copy=False doesn't guarantee it
    # won't be copied.
    check_runtime_cols_unsupported(df, "DataFrame.to_numpy()")
    if not is_df_values_numpy_supported_dftyp(df):
        raise_bodo_error(
            "DataFrame.to_numpy(): only supported for dataframes containing numeric values"
        )

    args_dict = {
        "dtype": dtype,
        "na_value": na_value,
    }

    args_default_dict = {
        "dtype": None,
        "na_value": _no_input,
    }
    check_unsupported_args(
        "DataFrame.to_numpy",
        args_dict,
        args_default_dict,
        package_name="pandas",
        module_name="DataFrame",
    )

    def impl(df, dtype=None, copy=False, na_value=_no_input):  # pragma: no cover
        return df.values

    return impl


@overload_attribute(DataFrameType, "ndim", inline="always")
def overload_dataframe_ndim(df):
    return lambda df: 2  # pragma: no cover


@overload_attribute(DataFrameType, "size")
def overload_dataframe_size(df):
    if df.has_runtime_cols:
        # If we have determine columns at runtime it can't be a
        # compile time constant.
        def impl(df):  # pragma: no cover
            t = bodo.hiframes.pd_dataframe_ext.get_dataframe_table(df)
            num_cols = bodo.hiframes.table.compute_num_runtime_columns(t)
            return num_cols * len(t)

        return impl

    ncols = len(df.columns)
    return lambda df: ncols * len(df)  # pragma: no cover


@lower_getattr(DataFrameType, "shape")
def lower_dataframe_shape(context, builder, typ, val):
    """
    Lowering call for df.shape. Used to separate typing
    from lowering.
    """
    impl = overload_dataframe_shape(typ)
    return context.compile_internal(
        builder, impl, types.Tuple([types.int64, types.int64])(typ), (val,)
    )


def overload_dataframe_shape(df):
    if df.has_runtime_cols:
        # If we have determine columns at runtime it can't be a
        # compile time constant.
        def impl(df):  # pragma: no cover
            t = bodo.hiframes.pd_dataframe_ext.get_dataframe_table(df)
            num_cols = bodo.hiframes.table.compute_num_runtime_columns(t)
            return (len(t), num_cols)

        return impl

    ncols = len(df.columns)
    return lambda df: (len(df), ncols)  # pragma: no cover


@overload_attribute(DataFrameType, "dtypes")
def overload_dataframe_dtypes(df):
    """Support df.dtypes by getting dtype values from underlying arrays"""
    check_runtime_cols_unsupported(df, "DataFrame.dtypes")

    func_text = "def impl(df):\n"

    data = ", ".join(
        f"bodo.hiframes.pd_dataframe_ext.get_dataframe_data(df, {i}).dtype\n"
        for i in range(len(df.columns))
    )
    comma = "," if len(df.columns) == 1 else ""
    # NOTE: using init_heter_index instead of just df.columns since df.dtypes could be
    # input to df.astype(), which requires constant input (see test_df_astype_dtypes)
    index = f"bodo.hiframes.pd_index_ext.init_heter_index({df.columns})"
    func_text += f"  return bodo.hiframes.pd_series_ext.init_series(({data}{comma}), {index}, None)\n"

    loc_vars = {}
    exec(
        func_text,
        {
            "bodo": bodo,
        },
        loc_vars,
    )
    impl = loc_vars["impl"]
    return impl


@overload_attribute(DataFrameType, "empty")
def overload_dataframe_empty(df):
    check_runtime_cols_unsupported(df, "DataFrame.empty")
    if len(df.columns) == 0:
        return lambda df: True  # pragma: no cover
    return lambda df: len(df) == 0  # pragma: no cover


@overload_method(DataFrameType, "assign", no_unliteral=True)
def overload_dataframe_assign(df, **kwargs):
    check_runtime_cols_unsupported(df, "DataFrame.assign()")
    # raise error to let typing pass handle it, since **kwargs is not supported in
    # overload
    raise_bodo_error("Invalid df.assign() call")


@overload_method(DataFrameType, "insert", no_unliteral=True)
def overload_dataframe_insert(df, loc, column, value, allow_duplicates=False):
    # raise error to let typing pass handle it, since it updates the dataframe inplace
    check_runtime_cols_unsupported(df, "DataFrame.insert()")
    raise_bodo_error("Invalid df.insert() call")


def _get_dtype_str(dtype):
    """return string representation of dtype value
    'dtype' could be actual value or type. TODO(ehsan): refactor to be consistent.
    """
    # function cases like str
    if isinstance(dtype, types.Function):  # pragma: no cover
        if dtype.key[0] is str:
            return "'str'"
        elif dtype.key[0] is float:
            return "float"
        elif dtype.key[0] is int:
            return "int"
        elif dtype.key[0] is bool:
            return "bool"
        else:
            raise BodoError(f"invalid dtype: {dtype}")
    # Handle Pandas Int type directly. This can occur when
    # we have a LiteralStrKeyDict so the type is the actual
    # Pandas dtype. See test_table_del_astype.
    if type(dtype) in bodo.libs.int_arr_ext.pd_int_dtype_classes:
        return dtype.name

    # e.g. dtype(int64)
    if isinstance(dtype, types.DTypeSpec):
        dtype = dtype.dtype

    # cases like np.float32
    if isinstance(dtype, types.functions.NumberClass):
        return f"'{dtype.key}'"

    # Object dtype. Place before string to avoid issues
    # with np.object_. For some reason this produces
    # TypeError: Cannot interpret 'StringDtype' as a data type
    if isinstance(dtype, types.PyObject) or dtype in (object, "object"):
        return "'object'"

    # pd.StringDtype
    if dtype in (bodo.libs.str_arr_ext.string_dtype, pd.StringDtype()):
        return "str"

    return f"'{dtype}'"


@overload_method(
    DataFrameType,
    "astype",
    inline="always",
    no_unliteral=True,
)
def overload_dataframe_astype(
    df,
    dtype,
    copy=True,
    errors="raise",
    _bodo_nan_to_str=True,
    _bodo_object_typeref=None,
):
    # _bodo_nan_to_str is a bodo specific argument that indicate is string should be converted
    # to "NaN" or NA

    # _bodo_object_typeref is a bodo internal argument used for optimizations of calls that
    # generate the dtype or dtypes inside Bodo code (i.e. `df1.astype(df2.dtypes)`)
    # This is used to get coverage parity with Spark's cast because some types are too generic
    # to do any actual cast in Pandas (i.e. object for datetime.date).
    check_runtime_cols_unsupported(df, "DataFrame.astype()")
    # check unsupported arguments
    args_dict = {
        "errors": errors,
    }
    args_default_dict = {"errors": "raise"}
    check_unsupported_args(
        "df.astype",
        args_dict,
        args_default_dict,
        package_name="pandas",
        module_name="DataFrame",
    )

    # If dtype is a string, force it to be a literal
    if dtype == types.unicode_type:
        raise_bodo_error(
            "DataFrame.astype(): 'dtype' when passed as string must be a constant value"
        )

    if not is_overload_bool(copy):  # pragma: no cover
        raise BodoError("DataFrame.astype(): 'copy' must be a boolean value")

    # just call astype() on all column arrays
    # TODO: support categorical, dt64, etc.
    extra_globals = None
    header = "def bodo_dataframe_astype(df, dtype, copy=True, errors='raise', _bodo_nan_to_str=True, _bodo_object_typeref=None):\n"
    if df.is_table_format:
        # Table format must always pass the final table as its output.
        extra_globals = {}
        header += "  table = bodo.hiframes.pd_dataframe_ext.get_dataframe_table(df)\n"
        # With table format we just generate the output table type in each case.
        arr_typ_list = []

    if _bodo_object_typeref is not None:
        # If _bodo_object_typeref is provided then we have a typeref that should be used to generate the appropriate
        # schema
        assert isinstance(_bodo_object_typeref, types.TypeRef), (
            "Bodo schema used in DataFrame.astype should be a TypeRef"
        )
        schema_type = _bodo_object_typeref.instance_type
        assert isinstance(schema_type, DataFrameType), (
            "Bodo schema used in DataFrame.astype is only supported for DataFrame schemas"
        )
        if df.is_table_format:
            # Find the names in the new schema and convert those.
            for i, name in enumerate(df.columns):
                if name in schema_type.column_index:
                    # We cast this column
                    idx = schema_type.column_index[name]
                    arr_typ = schema_type.data[idx]
                else:
                    arr_typ = df.data[i]
                arr_typ_list.append(arr_typ)
        else:
            extra_globals = {}
            name_map = {}
            # Generate the global variables for the types. To match the existing astype
            # implementation we convert certain arrays to scalars.
            for i, name in enumerate(schema_type.columns):
                arr_typ = schema_type.data[i]
                extra_globals[f"_bodo_schema{i}"] = get_castable_arr_dtype(arr_typ)
                name_map[name] = f"_bodo_schema{i}"
            data_args = ", ".join(
                f"bodo.utils.conversion.fix_arr_dtype(bodo.hiframes.pd_dataframe_ext.get_dataframe_data(df, {i}), {name_map[c]}, copy, nan_to_str=_bodo_nan_to_str, from_series=True)"
                if c in name_map
                else f"bodo.hiframes.pd_dataframe_ext.get_dataframe_data(df, {i})"
                for i, c in enumerate(df.columns)
            )
    elif is_overload_constant_dict(dtype) or is_overload_constant_series(dtype):
        dtype_const = (
            get_overload_constant_dict(dtype)
            if is_overload_constant_dict(dtype)
            else dict(get_overload_constant_series(dtype))
        )
        if df.is_table_format:
            dtype_const = {
                name: dtype_to_array_type(parse_dtype(dtype))
                for name, dtype in dtype_const.items()
            }
            for i, name in enumerate(df.columns):
                if name in dtype_const:
                    # We cast this column
                    arr_typ = dtype_const[name]
                else:
                    arr_typ = df.data[i]
                arr_typ_list.append(arr_typ)
        else:
            data_args = ", ".join(
                f"bodo.utils.conversion.fix_arr_dtype(bodo.hiframes.pd_dataframe_ext.get_dataframe_data(df, {i}), {_get_dtype_str(dtype_const[c])}, copy, nan_to_str=_bodo_nan_to_str, from_series=True)"
                if c in dtype_const
                else f"bodo.hiframes.pd_dataframe_ext.get_dataframe_data(df, {i})"
                for i, c in enumerate(df.columns)
            )
    else:
        if df.is_table_format:
            arr_typ = dtype_to_array_type(parse_dtype(dtype))
            arr_typ_list = [arr_typ] * len(df.columns)
        else:
            data_args = ", ".join(
                f"bodo.utils.conversion.fix_arr_dtype(bodo.hiframes.pd_dataframe_ext.get_dataframe_data(df, {i}), dtype, copy, nan_to_str=_bodo_nan_to_str, from_series=True)"
                for i in range(len(df.columns))
            )
    if df.is_table_format:
        table_type = bodo.types.TableType(tuple(arr_typ_list))
        extra_globals["out_table_typ"] = table_type
        data_args = "bodo.utils.table_utils.table_astype(table, out_table_typ, copy, _bodo_nan_to_str)"

    return _gen_init_df(header, df.columns, data_args, extra_globals=extra_globals)


@overload_method(
    DataFrameType,
    "copy",
    inline="always",
    no_unliteral=True,
    jit_options={"cache": True},
)
def overload_dataframe_copy(df, deep=True):
    # just call copy() on all arrays
    check_runtime_cols_unsupported(df, "DataFrame.copy()")

    header = "def bodo_dataframe_copy(df, deep=True):\n"
    extra_globals = None
    if df.is_table_format:
        header += "  table = bodo.hiframes.pd_dataframe_ext.get_dataframe_table(df)\n"
        output_arr_typ = types.none
        extra_globals = {"output_arr_typ": output_arr_typ}
        if is_overload_false(deep):
            data_args = (
                "bodo.utils.table_utils.generate_mappable_table_func("
                + "table, "
                + "None, "
                + "output_arr_typ, "
                + "True)"
            )
        elif is_overload_true(deep):
            data_args = (
                "bodo.utils.table_utils.generate_mappable_table_func("
                + "table, "
                + "'copy', "
                + "output_arr_typ, "
                + "True)"
            )
        else:
            data_args = (
                "bodo.utils.table_utils.generate_mappable_table_func("
                + "table, "
                + "'copy', "
                + "output_arr_typ, "
                + "True) if deep else bodo.utils.table_utils.generate_mappable_table_func("
                + "table, "
                + "None, "
                + "output_arr_typ, "
                + "True)"
            )
    else:
        data_outs = []
        for i in range(len(df.columns)):
            arr = f"bodo.hiframes.pd_dataframe_ext.get_dataframe_data(df, {i})"
            if is_overload_true(deep):
                data_outs.append(arr + ".copy()")
            elif is_overload_false(deep):
                data_outs.append(arr)
            else:
                data_outs.append(f"{arr}.copy() if deep else {arr}")
        data_args = ", ".join(data_outs)

    return _gen_init_df(
        header,
        df.columns,
        data_args,
        extra_globals=extra_globals,
    )


@overload_method(
    DataFrameType,
    "rename",
    inline="always",
    no_unliteral=True,
    jit_options={"cache": True},
)
def overload_dataframe_rename(
    df,
    mapper=None,
    index=None,
    columns=None,
    axis=None,
    copy=True,
    inplace=False,
    level=None,
    errors="ignore",
    _bodo_transformed=False,
):
    check_runtime_cols_unsupported(df, "DataFrame.rename()")

    handle_inplace_df_type_change(inplace, _bodo_transformed, "rename")

    # check unsupported arguments
    args_dict = {
        "index": index,
        "level": level,
        "errors": errors,
    }
    args_default_dict = {"index": None, "level": None, "errors": "ignore"}

    check_unsupported_args(
        "DataFrame.rename",
        args_dict,
        args_default_dict,
        package_name="pandas",
        module_name="DataFrame",
    )

    if not (is_overload_constant_bool(inplace)):
        raise BodoError(
            "DataFrame.rename(): 'inplace' keyword only supports boolean constant assignment"
        )

    # columns should be constant dictionary
    if not is_overload_none(mapper):
        if not is_overload_none(columns):
            raise BodoError(
                "DataFrame.rename(): Cannot specify both 'mapper' and 'columns'"
            )
        if not (is_overload_constant_int(axis) and get_overload_const_int(axis) == 1):
            raise BodoError("DataFrame.rename(): 'mapper' only supported with axis=1")
        if not is_overload_constant_dict(mapper):
            raise_bodo_error(
                "'mapper' argument to DataFrame.rename() should be a constant dictionary"
            )

        col_map = get_overload_constant_dict(mapper)

    elif not is_overload_none(columns):
        if not is_overload_none(axis):
            raise BodoError(
                "DataFrame.rename(): Cannot specify both 'axis' and 'columns'"
            )
        if not is_overload_constant_dict(columns):
            raise_bodo_error(
                "'columns' argument to DataFrame.rename() should be a constant dictionary"
            )

        col_map = get_overload_constant_dict(columns)
    else:
        raise_bodo_error(
            "DataFrame.rename(): must pass columns either via 'mapper' and 'axis'=1 or 'columns'"
        )
    new_cols = tuple(
        [col_map.get(df.columns[i], df.columns[i]) for i in range(len(df.columns))]
    )

    header = (
        "def bodo_dataframe_rename(df, mapper=None, index=None, columns=None, axis=None, "
        "copy=True, inplace=False, level=None, errors='ignore', _bodo_transformed=False):\n"
    )
    extra_globals = None

    if df.is_table_format:
        header += "  table = bodo.hiframes.pd_dataframe_ext.get_dataframe_table(df)\n"
        output_arr_typ = types.none
        extra_globals = {"output_arr_typ": output_arr_typ}
        if is_overload_false(copy):
            data_args = (
                "bodo.utils.table_utils.generate_mappable_table_func("
                + "table, "
                + "None, "
                + "output_arr_typ, "
                + "True)"
            )
        elif is_overload_true(copy):
            data_args = (
                "bodo.utils.table_utils.generate_mappable_table_func("
                + "table, "
                + "'copy', "
                + "output_arr_typ, "
                + "True)"
            )
        else:
            data_args = (
                "bodo.utils.table_utils.generate_mappable_table_func("
                + "table, "
                + "'copy', "
                + "output_arr_typ, "
                + "True) if copy else bodo.utils.table_utils.generate_mappable_table_func("
                + "table, "
                + "None, "
                + "output_arr_typ, "
                + "True)"
            )
    else:
        data_outs = []
        for i in range(len(df.columns)):
            arr = f"bodo.hiframes.pd_dataframe_ext.get_dataframe_data(df, {i})"
            if is_overload_true(copy):
                data_outs.append(arr + ".copy()")
            elif is_overload_false(copy):
                data_outs.append(arr)
            else:
                data_outs.append(f"{arr}.copy() if copy else {arr}")
        data_args = ", ".join(data_outs)

    return _gen_init_df(
        header,
        new_cols,
        data_args,
        extra_globals=extra_globals,
    )


@overload_method(
    DataFrameType, "filter", no_unliteral=True, jit_options={"cache": True}
)
def overload_dataframe_filter(df, items=None, like=None, regex=None, axis=None):
    check_runtime_cols_unsupported(df, "DataFrame.filter()")

    items_set = not is_overload_none(items)
    like_set = not is_overload_none(like)
    regex_set = not is_overload_none(regex)
    only_one_arg = items_set ^ like_set ^ regex_set
    no_args_set = not (items_set or like_set or regex_set)

    if no_args_set:
        raise BodoError(
            "DataFrame.filter(): one of keyword arguments `items`, `like`, and `regex` must be supplied"
        )
    if not only_one_arg:
        raise BodoError(
            "DataFrame.filter(): keyword arguments `items`, `like`, and `regex` are mutually exclusive"
        )

    # the default info axis is 'columns' for DataFrame
    if is_overload_none(axis):
        axis = "columns"

    # get the axis for filtering
    if is_overload_constant_str(axis):
        axis = get_overload_const_str(axis)
        if axis not in {"index", "columns"}:
            raise_bodo_error(
                'DataFrame.filter(): keyword arguments `axis` must be either "index" or "columns" if string'
            )
        axis_typ = 0 if axis == "index" else 1
    elif is_overload_constant_int(axis):
        axis = get_overload_const_int(axis)
        if axis not in {0, 1}:
            raise_bodo_error(
                "DataFrame.filter(): keyword arguments `axis` must be either 0 or 1 if integer"
            )
        axis_typ = axis
    else:
        raise_bodo_error(
            "DataFrame.filter(): keyword arguments `axis` must be constant string or integer"
        )

    assert axis_typ in {0, 1}

    # start with the signature of the function
    func_text = (
        "def bodo_dataframe_filter(df, items=None, like=None, regex=None, axis=None):\n"
    )

    if axis_typ == 0:
        raise BodoError(
            "DataFrame.filter(): filtering based on index is not supported."
        )

    # axis is columns
    if axis_typ == 1:
        var_names = []
        selected_cols = []
        selected_col_indices = []

        # extract the arguments if not none
        if items_set:
            if is_overload_constant_list(items):
                items_list = get_overload_const_list(items)
            else:
                raise BodoError(
                    "Dataframe.filter(): argument 'items' must be a list of constant strings."
                )
        if like_set:
            if is_overload_constant_str(like):
                like_val = get_overload_const_str(like)
            else:
                raise BodoError(
                    "Dataframe.filter(): argument 'like' must be a constant string."
                )
        if regex_set:
            if is_overload_constant_str(regex):
                regex_val = get_overload_const_str(regex)
                regex_prog = re.compile(regex_val)
            else:
                raise BodoError(
                    "Dataframe.filter(): argument 'regex' must be a constant string."
                )

        # get the selected columns
        for i, c in enumerate(df.columns):
            if (
                (not is_overload_none(items) and c in items_list)
                or (not is_overload_none(like) and like_val in str(c))
                or (not is_overload_none(regex) and regex_prog.search(str(c)))
            ):
                selected_cols.append(c)
                selected_col_indices.append(i)

        # generate the code text
        for i in selected_col_indices:
            var_name = f"data_{i}"
            var_names.append(var_name)
            func_text += f"  {var_name} = bodo.hiframes.pd_dataframe_ext.get_dataframe_data(df, {i})\n"

        data_args = ", ".join(var_names)

        # generate the df object using the init function
        return _gen_init_df(func_text, selected_cols, data_args)


@overload_method(
    DataFrameType,
    "isna",
    inline="always",
    no_unliteral=True,
    jit_options={"cache": True},
)
@overload_method(
    DataFrameType,
    "isnull",
    inline="always",
    no_unliteral=True,
    jit_options={"cache": True},
)
def overload_dataframe_isna(df):
    check_runtime_cols_unsupported(df, "DataFrame.isna()")

    header = "def bodo_dataframe_isna(df):\n"
    extra_globals = None

    if df.is_table_format:
        # isna generates a boolean array for every column
        output_arr_typ = bodo.types.boolean_array_type
        extra_globals = {"output_arr_typ": output_arr_typ}
        data_args = (
            "bodo.utils.table_utils.generate_mappable_table_func("
            + "bodo.hiframes.pd_dataframe_ext.get_dataframe_table(df), "
            + "'bodo.libs.array_ops.array_op_isna', "
            + "output_arr_typ, "
            + "False)"
        )
    else:
        # call isna() on column Series
        data_args = ", ".join(
            f"bodo.libs.array_ops.array_op_isna(bodo.hiframes.pd_dataframe_ext.get_dataframe_data(df, {i}))"
            for i in range(len(df.columns))
        )
    return _gen_init_df(
        header,
        df.columns,
        data_args,
        extra_globals=extra_globals,
    )


@overload_method(
    DataFrameType,
    "select_dtypes",
    inline="always",
    no_unliteral=True,
    jit_options={"cache": True},
)
def overload_dataframe_select_dtypes(df, include=None, exclude=None):
    check_runtime_cols_unsupported(df, "DataFrame.select_dtypes")
    # Check that at least one of include or exclude exists
    include_none = is_overload_none(include)
    exclude_none = is_overload_none(exclude)
    fname = "DataFrame.select_dtypes"

    bodo.hiframes.pd_timestamp_ext.check_tz_aware_unsupported(
        df, "DataFrame.select_dtypes()"
    )

    if include_none and exclude_none:
        raise_bodo_error(
            "DataFrame.select_dtypes() At least one of include or exclude must not be none"
        )

    def is_legal_input(elem):
        # TODO(Nick): Replace with the correct type check
        return (
            is_overload_constant_str(elem)
            or isinstance(elem, types.DTypeSpec)
            or isinstance(elem, types.Function)
        )

    if not include_none:
        # If the input is a list process each elem in the list
        if is_overload_constant_list(include):
            include = get_overload_const_list(include)
            include_types = [
                dtype_to_array_type(parse_dtype(elem, fname)) for elem in include
            ]
        # If its a scalar then just make it a list of 1 element
        elif is_legal_input(include):
            include_types = [dtype_to_array_type(parse_dtype(include, fname))]
        else:
            raise_bodo_error(
                "DataFrame.select_dtypes() only supports constant strings or types as arguments"
            )

        include_types = get_nullable_and_non_nullable_types(include_types)
        # Filter columns to those with a matching datatype
        # TODO(Nick): Add more general support for type rules:
        # ex. np.number for all numeric types, object for all obj types,
        # "string" for all string types
        chosen_columns = tuple(
            c for i, c in enumerate(df.columns) if df.data[i] in include_types
        )
    else:
        chosen_columns = df.columns

    if not exclude_none:
        # If the input is a list process each elem in the list
        if is_overload_constant_list(exclude):
            exclude = get_overload_const_list(exclude)
            exclude_types = [
                dtype_to_array_type(parse_dtype(elem, fname)) for elem in exclude
            ]
        # If its a scalar then just make it a list of 1 element
        elif is_legal_input(exclude):
            exclude_types = [dtype_to_array_type(parse_dtype(exclude, fname))]
        else:
            raise_bodo_error(
                "DataFrame.select_dtypes() only supports constant strings or types as arguments"
            )
        exclude_types = get_nullable_and_non_nullable_types(exclude_types)
        # Filter columns to those without a matching datatype
        # TODO(Nick): Add more general support for type rules:
        # ex. np.number for all numeric types, object for all obj types,
        # "string" for all string types
        chosen_columns = tuple(
            c
            for c in chosen_columns
            if df.data[df.column_index[c]] not in exclude_types
        )

    data_args = ", ".join(
        f"bodo.hiframes.pd_dataframe_ext.get_dataframe_data(df, {df.column_index[c]})"
        for c in chosen_columns
    )
    # Define our function
    header = "def bodo_dataframe_select_dtypes(df, include=None, exclude=None):\n"

    return _gen_init_df(header, chosen_columns, data_args)


@overload_method(
    DataFrameType,
    "notna",
    inline="always",
    no_unliteral=True,
    jit_options={"cache": True},
)
@overload_method(
    DataFrameType,
    "notnull",
    inline="always",
    no_unliteral=True,
    jit_options={"cache": True},
)
def overload_dataframe_notna(df):
    check_runtime_cols_unsupported(df, "DataFrame.notna()")
    header = "def bodo_dataframe_notna(df):\n"
    extra_globals = None
    if df.is_table_format:
        # notna generates a boolean array for every column
        output_arr_typ = bodo.types.boolean_array_type
        extra_globals = {"output_arr_typ": output_arr_typ}
        data_args = (
            "bodo.utils.table_utils.generate_mappable_table_func("
            + "bodo.hiframes.pd_dataframe_ext.get_dataframe_table(df), "
            + "'~bodo.libs.array_ops.array_op_isna', "
            + "output_arr_typ, "
            + "False)"
        )
    else:
        # call notna() on column Series
        data_args = ", ".join(
            f"bodo.libs.array_ops.array_op_isna(bodo.hiframes.pd_dataframe_ext.get_dataframe_data(df, {i})) == False"
            for i in range(len(df.columns))
        )
    return _gen_init_df(
        header,
        df.columns,
        data_args,
        extra_globals=extra_globals,
    )


def overload_dataframe_head(df, n=5):
    # This function is called by the inlining in compiler.py
    if df.is_table_format:
        data_args = "bodo.hiframes.pd_dataframe_ext.get_dataframe_table(df)[:n]"
    else:
        data_args = ", ".join(
            f"bodo.hiframes.pd_dataframe_ext.get_dataframe_data(df, {i})[:n]"
            for i in range(len(df.columns))
        )
    header = "def bodo_dataframe_head(df, n=5):\n"
    index = "bodo.hiframes.pd_dataframe_ext.get_dataframe_index(df)[:n]"
    return _gen_init_df(header, df.columns, data_args, index)


# Include lowering for safety.
@lower_builtin("df.head", DataFrameType, types.Integer)
# Include Omitted in case the arguement isn't provided
@lower_builtin("df.head", DataFrameType, types.Omitted)
def dataframe_head_lower(context, builder, sig, args):
    impl = overload_dataframe_head(*sig.args)
    return context.compile_internal(builder, impl, sig, args)


@overload_method(
    DataFrameType,
    "tail",
    inline="always",
    no_unliteral=True,
    jit_options={"cache": True},
)
def overload_dataframe_tail(df, n=5):
    check_runtime_cols_unsupported(df, "DataFrame.tail()")
    # n must be an integer for indexing.
    if not is_overload_int(n):
        raise BodoError("Dataframe.tail(): 'n' must be an Integer")
    # call tail() on column arrays
    if df.is_table_format:
        data_args = "bodo.hiframes.pd_dataframe_ext.get_dataframe_table(df)[m:]"
    else:
        # call tail() on column arrays
        data_args = ", ".join(
            f"bodo.hiframes.pd_dataframe_ext.get_dataframe_data(df, {i})[m:]"
            for i in range(len(df.columns))
        )
    header = "def bodo_dataframe_tail(df, n=5):\n"
    header += "  m = bodo.hiframes.series_impl.tail_slice(len(df), n)\n"
    index = "bodo.hiframes.pd_dataframe_ext.get_dataframe_index(df)[m:]"
    return _gen_init_df(header, df.columns, data_args, index)


@overload_method(
    DataFrameType,
    "first",
    inline="always",
    no_unliteral=True,
    jit_options={"cache": True},
)
def overload_dataframe_first(df, offset):
    check_runtime_cols_unsupported(df, "DataFrame.first()")
    supp_types = (
        types.unicode_type,
        bodo.types.month_begin_type,
        bodo.types.month_end_type,
        bodo.types.week_type,
        bodo.types.date_offset_type,
    )
    if not isinstance(df.index, DatetimeIndexType):
        raise BodoError("DataFrame.first(): only supports a DatetimeIndex index")
    if types.unliteral(offset) not in supp_types:
        raise BodoError("DataFrame.first(): 'offset' must be an string or DateOffset")

    # determine first() on underlying arrays
    index = "bodo.hiframes.pd_dataframe_ext.get_dataframe_index(df)[:valid_entries]"
    data_args = ", ".join(
        f"bodo.hiframes.pd_dataframe_ext.get_dataframe_data(df, {i})[:valid_entries]"
        for i in range(len(df.columns))
    )
    header = "def bodo_dataframe_first(df, offset):\n"
    header += "  df_index = bodo.hiframes.pd_dataframe_ext.get_dataframe_index(df)\n"
    header += "  if len(df_index):\n"
    header += "    start_date = df_index[0]\n"
    header += "    valid_entries = bodo.libs.array_kernels.get_valid_entries_from_date_offset(df_index, offset, start_date, False)\n"
    header += "  else:\n"
    header += "    valid_entries = 0\n"
    return _gen_init_df(header, df.columns, data_args, index)


@overload_method(
    DataFrameType,
    "last",
    inline="always",
    no_unliteral=True,
    jit_options={"cache": True},
)
def overload_dataframe_last(df, offset):
    check_runtime_cols_unsupported(df, "DataFrame.last()")
    supp_types = (
        types.unicode_type,
        bodo.types.month_begin_type,
        bodo.types.month_end_type,
        bodo.types.week_type,
        bodo.types.date_offset_type,
    )
    if not isinstance(df.index, DatetimeIndexType):
        raise BodoError("DataFrame.last(): only supports a DatetimeIndex index")
    if types.unliteral(offset) not in supp_types:
        raise BodoError("DataFrame.last(): 'offset' must be an string or DateOffset")

    # determine last() on underlying arrays
    index = (
        "bodo.hiframes.pd_dataframe_ext.get_dataframe_index(df)[len(df)-valid_entries:]"
    )
    data_args = ", ".join(
        f"bodo.hiframes.pd_dataframe_ext.get_dataframe_data(df, {i})[len(df)-valid_entries:]"
        for i in range(len(df.columns))
    )
    header = "def bodo_dataframe_last(df, offset):\n"
    header += "  df_index = bodo.hiframes.pd_dataframe_ext.get_dataframe_index(df)\n"
    header += "  if len(df_index):\n"
    header += "    final_date = df_index[-1]\n"
    header += "    valid_entries = bodo.libs.array_kernels.get_valid_entries_from_date_offset(df_index, offset, final_date, True)\n"
    header += "  else:\n"
    header += "    valid_entries = 0\n"
    return _gen_init_df(header, df.columns, data_args, index)


@overload_method(DataFrameType, "to_string", no_unliteral=True)
def to_string_overload(
    df,
    buf=None,
    columns=None,
    col_space=None,
    header=True,
    index=True,
    na_rep="NaN",
    formatters=None,
    float_format=None,
    sparsify=None,
    index_names=True,
    justify=None,
    max_rows=None,
    min_rows=None,
    max_cols=None,
    show_dimensions=False,
    decimal=".",
    line_width=None,
    max_colwidth=None,
    encoding=None,
):
    check_runtime_cols_unsupported(df, "DataFrame.to_string()")

    def impl(
        df,
        buf=None,
        columns=None,
        col_space=None,
        header=True,
        index=True,
        na_rep="NaN",
        formatters=None,
        float_format=None,
        sparsify=None,
        index_names=True,
        justify=None,
        max_rows=None,
        min_rows=None,
        max_cols=None,
        show_dimensions=False,
        decimal=".",
        line_width=None,
        max_colwidth=None,
        encoding=None,
    ):  # pragma: no cover
        with numba.objmode(res="string"):
            res = df.to_string(
                buf=buf,
                columns=columns,
                col_space=col_space,
                header=header,
                index=index,
                na_rep=na_rep,
                formatters=formatters,
                float_format=float_format,
                sparsify=sparsify,
                index_names=index_names,
                justify=justify,
                max_rows=max_rows,
                min_rows=min_rows,
                max_cols=max_cols,
                show_dimensions=show_dimensions,
                decimal=decimal,
                line_width=line_width,
                max_colwidth=max_colwidth,
                encoding=encoding,
            )
        return res

    return impl


@overload_method(
    DataFrameType,
    "isin",
    inline="always",
    no_unliteral=True,
    jit_options={"cache": True},
)
def overload_dataframe_isin(df, values):
    check_runtime_cols_unsupported(df, "DataFrame.isin()")
    # TODO: call isin on Series
    # TODO: make sure df indices match?
    # TODO: dictionary case
    from bodo.utils.typing import is_iterable_type

    func_text = "def bodo_dataframe_isin(df, values):\n"
    other_colmap = {}
    df_case = False
    # dataframe case
    if isinstance(values, DataFrameType):
        df_case = True
        for i, c in enumerate(df.columns):
            if c in values.column_index:
                v_name = f"val{i}"
                func_text += f"  {v_name} = bodo.hiframes.pd_dataframe_ext.get_dataframe_data(values, {values.column_index[c]})\n"
                other_colmap[c] = v_name
    # Series contains (x in y) does not seem to be supported?
    elif is_iterable_type(values) and not isinstance(values, SeriesType):
        # general iterable (e.g. list, set, array) case
        # TODO: handle passed in dict case (pass colname to func?)
        other_colmap = dict.fromkeys(df.columns, "values")
    else:
        raise_bodo_error(f"pd.isin(): not supported for type {values}")

    data = []
    for i in range(len(df.columns)):
        v_name = f"data{i}"
        func_text += (
            f"  {v_name} = bodo.hiframes.pd_dataframe_ext.get_dataframe_data(df, {i})\n"
        )
        data.append(v_name)

    out_data = [f"out{i}" for i in range(len(df.columns))]

    isin_func = """
  numba.parfors.parfor.init_prange()
  n = len({0})
  m = len({1})
  {2} = bodo.libs.bool_arr_ext.alloc_bool_array(n)
  for i in numba.parfors.parfor.internal_prange(n):
    {2}[i] = {0}[i] == {1}[i] if i < m else False
"""

    isin_vals_func = """
  numba.parfors.parfor.init_prange()
  n = len({0})
  {2} = bodo.libs.bool_arr_ext.alloc_bool_array(n)
  for i in numba.parfors.parfor.internal_prange(n):
    {2}[i] = {0}[i] in {1}
"""
    bool_arr_func = "  {} = bodo.libs.bool_arr_ext.alloc_false_bool_array(len(df))\n"
    for i, (cname, in_var) in enumerate(zip(df.columns, data)):
        if cname in other_colmap:
            other_col_var = other_colmap[cname]
            if df_case:
                func_text += isin_func.format(in_var, other_col_var, out_data[i])
            else:
                func_text += isin_vals_func.format(in_var, other_col_var, out_data[i])
        else:
            func_text += bool_arr_func.format(out_data[i])
    return _gen_init_df(func_text, df.columns, ",".join(out_data))


@overload_method(
    DataFrameType,
    "abs",
    inline="always",
    no_unliteral=True,
    jit_options={"cache": True},
)
def overload_dataframe_abs(df):
    check_runtime_cols_unsupported(df, "DataFrame.abs()")
    # only works for numerical data and Timedelta

    for arr_typ in df.data:
        if not (
            isinstance(arr_typ.dtype, types.Number)
            or arr_typ.dtype == bodo.types.timedelta64ns
        ):
            raise_bodo_error(
                f"DataFrame.abs(): Only supported for numeric and Timedelta. Encountered array with dtype {arr_typ.dtype}"
            )

    n_cols = len(df.columns)
    data_args = ", ".join(f"df.iloc[:, {i}].abs()" for i in range(n_cols))
    header = "def bodo_dataframe_abs(df):\n"
    return _gen_init_df(header, df.columns, data_args)


def overload_dataframe_corr(df, method="pearson", min_periods=1):
    # This function is called by the inlining in compiler.py
    numeric_cols = tuple(
        c
        for c, d in zip(df.columns, df.data)
        if bodo.utils.typing._is_pandas_numeric_dtype(d.dtype)
    )
    # TODO: support empty dataframe
    assert len(numeric_cols) != 0

    # convert input matrix to float64 if necessary
    typ_conv = ""
    if not any(d == types.float64 for d in df.data):
        typ_conv = ".astype(np.float64)"

    arr_args = ", ".join(
        "bodo.hiframes.pd_dataframe_ext.get_dataframe_data(df, {}){}".format(
            df.column_index[c],
            ".astype(np.float64)"
            if (
                isinstance(
                    df.data[df.column_index[c]], (IntegerArrayType, FloatingArrayType)
                )
                or df.data[df.column_index[c]] == boolean_array_type
            )
            else "",
        )
        for c in numeric_cols
    )
    mat = f"np.stack(({arr_args},), 1){typ_conv}"

    data_args = ", ".join(f"res[:,{i}]" for i in range(len(numeric_cols)))
    index = f"{generate_col_to_index_func_text(numeric_cols)}\n"

    header = "def bodo_dataframe_corr(df, method='pearson', min_periods=1):\n"
    header += f"  mat = {mat}\n"
    header += "  res = bodo.libs.array_kernels.nancorr(mat, 0, min_periods)\n"
    return _gen_init_df(header, numeric_cols, data_args, index)


@lower_builtin("df.corr", DataFrameType, types.VarArg(types.Any))
def dataframe_corr_lower(context, builder, sig, args):
    impl = overload_dataframe_corr(*sig.args)
    return context.compile_internal(builder, impl, sig, args)


@overload_method(
    DataFrameType,
    "cov",
    inline="always",
    no_unliteral=True,
    jit_options={"cache": True},
)
def overload_dataframe_cov(df, min_periods=None, ddof=1):
    check_runtime_cols_unsupported(df, "DataFrame.cov()")

    unsupported_args = {"ddof": ddof}
    arg_defaults = {"ddof": 1}
    check_unsupported_args(
        "DataFrame.cov",
        unsupported_args,
        arg_defaults,
        package_name="pandas",
        module_name="DataFrame",
    )

    # TODO: support calling np.cov() when there is no NA
    minpv = "1" if is_overload_none(min_periods) else "min_periods"

    numeric_cols = tuple(
        c
        for c, d in zip(df.columns, df.data)
        if bodo.utils.typing._is_pandas_numeric_dtype(d.dtype)
    )
    # TODO: support empty dataframe
    if len(numeric_cols) == 0:
        raise_bodo_error("DataFrame.cov(): requires non-empty dataframe")

    # convert input matrix to float64 if necessary
    typ_conv = ""
    if not any(d == types.float64 for d in df.data):
        typ_conv = ".astype(np.float64)"

    arr_args = ", ".join(
        "bodo.hiframes.pd_dataframe_ext.get_dataframe_data(df, {}){}".format(
            df.column_index[c],
            ".astype(np.float64)"
            if (
                isinstance(
                    df.data[df.column_index[c]], (IntegerArrayType, FloatingArrayType)
                )
                or df.data[df.column_index[c]] == boolean_array_type
            )
            else "",
        )
        for c in numeric_cols
    )
    mat = f"np.stack(({arr_args},), 1){typ_conv}"

    data_args = ", ".join(f"res[:,{i}]" for i in range(len(numeric_cols)))
    index = f"pd.Index({numeric_cols})\n"

    header = "def bodo_dataframe_cov(df, min_periods=None, ddof=1):\n"
    header += f"  mat = {mat}\n"
    header += f"  res = bodo.libs.array_kernels.nancorr(mat, 1, {minpv})\n"
    return _gen_init_df(header, numeric_cols, data_args, index)


@overload_method(DataFrameType, "count", inline="always", no_unliteral=True)
def overload_dataframe_count(df, axis=0, level=None, numeric_only=False):
    check_runtime_cols_unsupported(df, "DataFrame.count()")
    # TODO: numeric_only flag
    unsupported_args = {"axis": axis, "level": level, "numeric_only": numeric_only}
    arg_defaults = {"axis": 0, "level": None, "numeric_only": False}
    check_unsupported_args(
        "DataFrame.count",
        unsupported_args,
        arg_defaults,
        package_name="pandas",
        module_name="DataFrame",
    )

    data_args = ", ".join(
        f"bodo.libs.array_ops.array_op_count(bodo.hiframes.pd_dataframe_ext.get_dataframe_data(df, {i}))"
        for i in range(len(df.columns))
    )
    func_text = "def impl(df, axis=0, level=None, numeric_only=False):\n"
    func_text += f"  data = np.array([{data_args}])\n"
    col_index = bodo.hiframes.dataframe_impl.generate_col_to_index_func_text(df.columns)
    func_text += (
        f"  return bodo.hiframes.pd_series_ext.init_series(data, {col_index})\n"
    )
    loc_vars = {}
    exec(func_text, {"bodo": bodo, "np": np}, loc_vars)
    impl = loc_vars["impl"]
    return impl


@overload_method(DataFrameType, "nunique", inline="always", no_unliteral=True)
def overload_dataframe_nunique(df, axis=0, dropna=True):
    check_runtime_cols_unsupported(df, "DataFrame.unique()")
    unsupported_args = {"axis": axis}
    arg_defaults = {"axis": 0}
    if not is_overload_bool(dropna):
        raise BodoError("DataFrame.nunique: dropna must be a boolean value")
    check_unsupported_args(
        "DataFrame.nunique",
        unsupported_args,
        arg_defaults,
        package_name="pandas",
        module_name="DataFrame",
    )
    data_args = ", ".join(
        f"bodo.libs.array_kernels.nunique(bodo.hiframes.pd_dataframe_ext.get_dataframe_data(df, {i}), dropna)"
        for i in range(len(df.columns))
    )
    func_text = "def impl(df, axis=0, dropna=True):\n"
    func_text += f"  data = np.asarray(({data_args},))\n"
    col_index = bodo.hiframes.dataframe_impl.generate_col_to_index_func_text(df.columns)
    func_text += (
        f"  return bodo.hiframes.pd_series_ext.init_series(data, {col_index})\n"
    )
    loc_vars = {}
    exec(func_text, {"bodo": bodo, "np": np}, loc_vars)
    impl = loc_vars["impl"]
    return impl


@overload_method(DataFrameType, "prod", inline="always", no_unliteral=True)
@overload_method(DataFrameType, "product", inline="always", no_unliteral=True)
def overload_dataframe_prod(
    df, axis=None, skipna=None, level=None, numeric_only=None, min_count=0
):
    check_runtime_cols_unsupported(df, "DataFrame.prod()")
    unsupported_args = {
        "skipna": skipna,
        "level": level,
        "numeric_only": numeric_only,
        "min_count": min_count,
    }
    arg_defaults = {"skipna": None, "level": None, "numeric_only": None, "min_count": 0}
    check_unsupported_args(
        "DataFrame.prod",
        unsupported_args,
        arg_defaults,
        package_name="pandas",
        module_name="DataFrame",
    )

    return _gen_reduce_impl(df, "prod", axis=axis)


@overload_method(DataFrameType, "sum", inline="always", no_unliteral=True)
def overload_dataframe_sum(
    df, axis=None, skipna=None, level=None, numeric_only=None, min_count=0
):
    check_runtime_cols_unsupported(df, "DataFrame.sum()")
    unsupported_args = {
        "skipna": skipna,
        "level": level,
        "numeric_only": numeric_only,
        "min_count": min_count,
    }
    arg_defaults = {"skipna": None, "level": None, "numeric_only": None, "min_count": 0}
    check_unsupported_args(
        "DataFrame.sum",
        unsupported_args,
        arg_defaults,
        package_name="pandas",
        module_name="DataFrame",
    )

    return _gen_reduce_impl(df, "sum", axis=axis)


@overload_method(DataFrameType, "max", inline="always", no_unliteral=True)
def overload_dataframe_max(df, axis=None, skipna=None, level=None, numeric_only=None):
    check_runtime_cols_unsupported(df, "DataFrame.max()")
    unsupported_args = {"skipna": skipna, "level": level, "numeric_only": numeric_only}
    arg_defaults = {"skipna": None, "level": None, "numeric_only": None}
    check_unsupported_args(
        "DataFrame.max",
        unsupported_args,
        arg_defaults,
        package_name="pandas",
        module_name="DataFrame",
    )

    return _gen_reduce_impl(df, "max", axis=axis)


@overload_method(DataFrameType, "min", inline="always", no_unliteral=True)
def overload_dataframe_min(df, axis=None, skipna=None, level=None, numeric_only=None):
    check_runtime_cols_unsupported(df, "DataFrame.min()")
    unsupported_args = {"skipna": skipna, "level": level, "numeric_only": numeric_only}
    arg_defaults = {"skipna": None, "level": None, "numeric_only": None}
    check_unsupported_args(
        "DataFrame.min",
        unsupported_args,
        arg_defaults,
        package_name="pandas",
        module_name="DataFrame",
    )
    return _gen_reduce_impl(df, "min", axis=axis)


@overload_method(DataFrameType, "mean", inline="always", no_unliteral=True)
def overload_dataframe_mean(df, axis=None, skipna=None, level=None, numeric_only=None):
    check_runtime_cols_unsupported(df, "DataFrame.mean()")
    unsupported_args = {"skipna": skipna, "level": level, "numeric_only": numeric_only}
    arg_defaults = {"skipna": None, "level": None, "numeric_only": None}
    check_unsupported_args(
        "DataFrame.mean",
        unsupported_args,
        arg_defaults,
        package_name="pandas",
        module_name="DataFrame",
    )

    return _gen_reduce_impl(df, "mean", axis=axis)


@overload_method(DataFrameType, "var", inline="always", no_unliteral=True)
def overload_dataframe_var(
    df, axis=None, skipna=None, level=None, ddof=1, numeric_only=None
):
    check_runtime_cols_unsupported(df, "DataFrame.var()")
    unsupported_args = {
        "skipna": skipna,
        "level": level,
        "ddof": ddof,
        "numeric_only": numeric_only,
    }
    arg_defaults = {"skipna": None, "level": None, "ddof": 1, "numeric_only": None}
    check_unsupported_args(
        "DataFrame.var",
        unsupported_args,
        arg_defaults,
        package_name="pandas",
        module_name="DataFrame",
    )

    return _gen_reduce_impl(df, "var", axis=axis)


@overload_method(DataFrameType, "std", inline="always", no_unliteral=True)
def overload_dataframe_std(
    df, axis=None, skipna=None, level=None, ddof=1, numeric_only=None
):
    check_runtime_cols_unsupported(df, "DataFrame.std()")
    unsupported_args = {
        "skipna": skipna,
        "level": level,
        "ddof": ddof,
        "numeric_only": numeric_only,
    }
    arg_defaults = {"skipna": None, "level": None, "ddof": 1, "numeric_only": None}
    check_unsupported_args(
        "DataFrame.std",
        unsupported_args,
        arg_defaults,
        package_name="pandas",
        module_name="DataFrame",
    )

    return _gen_reduce_impl(df, "std", axis=axis)


@overload_method(DataFrameType, "median", inline="always", no_unliteral=True)
def overload_dataframe_median(
    df, axis=None, skipna=None, level=None, numeric_only=None
):
    check_runtime_cols_unsupported(df, "DataFrame.median()")
    unsupported_args = {"skipna": skipna, "level": level, "numeric_only": numeric_only}
    arg_defaults = {"skipna": None, "level": None, "numeric_only": None}
    check_unsupported_args(
        "DataFrame.median",
        unsupported_args,
        arg_defaults,
        package_name="pandas",
        module_name="DataFrame",
    )

    return _gen_reduce_impl(df, "median", axis=axis)


@overload_method(DataFrameType, "quantile", inline="always", no_unliteral=True)
def overload_dataframe_quantile(
    df, q=0.5, axis=0, numeric_only=True, interpolation="linear"
):
    check_runtime_cols_unsupported(df, "DataFrame.quantile()")
    unsupported_args = {"numeric_only": numeric_only, "interpolation": interpolation}
    arg_defaults = {"numeric_only": True, "interpolation": "linear"}
    check_unsupported_args(
        "DataFrame.quantile",
        unsupported_args,
        arg_defaults,
        package_name="pandas",
        module_name="DataFrame",
    )

    # TODO: name is str(q)
    return _gen_reduce_impl(df, "quantile", "q", axis=axis)


@overload_method(DataFrameType, "idxmax", inline="always", no_unliteral=True)
def overload_dataframe_idxmax(df, axis=0, skipna=True):
    check_runtime_cols_unsupported(df, "DataFrame.idxmax()")
    # TODO: [BE-281] Support idxmax with axis=1
    unsupported_args = {"axis": axis, "skipna": skipna}
    arg_defaults = {"axis": 0, "skipna": True}
    check_unsupported_args(
        "DataFrame.idxmax",
        unsupported_args,
        arg_defaults,
        package_name="pandas",
        module_name="DataFrame",
    )

    # Pandas restrictions:
    # Only supported for numeric types with numpy arrays
    # - int, floats, bool, dt64, td64. (maybe complex)
    # We also support categorical and nullable arrays
    for coltype in df.data:
        if not (
            bodo.utils.utils.is_np_array_typ(coltype)
            and (
                coltype.dtype in [bodo.types.datetime64ns, bodo.types.timedelta64ns]
                or isinstance(coltype.dtype, (types.Number, types.Boolean))
            )
            or isinstance(
                coltype,
                (
                    bodo.types.IntegerArrayType,
                    bodo.types.FloatingArrayType,
                    bodo.types.CategoricalArrayType,
                ),
            )
            or coltype
            in [bodo.types.boolean_array_type, bodo.types.datetime_date_array_type]
        ):
            raise BodoError(
                f"DataFrame.idxmax() only supported for numeric column types. Column type: {coltype} not supported."
            )
        if (
            isinstance(coltype, bodo.types.CategoricalArrayType)
            and not coltype.dtype.ordered
        ):
            raise BodoError("DataFrame.idxmax(): categorical columns must be ordered")

    return _gen_reduce_impl(df, "idxmax", axis=axis)


@overload_method(DataFrameType, "idxmin", inline="always", no_unliteral=True)
def overload_dataframe_idxmin(df, axis=0, skipna=True):
    check_runtime_cols_unsupported(df, "DataFrame.idxmin()")
    # TODO: [BE-281] Support idxmin with axis=1
    unsupported_args = {"axis": axis, "skipna": skipna}
    arg_defaults = {"axis": 0, "skipna": True}
    check_unsupported_args(
        "DataFrame.idxmin",
        unsupported_args,
        arg_defaults,
        package_name="pandas",
        module_name="DataFrame",
    )

    # Pandas restrictions:
    # Only supported for numeric types with numpy arrays
    # - int, floats, bool, dt64, td64. (maybe complex)
    # We also support categorical and nullable arrays
    for coltype in df.data:
        if not (
            bodo.utils.utils.is_np_array_typ(coltype)
            and (
                coltype.dtype in [bodo.types.datetime64ns, bodo.types.timedelta64ns]
                or isinstance(coltype.dtype, (types.Number, types.Boolean))
            )
            or isinstance(
                coltype,
                (
                    bodo.types.IntegerArrayType,
                    bodo.types.FloatingArrayType,
                    bodo.types.CategoricalArrayType,
                ),
            )
            or coltype
            in [bodo.types.boolean_array_type, bodo.types.datetime_date_array_type]
        ):
            raise BodoError(
                f"DataFrame.idxmin() only supported for numeric column types. Column type: {coltype} not supported."
            )
        if (
            isinstance(coltype, bodo.types.CategoricalArrayType)
            and not coltype.dtype.ordered
        ):
            raise BodoError("DataFrame.idxmin(): categorical columns must be ordered")

    return _gen_reduce_impl(df, "idxmin", axis=axis)


@overload_method(DataFrameType, "infer_objects", inline="always")
def overload_dataframe_infer_objects(df):
    """
    Performs deep copy as per pandas FrameOrSeries infer_objects() implementation:
    https://github.com/pandas-dev/pandas/blob/v1.3.5/pandas/core/generic.py#L5987-L6031
    (eventually calls https://github.com/pandas-dev/pandas/blob/master/pandas/core/internals/blocks.py#L580-L592)
    """
    check_runtime_cols_unsupported(df, "DataFrame.infer_objects()")
    return lambda df: df.copy()  # pragma: no cover


def _gen_reduce_impl(df, func_name, args=None, axis=None):
    """generate implementation for dataframe reduction functions like min, max, sum, ..."""
    args = "" if is_overload_none(args) else args

    # axis is 0 by default. Some reduce functions have None as the default value.
    if is_overload_none(axis):
        axis = 0
    elif is_overload_constant_int(axis):
        axis = get_overload_const_int(axis)
    else:
        raise_bodo_error(f"DataFrame.{func_name}: axis must be a constant Integer")

    assert axis in (0, 1), f"invalid axis argument for DataFrame.{func_name}"

    if func_name in ("idxmax", "idxmin"):
        out_colnames = df.columns
    else:
        # TODO: numeric_only=None tries its best: core/frame.py/DataFrame/_reduce
        numeric_cols = tuple(
            c
            for c, d in zip(df.columns, df.data)
            if bodo.utils.typing._is_pandas_numeric_dtype(d.dtype)
        )
        out_colnames = numeric_cols

    # TODO: support empty dataframe
    assert len(out_colnames) != 0

    # Ensure that the dtypes can be supported and raise a BodoError if
    # they cannot be combined. This is a safety net and should generally
    # be handled by the individual functions.
    try:
        # TODO: Only generate common types when necessary to prevent errors.
        if func_name in ("idxmax", "idxmin") and axis == 0:
            comm_dtype = None
        else:
            dtypes = [
                numba.np.numpy_support.as_dtype(df.data[df.column_index[c]].dtype)
                for c in out_colnames
            ]
            comm_dtype = numba.np.numpy_support.from_dtype(np.result_type(*dtypes))
    # If we have a Bodo or Numba type that isn't implemented in
    # Numpy, we will get a NumbaNotImplementedError
    except numba.core.errors.NumbaNotImplementedError:
        raise BodoError(
            f"Dataframe.{func_name}() with column types: {df.data} could not be merged to a common type."
        )
    # If we get types that aren't compatible in Numpy, we will get a
    # DTypePromotionError
    except np.exceptions.DTypePromotionError:
        raise BodoError(
            f"Dataframe.{func_name}() with column types: {df.data} could not be merged to a common type."
        )

    # generate function signature
    minc = ""
    if func_name in ("sum", "prod"):
        minc = ", min_count=0"

    ddof = ""
    if func_name in ("var", "std"):
        ddof = "ddof=1, "

    func_text = f"def impl(df, axis=None, skipna=None, level=None,{ddof} numeric_only=None{minc}):\n"
    if func_name == "quantile":
        func_text = (
            "def impl(df, q=0.5, axis=0, numeric_only=True, interpolation='linear'):\n"
        )
    if func_name in ("idxmax", "idxmin"):
        func_text = "def impl(df, axis=0, skipna=True):\n"

    if axis == 0:
        func_text += _gen_reduce_impl_axis0(
            df, func_name, out_colnames, comm_dtype, args
        )
    else:
        func_text += _gen_reduce_impl_axis1(func_name, out_colnames, comm_dtype, df)

    loc_vars = {}
    exec(func_text, {"bodo": bodo, "np": np, "pd": pd, "numba": numba}, loc_vars)
    impl = loc_vars["impl"]
    return impl


def _gen_reduce_impl_axis0(df, func_name, out_colnames, comm_dtype, args):
    """generate function body for dataframe reduction across rows"""
    # XXX: use common type for min/max to avoid float for ints due to NaN
    # TODO: handle NaN for ints better
    typ_cast = ""
    if func_name in ("min", "max"):
        typ_cast = f", dtype=np.{comm_dtype}"

    # XXX pandas combines all column values so int8/float32 results in float32
    # not float64
    if comm_dtype == types.float32 and func_name in (
        "sum",
        "prod",
        "mean",
        "var",
        "std",
        "median",
    ):
        typ_cast = ", dtype=np.float32"

    kernel_func_name = f"bodo.libs.array_ops.array_op_{func_name}"
    kernel_args = ""
    if func_name in ["sum", "prod"]:
        # skipna isn't supported by sum or prod
        kernel_args = "True, min_count"
    elif func_name in ["idxmax", "idxmin"]:
        kernel_args = "index"
    elif func_name == "quantile":
        kernel_args = "q"
    elif func_name in ["std", "var"]:
        # skipna isn't supported by std or var
        kernel_args = "True, ddof"
    elif func_name == "median":
        # skipna isn't supported by median
        kernel_args = "True"

    data_args = ", ".join(
        f"{kernel_func_name}(bodo.hiframes.pd_dataframe_ext.get_dataframe_data(df, {df.column_index[c]}), {kernel_args})"
        for c in out_colnames
    )

    func_text = ""
    # data conversion
    if func_name in ("idxmax", "idxmin"):
        # idxmax/idxmin don't cast type since just index value is produced
        # but need to convert tuple of Timestamp to dt64 array
        # see idxmax test numeric_df_value[6]
        func_text += (
            "  index = bodo.hiframes.pd_dataframe_ext.get_dataframe_index(df)\n"
        )
        func_text += f"  data = bodo.utils.conversion.coerce_to_array(({data_args},))\n"
    else:
        func_text += f"  data = np.asarray(({data_args},){typ_cast})\n"
    func_text += f"  return bodo.hiframes.pd_series_ext.init_series(data, pd.Index({out_colnames}))\n"
    return func_text


def _gen_reduce_impl_axis1(func_name, out_colnames, comm_dtype, df_type):
    """generate function body for dataframe reduction across columns"""
    col_inds = [df_type.column_index[c] for c in out_colnames]
    index = "bodo.hiframes.pd_dataframe_ext.get_dataframe_index(df)"
    data_args = "\n    ".join(
        f"arr_{i} = bodo.hiframes.pd_dataframe_ext.get_dataframe_data(df, {i})"
        for i in col_inds
    )
    data_accesses = "\n        ".join(
        f"row[{i}] = arr_{col_inds[i]}[i]" for i in range(len(out_colnames))
    )
    # TODO: support empty dataframes
    assert len(data_args) > 0, f"empty dataframe in DataFrame.{func_name}()"
    df_len = f"len(arr_{col_inds[0]})"

    func_np_func_map = {
        "max": "np.nanmax",
        "min": "np.nanmin",
        "sum": "np.nansum",
        "prod": "np.nanprod",
        "mean": "np.nanmean",
        "median": "np.nanmedian",
        # TODO: Handle these cases. Numba doesn't support the
        # ddof argument and pd & np
        # implementations vary (sample vs population)
        #'var': '(lambda A: np.nanvar(A, ddof=1))',
        "var": "bodo.utils.utils.nanvar_ddof1",
        #'std': '(lambda A: np.nanstd(A, ddof=1))',
        "std": "bodo.utils.utils.nanstd_ddof1",
    }

    if func_name in func_np_func_map:
        np_func = func_np_func_map[func_name]
        # NOTE: Pandas outputs float64 output even for int64 dataframes
        # when using df.mean() and df.median()
        # TODO: More sophisticated manner of computing this output_dtype
        output_dtype = (
            "float64" if func_name in ["mean", "median", "std", "var"] else comm_dtype
        )
        func_text = f"""
    {data_args}
    numba.parfors.parfor.init_prange()
    n = {df_len}
    row = np.empty({len(out_colnames)}, np.{comm_dtype})
    A = np.empty(n, np.{output_dtype})
    for i in numba.parfors.parfor.internal_prange(n):
        {data_accesses}
        A[i] = {np_func}(row)
    return bodo.hiframes.pd_series_ext.init_series(A, {index})
"""
        return func_text
    else:
        # Prevent internal error from func_text not existing
        raise BodoError(f"DataFrame.{func_name}(): Not supported for axis=1")


@overload_method(
    DataFrameType,
    "pct_change",
    inline="always",
    no_unliteral=True,
    jit_options={"cache": True},
)
def overload_dataframe_pct_change(
    df, periods=1, fill_method="pad", limit=None, freq=None
):
    check_runtime_cols_unsupported(df, "DataFrame.pct_change()")
    unsupported_args = {"fill_method": fill_method, "limit": limit, "freq": freq}
    arg_defaults = {"fill_method": "pad", "limit": None, "freq": None}
    check_unsupported_args(
        "DataFrame.pct_change",
        unsupported_args,
        arg_defaults,
        package_name="pandas",
        module_name="DataFrame",
    )

    data_args = ", ".join(
        f"bodo.hiframes.rolling.pct_change(bodo.hiframes.pd_dataframe_ext.get_dataframe_data(df, {i}), periods, False)"
        for i in range(len(df.columns))
    )
    header = "def bodo_dataframe_pct_change(df, periods=1, fill_method='pad', limit=None, freq=None):\n"
    return _gen_init_df(header, df.columns, data_args)


@overload_method(
    DataFrameType,
    "cumprod",
    inline="always",
    no_unliteral=True,
    jit_options={"cache": True},
)
def overload_dataframe_cumprod(df, axis=None, skipna=True):
    check_runtime_cols_unsupported(df, "DataFrame.cumprod()")
    unsupported_args = {"axis": axis, "skipna": skipna}
    arg_defaults = {"axis": None, "skipna": True}
    check_unsupported_args(
        "DataFrame.cumprod",
        unsupported_args,
        arg_defaults,
        package_name="pandas",
        module_name="DataFrame",
    )

    data_args = ", ".join(
        f"bodo.hiframes.pd_dataframe_ext.get_dataframe_data(df, {i}).cumprod()"
        for i in range(len(df.columns))
    )
    header = "def bodo_dataframe_cumprod(df, axis=None, skipna=True):\n"
    return _gen_init_df(header, df.columns, data_args)


@overload_method(
    DataFrameType,
    "cumsum",
    inline="always",
    no_unliteral=True,
    jit_options={"cache": True},
)
def overload_dataframe_cumsum(df, axis=None, skipna=True):
    check_runtime_cols_unsupported(df, "DataFrame.cumsum()")
    unsupported_args = {"skipna": skipna}
    arg_defaults = {"skipna": True}
    check_unsupported_args(
        "DataFrame.cumsum",
        unsupported_args,
        arg_defaults,
        package_name="pandas",
        module_name="DataFrame",
    )

    data_args = ", ".join(
        f"bodo.hiframes.pd_dataframe_ext.get_dataframe_data(df, {i}).cumsum()"
        for i in range(len(df.columns))
    )

    header = "def bodo_dataframe_cumsum(df, axis=None, skipna=True):\n"
    return _gen_init_df(header, df.columns, data_args)


def _is_describe_type(data):
    """Check if df.data has supported datatype for describe"""
    return (
        isinstance(data, (IntegerArrayType, FloatingArrayType))
        or (isinstance(data, types.Array) and isinstance(data.dtype, (types.Number)))
        or data.dtype == bodo.types.datetime64ns
    )


@overload_method(
    DataFrameType,
    "describe",
    inline="always",
    no_unliteral=True,
    jit_options={"cache": True},
)
def overload_dataframe_describe(df, percentiles=None, include=None, exclude=None):
    """
    Support df.describe with numeric and datetime column.
    """
    check_runtime_cols_unsupported(df, "DataFrame.describe()")
    unsupported_args = {
        "percentiles": percentiles,
        "include": include,
        "exclude": exclude,
    }
    arg_defaults = {"percentiles": None, "include": None, "exclude": None}
    check_unsupported_args(
        "DataFrame.describe",
        unsupported_args,
        arg_defaults,
        package_name="pandas",
        module_name="DataFrame",
    )

    # By default, For mixed data types columns Pandas return only an analysis of numeric columns(i.e. remove non-numeric columns)
    # Drop any non-Bodo supported columns (only keep: int, float, and nullable int)
    # If all column types are not supported, raise BodoError
    numeric_cols = [c for c, d in zip(df.columns, df.data) if _is_describe_type(d)]
    if len(numeric_cols) == 0:
        raise BodoError("df.describe() only supports numeric columns")

    # number of datetime columns
    num_dt = sum(
        df.data[df.column_index[c]].dtype == bodo.types.datetime64ns
        for c in numeric_cols
    )

    def _get_describe(col_ind):
        """get describe values: move std to the end if df has a mix of datetime/numeric
        columns to match Pandas.
        https://github.com/pandas-dev/pandas/blob/059c8bac51e47d6eaaa3e36d6a293a22312925e6/pandas/core/describe.py#L179
        """
        is_dt = df.data[col_ind].dtype == bodo.types.datetime64ns
        if num_dt and num_dt != len(numeric_cols):
            if is_dt:
                return f"des_{col_ind} + (np.nan,)"
            return f"des_{col_ind}[:2] + des_{col_ind}[3:] + (des_{col_ind}[2],)"

        return f"des_{col_ind}"

    header = "def bodo_dataframe_describe(df, percentiles=None, include=None, exclude=None):\n"
    for c in numeric_cols:
        col_ind = df.column_index[c]
        header += f"  des_{col_ind} = bodo.libs.array_ops.array_op_describe(bodo.hiframes.pd_dataframe_ext.get_dataframe_data(df, {col_ind}))\n"

    data_args = ", ".join(_get_describe(df.column_index[c]) for c in numeric_cols)

    index_vals = "['count', 'mean', 'std', 'min', '25%', '50%', '75%', 'max']"
    # Pandas avoids std for datetime-only cases
    if num_dt == len(numeric_cols):
        index_vals = "['count', 'mean', 'min', '25%', '50%', '75%', 'max']"
    # Pandas moves std to the end if df has a mix of datetime/numeric
    elif num_dt:
        index_vals = "['count', 'mean', 'min', '25%', '50%', '75%', 'max', 'std']"

    index = f"bodo.utils.conversion.convert_to_index({index_vals})"
    return _gen_init_df(header, numeric_cols, data_args, index)


@overload_method(
    DataFrameType,
    "take",
    inline="always",
    no_unliteral=True,
    jit_options={"cache": True},
)
def overload_dataframe_take(df, indices, axis=0, convert=None, is_copy=True):
    check_runtime_cols_unsupported(df, "DataFrame.take()")
    unsupported_args = {"axis": axis, "convert": convert, "is_copy": is_copy}
    arg_defaults = {"axis": 0, "convert": None, "is_copy": True}
    check_unsupported_args(
        "DataFrame.take",
        unsupported_args,
        arg_defaults,
        package_name="pandas",
        module_name="DataFrame",
    )

    data_args = ", ".join(
        f"bodo.hiframes.pd_dataframe_ext.get_dataframe_data(df, {i})[indices_t]"
        for i in range(len(df.columns))
    )
    header = (
        "def bodo_dataframe_take(df, indices, axis=0, convert=None, is_copy=True):\n"
    )
    header += "  indices_t = bodo.utils.conversion.coerce_to_ndarray(indices)\n"
    index = "bodo.hiframes.pd_dataframe_ext.get_dataframe_index(df)[indices_t]"
    return _gen_init_df(header, df.columns, data_args, index)


@overload_method(DataFrameType, "shift", inline="always", no_unliteral=True)
def overload_dataframe_shift(df, periods=1, freq=None, axis=0, fill_value=None):
    check_runtime_cols_unsupported(df, "DataFrame.shift()")
    # TODO: handle freq, int NA
    # TODO: Support nullable float types
    unsupported_args = {
        "freq": freq,
        "axis": axis,
    }
    arg_defaults = {
        "freq": None,
        "axis": 0,
    }
    check_unsupported_args(
        "DataFrame.shift",
        unsupported_args,
        arg_defaults,
        package_name="pandas",
        module_name="DataFrame",
    )

    # Bodo specific limitations for supported types
    # Currently only float (not nullable), int (not nullable), and dt64 are supported
    for column_type in df.data:
        if not is_supported_shift_array_type(column_type):
            # TODO: Link to supported Column input types.
            raise BodoError(
                f"Dataframe.shift() column input type {column_type.dtype} not supported yet."
            )

    # Ensure period is int
    if not is_overload_int(periods):
        raise BodoError("DataFrame.shift(): 'periods' input must be an integer.")

    data_args = ", ".join(
        f"bodo.hiframes.rolling.shift(bodo.hiframes.pd_dataframe_ext.get_dataframe_data(df, {i}), periods, False, fill_value)"
        for i in range(len(df.columns))
    )
    header = (
        "def bodo_dataframe_shift(df, periods=1, freq=None, axis=0, fill_value=None):\n"
    )
    return _gen_init_df(header, df.columns, data_args)


@overload_method(
    DataFrameType,
    "diff",
    inline="always",
    no_unliteral=True,
    jit_options={"cache": True},
)
def overload_dataframe_diff(df, periods=1, axis=0):
    """DataFrame.diff() support which is the same as df - df.shift(periods)"""
    check_runtime_cols_unsupported(df, "DataFrame.diff()")
    # TODO: Support nullable integer/float types
    unsupported_args = {"axis": axis}
    arg_defaults = {"axis": 0}
    check_unsupported_args(
        "DataFrame.diff",
        unsupported_args,
        arg_defaults,
        package_name="pandas",
        module_name="DataFrame",
    )

    # Bodo specific limitations for supported types
    # Currently only float, int, and dt64 are supported
    for column_type in df.data:
        if not (
            isinstance(column_type, (types.Array, IntegerArrayType, FloatingArrayType))
            and (
                isinstance(column_type.dtype, (types.Number))
                or column_type.dtype == bodo.types.datetime64ns
            )
        ):
            # TODO: Link to supported Column input types.
            raise BodoError(
                f"DataFrame.diff() column input type {column_type} not supported."
            )

    # Ensure period is int
    if not is_overload_int(periods):
        raise BodoError("DataFrame.diff(): 'periods' input must be an integer.")

    header = "def bodo_dataframe_diff(df, periods=1, axis= 0):\n"
    for i in range(len(df.columns)):
        header += (
            f"  data_{i} = bodo.hiframes.pd_dataframe_ext.get_dataframe_data(df, {i})\n"
        )
    data_args = ", ".join(
        # NOTE: using our sub function for dt64 due to bug in Numba (TODO: fix)
        f"bodo.hiframes.series_impl.dt64_arr_sub(data_{i}, bodo.hiframes.rolling.shift(data_{i}, periods, False))"
        if df.data[i] == types.Array(bodo.types.datetime64ns, 1, "C")
        else f"data_{i} - bodo.hiframes.rolling.shift(data_{i}, periods, False)"
        for i in range(len(df.columns))
    )
    return _gen_init_df(header, df.columns, data_args)


@overload_method(
    DataFrameType,
    "explode",
    inline="always",
    no_unliteral=True,
    jit_options={"cache": True},
)
def overload_dataframe_explode(df, column, ignore_index=False):
    """
    DataFrame.explode support: explodes columns specified, asserting all desired columns be array-like and have equal
    length per entry while all other columns undergo repeat() with counts equal to the entry-lengths of exploded columns.
    NOTE: Differs from Pandas in that [], scalars, and NA are all length 1 when error checking
    counts against all columns. I.e. pd.DataFrame({'A':[[1,2], [], [1]], 'B': [[1], np.nan, [1]]}) will
    fail in Pandas, but works with Bodo.
    """

    err_msg = "DataFrame.explode(): 'column' must a constant label or list of labels"
    if not is_literal_type(column):
        raise_bodo_error(err_msg)
    if is_overload_constant_list(column) or is_overload_constant_tuple(column):
        explode_cols = get_overload_const_list(column)
    else:
        explode_cols = [get_literal_value(column)]

    explode_inds = [df.column_index[c] for c in explode_cols]
    for i in explode_inds:
        if (
            not isinstance(df.data[i], ArrayItemArrayType)
            and df.data[i].dtype != string_array_split_view_type
        ):
            raise BodoError("DataFrame.explode(): columns must have array-like entries")
    n = len(df.columns)
    header = "def bodo_dataframe_explode(df, column, ignore_index=False):\n"
    header += "  index = bodo.hiframes.pd_dataframe_ext.get_dataframe_index(df)\n"
    header += "  index_arr = bodo.utils.conversion.index_to_array(index)\n"
    for i in range(n):
        header += (
            f"  data{i} = bodo.hiframes.pd_dataframe_ext.get_dataframe_data(df, {i})\n"
        )
    header += (
        f"  counts = bodo.libs.array_kernels.get_arr_lens(data{explode_inds[0]})\n"
    )
    for i in range(n):
        if i in explode_inds:
            header += f"  out_data{i} = bodo.libs.array_kernels.explode_no_index(data{i}, counts)\n"
        else:
            header += f"  out_data{i} = bodo.libs.array_kernels.repeat_kernel(data{i}, counts)\n"
    header += "  new_index = bodo.libs.array_kernels.repeat_kernel(index_arr, counts)\n"
    data_args = ", ".join(f"out_data{i}" for i in range(n))
    index = "bodo.utils.conversion.convert_to_index(new_index)"

    return _gen_init_df(header, df.columns, data_args, index)


@overload_method(
    DataFrameType,
    "set_index",
    inline="always",
    no_unliteral=True,
    jit_options={"cache": True},
)
def overload_dataframe_set_index(
    df, keys, drop=True, append=False, inplace=False, verify_integrity=False
):
    check_runtime_cols_unsupported(df, "DataFrame.set_index()")
    args_dict = {
        "inplace": inplace,
        "append": append,
        "verify_integrity": verify_integrity,
    }
    args_default_dict = {"inplace": False, "append": False, "verify_integrity": False}

    check_unsupported_args(
        "DataFrame.set_index",
        args_dict,
        args_default_dict,
        package_name="pandas",
        module_name="DataFrame",
    )

    # Column name only supported on constant string
    if not is_overload_constant_str(keys):
        raise_bodo_error("DataFrame.set_index(): 'keys' must be a constant string")
    col_name = get_overload_const_str(keys)
    col_ind = df.columns.index(col_name)

    header = "def bodo_dataframe_set_index(df, keys, drop=True, append=False, inplace=False, verify_integrity=False):\n"
    data_args = ", ".join(
        f"bodo.hiframes.pd_dataframe_ext.get_dataframe_data(df, {i})"
        for i in range(len(df.columns))
        if i != col_ind
    )
    columns = tuple(c for c in df.columns if c != col_name)
    index = (
        "bodo.utils.conversion.index_from_array("
        "bodo.hiframes.pd_dataframe_ext.get_dataframe_data(df, {}), {})"
    ).format(col_ind, f"'{col_name}'" if isinstance(col_name, str) else col_name)
    return _gen_init_df(header, columns, data_args, index)


@overload_method(DataFrameType, "query", no_unliteral=True)
def overload_dataframe_query(df, expr, inplace=False):
    """Support query only for the case where expr is a constant string and expr output
    is a 1D boolean array.
    Refering to named index by name is not supported.
    Series.dt.* is not supported. issue #451
    """
    check_runtime_cols_unsupported(df, "DataFrame.query()")
    # check unsupported "inplace"
    args_dict = {
        "inplace": inplace,
    }
    args_default_dict = {
        "inplace": False,
    }

    check_unsupported_args(
        "query",
        args_dict,
        args_default_dict,
        package_name="pandas",
        module_name="DataFrame",
    )

    # check expr is of type string
    if not isinstance(expr, (types.StringLiteral, types.UnicodeType)):
        raise BodoError("query(): expr argument should be a string")

    # TODO: support df.loc for normal case and getitem for multi-dim case similar to
    # Pandas
    def impl(df, expr, inplace=False):  # pragma: no cover
        b = bodo.hiframes.pd_dataframe_ext.query_dummy(df, expr)
        return df[b]

    return impl


@overload_method(DataFrameType, "duplicated", inline="always", no_unliteral=True)
def overload_dataframe_duplicated(df, subset=None, keep="first"):
    check_runtime_cols_unsupported(df, "DataFrame.duplicated()")
    # TODO: support subset and first
    args_dict = {
        "subset": subset,
        "keep": keep,
    }
    args_default_dict = {
        "subset": None,
        "keep": "first",
    }

    check_unsupported_args(
        "DataFrame.duplicated",
        args_dict,
        args_default_dict,
        package_name="pandas",
        module_name="DataFrame",
    )

    n_cols = len(df.columns)

    func_text = "def impl(df, subset=None, keep='first'):\n"
    for i in range(n_cols):
        func_text += (
            f"  data_{i} = bodo.hiframes.pd_dataframe_ext.get_dataframe_data(df, {i})\n"
        )
    data_cols = ", ".join(f"data_{i}" for i in range(n_cols))
    data_cols += "," if n_cols == 1 else ""
    func_text += f"  duplicated = bodo.libs.array_kernels.duplicated(({data_cols}))\n"
    func_text += "  index = bodo.hiframes.pd_dataframe_ext.get_dataframe_index(df)\n"
    func_text += "  return bodo.hiframes.pd_series_ext.init_series(duplicated, index)\n"
    loc_vars = {}
    exec(func_text, {"bodo": bodo}, loc_vars)
    impl = loc_vars["impl"]
    return impl


@overload_method(DataFrameType, "drop_duplicates", inline="always", no_unliteral=True)
def overload_dataframe_drop_duplicates(
    df, subset=None, keep="first", inplace=False, ignore_index=False
):
    check_runtime_cols_unsupported(df, "DataFrame.drop_duplicates()")
    # TODO: support inplace
    args_dict = {
        "inplace": inplace,
    }
    args_default_dict = {
        "inplace": False,
    }

    subset_columns = []
    if is_overload_constant_list(subset):
        # List is a single of column names
        subset_columns = get_overload_const_list(subset)
    elif is_overload_constant_str(subset):
        # String is a single column name
        subset_columns = [get_overload_const_str(subset)]
    elif is_overload_constant_int(subset):
        # Integer is a single column name
        subset_columns = [get_overload_const_int(subset)]
    elif not is_overload_none(subset):
        raise_bodo_error(
            "DataFrame.drop_duplicates(): subset must be a constant column name, constant list of column names or None"
        )

    subset_idx = []
    for col_name in subset_columns:
        if col_name not in df.column_index:
            raise BodoError(
                "DataFrame.drop_duplicates(): All subset columns must be found in the DataFrame."
                + f"Column {col_name} not found in DataFrame columns {df.columns}"
            )
        subset_idx.append(df.column_index[col_name])

    # keep: "first" => 0, "last" => 1, False => 2
    if is_overload_constant_str(keep):
        keep_str = get_overload_const_str(keep)
        if keep_str == "first":
            keep_i = 0
        elif keep_str == "last":
            keep_i = 1
        else:  # pragma: no cover
            raise_bodo_error(
                "DataFrame.drop_duplicates(): keep must be 'first', 'last', or False"
            )
    elif is_overload_constant_bool(keep) and get_overload_const_bool(keep) == False:
        keep_i = 2
    else:  # pragma: no cover
        raise_bodo_error(
            "DataFrame.drop_duplicates(): keep must be 'first', 'last', or False"
        )

    # get ignore_index parameter
    if is_overload_constant_bool(ignore_index):
        ignore_index = get_overload_const_bool(ignore_index)
    else:  # pragma: no cover
        raise_bodo_error(
            "DataFrame.drop_duplicates(): ignore_index must be a constant boolean"
        )

    check_unsupported_args(
        "DataFrame.drop_duplicates",
        args_dict,
        args_default_dict,
        package_name="pandas",
        module_name="DataFrame",
    )

    # dict_cols are columns with dictionaries that don't have drop_duplicates
    # implemented because dictionaries may not have the same order. We can
    # still run drop_duplicates on structs because those are always ordered.
    # Note: Pandas doesn't support drop_duplicates with dictionaries.
    dict_cols = []
    if subset_idx:
        for col_idx in subset_idx:
            if isinstance(df.data[col_idx], bodo.types.MapArrayType):
                dict_cols.append(df.columns[col_idx])
    else:
        for i, col_name in enumerate(df.columns):
            if isinstance(df.data[i], bodo.types.MapArrayType):
                dict_cols.append(col_name)
    if dict_cols:
        raise BodoError(
            f"DataFrame.drop_duplicates(): Columns {dict_cols} "
            + "have dictionary types which cannot be used to drop duplicates. "
            + "Please consider using the 'subset' argument to skip these columns."
        )

    # XXX: can't reuse duplicated() here since it shuffles data and chunks
    # may not match

    n_cols = len(df.columns)

    # handle empty dataframe corner case
    if n_cols == 0:
        return (
            lambda df, subset=None, keep="first", inplace=False, ignore_index=False: df
        )  # pragma: no cover

    subset_args = [f"data_{i}" for i in subset_idx]
    non_subset_args = [f"data_{i}" for i in range(n_cols) if i not in subset_idx]

    if subset_args:
        num_drop_cols = len(subset_args)
    else:
        num_drop_cols = n_cols

    index = "bodo.utils.conversion.index_to_array(bodo.hiframes.pd_dataframe_ext.get_dataframe_index(df))"

    if ignore_index:
        index = "None"

    func_text = "def bodo_dataframe_drop_duplicates(df, subset=None, keep='first', inplace=False, ignore_index=False):\n"

    # TODO(ehsan): support subset with table format
    if df.is_table_format and is_overload_none(subset):
        func_text += (
            "  in_table = bodo.hiframes.pd_dataframe_ext.get_dataframe_table(df)\n"
        )
        func_text += f"  out_table, index_arr = bodo.utils.table_utils.drop_duplicates_table(in_table, {index}, {n_cols}, {keep_i})\n"
        data_args = "out_table"
        out_len = "len(out_table)"
    else:
        drop_duplicates_args = ", ".join(subset_args + non_subset_args)

        data_args = ", ".join(f"data_{i}" for i in range(n_cols))

        for i in range(n_cols):
            func_text += f"  data_{i} = bodo.hiframes.pd_dataframe_ext.get_dataframe_data(df, {i})\n"

        func_text += f"  ({drop_duplicates_args},), index_arr = bodo.libs.array_kernels.drop_duplicates(({drop_duplicates_args},), {index}, {num_drop_cols}, {keep_i})\n"
        out_len = "len(data_0)"

    if ignore_index:
        func_text += f"  index = bodo.hiframes.pd_index_ext.init_range_index(0, {out_len}, 1, None)\n"
    else:
        func_text += "  index = bodo.utils.conversion.index_from_array(index_arr)\n"

    return _gen_init_df(func_text, df.columns, data_args, "index")


def create_dataframe_mask_where_overload(func_name):
    def overload_dataframe_mask_where(
        df,
        cond,
        other=np.nan,
        inplace=False,
        axis=None,
        level=None,
        errors="raise",
        try_cast=False,
    ):
        """
        Overload DataFrame.mask or DataFrame.where. It replaces element with other depending on cond
        (if DataFrame.where, will replace iff cond is False; if DataFrame.mask, will replace iff cond is True).
        """
        _validate_arguments_mask_where(
            f"DataFrame.{func_name}",
            df,
            cond,
            other,
            inplace,
            axis,
            level,
            errors,
            try_cast,
        )

        header = "def bodo_dataframe_mask_where_overload(df, cond, other=np.nan, inplace=False, axis=None, level=None, errors='raise', try_cast=False):\n"
        if func_name == "mask":
            header += "  cond = ~cond\n"
        gen_all_false = [False]

        if cond.ndim == 1:
            cond_str = lambda i, _: "cond"
        elif cond.ndim == 2:
            if isinstance(cond, DataFrameType):

                def cond_str(i, gen_all_false):
                    if df.columns[i] in cond.column_index:
                        return f"bodo.hiframes.pd_dataframe_ext.get_dataframe_data(cond, {cond.column_index[df.columns[i]]})"
                    else:
                        gen_all_false[0] = True
                        return "all_false"

            elif isinstance(cond, types.Array):
                cond_str = lambda i, _: f"cond[:,{i}]"

        if not hasattr(other, "ndim") or other.ndim == 1:
            other_str = lambda i: "other"
        elif other.ndim == 2:
            if isinstance(other, DataFrameType):
                other_str = (
                    lambda i: f"bodo.hiframes.pd_dataframe_ext.get_dataframe_data(other, {other.column_index[df.columns[i]]})"
                    if df.columns[i] in other.column_index
                    else "None"
                )
            elif isinstance(other, types.Array):
                other_str = lambda i: f"other[:,{i}]"

        n_cols = len(df.columns)
        data_args = ", ".join(
            f"bodo.hiframes.series_impl.where_impl({cond_str(i, gen_all_false)}, bodo.hiframes.pd_dataframe_ext.get_dataframe_data(df, {i}), {other_str(i)})"
            for i in range(n_cols)
        )

        if gen_all_false[0]:
            header += (
                "  all_false = bodo.libs.bool_arr_ext.alloc_false_bool_array(len(df))\n"
            )

        return _gen_init_df(header, df.columns, data_args)

    return overload_dataframe_mask_where


def _install_dataframe_mask_where_overload():
    for func_name in ("mask", "where"):
        overload_impl = create_dataframe_mask_where_overload(func_name)
        overload_method(DataFrameType, func_name, no_unliteral=True)(overload_impl)


_install_dataframe_mask_where_overload()


def _validate_arguments_mask_where(
    func_name,
    df,
    cond,
    other,
    inplace,
    axis,
    level,
    errors,
    try_cast,
):
    """
    Helper function to perform the necessary error checking for
    Series.where() and Series.mask().
    """
    unsupported_args = {
        "inplace": inplace,
        "level": level,
        "errors": errors,
        "try_cast": try_cast,
    }
    arg_defaults = {
        "inplace": False,
        "level": None,
        "errors": "raise",
        "try_cast": False,
    }
    check_unsupported_args(
        f"{func_name}",
        unsupported_args,
        arg_defaults,
        package_name="pandas",
        module_name="DataFrame",
    )
    if not (is_overload_none(axis) or is_overload_zero(axis)):  # pragma: no cover
        raise_bodo_error(f"{func_name}(): axis argument not supported")

    if not (
        isinstance(cond, (SeriesType, types.Array, BooleanArrayType))
        and (cond.ndim == 1 or cond.ndim == 2)
        and cond.dtype == types.bool_
    ) and not (
        isinstance(cond, DataFrameType)
        and cond.ndim == 2
        and all(cond.data[i].dtype == types.bool_ for i in range(len(df.columns)))
    ):
        raise BodoError(
            f"{func_name}(): 'cond' argument must be a DataFrame, Series, 1- or 2-dimensional array of booleans"
        )

    n_cols = len(df.columns)

    if hasattr(other, "ndim") and (other.ndim != 1 or other.ndim != 2):
        if other.ndim == 2:
            if not isinstance(other, (DataFrameType, types.Array)):
                raise BodoError(
                    f"{func_name}(): 'other', if 2-dimensional, must be a DataFrame or array."
                )
        elif other.ndim != 1:
            raise BodoError(f"{func_name}(): 'other' must be either 1 or 2-dimensional")
    if isinstance(other, DataFrameType):
        for i in range(n_cols):
            if df.columns[i] in other.column_index:
                bodo.hiframes.series_impl._validate_self_other_mask_where(
                    func_name,
                    "Series",
                    df.data[i],
                    other.data[other.column_index[df.columns[i]]],
                )
            else:
                # Essentially only validates df.data[i]
                bodo.hiframes.series_impl._validate_self_other_mask_where(
                    func_name, "Series", df.data[i], None, is_default=True
                )
    elif isinstance(other, SeriesType):
        for i in range(n_cols):
            bodo.hiframes.series_impl._validate_self_other_mask_where(
                func_name, "Series", df.data[i], other.data
            )
    else:
        for i in range(n_cols):
            bodo.hiframes.series_impl._validate_self_other_mask_where(
                func_name, "Series", df.data[i], other, max_ndim=2
            )


def _gen_init_df(
    header,
    columns,
    data_args,
    index=None,
    extra_globals=None,
):
    """generate a function that initializes a dataframe.
    'header' is the initial parts of the function text (defines data arrays etc.)
    'output_df_type' resembles the output dataframe types, but only column names have to
    be accurate.
    """
    if index is None:
        index = "bodo.hiframes.pd_dataframe_ext.get_dataframe_index(df)"

    if extra_globals is None:
        extra_globals = {}

    meta_data = ColNamesMetaType(tuple(columns))

    data_args = "({}{})".format(data_args, "," if data_args else "")

    func_text = f"{header}  return bodo.hiframes.pd_dataframe_ext.init_dataframe({data_args}, {index}, __col_name_meta_value_gen_init_df)\n"
    _global = {
        "bodo": bodo,
        "np": np,
        "pd": pd,
        "numba": numba,
        "__col_name_meta_value_gen_init_df": meta_data,
    }
    _global.update(extra_globals)

    return bodo_exec(func_text, _global, {}, __name__)


############################ binary operators #############################


def _get_binop_columns(lhs, rhs, is_inplace=False):
    """
    Get output column names and corresponding input column indices for binary operators
    similar to Pandas.
    For example, lhs = ('A', 'B') and rhs = ('B', 'A', 'C') will return
    out_cols = ('A', 'B', 'C'), lcol_inds = [0, 1, -1], rcol_inds = [1, 0, 2]
    Inplace operators only consider left input's columns.
    """
    # find corresponding column indices if schemas don't exactly match
    if lhs.columns != rhs.columns:
        # same computation as Pandas here:
        # https://github.com/pandas-dev/pandas/blob/c979bd8b84e95bab0f69b4be71d2cd340e71912f/pandas/core/ops/__init__.py#L369
        l_cols = pd.Index(lhs.columns)
        r_cols = pd.Index(rhs.columns)
        out_cols, lcol_inds, rcol_inds = l_cols.join(
            r_cols,
            how="left" if is_inplace else "outer",
            level=None,
            return_indexers=True,
        )
        return tuple(out_cols), lcol_inds, rcol_inds

    # schemas match, output is trivial
    return lhs.columns, range(len(lhs.columns)), range(len(lhs.columns))


def create_binary_op_overload(op):
    def overload_dataframe_binary_op(lhs, rhs):
        op_str = numba.core.utils.OPERATORS_TO_BUILTINS[op]
        # Handle equality specially because we can determine the result
        # when there are mismatched types.
        eq_ops = (operator.eq, operator.ne)
        check_runtime_cols_unsupported(lhs, op_str)
        check_runtime_cols_unsupported(rhs, op_str)
        if isinstance(lhs, DataFrameType):
            # df/df case
            if isinstance(rhs, DataFrameType):
                out_cols, lcol_inds, rcol_inds = _get_binop_columns(lhs, rhs)

                data_args = ", ".join(
                    (
                        f"bodo.hiframes.pd_dataframe_ext.get_dataframe_data(lhs, {l_ind}) {op_str}"
                        f"bodo.hiframes.pd_dataframe_ext.get_dataframe_data(rhs, {r_ind})"
                    )
                    if l_ind != -1 and r_ind != -1
                    # Pandas always generates an array of NaNs if an intput is missing a
                    # column
                    else "bodo.libs.array_kernels.gen_na_array(len(lhs), float64_arr_type)"
                    for l_ind, r_ind in zip(lcol_inds, rcol_inds)
                )
                header = "def bodo_dataframe_binary_op_dfdf(lhs, rhs):\n"
                index = "bodo.hiframes.pd_dataframe_ext.get_dataframe_index(lhs)"
                return _gen_init_df(
                    header,
                    out_cols,
                    data_args,
                    index,
                    extra_globals={
                        "float64_arr_type": types.Array(types.float64, 1, "C")
                    },
                )
            elif isinstance(rhs, SeriesType):
                raise_bodo_error(
                    "Comparison operation between Dataframe and Series is not supported yet."
                )

            # scalar case, TODO: Proper error handling for all operators
            # TODO: Test with ArrayItemArrayType
            data_impl = []
            # For each array with a different type, we generate an array
            # of all True/False using a prange. This is because np.full
            # support can have parallelism issues in Numba.
            diff_types = []
            # TODO: What is the best way to place these constants in the code.
            if op in eq_ops:
                for i, col in enumerate(lhs.data):
                    # If the types don't match, generate an array of False/True values
                    if is_common_scalar_dtype([col.dtype, rhs]):
                        data_impl.append(
                            f"bodo.hiframes.pd_dataframe_ext.get_dataframe_data(lhs, {i}) {op_str} rhs"
                        )
                    else:
                        arr_name = f"arr{i}"
                        diff_types.append(arr_name)
                        data_impl.append(arr_name)
                data_args = ", ".join(data_impl)
            else:
                data_args = ", ".join(
                    f"bodo.hiframes.pd_dataframe_ext.get_dataframe_data(lhs, {i}) {op_str} rhs"
                    for i in range(len(lhs.columns))
                )

            header = "def bodo_dataframe_binary_op(lhs, rhs):\n"
            if len(diff_types) > 0:
                header += "  numba.parfors.parfor.init_prange()\n"
                header += "  n = len(lhs)\n"
                header += "".join(
                    f"  {arr_name} = bodo.libs.bool_arr_ext.alloc_bool_array(n)\n"
                    for arr_name in diff_types
                )
                header += "  for i in numba.parfors.parfor.internal_prange(n):\n"
                header += "".join(
                    f"    {arr_name}[i] = {op == operator.ne}\n"
                    for arr_name in diff_types
                )
            index = "bodo.hiframes.pd_dataframe_ext.get_dataframe_index(lhs)"
            return _gen_init_df(header, lhs.columns, data_args, index)

        if isinstance(rhs, DataFrameType):
            if isinstance(lhs, SeriesType):
                raise_bodo_error(
                    "Comparison operation between Dataframe and Series is not supported yet."
                )
            # scalar case, TODO: Proper error handling for all operators
            # TODO: Test with ArrayItemArrayType
            data_impl = []
            # For each array with a different type, we generate an array
            # of all True/False using a prange. This is because np.full
            # support can have parallelism issues in Numba.
            diff_types = []
            # TODO: What is the best way to place these constants in the code.
            if op in eq_ops:
                for i, col in enumerate(rhs.data):
                    # If the types don't match, generate an array of False/True values
                    if is_common_scalar_dtype([lhs, col.dtype]):
                        data_impl.append(
                            f"lhs {op_str} bodo.hiframes.pd_dataframe_ext.get_dataframe_data(rhs, {i})"
                        )
                    else:
                        arr_name = f"arr{i}"
                        diff_types.append(arr_name)
                        data_impl.append(arr_name)
                data_args = ", ".join(data_impl)
            else:
                data_args = ", ".join(
                    f"lhs {op_str} bodo.hiframes.pd_dataframe_ext.get_dataframe_data(rhs, {i})"
                    for i in range(len(rhs.columns))
                )
            header = "def bodo_dataframe_binary_op_df_series(lhs, rhs):\n"
            if len(diff_types) > 0:
                header += "  numba.parfors.parfor.init_prange()\n"
                header += "  n = len(rhs)\n"
                header += "".join(
                    f"  {arr_name} = bodo.libs.bool_arr_ext.alloc_bool_array(n)\n"
                    for arr_name in diff_types
                )
                header += "  for i in numba.parfors.parfor.internal_prange(n):\n"
                header += "".join(
                    f"    {arr_name}[i] = {op == operator.ne}\n"
                    for arr_name in diff_types
                )
            index = "bodo.hiframes.pd_dataframe_ext.get_dataframe_index(rhs)"
            return _gen_init_df(header, rhs.columns, data_args, index)

    return overload_dataframe_binary_op


# operators taken care of in binops_ext.py
skips = [
    operator.lt,
    operator.le,
    operator.eq,
    operator.ne,
    operator.gt,
    operator.ge,
    operator.add,
    operator.sub,
    operator.mul,
    operator.truediv,
    operator.floordiv,
    operator.pow,
    operator.mod,
]


def _install_binary_ops():
    # install binary ops such as add, sub, pow, eq, ...
    for op in bodo.hiframes.pd_series_ext.series_binary_ops:
        if op in skips:
            continue
        overload_impl = create_binary_op_overload(op)
        overload(op)(overload_impl)


_install_binary_ops()


####################### binary inplace operators #############################


def create_inplace_binary_op_overload(op):
    def overload_dataframe_inplace_binary_op(left, right):
        op_str = numba.core.utils.OPERATORS_TO_BUILTINS[op]
        check_runtime_cols_unsupported(left, op_str)
        check_runtime_cols_unsupported(right, op_str)
        if isinstance(left, DataFrameType):
            if isinstance(right, DataFrameType):
                out_cols, _, rcol_inds = _get_binop_columns(left, right, True)

                func_text = "def bodo_dataframe_inplace_binary_op_dfdf(left, right):\n"
                for i, r_ind in enumerate(rcol_inds):
                    if r_ind == -1:
                        func_text += f"  df_arr{i} = bodo.libs.array_kernels.gen_na_array(len(left), float64_arr_type)\n"
                        continue
                    func_text += f"  df_arr{i} = bodo.hiframes.pd_dataframe_ext.get_dataframe_data(left, {i})\n"
                    func_text += f"  df_arr{i} {op_str} bodo.hiframes.pd_dataframe_ext.get_dataframe_data(right, {r_ind})\n"
                data_args = ", ".join(f"df_arr{i}" for i in range(len(out_cols)))
                index = "bodo.hiframes.pd_dataframe_ext.get_dataframe_index(left)"
                return _gen_init_df(
                    func_text,
                    out_cols,
                    data_args,
                    index,
                    extra_globals={
                        "float64_arr_type": types.Array(types.float64, 1, "C")
                    },
                )

            # scalar case
            func_text = "def bodo_dataframe_inplace_binary_op_scalar(left, right):\n"
            for i in range(len(left.columns)):
                func_text += f"  df_arr{i} = bodo.hiframes.pd_dataframe_ext.get_dataframe_data(left, {i})\n"
                func_text += f"  df_arr{i} {op_str} right\n"
            data_args = ", ".join((f"df_arr{i}") for i in range(len(left.columns)))
            index = "bodo.hiframes.pd_dataframe_ext.get_dataframe_index(left)"
            return _gen_init_df(func_text, left.columns, data_args, index)

    return overload_dataframe_inplace_binary_op


def _install_inplace_binary_ops():
    # install inplace binary ops such as iadd, isub, ...
    for op in bodo.hiframes.pd_series_ext.series_inplace_binary_ops:
        overload_impl = create_inplace_binary_op_overload(op)
        overload(op, no_unliteral=True)(overload_impl)


_install_inplace_binary_ops()


########################## unary operators ###############################


def create_unary_op_overload(op):
    def overload_dataframe_unary_op(df):
        if isinstance(df, DataFrameType):
            op_str = numba.core.utils.OPERATORS_TO_BUILTINS[op]
            check_runtime_cols_unsupported(df, op_str)
            data_args = ", ".join(
                f"{op_str} bodo.hiframes.pd_dataframe_ext.get_dataframe_data(df, {i})"
                for i in range(len(df.columns))
            )
            header = "def bodo_dataframe_unary_op(df):\n"
            return _gen_init_df(header, df.columns, data_args)

    return overload_dataframe_unary_op


def _install_unary_ops():
    # install unary operators: ~, -, +
    for op in bodo.hiframes.pd_series_ext.series_unary_ops:
        overload_impl = create_unary_op_overload(op)
        overload(op, no_unliteral=True)(overload_impl)


_install_unary_ops()


# TODO: move to other file
########### top level functions ###############


# inline IR for parallelizable data structures, but don't inline for scalars since we
# pattern match pd.isna(A[i]) in SeriesPass to handle it properly
def overload_isna(obj):
    check_runtime_cols_unsupported(obj, "pd.isna()")
    # DataFrame, Series, Index
    if isinstance(
        obj, (DataFrameType, SeriesType)
    ) or bodo.hiframes.pd_index_ext.is_pd_index_type(obj):
        return lambda obj: obj.isna()  # pragma: no cover

    # Bodo arrays, use array_kernels.isna()
    if is_array_typ(obj):

        def impl(obj):  # pragma: no cover
            numba.parfors.parfor.init_prange()
            n = len(obj)
            out_arr = bodo.libs.bool_arr_ext.alloc_bool_array(n)
            for i in numba.parfors.parfor.internal_prange(n):
                out_arr[i] = bodo.libs.array_kernels.isna(obj, i)
            return out_arr

        return impl


# Use function decorator to enable stacked inlining
overload(pd.isna, inline="always")(overload_isna)
overload(pd.isnull, inline="always")(overload_isna)

# Enable Bodo's exports of Pandas
overload(bd.isna, inline="always")(overload_isna)
overload(bd.isnull, inline="always")(overload_isna)


@overload(bd.isna)
@overload(bd.isnull)
@overload(pd.isna)
@overload(pd.isnull)
def overload_isna_scalar(obj):
    # ignore cases handled above
    if (
        isinstance(obj, (DataFrameType, SeriesType))
        or bodo.hiframes.pd_index_ext.is_pd_index_type(obj)
        or is_array_typ(obj)
    ):
        return

    # array-like: list, tuple
    if isinstance(obj, (types.List, types.UniTuple)):
        # no reuse of array implementation to avoid prange (unexpected threading etc.)
        def impl(obj):  # pragma: no cover
            n = len(obj)
            out_arr = bodo.libs.bool_arr_ext.alloc_bool_array(n)
            for i in range(n):
                out_arr[i] = pd.isna(obj[i])
            return out_arr

        return impl

    # scalars
    # using unliteral_val() to avoid literal type in output type since we may replace
    # this call in Series pass with array_kernels.isna()
    obj = types.unliteral(obj)
    if obj == bodo.types.string_type:
        return lambda obj: unliteral_val(False)  # pragma: no cover
    if isinstance(obj, types.Integer):
        return lambda obj: unliteral_val(False)  # pragma: no cover
    if isinstance(obj, types.Float):
        return lambda obj: np.isnan(obj)  # pragma: no cover
    if isinstance(obj, (types.NPDatetime, types.NPTimedelta)):
        return lambda obj: np.isnat(obj)  # pragma: no cover
    if obj == types.none:
        return lambda obj: unliteral_val(True)
    if isinstance(obj, bodo.hiframes.pd_timestamp_ext.PandasTimestampType):
        return lambda obj: np.isnat(
            bodo.hiframes.pd_timestamp_ext.integer_to_dt64(obj.value)
        )  # pragma: no cover
    if obj == bodo.hiframes.datetime_timedelta_ext.pd_timedelta_type:
        return lambda obj: np.isnat(
            bodo.hiframes.pd_timestamp_ext.integer_to_timedelta64(obj.value)
        )  # pragma: no cover

    # OptionalType case
    if isinstance(obj, types.Optional):
        return lambda obj: obj is None  # pragma: no cover

    # TODO: catch other cases
    return lambda obj: unliteral_val(False)  # pragma: no cover


# support A[i] = None array setitem using our array NA setting function
# TODO: inline when supported in Numba
@overload(operator.setitem, no_unliteral=True)
def overload_setitem_arr_none(A, idx, val):
    if is_array_typ(A, False) and isinstance(idx, types.Integer) and val == types.none:
        return lambda A, idx, val: bodo.libs.array_kernels.setna(
            A, idx
        )  # pragma: no cover


def overload_notna(obj):
    # non-scalars
    # TODO: ~pd.isna(obj) implementation fails for some reason in
    # test_dataframe.py::test_pd_notna[na_test_obj7] with 1D_Var input
    check_runtime_cols_unsupported(obj, "pd.notna()")
    if isinstance(obj, (DataFrameType, SeriesType)):
        return lambda obj: obj.notna()  # pragma: no cover

    if isinstance(obj, (types.List, types.UniTuple)) or is_array_typ(
        obj, include_index_series=True
    ):
        return lambda obj: ~pd.isna(obj)  # pragma: no cover

    # scalars
    return lambda obj: not pd.isna(obj)  # pragma: no cover


# Use function decorator to enable stacked inlining
overload(pd.notna, inline="always", no_unliteral=True)(overload_notna)
overload(pd.notnull, inline="always", no_unliteral=True)(overload_notna)

overload(bd.notna, inline="always", no_unliteral=True)(overload_notna)
overload(bd.notnull, inline="always", no_unliteral=True)(overload_notna)


def _get_pd_dtype_str(t):
    """return dtype string for 'dtype' values in read_excel()
    it's not fully consistent for read_csv(), since datetime64 requires 'datetime64[ns]'
    instead of 'str'
    """
    if t.dtype == types.NPDatetime("ns"):
        return "'datetime64[ns]'"

    return bodo.ir.csv_ext._get_pd_dtype_str(t)


@overload_method(DataFrameType, "replace", inline="always", no_unliteral=True)
def overload_dataframe_replace(
    df,
    to_replace=None,
    value=None,
    inplace=False,
    limit=None,
    regex=False,
    method="pad",
):
    check_runtime_cols_unsupported(df, "DataFrame.replace()")

    # Check that to_replace is never none
    if is_overload_none(to_replace):
        raise BodoError("replace(): to_replace value of None is not supported")

    # TODO: Add error checking to ensure this is only called on supported types.e

    # Handle type error checking for defaults only supported
    # Right now this will be everything except to_replace
    # and value
    args_dict = {
        "inplace": inplace,
        "limit": limit,
        "regex": regex,
        "method": method,
    }
    args_default_dict = {
        "inplace": False,
        "limit": None,
        "regex": False,
        "method": "pad",
    }

    check_unsupported_args(
        "replace",
        args_dict,
        args_default_dict,
        package_name="pandas",
        module_name="DataFrame",
    )

    data_args = ", ".join(
        f"df.iloc[:, {i}].replace(to_replace, value).values"
        for i in range(len(df.columns))
    )
    header = "def bodo_dataframe_replace(df, to_replace=None, value=None, inplace=False, limit=None, regex=False, method='pad'):\n"
    return _gen_init_df(header, df.columns, data_args)


def _is_col_access(expr_node):
    """return True if the expression is a column access"""
    expr_str = str(expr_node)
    return expr_str.startswith("(left.") or expr_str.startswith("(right.")


def _insert_NA_cond(expr_node, left_columns, left_data, right_columns, right_data):
    """
    Insert NA checks for the general cond used in a join. NA values
    can be inserted at the start of expressions in which they are used.

    The one exception to this is an OR, which must do separate NA checks in
    each section of the OR, unless a value appears on both sides.

    This returns the expression with NOT_NA(...) for each table.

    Example:
    input: left.A > right.B & (left.C > right.D | left.C < right.A)
    output: NOT_NA(left.A) & NOT_NA(right.B) & NOT_NA(left.C)
            & left.A > right.B
            & ((NOT_NA(right.D) & left.C > right.D) | (NOT_NA(right.A) & left.C < right.A)

    Notice how right.D and right.A must be checked inside the OR because each only impacts
    part of the expression, but left.C is checked once because it impacts both sides.

    NA checks are only inserted if the types is nullable.
    """
    # create fake environment for Expr to enable parsing
    # include NOT_NA for the checks we insert
    resolver = {"left": 0, "right": 0, "NOT_NA": 0}
    env = pandas.core.computation.scope.ensure_scope(2, {}, {}, (resolver,))
    clean_func = pandas.core.computation.parsing.clean_column_name

    def append_null_checks(expr_node, null_set: set[str]):
        """Create a new expr appending by append NOTNA & checks
        to the front of expr node for each element in null_set."""

        if not null_set:
            return expr_node
        # Generate the null check strings, inserting ` `
        null_check_str = " & ".join(["NOT_NA.`" + x + "`" for x in null_set])
        # Generate the clean versions of the column name for the NOT_NA checks.
        # These should ensure that column names that can't be parsed properly
        # as Python identifiers are not modified. We append "NOT_NA" as the
        # first half of the key because all elements in the null set should
        # start with NOT_NA.
        null_cleaned = {("NOT_NA", clean_func(col)): col for col in null_set}
        null_expr, _, _ = _parse_query_expr(
            null_check_str, env, [], [], None, join_cleaned_cols=null_cleaned
        )
        # _disallow_scalar_only_bool_ops accesses actual value which is not possible
        saved__disallow_scalar_only_bool_ops = (
            pandas.core.computation.ops.BinOp._disallow_scalar_only_bool_ops
        )
        pandas.core.computation.ops.BinOp._disallow_scalar_only_bool_ops = (
            lambda self: None
        )

        try:
            binop = pandas.core.computation.ops.BinOp("&", null_expr, expr_node)
        finally:
            # Restore _disallow_scalar_only_bool_ops
            pandas.core.computation.ops.BinOp._disallow_scalar_only_bool_ops = (
                saved__disallow_scalar_only_bool_ops
            )
        return binop

    def _insert_NA_cond_body(expr_node, null_set) -> None:
        """
        Scans through expr_node and inserts names for all
        columns that have a column check (i.e. right.A) and need a
        null value inserted.

        Also modified expr_node in-place to add null checks,
        but only for the OR case.
        """
        if isinstance(expr_node, pandas.core.computation.ops.BinOp):
            if expr_node.op == "|":
                # If we encounter OR we need special handling because
                # a null value may only impact the lhs or rhs.
                left_null = set()
                right_null = set()

                _insert_NA_cond_body(expr_node.lhs, left_null)
                _insert_NA_cond_body(expr_node.rhs, right_null)

                # Elements found in both sets can be bubbled up
                joint_nulls = left_null.intersection(right_null)

                # Remove any joint nulls from each set
                left_null.difference_update(joint_nulls)
                right_null.difference_update(joint_nulls)

                # Update the existing set
                null_set.update(joint_nulls)

                # Add null checks where needed
                expr_node.lhs = append_null_checks(expr_node.lhs, left_null)
                expr_node.rhs = append_null_checks(expr_node.rhs, right_null)
                # Update operands so print works properly
                expr_node.operands = (expr_node.lhs, expr_node.rhs)
            else:
                _insert_NA_cond_body(expr_node.lhs, null_set)
                _insert_NA_cond_body(expr_node.rhs, null_set)
        elif _is_col_access(expr_node):
            # If we have a column add it to the nullset if the type is nullable.
            full_name = expr_node.name
            table_name, col_name = full_name.split(".")
            if table_name == "left":
                cols = left_columns
                data = left_data
            else:
                cols = right_columns
                data = right_data
            arr_type = data[cols.index(col_name)]
            if bodo.utils.typing.is_nullable(arr_type):
                null_set.add(expr_node.name)

    null_set = set()
    _insert_NA_cond_body(expr_node, null_set)
    return append_null_checks(expr_node, null_set)


def _extract_equal_conds(expr_node):
    """extract equality terms of parsed expression node and remove them
    e.g. "left.A == right.A & left.B < 3" -> ["A"], ["B"], "left.B < 3"
    """
    # "left.A == right.A" -> ["A"], ["A"], None (means there is no non-equality condition)

    if not hasattr(expr_node, "op"):
        # If the expression node is a scalar, just return the expression
        return [], [], expr_node
    if (
        expr_node.op == "=="
        and _is_col_access(expr_node.lhs)
        and _is_col_access(expr_node.rhs)
    ):
        l_str = str(expr_node.lhs)
        r_str = str(expr_node.rhs)

        # if both refer to the same table (strange corner case)
        if (l_str.startswith("(left.") and r_str.startswith("(left.")) or (
            l_str.startswith("(right.") and r_str.startswith("(right.")
        ):
            return [], [], expr_node

        # remove "left." and "right."
        # and the trailing ")"
        left_on = [l_str.split(".")[1][:-1]]
        right_on = [r_str.split(".")[1][:-1]]

        # reverse order
        if l_str.startswith("(right."):
            return right_on, left_on, None

        return left_on, right_on, None

    # for '&', extract equality terms from lhs and rhs
    if expr_node.op == "&":
        l_left_on, l_right_on, l_expr = _extract_equal_conds(expr_node.lhs)
        r_left_on, r_right_on, r_expr = _extract_equal_conds(expr_node.rhs)
        left_on = l_left_on + r_left_on
        right_on = l_right_on + r_right_on

        # lhs is removed, only rhs remains
        if l_expr is None:
            return left_on, right_on, r_expr

        # rhs is removed, only lhs remains
        if r_expr is None:
            return left_on, right_on, l_expr

        # update '&' expr with new lhs/rhs
        expr_node.lhs = l_expr
        expr_node.rhs = r_expr
        # Update operands so print works properly
        expr_node.operands = (expr_node.lhs, expr_node.rhs)
        return left_on, right_on, expr_node

    # no term can be extracted for other nodes like '|' (unless if transformed)
    # TODO(ehsan): transform the expression into conjunctive normal form (CNF) to
    # extract more equality conditions
    return [], [], expr_node


def _parse_merge_cond(on_str, left_columns, left_data, right_columns, right_data):
    """Parse general merge condition such as "left.A == right.A & left.B < 3" and
    extract its column equality terms
    """
    resolver = {"left": 0, "right": 0}
    # create fake environment for Expr to enable parsing
    env = pandas.core.computation.scope.ensure_scope(2, {}, {}, (resolver,))
    col_map = {}
    # When Pandas parses the query expression, all column names
    # that are not valid identifiers will be "cleaned" to become a
    # name that is a valid identifier
    #
    # For example:  2#7 -> 2__HASH__7
    #
    # We need to keep the original column names, so we "clean" each
    # column to determined what it will be transformed into. We then
    # map this name back to the original column names so the conversion
    # avoids errors. If two "cleaned" columns conflict, we raise an exception
    # because we cannot tell what column is being referenced.
    #
    # https://github.com/pandas-dev/pandas/blob/cd13e3a42d03f2c3f93b1de3e04352d01aac2241/pandas/core/computation/parsing.py#L96
    clean_func = pandas.core.computation.parsing.clean_column_name
    for name, col_list in (
        ("left", left_columns),
        ("right", right_columns),
    ):
        for col in col_list:
            clean_col = clean_func(col)
            escape_key = (name, clean_col)
            # TODO: test this error handling
            if escape_key in col_map:
                raise_bodo_error(
                    f"pd.merge(): {name} table contains two columns that are escaped to the same Python identifier '{col}' and '{col_map[clean_col]}' Please rename one of these columns. To avoid this issue, please use names that are valid Python identifiers."
                )
            col_map[escape_key] = col
    parsed_expr, _, _ = _parse_query_expr(
        on_str, env, [], [], None, join_cleaned_cols=col_map
    )
    left_on, right_on, simplified_expr = _extract_equal_conds(parsed_expr.terms)
    return (
        left_on,
        right_on,
        _insert_NA_cond(
            simplified_expr, left_columns, left_data, right_columns, right_data
        ),
    )


@overload_method(
    DataFrameType,
    "merge",
    inline="always",
    no_unliteral=True,
    jit_options={"cache": True},
)
@overload(pd.merge, inline="always", no_unliteral=True, jit_options={"cache": True})
@overload(bd.merge, inline="always", no_unliteral=True, jit_options={"cache": True})
def overload_dataframe_merge(
    left,
    right,
    how="inner",
    on=None,
    left_on=None,
    right_on=None,
    left_index=False,
    right_index=False,
    sort=False,
    suffixes=("_x", "_y"),
    copy=True,
    indicator=False,
    validate=None,
    _bodo_na_equal=True,
    _bodo_rebalance_output_if_skewed=False,
):
    # _bodo_na_equal is a bodo extension to Pandas used to indicate
    # that NA values should not be considered equal. BodoSQL uses
    # this behavior because NA is not considered equal in SQL (while
    # Pandas does)
    # _bodo_rebalance_output_if_skewed is a BodoSQL plan based attribute to
    # check for skew and potentially rebalance because we expect to do
    # several independent operations over the output before the next shuffle.
    check_runtime_cols_unsupported(left, "DataFrame.merge()")
    check_runtime_cols_unsupported(right, "DataFrame.merge()")
    unsupported_args = {"sort": sort, "copy": copy, "validate": validate}
    arg_defaults = {"sort": False, "copy": True, "validate": None}
    check_unsupported_args(
        "DataFrame.merge",
        unsupported_args,
        arg_defaults,
        package_name="pandas",
        module_name="DataFrame",
    )

    validate_merge_spec(
        left,
        right,
        how,
        on,
        left_on,
        right_on,
        left_index,
        right_index,
        sort,
        suffixes,
        copy,
        indicator,
        validate,
    )

    how = get_overload_const_str(how)
    # NOTE: using sorted to avoid inconsistent ordering across processors
    # passing sort lambda to avoid errors when str and non-str column names are mixed
    comm_cols = tuple(
        sorted(set(left.columns) & set(right.columns), key=lambda k: str(k))
    )

    gen_cond = ""
    if not is_overload_none(on):
        left_on = right_on = on
        # check for general condition like "left.A == right.A & left.B < 3"
        if is_overload_constant_str(on):
            on_str = get_overload_const_str(on)
            if on_str not in comm_cols and ("left." in on_str or "right." in on_str):
                # extract equality portions to reuse existing infrastructure
                # e.g. "left.A == right.A & left.B < 3" -> ["A"], ["A"], "left.B < 3"
                left_on, right_on, gen_expr = _parse_merge_cond(
                    on_str, left.columns, left.data, right.columns, right.data
                )
                if gen_expr is None:
                    gen_cond = ""
                else:
                    gen_cond = str(gen_expr)

    if (
        is_overload_none(on)
        and is_overload_none(left_on)
        and is_overload_none(right_on)
        and is_overload_false(left_index)
        and is_overload_false(right_index)
    ):
        left_keys = comm_cols
        right_keys = comm_cols
    else:
        if is_overload_true(left_index):
            left_keys = ["$_bodo_index_"]
        else:
            left_keys = get_overload_const_list(left_on)
            # make sure all left_keys is a valid column in left
            validate_keys(left_keys, left)
        if is_overload_true(right_index):
            right_keys = ["$_bodo_index_"]
        else:
            right_keys = get_overload_const_list(right_on)
            # make sure all right_keys is a valid column in right
            validate_keys(right_keys, right)

    if not is_overload_bool(indicator):
        raise_bodo_error("DataFrame.merge(): indicator must be a constant boolean")
    indicator_val = get_overload_const_bool(indicator)
    if not is_overload_bool(_bodo_na_equal):
        raise_bodo_error(
            "DataFrame.merge(): bodo extension '_bodo_na_equal' must be a constant boolean"
        )
    _bodo_na_equal_val = get_overload_const_bool(_bodo_na_equal)
    if not is_overload_bool(_bodo_rebalance_output_if_skewed):
        raise_bodo_error(
            "DataFrame.merge(): bodo extension '_bodo_rebalance_output_if_skewed' must be a constant boolean"
        )
    _bodo_rebalance_output_if_skewed_val = get_overload_const_bool(
        _bodo_rebalance_output_if_skewed
    )

    validate_keys_length(left_index, right_index, left_keys, right_keys)
    validate_keys_dtypes(left, right, left_index, right_index, left_keys, right_keys)

    # The suffixes
    if is_overload_constant_tuple(suffixes):
        suffixes_val = get_overload_const_tuple(suffixes)
    if is_overload_constant_list(suffixes):
        suffixes_val = list(get_overload_const_list(suffixes))

    suffix_x = suffixes_val[0]
    suffix_y = suffixes_val[1]
    validate_unicity_output_column_names(
        suffix_x,
        suffix_y,
        left_keys,
        right_keys,
        left.columns,
        right.columns,
        indicator_val,
    )

    left_keys = gen_const_tup(left_keys)
    right_keys = gen_const_tup(right_keys)

    # generating code since typers can't find constants easily
    func_text = (
        "def bodo_dataframe_merge(left, right, how='inner', on=None, left_on=None,\n"
    )
    func_text += "    right_on=None, left_index=False, right_index=False, sort=False,\n"
    func_text += "    suffixes=('_x', '_y'), copy=True, indicator=False, validate=None, _bodo_na_equal=True, _bodo_rebalance_output_if_skewed=False):\n"
    func_text += f"  return bodo.hiframes.pd_dataframe_ext.join_dummy(left, right, {left_keys}, {right_keys}, '{how}', '{suffix_x}', '{suffix_y}', False, {indicator_val}, {_bodo_na_equal_val}, {_bodo_rebalance_output_if_skewed_val}, {gen_cond!r})\n"
    return bodo_exec(func_text, {"bodo": bodo}, {}, __name__)


def common_validate_merge_merge_asof_spec(
    name_func, left, right, on, left_on, right_on, left_index, right_index, suffixes
):
    """Validate checks that are common to merge and merge_asof"""
    # make sure left and right are dataframes
    if not isinstance(left, DataFrameType) or not isinstance(right, DataFrameType):
        raise BodoError(name_func + "() requires dataframe inputs")

    # make sure the column datatypes of the left and right are valid
    valid_dataframe_column_types = (
        ArrayItemArrayType,
        MapArrayType,
        StructArrayType,
        CategoricalArrayType,
        types.Array,
        IntegerArrayType,
        FloatingArrayType,
        DecimalArrayType,
        IntervalArrayType,
        bodo.types.DatetimeArrayType,
        TimeArrayType,
    )
    valid_dataframe_column_insts = {
        string_array_type,
        dict_str_arr_type,
        binary_array_type,
        datetime_date_array_type,
        timedelta_array_type,
        boolean_array_type,
        timestamptz_array_type,
    }
    merge_on_names = {
        get_overload_const_str(on_const)
        for on_const in (left_on, right_on, on)
        if is_overload_constant_str(on_const)
    }
    for df in (left, right):
        for i, col in enumerate(df.data):
            if (
                not isinstance(col, valid_dataframe_column_types)
                and col not in valid_dataframe_column_insts
            ):
                raise BodoError(
                    f"{name_func}(): use of column with "
                    f"{type(col)} in merge unsupported"
                )
            # ensure merge isn't being performed on non-hashable `MapArrayType`
            if df.columns[i] in merge_on_names and isinstance(col, MapArrayType):
                raise BodoError(f"{name_func}(): merge on MapArrayType unsupported")

    # make sure leftindex is of type bool
    ensure_constant_arg(name_func, "left_index", left_index, bool)
    ensure_constant_arg(name_func, "right_index", right_index, bool)

    # make sure suffixes is not passed in
    # make sure on is of type str or strlist
    if (not is_overload_constant_tuple(suffixes)) and (
        not is_overload_constant_list(suffixes)
    ):
        raise_bodo_error(
            name_func + "(): suffixes parameters should be ['_left', '_right']"
        )

    if is_overload_constant_tuple(suffixes):
        suffixes_val = get_overload_const_tuple(suffixes)
    if is_overload_constant_list(suffixes):
        suffixes_val = list(get_overload_const_list(suffixes))

    if len(suffixes_val) != 2:
        raise BodoError(name_func + "(): The number of suffixes should be exactly 2")

    comm_cols = tuple(set(left.columns) & set(right.columns))
    if not is_overload_none(on):
        # make sure two dataframes have common columns if 'on' columns are specified
        is_gen_cond = False
        if is_overload_constant_str(on):
            on_str = get_overload_const_str(on)
            is_gen_cond = on_str not in comm_cols and (
                "left." in on_str or "right." in on_str
            )

        if len(comm_cols) == 0 and not is_gen_cond:
            raise_bodo_error(
                name_func + "(): No common columns to perform merge on. "
                f"Merge options: left_on={is_overload_true(left_on)}, right_on={is_overload_true(right_on)}, "
                f"left_index={is_overload_true(left_index)}, right_index={is_overload_true(right_index)}"
            )
        # make sure "on" does not coexist with left_on or right_on
        if (not is_overload_none(left_on)) or (not is_overload_none(right_on)):
            raise BodoError(
                name_func + '(): Can only pass argument "on" OR "left_on" '
                'and "right_on", not a combination of both.'
            )

    # make sure right_on, right_index, left_on, left_index are specified properly
    if (
        (is_overload_true(left_index) or not is_overload_none(left_on))
        and is_overload_none(right_on)
        and not is_overload_true(right_index)
    ):
        raise BodoError(name_func + "(): Must pass right_on or right_index=True")
    if (
        (is_overload_true(right_index) or not is_overload_none(right_on))
        and is_overload_none(left_on)
        and not is_overload_true(left_index)
    ):
        raise BodoError(name_func + "(): Must pass left_on or left_index=True")


def validate_merge_spec(
    left,
    right,
    how,
    on,
    left_on,
    right_on,
    left_index,
    right_index,
    sort,
    suffixes,
    copy,
    indicator,
    validate,
):
    """validate arguments to merge()"""
    common_validate_merge_merge_asof_spec(
        "merge", left, right, on, left_on, right_on, left_index, right_index, suffixes
    )
    # make sure how is constant and one of ("left", "right", "outer", "inner", "cross")
    ensure_constant_values(
        "merge", "how", how, ("left", "right", "outer", "inner", "cross")
    )


def validate_merge_asof_spec(
    left,
    right,
    on,
    left_on,
    right_on,
    left_index,
    right_index,
    by,
    left_by,
    right_by,
    suffixes,
    tolerance,
    allow_exact_matches,
    direction,
):
    """validate checks of the merge_asof() function"""
    common_validate_merge_merge_asof_spec(
        "merge_asof",
        left,
        right,
        on,
        left_on,
        right_on,
        left_index,
        right_index,
        suffixes,
    )
    if not is_overload_true(allow_exact_matches):
        raise BodoError(
            "merge_asof(): allow_exact_matches parameter only supports default value True"
        )
    # make sure validate is None
    if not is_overload_none(tolerance):
        raise BodoError(
            "merge_asof(): tolerance parameter only supports default value None"
        )
    if not is_overload_none(by):
        raise BodoError("merge_asof(): by parameter only supports default value None")
    if not is_overload_none(left_by):
        raise BodoError(
            "merge_asof(): left_by parameter only supports default value None"
        )
    if not is_overload_none(right_by):
        raise BodoError(
            "merge_asof(): right_by parameter only supports default value None"
        )
    if not is_overload_constant_str(direction):
        raise BodoError("merge_asof(): direction parameter should be of type str")
    else:
        direction = get_overload_const_str(direction)
        if direction != "backward":
            raise BodoError(
                "merge_asof(): direction parameter only supports default value 'backward'"
            )


def validate_merge_asof_keys_length(
    left_on, right_on, left_index, right_index, left_keys, right_keys
):
    # make sure right_keys and left_keys have the same size
    if (not is_overload_true(left_index)) and (not is_overload_true(right_index)):
        if len(right_keys) != len(left_keys):
            raise BodoError("merge(): len(right_on) must equal len(left_on)")
    if not is_overload_none(left_on) and is_overload_true(right_index):
        raise BodoError(
            "merge(): right_index = True and specifying left_on is not suppported yet."
        )
    if not is_overload_none(right_on) and is_overload_true(left_index):
        raise BodoError(
            "merge(): left_index = True and specifying right_on is not suppported yet."
        )


def validate_keys_length(left_index, right_index, left_keys, right_keys):
    # make sure right_keys and left_keys have the same size
    if (not is_overload_true(left_index)) and (not is_overload_true(right_index)):
        if len(right_keys) != len(left_keys):
            raise BodoError("merge(): len(right_on) must equal len(left_on)")
    if is_overload_true(right_index):
        if len(left_keys) != 1:
            raise BodoError(
                "merge(): len(left_on) must equal the number "
                'of levels in the index of "right", which is 1'
            )
    if is_overload_true(left_index):
        if len(right_keys) != 1:
            raise BodoError(
                "merge(): len(right_on) must equal the number "
                'of levels in the index of "left", which is 1'
            )


def validate_keys_dtypes(left, right, left_index, right_index, left_keys, right_keys):
    # make sure left keys and right keys have comparable dtypes

    typing_context = numba.core.registry.cpu_target.typing_context

    if is_overload_true(left_index) or is_overload_true(right_index):
        # cases where index is used in merging
        if is_overload_true(left_index) and is_overload_true(right_index):
            lk_type = left.index
            is_l_str = isinstance(lk_type, StringIndexType)
            rk_type = right.index
            is_r_str = isinstance(rk_type, StringIndexType)
        elif is_overload_true(left_index):
            lk_type = left.index
            is_l_str = isinstance(lk_type, StringIndexType)
            rk_type = right.data[right.columns.index(right_keys[0])]
            is_r_str = rk_type.dtype == string_type
        elif is_overload_true(right_index):
            lk_type = left.data[left.columns.index(left_keys[0])]
            is_l_str = lk_type.dtype == string_type
            rk_type = right.index
            is_r_str = isinstance(rk_type, StringIndexType)

        if is_l_str and is_r_str:
            return
        lk_type = lk_type.dtype
        rk_type = rk_type.dtype
        try:
            typing_context.resolve_function_type(operator.eq, (lk_type, rk_type), {})
        except Exception:
            raise_bodo_error(
                f"merge: You are trying to merge on {lk_type} and "
                f"{rk_type} columns. If you wish to proceed "
                "you should use pd.concat"
            )
    else:  # cases where only columns are used in merge
        for lk, rk in zip(left_keys, right_keys):
            lk_type = left.data[left.columns.index(lk)].dtype
            lk_arr_type = left.data[left.columns.index(lk)]
            rk_type = right.data[right.columns.index(rk)].dtype
            rk_arr_type = right.data[right.columns.index(rk)]

            if lk_arr_type == rk_arr_type:
                continue

            msg = (
                f"merge: You are trying to merge on column {lk} of {lk_type} and "
                f"column {rk} of {rk_type}. If you wish to proceed "
                "you should use pd.concat"
            )

            # Make sure non-string columns are not merged with string columns.
            # As of Numba 0.47, string comparison with non-string works and is always
            # False, so using type inference below doesn't work
            # TODO: check all incompatible key types similar to Pandas in
            # _maybe_coerce_merge_keys
            l_is_str = lk_type == string_type
            r_is_str = rk_type == string_type
            if l_is_str ^ r_is_str:
                raise_bodo_error(msg)

            try:
                typing_context.resolve_function_type(
                    operator.eq, (lk_type, rk_type), {}
                )
            except Exception:  # pragma: no cover
                # TODO: cover this case in unittests
                raise_bodo_error(msg)


def validate_keys(keys, df):
    key_diff = set(keys).difference(set(df.columns))
    if len(key_diff) > 0:
        if (
            is_overload_constant_str(df.index.name_typ)
            and get_overload_const_str(df.index.name_typ) in key_diff
        ):
            raise_bodo_error(
                f"merge(): use of index {df.index.name_typ} as key for "
                "on/left_on/right_on is unsupported"
            )
        raise_bodo_error(
            f"merge(): invalid key {key_diff} for on/left_on/right_on\n"
            f"merge supports only valid column names {df.columns}"
        )


@overload_method(DataFrameType, "join", inline="always", no_unliteral=True)
def overload_dataframe_join(
    left, other, on=None, how="left", lsuffix="", rsuffix="", sort=False
):
    check_runtime_cols_unsupported(left, "DataFrame.join()")
    check_runtime_cols_unsupported(other, "DataFrame.join()")
    unsupported_args = {"lsuffix": lsuffix, "rsuffix": rsuffix}
    arg_defaults = {"lsuffix": "", "rsuffix": ""}
    check_unsupported_args(
        "DataFrame.join",
        unsupported_args,
        arg_defaults,
        package_name="pandas",
        module_name="DataFrame",
    )

    validate_join_spec(left, other, on, how, lsuffix, rsuffix, sort)

    how = get_overload_const_str(how)

    if not is_overload_none(on):
        left_keys = get_overload_const_list(on)
    else:
        left_keys = ["$_bodo_index_"]

    right_keys = ["$_bodo_index_"]

    left_keys = gen_const_tup(left_keys)
    right_keys = gen_const_tup(right_keys)

    # generating code since typers can't find constants easily
    func_text = "def _impl(left, other, on=None, how='left',\n"
    func_text += "    lsuffix='', rsuffix='', sort=False):\n"
    func_text += f"  return bodo.hiframes.pd_dataframe_ext.join_dummy(left, other, {left_keys}, {right_keys}, '{how}', '{lsuffix}', '{rsuffix}', True, False, True, False, '')\n"

    loc_vars = {}
    exec(func_text, {"bodo": bodo}, loc_vars)
    _impl = loc_vars["_impl"]
    return _impl


def validate_join_spec(left, other, on, how, lsuffix, rsuffix, sort):
    # make sure left and other are dataframes
    if not isinstance(other, DataFrameType):
        raise BodoError("join() requires dataframe inputs")

    # make sure how is constant and one of ("left", "right", "outer", "inner")
    ensure_constant_values("merge", "how", how, ("left", "right", "outer", "inner"))

    # make sure 'on' has length 1 since we don't support Multiindex
    if not is_overload_none(on) and len(get_overload_const_list(on)) != 1:
        raise BodoError("join(): len(on) must equals to 1 when specified.")
    # make sure 'on' is a valid column in other
    if not is_overload_none(on):
        on_keys = get_overload_const_list(on)
        validate_keys(on_keys, left)
    # make sure sort is the default value, sort=True not supported
    if not is_overload_false(sort):
        raise BodoError("join(): sort parameter only supports default value False")

    comm_cols = tuple(set(left.columns) & set(other.columns))
    if len(comm_cols) > 0:
        # make sure two dataframes do not have common columns
        # because we are not supporting lsuffix and rsuffix
        raise_bodo_error(
            "join(): not supporting joining on overlapping columns:"
            f"{comm_cols} Use DataFrame.merge() instead."
        )


def validate_unicity_output_column_names(
    suffix_x,
    suffix_y,
    left_keys,
    right_keys,
    left_columns,
    right_columns,
    indicator_val,
):
    """Raise a BodoError if the column in output of the join operation collide"""
    comm_keys = set(left_keys) & set(right_keys)
    comm_data = set(left_columns) & set(right_columns)
    add_suffix = comm_data - comm_keys
    other_left = set(left_columns) - comm_data
    other_right = set(right_columns) - comm_data

    NatureLR = {}

    def insertOutColumn(col_name):
        if col_name in NatureLR:
            raise_bodo_error(
                f"join(): two columns happen to have the same name : {col_name}"
            )
        NatureLR[col_name] = 0

    for eVar in comm_keys:
        insertOutColumn(eVar)

    for eVar in add_suffix:
        eVarX = str(eVar) + suffix_x
        eVarY = str(eVar) + suffix_y
        insertOutColumn(eVarX)
        insertOutColumn(eVarY)

    for eVar in other_left:
        insertOutColumn(eVar)

    for eVar in other_right:
        insertOutColumn(eVar)

    # If indicator=True, it creates a column called _merge.
    if indicator_val:
        insertOutColumn("_merge")


@overload_method(
    DataFrameType,
    "groupby",
    inline="always",
    no_unliteral=True,
    jit_options={"cache": True},
)
def overload_dataframe_groupby(
    df,
    by=None,
    axis=0,
    level=None,
    as_index=True,
    sort=False,
    group_keys=True,
    squeeze=False,
    observed=True,
    dropna=True,
    # Bodo specific argument. When provided we shuffle on the first n keys
    _bodo_num_shuffle_keys=-1,
    # Bodo specific argument to determine Pandas vs BodoSQL.
    _is_bodosql=False,
):
    check_runtime_cols_unsupported(df, "DataFrame.groupby()")

    validate_groupby_spec(
        df,
        by,
        axis,
        level,
        as_index,
        sort,
        group_keys,
        squeeze,
        observed,
        dropna,
        _bodo_num_shuffle_keys,
        _is_bodosql,
    )

    def _impl(
        df,
        by=None,
        axis=0,
        level=None,
        as_index=True,
        sort=False,
        group_keys=True,
        squeeze=False,
        observed=True,
        dropna=True,
        _bodo_num_shuffle_keys=-1,
        _is_bodosql=False,
    ):  # pragma: no cover
        return bodo.hiframes.pd_groupby_ext.init_groupby(
            df, by, as_index, dropna, _bodo_num_shuffle_keys, _is_bodosql
        )

    return _impl


def validate_groupby_spec(
    df,
    by,
    axis,
    level,
    as_index,
    sort,
    group_keys,
    squeeze,
    observed,
    dropna,
    _num_shuffle_keys,
    _is_bodosql,
):
    """
    validate df.groupby() specifications: In addition to consistent error checking
    with pandas, we also check for unsupported specs.

    An error is raised if the spec is invalid.
    """

    # make sure 'by' is supplied
    if is_overload_none(by):
        raise BodoError("groupby(): 'by' must be supplied.")

    # make sure axis has default value 0
    if not is_overload_zero(axis):
        raise BodoError("groupby(): 'axis' parameter only supports integer value 0.")

    # make sure level is not specified
    if not is_overload_none(level):
        raise BodoError(
            "groupby(): 'level' is not supported since MultiIndex is not supported."
        )

    # make sure by is a const str list
    if not is_literal_type(by) and not is_overload_constant_list(by):
        raise_bodo_error(
            f"groupby(): 'by' parameter only supports a constant column label or column labels, not {by}."
        )

    # make sure by has valid label(s)
    if len(set(get_overload_const_list(by)).difference(set(df.columns))) > 0:
        raise_bodo_error(
            f"groupby(): invalid key {get_overload_const_list(by)} for 'by' (not available in columns {df.columns})."
        )

    # make sure as_index is of type bool
    if not is_overload_constant_bool(as_index):
        raise_bodo_error(
            f"groupby(): 'as_index' parameter must be a constant bool, not {as_index}.",
        )

    # make sure dropna is of type bool
    if not is_overload_constant_bool(dropna):
        raise_bodo_error(
            f"groupby(): 'dropna' parameter must be a constant bool, not {dropna}.",
        )

    if not is_overload_constant_int(_num_shuffle_keys):  # pragma: no cover
        raise_bodo_error(
            f"groupby(): '_num_shuffle_keys' parameter must be a constant integer, not {_num_shuffle_keys}."
        )
    if not is_overload_constant_bool(_is_bodosql):  # pragma: no cover
        raise_bodo_error(
            f"groupby(): '_is_bodosql' parameter must be a constant integer, not {_is_bodosql}."
        )

    # NOTE: sort default value is True in pandas. We opt to set it to False by default for performance
    unsupported_args = {
        "sort": sort,
        "group_keys": group_keys,
        "squeeze": squeeze,
        "observed": observed,
    }
    args_defaults = {
        "sort": False,
        "group_keys": True,
        "squeeze": False,
        "observed": True,
    }

    check_unsupported_args(
        "Dataframe.groupby",
        unsupported_args,
        args_defaults,
        package_name="pandas",
        module_name="GroupBy",
    )


def pivot_error_checking(df, index, columns, values, func_name):
    """
    Performs common error checking shared by pivot and functions
    that depend on pivot.

    Returns the literals string for the column names and their index
    in the columns.
    """
    # Certain differences arise when doing pivot vs pivot_table. For
    # example, what needs to be checked/supported for index=None
    is_pivot_table = func_name == "DataFrame.pivot_table"
    if is_pivot_table:
        if is_overload_none(index) or not is_literal_type(index):
            # Failing to provide the index requires just transposing the table into n columns,
            # where n is the length(values). As a result, at this time we don't support it.
            raise_bodo_error(
                "DataFrame.pivot_table(): 'index' argument is required and must be constant column labels"
            )
    else:
        # pivot supports index=None
        if not is_overload_none(index) and not is_literal_type(index):
            raise_bodo_error(
                f"{func_name}(): if 'index' argument is provided it must be constant column labels"
            )
    if is_overload_none(columns) or not is_literal_type(columns):
        raise_bodo_error(
            f"{func_name}(): 'columns' argument is required and must be a constant column label"
        )
    if not is_overload_none(values) and not is_literal_type(values):
        raise_bodo_error(
            f"{func_name}(): if 'values' argument is provided it must be constant column labels"
        )

    columns_lit = get_literal_value(columns)
    # Only lists/tuples with 1 element are supported.
    if isinstance(columns_lit, (list, tuple)):
        if len(columns_lit) > 1:
            raise BodoError(
                f"{func_name}(): 'columns' argument must be a constant column label not a {columns_lit}"
            )
        columns_lit = columns_lit[0]

    # Verify that each column can be found in the DataFrame
    if columns_lit not in df.columns:
        raise BodoError(
            f"{func_name}(): 'columns' column {columns_lit} not found in DataFrame {df}."
        )

    # Get the column numbers
    columns_idx = df.column_index[columns_lit]

    # Handle index
    if is_overload_none(index):
        # If index isn't provided then we use the existing index
        index_idxs = []
        index_lit = []
    else:
        index_lit = get_literal_value(index)
        if not isinstance(index_lit, (list, tuple)):
            index_lit = [index_lit]
        index_idxs = []
        for index in index_lit:
            if index not in df.column_index:
                raise BodoError(
                    f"{func_name}(): 'index' column {index} not found in DataFrame {df}."
                )
            index_idxs.append(df.column_index[index])

    # Validate that the index values can be lowered as a list (for groupby).
    # Note if the list is empty (which means use the index),
    # then the array won't be used at runtime.
    if not (
        all(isinstance(c, int) for c in index_lit)
        or all(isinstance(c, str) for c in index_lit)
    ):
        raise BodoError(
            f"{func_name}(): column names selected for 'index' must all share a common int or string type. Please convert your names to a common type using DataFrame.rename()"
        )

    # Handle values
    if is_overload_none(values):
        # If values isn't provided, all columns except the index
        # columns are used.
        values_idxs = []
        values_lit = []
        seen_columns = index_idxs + [columns_idx]
        for i, c in enumerate(df.columns):
            if i not in seen_columns:
                values_idxs.append(i)
                values_lit.append(c)
    else:
        values_lit = get_literal_value(values)
        if not isinstance(values_lit, (list, tuple)):
            values_lit = [values_lit]
        values_idxs = []
        for val in values_lit:
            if val not in df.column_index:
                raise BodoError(
                    f"{func_name}(): 'values' column {val} not found in DataFrame {df}."
                )
            values_idxs.append(df.column_index[val])

    # Verify that none of the columns are the same.
    index_set = set(values_idxs) | set(index_idxs) | {columns_idx}
    if len(index_set) != (len(values_idxs) + len(index_idxs) + 1):
        raise BodoError(
            f"{func_name}(): 'index', 'columns', and 'values' must all refer to different columns"
        )

    # Verify that the allowed column types.
    def check_valid_index_typ(index_column):
        if isinstance(
            index_column,
            (
                bodo.types.ArrayItemArrayType,
                bodo.types.MapArrayType,
                bodo.types.StructArrayType,
                bodo.types.TupleArrayType,
                bodo.types.IntervalArrayType,
            ),
        ):
            raise BodoError(
                f"{func_name}(): 'index' DataFrame column must have scalar rows"
            )

        # TODO: Support
        if isinstance(index_column, bodo.types.CategoricalArrayType):
            raise BodoError(
                f"{func_name}(): 'index' DataFrame column does not support categorical data"
            )

    # If no index is specified we must check the actual index.
    if len(index_idxs) == 0:
        index = df.index
        if isinstance(index, MultiIndexType):
            raise BodoError(
                f"{func_name}(): 'index' cannot be None with a DataFrame with a multi-index"
            )
        if not isinstance(index, RangeIndexType):
            # Range Index doesn't have a data field, but it should be supported.
            check_valid_index_typ(index.data)
        if not is_literal_type(df.index.name_typ):
            raise BodoError(
                f"{func_name}(): If 'index' is None, the name of the DataFrame's Index must be constant at compile-time"
            )
    else:
        for index_idx in index_idxs:
            index_column = df.data[index_idx]
            check_valid_index_typ(index_column)

    columns_column = df.data[columns_idx]
    if isinstance(
        columns_column,
        (
            bodo.types.ArrayItemArrayType,
            bodo.types.MapArrayType,
            bodo.types.StructArrayType,
            bodo.types.TupleArrayType,
            bodo.types.IntervalArrayType,
        ),
    ):
        raise BodoError(
            f"{func_name}(): 'columns' DataFrame column must have scalar rows"
        )

    # TODO: Support and generate a DataFrame with column known at compile time if the
    # categories are known at compile time.
    if isinstance(columns_column, bodo.types.CategoricalArrayType):
        raise BodoError(
            f"{func_name}(): 'columns' DataFrame column does not support categorical data"
        )

    # TODO: Support. The existing implementation doesn't support setting data with immutable array
    # types except strings.
    for val_idx in values_idxs:
        values_column = df.data[val_idx]
        if (
            isinstance(
                values_column,
                (
                    bodo.types.ArrayItemArrayType,
                    bodo.types.MapArrayType,
                    bodo.types.StructArrayType,
                    bodo.types.TupleArrayType,
                ),
            )
            or values_column == bodo.types.binary_array_type
        ):
            raise BodoError(
                f"{func_name}(): 'values' DataFrame column must have scalar rows"
            )
    return (
        index_lit,
        columns_lit,
        values_lit,
        index_idxs,
        columns_idx,
        values_idxs,
    )


@overload(pd.pivot, inline="always", no_unliteral=True)
@overload(bd.pivot, inline="always", no_unliteral=True)
@overload_method(DataFrameType, "pivot", inline="always", no_unliteral=True)
def overload_dataframe_pivot(data, index=None, columns=None, values=None):
    """
    This implementation verifies that we
    have a single column, which is distinct, for each argument and then
    calls an intermedate function that is handled through infer_global.

    The infer_global function is used for constructing a DataFrame type
    with number of output columns determined at runtime.
    """
    check_runtime_cols_unsupported(data, "DataFrame.pivot()")
    bodo.hiframes.pd_timestamp_ext.check_tz_aware_unsupported(data, "DataFrame.pivot()")
    if not isinstance(data, DataFrameType):
        raise BodoError("pandas.pivot(): 'data' argument must be a DataFrame")

    (
        index_lit,
        columns_lit,
        values_lit,
        index_idx,
        columns_idx,
        values_idx,
    ) = pivot_error_checking(data, index, columns, values, "DataFrame.pivot")
    if len(index_lit) == 0:
        if is_overload_none(data.index.name_typ):
            index_name_val = (None,)
        else:
            # Checking for literal type is done in pivot_error_checking
            index_name_val = (get_literal_value(data.index.name_typ),)
    else:
        index_name_val = tuple(index_lit)
    index_lit = ColNamesMetaType(index_name_val)
    values_lit = ColNamesMetaType(tuple(values_lit))
    columns_lit = ColNamesMetaType((columns_lit,))
    # TODO: Provide a Bodo specific optional argument for specifying pivot_values
    # without requiring communication. If this value is constant at compile time
    # we don't need table format.
    func_text = "def impl(data, index=None, columns=None, values=None):\n"
    func_text += "    ev = tracing.Event('df.pivot')\n"
    # Compute the pivot columns
    func_text += f"    pivot_values = data.iloc[:, {columns_idx}].unique()\n"
    # Call the main pivot_impl
    func_text += "    result = bodo.hiframes.pd_dataframe_ext.pivot_impl(\n"
    # Select all of the arrays. TODO: Support table format.

    if len(index_idx) == 0:
        # If we use the index just get the index data
        func_text += "        (bodo.utils.conversion.index_to_array(bodo.hiframes.pd_dataframe_ext.get_dataframe_index(data)),),\n"
    else:
        func_text += "        (\n"
        for ind_idx in index_idx:
            func_text += f"            bodo.hiframes.pd_dataframe_ext.get_dataframe_data(data, {ind_idx}),\n"
        func_text += "        ),\n"
    func_text += f"        (bodo.hiframes.pd_dataframe_ext.get_dataframe_data(data, {columns_idx}),),\n"
    func_text += "        (\n"
    for val_idx in values_idx:
        func_text += f"            bodo.hiframes.pd_dataframe_ext.get_dataframe_data(data, {val_idx}),\n"
    func_text += "        ),\n"
    func_text += "        pivot_values,\n"
    func_text += "        index_lit,\n"
    func_text += "        columns_lit,\n"
    func_text += "        values_lit,\n"
    func_text += "    )\n"
    func_text += "    ev.finalize()\n"
    func_text += "    return result\n"
    loc_vars = {}
    exec(
        func_text,
        {
            "bodo": bodo,
            "index_lit": index_lit,
            "columns_lit": columns_lit,
            "values_lit": values_lit,
            "tracing": tracing,
        },
        loc_vars,
    )
    impl = loc_vars["impl"]
    return impl


@overload(pd.pivot_table, inline="always", no_unliteral=True)
@overload(bd.pivot_table, inline="always", no_unliteral=True)
@overload_method(DataFrameType, "pivot_table", inline="always", no_unliteral=True)
def overload_dataframe_pivot_table(
    data,
    values=None,
    index=None,
    columns=None,
    aggfunc="mean",
    fill_value=None,
    margins=False,
    dropna=True,
    margins_name="All",
    observed=False,
    sort=True,
    _pivot_values=None,  # bodo argument
):
    check_runtime_cols_unsupported(data, "DataFrame.pivot_table()")
    bodo.hiframes.pd_timestamp_ext.check_tz_aware_unsupported(
        data, "DataFrame.pivot_table()"
    )
    unsupported_args = {
        "fill_value": fill_value,
        "margins": margins,
        "dropna": dropna,
        "margins_name": margins_name,
        "observed": observed,
        "sort": sort,
    }
    arg_defaults = {
        "fill_value": None,
        "margins": False,
        "dropna": True,
        "margins_name": "All",
        "observed": False,
        "sort": True,
    }
    check_unsupported_args(
        "DataFrame.pivot_table",
        unsupported_args,
        arg_defaults,
        package_name="pandas",
        module_name="DataFrame",
    )
    if not isinstance(data, DataFrameType):
        raise BodoError("pandas.pivot_table(): 'data' argument must be a DataFrame")

    # TODO: Add error checking for supported aggfunc?
    (
        index_lit,
        columns_lit,
        values_lit,
        index_idx,
        columns_idx,
        values_idx,
    ) = pivot_error_checking(data, index, columns, values, "DataFrame.pivot_table")

    # If len(values_lit) == 1, the names aren't needed because we create a
    # regular index instead of a multi-index. Since we lower an array, we pass
    # None if this case to make sure we can determine multi-index at compile
    # time.
    index_lit_orig = index_lit
    index_lit = ColNamesMetaType(tuple(index_lit))
    values_lit = ColNamesMetaType(tuple(values_lit))
    columns_lit_org = columns_lit
    columns_lit = ColNamesMetaType((columns_lit,))

    func_text = "def impl(\n"
    func_text += "    data,\n"
    func_text += "    values=None,\n"
    func_text += "    index=None,\n"
    func_text += "    columns=None,\n"
    func_text += '    aggfunc="mean",\n'
    func_text += "    fill_value=None,\n"
    func_text += "    margins=False,\n"
    func_text += "    dropna=True,\n"
    func_text += '    margins_name="All",\n'
    func_text += "    observed=False,\n"
    func_text += "    sort=True,\n"
    func_text += "    _pivot_values=None,\n"
    func_text += "):\n"
    # Truncate the dataframe to just the columns in question.
    func_text += "    ev = tracing.Event('df.pivot_table')\n"
    total_idx = index_idx + [columns_idx] + values_idx
    func_text += f"    data = data.iloc[:, {total_idx}]\n"
    groupby_lit = index_lit_orig + [columns_lit_org]

    if not is_overload_none(_pivot_values):
        # Sort to be consistent with the runtime columns assumption.
        _pivot_values_tup = tuple(sorted(_pivot_values.meta))
        _pivot_values = ColNamesMetaType(_pivot_values_tup)
        func_text += "    pivot_values = _pivot_values_arr\n"
        # Filter the unique values column. This is now always column number
        # len(index_idx), since we have done an iloc on the DataFrame.
        func_text += (
            f"    data = data[data.iloc[:, {len(index_idx)}].isin(pivot_values)]\n"
        )
        if all(isinstance(c, str) for c in _pivot_values_tup):
            _pivot_values_arr = pd.array(_pivot_values_tup, "string")
        elif all(isinstance(c, int) for c in _pivot_values_tup):
            _pivot_values_arr = np.array(_pivot_values_tup, "int64")
        else:  # pragma: no cover
            # This should be unreachable
            raise BodoError(
                "pivot(): pivot values selcected via pivot JIT argument must all share a common int or string type."
            )
    else:
        _pivot_values_arr = None

    # Perform the groupby with the agg function. We opt to only
    # shuffle using the index portion of the key to skip an extra
    # shuffle at the pivot step. The exception is if we see nunique, which
    # is not supported.
    is_nunique = is_overload_constant_str(aggfunc) and (
        get_overload_const_str(aggfunc) == "nunique"
    )
    num_shuffle_keys = len(groupby_lit) if is_nunique else len(index_lit_orig)
    func_text += f"    data = data.groupby({groupby_lit!r}, as_index=False, _bodo_num_shuffle_keys={num_shuffle_keys}).agg(aggfunc)\n"
    if is_overload_none(_pivot_values):
        # Compute the unique columns. This is now always column number
        # len(index_idx), since we have done an iloc on the DataFrame.
        func_text += f"    pivot_values = data.iloc[:, {len(index_idx)}].unique()\n"
    # Call the main pivot_impl
    func_text += "    result = bodo.hiframes.pd_dataframe_ext.pivot_impl(\n"
    # Select all of the arrays. Since we have applied an iloc/groupby the new
    # locations are now always [0-m), m, [m+1, n)
    func_text += "        (\n"
    for i in range(0, len(index_idx)):
        func_text += f"            bodo.hiframes.pd_dataframe_ext.get_dataframe_data(data, {i}),\n"
    func_text += "        ),\n"
    func_text += f"        (bodo.hiframes.pd_dataframe_ext.get_dataframe_data(data, {len(index_idx)}),),\n"
    func_text += "        (\n"
    for i in range(len(index_idx) + 1, len(values_idx) + len(index_idx) + 1):
        func_text += f"            bodo.hiframes.pd_dataframe_ext.get_dataframe_data(data, {i}),\n"
    func_text += "        ),\n"
    func_text += "        pivot_values,\n"
    func_text += "        index_lit,\n"
    func_text += "        columns_lit,\n"
    func_text += "        values_lit,\n"
    # We don't need to check for duplicates because we have already
    # done an aggregation.
    func_text += "        check_duplicates=False,\n"
    # We don't need to shuffle if we matched the shuffle keys when doing the groupby.
    func_text += f"        is_already_shuffled={not is_nunique},\n"
    func_text += "        _constant_pivot_values=_constant_pivot_values,\n"
    func_text += "    )\n"
    func_text += "    ev.finalize()\n"
    func_text += "    return result\n"
    loc_vars = {}
    exec(
        func_text,
        {
            "bodo": bodo,
            "numba": numba,
            "index_lit": index_lit,
            "columns_lit": columns_lit,
            "values_lit": values_lit,
            "_pivot_values_arr": _pivot_values_arr,
            "_constant_pivot_values": _pivot_values,
            "tracing": tracing,
        },
        loc_vars,
    )
    impl = loc_vars["impl"]
    return impl


@overload(pd.melt, inline="always", no_unliteral=True)
@overload(bd.melt, inline="always", no_unliteral=True)
@overload_method(DataFrameType, "melt", inline="always", no_unliteral=True)
def overload_dataframe_melt(
    frame,
    id_vars=None,
    value_vars=None,
    var_name=None,
    value_name="value",
    col_level=None,
    ignore_index=True,
):
    unsupported_args = {
        "col_level": col_level,
        "ignore_index": ignore_index,
    }
    arg_defaults = {
        "col_level": None,
        "ignore_index": True,
    }
    check_unsupported_args(
        "DataFrame.melt",
        unsupported_args,
        arg_defaults,
        package_name="pandas",
        module_name="DataFrame",
    )

    if not isinstance(frame, DataFrameType):
        raise BodoError("pandas.melt(): 'frame' argument must be a DataFrame.")
    if not is_overload_none(id_vars) and not is_literal_type(id_vars):
        raise_bodo_error(
            "DataFrame.melt(): 'id_vars', if specified, must be a literal."
        )
    if not is_overload_none(value_vars) and not is_literal_type(value_vars):
        raise_bodo_error(
            "DataFrame.melt(): 'value_vars', if specified, must be a literal."
        )
    if not is_overload_none(var_name) and not (
        is_literal_type(var_name)
        and (is_scalar_type(var_name) or isinstance(value_name, types.Omitted))
    ):
        raise_bodo_error(
            "DataFrame.melt(): 'var_name', if specified, must be a literal."
        )
    if value_name != "value" and not (
        is_literal_type(value_name)
        and (is_scalar_type(value_name) or isinstance(value_name, types.Omitted))
    ):
        raise_bodo_error(
            "DataFrame.melt(): 'value_name', if specified, must be a literal."
        )

    var_name = (
        get_literal_value(var_name) if not is_overload_none(var_name) else "variable"
    )
    value_name = get_literal_value(value_name) if value_name != "value" else "value"

    id_lit = get_literal_value(id_vars) if not is_overload_none(id_vars) else []
    if not isinstance(id_lit, (list, tuple)):
        id_lit = [id_lit]

    for c in id_lit:
        if c not in frame.columns:
            raise BodoError(
                f"DataFrame.melt(): 'id_vars' column {c} not found in {frame}."
            )

    id_idxs = [frame.column_index[i] for i in id_lit]
    # Handle value_vars
    if is_overload_none(value_vars):
        # If value_vars isn't provided, all columns except the id_vars are used
        value_idxs = []
        value_lit = []
        for i, c in enumerate(frame.columns):
            if i not in id_idxs:
                # We assume that len(id_vars) will be relatively small
                value_idxs.append(i)
                value_lit.append(c)
    else:
        value_lit = get_literal_value(value_vars)
        if not isinstance(value_lit, (list, tuple)):
            value_lit = [value_lit]
        # In case value_lit has items from id_lit, matches pandas,
        # TODO: support value_lit=[]
        value_lit = [v for v in value_lit if v not in id_lit]
        if not value_lit:
            raise BodoError(
                "DataFrame.melt(): currently empty 'value_vars' is unsupported."
            )
        value_idxs = []
        for val in value_lit:
            if val not in frame.column_index:
                raise BodoError(
                    f"DataFrame.melt(): 'value_vars' column {val} not found in DataFrame {frame}."
                )
            value_idxs.append(frame.column_index[val])
    for c in value_lit:
        if c not in frame.columns:
            raise BodoError(
                f"DataFrame.melt(): 'value_vars' column {c} not found in {frame}."
            )
    if not (
        all(isinstance(c, int) for c in value_lit)
        or all(isinstance(c, str) for c in value_lit)
    ):
        raise BodoError(
            "DataFrame.melt(): column names selected for 'value_vars' must all share a common int or string type. Please convert your names to a common type using DataFrame.rename()"
        )
    val_type = frame.data[value_idxs[0]]
    value_dtypes = [frame.data[i].dtype for i in value_idxs]
    value_idxs = np.array(value_idxs, dtype=np.int64)
    id_idxs = np.array(id_idxs, dtype=np.int64)
    unified, _ = bodo.utils.typing.get_common_scalar_dtype(value_dtypes)
    if unified is None:
        raise BodoError(
            "DataFrame.melt(): columns selected in 'value_vars' must have a unifiable type."
        )

    extra_globals = {
        "np": np,
        "value_lit": value_lit,
        "val_type": val_type,
    }
    header = "def bodo_dataframe_melt(\n"
    header += "  frame,\n"
    header += "  id_vars=None,\n"
    header += "  value_vars=None,\n"
    header += "  var_name=None,\n"
    header += "  value_name='value',\n"
    header += "  col_level=None,\n"
    header += "  ignore_index=True,\n"
    header += "):\n"
    header += (
        "  dummy_id = bodo.hiframes.pd_dataframe_ext.get_dataframe_data(frame, 0)\n"
    )
    if frame.is_table_format and all(v == val_type.dtype for v in value_dtypes):
        # Use a meta type to make sure the column numbers are still
        # available in the IR for optimizations.
        extra_globals["value_idxs"] = bodo.utils.typing.MetaType(tuple(value_idxs))
        header += (
            "  table = bodo.hiframes.pd_dataframe_ext.get_dataframe_table(frame)\n"
        )
        header += "  val_col = bodo.utils.table_utils.table_concat(table, value_idxs, val_type)\n"
    else:
        if len(value_lit) == 1:
            header += f"  val_col = bodo.hiframes.pd_dataframe_ext.get_dataframe_data(frame, {value_idxs[0]})\n"
        else:
            val_tup = ", ".join(
                f"bodo.hiframes.pd_dataframe_ext.get_dataframe_data(frame, {i})"
                for i in value_idxs
            )
            header += f"  val_col = bodo.libs.array_kernels.concat(({val_tup},))\n"
    header += "  var_col = bodo.libs.array_kernels.repeat_like(bodo.utils.conversion.coerce_to_array(value_lit), dummy_id)\n"
    for i in id_idxs:
        header += (
            f"  id{i} = bodo.hiframes.pd_dataframe_ext.get_dataframe_data(frame, {i})\n"
        )
        # TODO [BE-2421]: Support for parallel np.tile
        # header += f"  out_id{i} = np.tile(id{i}, {len(value_lit)})\n"
        header += f"  out_id{i} = bodo.libs.array_kernels.concat([id{i}] * {len(value_lit)})\n"
    id_args = ", ".join(f"out_id{i}" for i in id_idxs) + (
        ", " if len(id_idxs) > 0 else ""
    )
    data_args = id_args + "var_col, val_col"
    columns = tuple(id_lit + [var_name, value_name])
    index = f"bodo.hiframes.pd_index_ext.init_range_index(0, len(frame) * {len(value_lit)}, 1, None)"
    return _gen_init_df(
        header,
        columns,
        data_args,
        index,
        extra_globals,
    )


@overload(pd.crosstab, inline="always", no_unliteral=True)
@overload(bd.crosstab, inline="always", no_unliteral=True)
def crosstab_overload(
    index,
    columns,
    values=None,
    rownames=None,
    colnames=None,
    aggfunc=None,
    margins=False,
    margins_name="All",
    dropna=True,
    normalize=False,
    _pivot_values=None,
):
    # TODO[BE-3188]: Disabling since needs to use new pivot infrastructure
    raise BodoError("pandas.crosstab() not supported yet")

    unsupported_args = {
        "values": values,
        "rownames": rownames,
        "colnames": colnames,
        "aggfunc": aggfunc,
        "margins": margins,
        "margins_name": margins_name,
        "dropna": dropna,
        "normalize": normalize,
    }
    arg_defaults = {
        "values": None,
        "rownames": None,
        "colnames": None,
        "aggfunc": None,
        "margins": False,
        "margins_name": "All",
        "dropna": True,
        "normalize": False,
    }
    check_unsupported_args(
        "pandas.crosstab",
        unsupported_args,
        arg_defaults,
        package_name="pandas",
        module_name="DataFrame",
    )
    # TODO: Add error checking on which Series Types are actually supported
    if not isinstance(index, SeriesType):
        raise BodoError(
            f"pandas.crosstab(): 'index' argument only supported for Series types, found {index}"
        )
    if not isinstance(columns, SeriesType):
        raise BodoError(
            f"pandas.crosstab(): 'columns' argument only supported for Series types, found {columns}"
        )

    # TODO: handle multiple keys (index args).
    # TODO: handle values and aggfunc options
    def _impl(
        index,
        columns,
        values=None,
        rownames=None,
        colnames=None,
        aggfunc=None,
        margins=False,
        margins_name="All",
        dropna=True,
        normalize=False,
        _pivot_values=None,
    ):  # pragma: no cover
        return bodo.hiframes.pd_groupby_ext.crosstab_dummy(
            index, columns, _pivot_values
        )

    return _impl


@overload_method(
    DataFrameType,
    "sort_values",
    inline="always",
    no_unliteral=True,
    jit_options={"cache": True},
)
def overload_dataframe_sort_values(
    df,
    by,
    axis=0,
    ascending=True,
    inplace=False,
    kind="quicksort",
    na_position="last",
    ignore_index=False,
    key=None,
    _bodo_chunk_bounds=None,
    _bodo_interval_sort=False,
    _bodo_transformed=False,
):
    check_runtime_cols_unsupported(df, "DataFrame.sort_values()")
    unsupported_args = {"ignore_index": ignore_index, "key": key}
    arg_defaults = {"ignore_index": False, "key": None}
    check_unsupported_args(
        "DataFrame.sort_values",
        unsupported_args,
        arg_defaults,
        package_name="pandas",
        module_name="DataFrame",
    )

    # df type can change if inplace is set (e.g. RangeIndex to Int64Index)
    handle_inplace_df_type_change(inplace, _bodo_transformed, "sort_values")

    validate_sort_values_spec(
        df,
        by,
        axis,
        ascending,
        inplace,
        kind,
        na_position,
        _bodo_chunk_bounds,
        _bodo_interval_sort,
    )

    def _impl(
        df,
        by,
        axis=0,
        ascending=True,
        inplace=False,
        kind="quicksort",
        na_position="last",
        ignore_index=False,
        key=None,
        _bodo_chunk_bounds=None,
        _bodo_interval_sort=False,
        _bodo_transformed=False,
    ):  # pragma: no cover
        return bodo.hiframes.pd_dataframe_ext.sort_values_dummy(
            df,
            by,
            ascending,
            inplace,
            na_position,
            _bodo_chunk_bounds,
            _bodo_interval_sort,
        )

    return _impl


def validate_sort_values_spec(
    df,
    by,
    axis,
    ascending,
    inplace,
    kind,
    na_position,
    _bodo_chunk_bounds,
    _bodo_interval_sort,
):
    """validates sort_values spec
    Note that some checks are due to unsupported functionalities
    """

    # whether 'by' is supplied is checked by numba
    # make sure 'by' is a const str or str list
    if is_overload_none(by) or (
        not is_literal_type(by) and not is_overload_constant_list(by)
    ):
        raise_bodo_error(
            "sort_values(): 'by' parameter only supports "
            f"a constant column label or column labels. by={by}"
        )
    # make sure by has valid label(s)
    valid_keys_set = set(df.columns)
    if is_overload_constant_str(df.index.name_typ):
        valid_keys_set.add(get_overload_const_str(df.index.name_typ))
    if is_overload_constant_tuple(by):
        key_names = [get_overload_const_tuple(by)]
    else:
        key_names = get_overload_const_list(by)
    # "A" is equivalent to ("A", "")
    key_names = {(k, "") if (k, "") in valid_keys_set else k for k in key_names}
    if len(key_names.difference(valid_keys_set)) > 0:
        invalid_keys = list(set(get_overload_const_list(by)).difference(valid_keys_set))
        raise_bodo_error(f"sort_values(): invalid keys {invalid_keys} for by.")

    # _bodo_interval_sort cannot be used without _bodo_chunk_bounds
    if is_overload_true(_bodo_interval_sort) and is_overload_none(_bodo_chunk_bounds):
        raise_bodo_error(
            "sort_values(): _bodo_chunk_bounds with at most 2 keys must be provided when _bodo_interval_sort=True."
        )

    if not is_overload_none(_bodo_chunk_bounds):
        if is_overload_true(_bodo_interval_sort):
            # If _bodo_interval_sort, at most 2 keys should be provided.
            if len(key_names) > 2:
                raise_bodo_error(
                    "sort_values(): When using _bodo_interval_sort, you must specify at most 2 keys"
                )
        elif len(key_names) != 1:
            # make sure there is only one key when bounds are passed and not _bodo_interval_sort (as currently supported)
            raise_bodo_error(
                "sort_values(): _bodo_chunk_bounds only supported when there is a single key."
            )

    # make sure axis has default value 0
    if not is_overload_zero(axis):
        raise_bodo_error(
            "sort_values(): 'axis' parameter only supports integer value 0."
        )

    # make sure 'ascending' is of type bool
    if not is_overload_bool(ascending) and not is_overload_bool_list(ascending):
        raise_bodo_error(
            "sort_values(): 'ascending' parameter must be of type bool or list of bool, "
            f"not {ascending}."
        )

    if is_overload_true(_bodo_interval_sort):
        if is_overload_bool(ascending) and not get_overload_const_bool(ascending):
            raise_bodo_error(
                "sort_values(): 'ascending' parameter must be true when using _bodo_interval_sort"
            )
        if is_overload_bool_list(ascending):
            ascending_list = get_overload_const_list(ascending)
            for asc in ascending_list:
                if not asc:
                    raise_bodo_error(
                        "sort_values(): Every value in 'ascending' must be true when using _bodo_interval_sort"
                    )

    # make sure 'inplace' is of type bool
    if not is_overload_bool(inplace):
        raise_bodo_error(
            f"sort_values(): 'inplace' parameter must be of type bool, not {inplace}."
        )

    # make sure 'kind' is not specified
    if kind != "quicksort" and not isinstance(kind, types.Omitted):
        warnings.warn(
            BodoWarning(
                "sort_values(): specifying sorting algorithm "
                "is not supported in Bodo. Bodo uses stable sort."
            )
        )

    # make sure 'na_position' is correctly specified
    if is_overload_constant_str(na_position):
        # Pandas by default only supports a string
        na_position = get_overload_const_str(na_position)
        if na_position not in ("first", "last"):
            raise BodoError(
                "sort_values(): na_position should either be 'first' or 'last'"
            )
        if is_overload_true(_bodo_interval_sort) and na_position != "last":
            raise_bodo_error(
                "sort_values(): na_position must be 'last' when using _bodo_interval_sort"
            )
    elif is_overload_constant_list(na_position):
        # Bodo supports a list to sort DataFrame column orders differently
        na_position_list = get_overload_const_list(na_position)
        for na_position in na_position_list:
            if na_position not in ("first", "last"):
                raise BodoError(
                    "sort_values(): Every value in na_position should either be 'first' or 'last'"
                )
            if is_overload_true(_bodo_interval_sort) and na_position != "last":
                raise_bodo_error(
                    "sort_values(): Every value in na_position must be 'last' when using _bodo_interval_sort"
                )
    else:
        raise_bodo_error(
            "sort_values(): na_position parameter must be a literal constant of type str or a constant "
            f"list of str with 1 entry per key column, not {na_position}"
        )


@overload_method(DataFrameType, "sort_index", inline="always", no_unliteral=True)
def overload_dataframe_sort_index(
    df,
    axis=0,
    level=None,
    ascending=True,
    inplace=False,
    kind="quicksort",
    na_position="last",
    sort_remaining=True,
    ignore_index=False,
    key=None,
    _bodo_chunk_bounds=None,
):
    check_runtime_cols_unsupported(df, "DataFrame.sort_index()")
    unsupported_args = {
        "axis": axis,
        "level": level,
        "kind": kind,
        "sort_remaining": sort_remaining,
        "ignore_index": ignore_index,
        "key": key,
    }
    arg_defaults = {
        "axis": 0,
        "level": None,
        "kind": "quicksort",
        "sort_remaining": True,
        "ignore_index": False,
        "key": None,
    }
    check_unsupported_args(
        "DataFrame.sort_index",
        unsupported_args,
        arg_defaults,
        package_name="pandas",
        module_name="DataFrame",
    )

    if not is_overload_bool(ascending):
        raise BodoError(
            "DataFrame.sort_index(): 'ascending' parameter must be of type bool"
        )

    if not is_overload_bool(inplace):
        raise BodoError(
            "DataFrame.sort_index(): 'inplace' parameter must be of type bool"
        )

    if not is_overload_constant_str(na_position) or get_overload_const_str(
        na_position
    ) not in ("first", "last"):
        raise_bodo_error(
            "DataFrame.sort_index(): 'na_position' should either be 'first' or 'last'"
        )

    def _impl(
        df,
        axis=0,
        level=None,
        ascending=True,
        inplace=False,
        kind="quicksort",
        na_position="last",
        sort_remaining=True,
        ignore_index=False,
        key=None,
        _bodo_chunk_bounds=None,
    ):  # pragma: no cover
        return bodo.hiframes.pd_dataframe_ext.sort_values_dummy(
            df,
            "$_bodo_index_",
            ascending,
            inplace,
            na_position,
            _bodo_chunk_bounds,
            False,  # _bodo_interval_sort
        )

    return _impl


@overload_method(DataFrameType, "rank", inline="always", no_unliteral=True)
def overload_dataframe_rank(
    df,
    axis=0,
    method="average",
    numeric_only=None,
    na_option="keep",
    ascending=True,
    pct=False,
):
    func_text = "def bodo_dataframe_rank(df, axis=0, method='average', numeric_only=None, na_option='keep', ascending=True, pct=False):\n"
    n_cols = len(df.columns)
    data_args = ", ".join(
        f"bodo.libs.array_kernels.rank(data_{i}, method=method, na_option=na_option, ascending=ascending, pct=pct)"
        for i in range(n_cols)
    )
    for i in range(n_cols):
        func_text += (
            f"  data_{i} = bodo.hiframes.pd_dataframe_ext.get_dataframe_data(df, {i})\n"
        )
    index = "bodo.hiframes.pd_dataframe_ext.get_dataframe_index(df)"
    return _gen_init_df(func_text, df.columns, data_args, index)


@overload_method(DataFrameType, "fillna", inline="always", no_unliteral=True)
def overload_dataframe_fillna(
    df, value=None, method=None, axis=None, inplace=False, limit=None, downcast=None
):
    check_runtime_cols_unsupported(df, "DataFrame.fillna()")
    unsupported_args = {"limit": limit, "downcast": downcast}
    arg_defaults = {"limit": None, "downcast": None}
    check_unsupported_args(
        "DataFrame.fillna",
        unsupported_args,
        arg_defaults,
        package_name="pandas",
        module_name="DataFrame",
    )

    if not (is_overload_none(axis) or is_overload_zero(axis)):  # pragma: no cover
        raise BodoError("DataFrame.fillna(): 'axis' argument not supported.")

    is_value_provided = not is_overload_none(value)
    is_method_provided = not is_overload_none(method)

    if is_value_provided and is_method_provided:
        raise BodoError("DataFrame.fillna(): Cannot specify both 'value' and 'method'.")

    if not is_value_provided and not is_method_provided:
        raise BodoError("DataFrame.fillna(): Must specify one of 'value' and 'method'.")

    # TODO: handle possible **kwargs options?

    # TODO: inplace of df with parent that has a string column (reflection)
    if is_value_provided:
        fill_na_arg = "value=value"
    else:
        fill_na_arg = "method=method"
    data_args = [
        f"df['{c}'].fillna({fill_na_arg}, inplace=inplace)"
        if isinstance(c, str)
        else f"df[{c}].fillna({fill_na_arg}, inplace=inplace)"
        for c in df.columns
    ]
    func_text = "def bodo_dataframe_fillna(df, value=None, method=None, axis=None, inplace=False, limit=None, downcast=None):\n"
    if is_overload_true(inplace):
        func_text += "  " + "  \n".join(data_args) + "\n"
        return bodo_exec(func_text, {}, {}, __name__)
    else:
        return _gen_init_df(
            func_text, df.columns, ", ".join(d + ".values" for d in data_args)
        )


@overload_method(DataFrameType, "reset_index", inline="always", no_unliteral=True)
def overload_dataframe_reset_index(
    df,
    level=None,
    drop=False,
    inplace=False,
    col_level=0,
    col_fill="",
    _bodo_transformed=False,
):
    check_runtime_cols_unsupported(df, "DataFrame.reset_index()")
    unsupported_args = {"col_level": col_level, "col_fill": col_fill}
    arg_defaults = {"col_level": 0, "col_fill": ""}
    check_unsupported_args(
        "DataFrame.reset_index",
        unsupported_args,
        arg_defaults,
        package_name="pandas",
        module_name="DataFrame",
    )

    handle_inplace_df_type_change(inplace, _bodo_transformed, "reset_index")

    # we only support dropping all levels currently
    if not _is_all_levels(df, level):  # pragma: no cover
        raise_bodo_error(
            "DataFrame.reset_index(): only dropping all index levels supported"
        )

    # make sure 'drop' is a constant bool
    if not is_overload_constant_bool(drop):  # pragma: no cover
        raise BodoError(
            "DataFrame.reset_index(): 'drop' parameter should be a constant boolean value"
        )

    # make sure 'inplace' is a constant bool
    if not is_overload_constant_bool(inplace):
        raise BodoError(
            "DataFrame.reset_index(): 'inplace' parameter should be a constant boolean value"
        )

    # impl: for each column, copy data and create a new dataframe
    func_text = "def bodo_dataframe_reset_index(df, level=None, drop=False, inplace=False, col_level=0, col_fill='', _bodo_transformed=False,):\n"
    func_text += (
        "  index = bodo.hiframes.pd_index_ext.init_range_index(0, len(df), 1, None)\n"
    )

    drop = is_overload_true(drop)
    inplace = is_overload_true(inplace)
    columns = df.columns
    data_args = [
        "bodo.hiframes.pd_dataframe_ext.get_dataframe_data(df, {}){}\n".format(
            i, "" if inplace else ".copy()"
        )
        for i in range(len(df.columns))
    ]
    # add index array arguments if not dropping index
    if not drop:
        # pandas assigns "level_0" if "index" is already used as a column name
        # https://github.com/pandas-dev/pandas/blob/08b70d837dd017d49d2c18e02369a15272b662b2/pandas/core/frame.py#L4547
        default_name = "index" if "index" not in columns else "level_0"
        index_names = get_index_names(df.index, "DataFrame.reset_index()", default_name)
        columns = index_names + columns
        if isinstance(df.index, MultiIndexType):
            # MultiIndex case takes multiple arrays from MultiIndex
            func_text += "  m_index = bodo.hiframes.pd_index_ext.get_index_data(bodo.hiframes.pd_dataframe_ext.get_dataframe_index(df))\n"
            ind_arrs = [f"m_index[{i}]" for i in range(df.index.nlevels)]
            data_args = ind_arrs + data_args
        else:
            ind_arr = "bodo.utils.conversion.index_to_array(bodo.hiframes.pd_dataframe_ext.get_dataframe_index(df))"
            data_args = [ind_arr] + data_args

    # TODO: inplace of df with parent (reflection)
    # return new df even for inplace case, since typing pass replaces input variable
    # using output of the call
    return _gen_init_df(func_text, columns, ", ".join(data_args), "index")


def _is_all_levels(df, level):
    """return True if 'level' argument selects all Index levels in dataframe 'df'"""
    n_levels = len(get_index_data_arr_types(df.index))
    return (
        is_overload_none(level)
        or (
            is_overload_constant_int(level)
            and get_overload_const_int(level) == 0
            and n_levels == 1
        )
        or (
            is_overload_constant_list(level)
            and list(get_overload_const_list(level)) == list(range(n_levels))
        )
    )


@overload_method(DataFrameType, "dropna", inline="always", no_unliteral=True)
def overload_dataframe_dropna(
    df, axis=0, how="any", thresh=None, subset=None, inplace=False
):
    check_runtime_cols_unsupported(df, "DataFrame.dropna()")
    # error-checking for inplace=True
    if not is_overload_constant_bool(inplace) or is_overload_true(inplace):
        raise BodoError("DataFrame.dropna(): inplace=True is not supported")

    # check axis=0
    if not is_overload_zero(axis):
        raise_bodo_error("df.dropna(): only axis=0 supported")

    ensure_constant_values("dropna", "how", how, ("any", "all"))

    # get the index of columns to consider for NA check
    if is_overload_none(subset):
        subset_ints = list(range(len(df.columns)))
    elif not is_overload_constant_list(subset):
        raise_bodo_error(
            f"df.dropna(): subset argument should a constant list, not {subset}"
        )
    else:
        subset_vals = get_overload_const_list(subset)
        subset_ints = []
        for s in subset_vals:
            if s not in df.column_index:
                raise_bodo_error(
                    f"df.dropna(): column '{s}' not in data frame columns {df}"
                )
            subset_ints.append(df.column_index[s])

    n_cols = len(df.columns)
    data_args = ", ".join(f"data_{i}" for i in range(n_cols))

    func_text = "def bodo_dataframe_dropna(df, axis=0, how='any', thresh=None, subset=None, inplace=False):\n"
    for i in range(n_cols):
        func_text += (
            f"  data_{i} = bodo.hiframes.pd_dataframe_ext.get_dataframe_data(df, {i})\n"
        )
    index = "bodo.utils.conversion.index_to_array(bodo.hiframes.pd_dataframe_ext.get_dataframe_index(df))"
    func_text += "  ({0}, index_arr) = bodo.libs.array_kernels.dropna(({0}, {1}), how, thresh, ({2},))\n".format(
        data_args, index, ", ".join(str(a) for a in subset_ints)
    )
    func_text += "  index = bodo.utils.conversion.index_from_array(index_arr)\n"
    return _gen_init_df(func_text, df.columns, data_args, "index")


@overload_method(DataFrameType, "drop", inline="always", no_unliteral=True)
def overload_dataframe_drop(
    df,
    labels=None,
    axis=0,
    index=None,
    columns=None,
    level=None,
    inplace=False,
    errors="raise",
    _bodo_transformed=False,
):
    check_runtime_cols_unsupported(df, "DataFrame.drop()")
    unsupported_args = {"index": index, "level": level, "errors": errors}
    arg_defaults = {"index": None, "level": None, "errors": "raise"}
    check_unsupported_args(
        "DataFrame.drop",
        unsupported_args,
        arg_defaults,
        package_name="pandas",
        module_name="DataFrame",
    )

    handle_inplace_df_type_change(inplace, _bodo_transformed, "drop")

    if not is_overload_constant_bool(inplace):  # pragma: no cover
        raise_bodo_error(
            "DataFrame.drop(): 'inplace' parameter should be a constant bool"
        )

    if not is_overload_none(labels):
        if not is_overload_none(columns):
            raise BodoError(
                "Dataframe.drop(): Cannot specify both 'labels' and 'columns'"
            )

        # make sure axis=1
        if (
            not is_overload_constant_int(axis) or get_overload_const_int(axis) != 1
        ):  # pragma: no cover
            raise_bodo_error("DataFrame.drop(): only axis=1 supported")
        # get 'labels' column list
        if is_overload_constant_str(labels):
            drop_cols = (get_overload_const_str(labels),)
        elif is_overload_constant_list(labels):  # pragma: no cover
            drop_cols = get_overload_const_list(labels)
        else:  # pragma: no cover
            raise_bodo_error(
                "constant list of columns expected for labels in DataFrame.drop()"
            )
    else:
        if is_overload_none(columns):
            raise BodoError(
                "DataFrame.drop(): Need to specify at least one of 'labels' or 'columns'"
            )

        if is_overload_constant_str(columns):  # pragma: no cover
            drop_cols = (get_overload_const_str(columns),)
        elif is_overload_constant_list(columns):
            drop_cols = get_overload_const_list(columns)
        else:  # pragma: no cover
            raise_bodo_error(
                "constant list of columns expected for labels in DataFrame.drop()"
            )

    # check drop columns to be in df schema
    for c in drop_cols:
        if c not in df.columns:
            raise_bodo_error(
                f"DataFrame.drop(): column {c} not in DataFrame columns {df.columns}"
            )
    if len(set(drop_cols)) == len(df.columns):
        raise BodoError("DataFrame.drop(): Dropping all columns not supported.")

    inplace = is_overload_true(inplace)
    # TODO: inplace of df with parent (reflection)

    new_cols = tuple(c for c in df.columns if c not in drop_cols)
    data_args = ", ".join(
        "bodo.hiframes.pd_dataframe_ext.get_dataframe_data(df, {}){}".format(
            df.column_index[c], ".copy()" if not inplace else ""
        )
        for c in new_cols
    )

    func_text = "def impl(df, labels=None, axis=0, index=None, columns=None,\n"
    func_text += (
        "     level=None, inplace=False, errors='raise', _bodo_transformed=False):\n"
    )
    index = "bodo.hiframes.pd_dataframe_ext.get_dataframe_index(df)"
    # return new df even for inplace case, since typing pass replaces input variable
    # using output of the call
    return _gen_init_df(func_text, new_cols, data_args, index)


@overload_method(DataFrameType, "sample", inline="always", no_unliteral=True)
def overload_dataframe_sample(
    df,
    n=None,
    frac=None,
    replace=False,
    weights=None,
    random_state=None,
    axis=None,
    ignore_index=False,
):
    """Implementation of the sample functionality from
    https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sample.html
    """
    check_runtime_cols_unsupported(df, "DataFrame.sample()")
    unsupported_args = {"weights": weights, "axis": axis, "ignore_index": ignore_index}
    sample_defaults = {"weights": None, "axis": None, "ignore_index": False}

    if not is_overload_none(n) and not is_overload_none(frac):
        raise BodoError(
            "DataFrame.sample(): only one of n and frac option can be selected"
        )

    if not is_overload_none(random_state) and not is_overload_int(random_state):
        raise BodoError(
            "DataFrame.sample(): random_state must be None or an int argument"
        )

    check_unsupported_args(
        "DataFrame.sample",
        unsupported_args,
        sample_defaults,
        package_name="pandas",
        module_name="DataFrame",
    )

    n_cols = len(df.columns)
    data_args = ", ".join(f"data_{i}" for i in range(n_cols))
    rhs_data_args = ", ".join(f"rhs_data_{i}" for i in range(n_cols))

    func_text = "def impl(df, n=None, frac=None, replace=False, weights=None, random_state=None, axis=None, ignore_index=False):\n"
    for i in range(n_cols):
        func_text += f"  rhs_data_{i} = bodo.hiframes.pd_dataframe_ext.get_dataframe_data(df, {i})\n"
    func_text += "  frac_d = -1.0 if frac is None else frac\n"
    func_text += "  n_i = 0 if n is None else n\n"
    func_text += "  if random_state is None:\n"
    func_text += "    random_state_i = bodo.libs.distributed_api.bcast_scalar(np.random.randint(0, 2**31))\n"
    func_text += "  else:\n"
    func_text += "    random_state_i = random_state\n"

    index = "bodo.utils.conversion.index_to_array(bodo.hiframes.pd_dataframe_ext.get_dataframe_index(df))"
    func_text += f"  ({data_args},), index_arr = bodo.libs.array_kernels.sample_table_operation(({rhs_data_args},), {index}, n_i, frac_d, replace, random_state_i)\n"
    func_text += "  index = bodo.utils.conversion.index_from_array(index_arr)\n"
    return bodo.hiframes.dataframe_impl._gen_init_df(
        func_text,
        df.columns,
        data_args,
        "index",
    )


@numba.njit
def _sizeof_fmt(num, size_qualifier=""):  # pragma: no cover
    """
    Return size in human readable format.
    Copied from Pandas https://github.com/pandas-dev/pandas/blob/master/pandas/io/formats/info.py#L59
    """
    for x in ["bytes", "KB", "MB", "GB", "TB"]:
        if num < 1024.0:
            return f"{num:3.1f}{size_qualifier} {x}"
        num /= 1024.0
    return f"{num:3.1f}{size_qualifier} PB"


@overload_method(DataFrameType, "info", no_unliteral=True)
def overload_dataframe_info(
    df,
    verbose=None,
    buf=None,
    max_cols=None,
    memory_usage=None,
    show_counts=None,
    null_counts=None,
):
    check_runtime_cols_unsupported(df, "DataFrame.info()")
    args_dict = {
        "verbose": verbose,
        "buf": buf,
        "max_cols": max_cols,
        "memory_usage": memory_usage,
        "show_counts": show_counts,
        "null_counts": null_counts,
    }
    args_default_dict = {
        "verbose": None,
        "buf": None,
        "max_cols": None,
        "memory_usage": None,
        "show_counts": None,
        "null_counts": None,
    }
    check_unsupported_args(
        "DataFrame.info",
        args_dict,
        args_default_dict,
        package_name="pandas",
        module_name="DataFrame",
    )
    class_name = f"<class '{str(type(df)).split('.')[-1]}"
    # Empty dataframe
    if len(df.columns) == 0:

        def _info_impl(
            df,
            verbose=None,
            buf=None,
            max_cols=None,
            memory_usage=None,
            show_counts=None,
            null_counts=None,
        ):  # pragma: no cover\n"
            lines = class_name + "\n"
            lines += "Index: 0 entries\n"
            lines += "Empty DataFrame"
            print(lines)

        return _info_impl
    else:
        func_text = "def _info_impl(df, verbose=None, buf=None, max_cols=None, memory_usage=None, show_counts=None, null_counts=None): #pragma: no cover\n"
        func_text += "    ncols = df.shape[1]\n"
        func_text += f'    lines = "{class_name}\\n"\n'
        func_text += f'    lines += "{df.index}: "\n'
        func_text += (
            "    index = bodo.hiframes.pd_dataframe_ext.get_dataframe_index(df)\n"
        )
        # total_number entries, start to end
        if isinstance(df.index, bodo.hiframes.pd_index_ext.RangeIndexType):
            func_text += '    lines += f"{len(index)} entries, {index.start} to {index.stop-1}\\n"\n'
        # TODO: Remove when [BE-653] is fixed
        elif isinstance(df.index, bodo.hiframes.pd_index_ext.StringIndexType):
            func_text += '    lines += f"{len(index)} entries, {index[0]} to {index[len(index)-1]}\\n"\n'
        else:
            func_text += (
                '    lines += f"{len(index)} entries, {index[0]} to {index[-1]}\\n"\n'
            )
        func_text += '    lines += f"Data columns (total {ncols} columns):\\n"\n'
        # Formatting re-used from Dask (https://docs.dask.org/en/latest/_modules/dask/dataframe/core.html#DataFrame.info)
        # Store max. length of column name (needed for formatting table in print)
        func_text += f"    space = {max(len(str(k)) for k in df.columns) + 1}\n"
        func_text += "    column_width = max(space, 7)\n"
        # Table Header
        func_text += '    column= "Column"\n'
        func_text += '    underl= "------"\n'
        func_text += (
            '    lines += f"#   {column:<{column_width}} Non-Null Count  Dtype\\n"\n'
        )
        func_text += (
            '    lines += f"--- {underl:<{column_width}} --------------  -----\\n"\n'
        )
        # Compute memory usage (sum nbytes for columns and index)
        func_text += "    mem_size = 0\n"
        # Arrays to store column information (name, non-null count, and dtype)
        func_text += (
            "    col_name = bodo.libs.str_arr_ext.pre_alloc_string_array(ncols, -1)\n"
        )
        func_text += "    non_null_count = bodo.libs.str_arr_ext.pre_alloc_string_array(ncols, -1)\n"
        func_text += (
            "    col_dtype = bodo.libs.str_arr_ext.pre_alloc_string_array(ncols, -1)\n"
        )
        # Dictionary to store how many column types in the df
        dtype_count = {}
        # Loop over each column and get its nbytes, name, how many non-null, and dtype
        for i in range(len(df.columns)):
            func_text += f"    non_null_count[{i}] = str(bodo.libs.array_ops.array_op_count(bodo.hiframes.pd_dataframe_ext.get_dataframe_data(df, {i})))\n"
            # Get type of column and rename Categorical and nullable int to match Pandas
            dtype_name = f"{df.data[i].dtype}"
            if isinstance(df.data[i], bodo.types.CategoricalArrayType):
                dtype_name = "category"
            elif isinstance(df.data[i], bodo.types.IntegerArrayType):
                int_typ_name = bodo.libs.int_arr_ext.IntDtype(df.data[i].dtype).name
                dtype_name = int_typ_name[:-7]  # remove trailing "Dtype()"
            elif isinstance(df.data[i], FloatingArrayType):
                float_typ_name = bodo.libs.float_arr_ext.FloatDtype(
                    df.data[i].dtype
                ).name
                dtype_name = float_typ_name[:-7]  # remove trailing "Dtype()"
            func_text += f'    col_dtype[{i}] = "{dtype_name}"\n'

            if dtype_name in dtype_count:
                dtype_count[dtype_name] += 1
            else:
                dtype_count[dtype_name] = 1
            func_text += f'    col_name[{i}] = "{df.columns[i]}"\n'
            func_text += f"    mem_size += bodo.hiframes.pd_dataframe_ext.get_dataframe_data(df, {i}).nbytes\n"
        # Generate column information per line
        func_text += "    column_info = [f'{i:^3} {name:<{column_width}} {count} non-null      {dtype}' for i, (name, count, dtype) in enumerate(zip(col_name, non_null_count, col_dtype))]\n"
        func_text += "    for i in column_info:\n"
        func_text += "        lines += f'{i}\\n'\n"

        # TODO: [BE-652]
        # Pandas: df.dtypes.value_counts().groupby(lambda x: x.name).sum()
        # Workaround: build dictionary of dtypes at compile time
        dtype_line = ", ".join(f"{k}({dtype_count[k]})" for k in sorted(dtype_count))
        func_text += f"    lines += 'dtypes: {dtype_line}\\n'\n"
        # Add index nbytes to memory usage
        func_text += "    mem_size += df.index.nbytes\n"
        # Format memory size to be in human readable format
        func_text += "    total_size = _sizeof_fmt(mem_size)\n"
        func_text += "    lines += f'memory usage: {total_size}'\n"
        func_text += "    print(lines)\n"
        loc_vars = {}
        exec(
            func_text,
            {"_sizeof_fmt": _sizeof_fmt, "pd": pd, "bodo": bodo, "np": np},
            loc_vars,
        )
        _info_impl = loc_vars["_info_impl"]

        return _info_impl


@overload_method(DataFrameType, "memory_usage", inline="always", no_unliteral=True)
def overload_dataframe_memory_usage(df, index=True, deep=False):
    """Support df.memory_usage by getting nbytes from underlying arrays for each column
    and return result as a Series
    index argument is supported
    Pandas deep is related to object datatype which isn't available in Bodo.
    Hence, deep argument is meaningless inside Bodo.
    """
    check_runtime_cols_unsupported(df, "DataFrame.memory_usage()")
    func_text = "def impl(df, index=True, deep=False):\n"

    # Compute nbytes for index, only used if index=True
    index_nbytes = "bodo.hiframes.pd_dataframe_ext.get_dataframe_index(df).nbytes"
    use_index = is_overload_true(index)
    columns = df.columns
    if use_index:
        # create index values for the Series output that has 'Index' and column names
        columns = ("Index",) + columns
    if len(columns) == 0:
        # Handle empty columns edge case to output
        # a regular index.
        column_vals = ()
    elif all(isinstance(c, int) for c in columns):
        column_vals = np.array(columns, "int64")
    elif all(isinstance(c, str) for c in columns):
        column_vals = pd.array(columns, "string")
    else:
        column_vals = columns
    if df.is_table_format and len(df.columns) > 0:
        # Don't use table format if the table is unused.
        start_offset = int(use_index)
        num_cols = len(columns)
        func_text += f"  nbytes_arr = np.empty({num_cols}, np.int64)\n"
        func_text += (
            "  table = bodo.hiframes.pd_dataframe_ext.get_dataframe_table(df)\n"
        )
        func_text += f"  bodo.utils.table_utils.generate_table_nbytes(table, nbytes_arr, {start_offset})\n"
        if use_index:
            func_text += f"  nbytes_arr[0] = {index_nbytes}\n"
        func_text += "  return bodo.hiframes.pd_series_ext.init_series(nbytes_arr, pd.Index(column_vals), None)\n"
    else:
        data = ", ".join(
            f"bodo.libs.array_ops.array_op_nbytes(bodo.hiframes.pd_dataframe_ext.get_dataframe_data(df, {i}))"
            for i in range(len(df.columns))
        )
        if use_index:
            data = f"{index_nbytes},{data}"
        else:
            comma = "," if len(columns) == 1 else ""
            data = f"{data}{comma}"
        func_text += f"  return bodo.hiframes.pd_series_ext.init_series(({data}), pd.Index(column_vals), None)\n"

    loc_vars = {}
    exec(
        func_text,
        {
            "bodo": bodo,
            "np": np,
            "pd": pd,
            "column_vals": column_vals,
        },
        loc_vars,
    )
    impl = loc_vars["impl"]
    return impl


@overload(pd.read_excel, no_unliteral=True)
@overload(bd.read_excel, no_unliteral=True)
def overload_read_excel(
    io,
    sheet_name=0,
    header=0,
    names=None,
    index_col=None,
    usecols=None,
    dtype=None,
    engine=None,
    converters=None,
    true_values=None,
    false_values=None,
    skiprows=None,
    nrows=None,
    na_values=None,
    keep_default_na=True,
    na_filter=True,
    verbose=False,
    parse_dates=False,
    date_parser=None,
    date_format=None,
    thousands=None,
    decimal=".",
    comment=None,
    skipfooter=0,
    storage_options=None,
    _bodo_df_type=None,
):
    # implement pd.read_excel() by just calling Pandas
    # utyped pass adds _bodo_df_type argument which is a TypeRef of output type
    df_type = _bodo_df_type.instance_type

    # add output type to numba 'types' module with a unique name, needed for objmode
    t_name = f"read_excel_df{next_label()}"
    setattr(types, t_name, df_type)

    # objmode doesn't allow lists, embed 'parse_dates' as a constant inside objmode
    parse_dates_const = False
    if is_overload_constant_list(parse_dates):
        parse_dates_const = get_overload_const_list(parse_dates)

    # embed dtype since objmode doesn't allow list/dict
    pd_dtype_strs = ", ".join(
        [
            f"'{cname}':{_get_pd_dtype_str(t)}"
            for cname, t in zip(df_type.columns, df_type.data)
        ]
    )

    func_text = f"""
def impl(
    io,
    sheet_name=0,
    header=0,
    names=None,
    index_col=None,
    usecols=None,
    dtype=None,
    engine=None,
    converters=None,
    true_values=None,
    false_values=None,
    skiprows=None,
    nrows=None,
    na_values=None,
    keep_default_na=True,
    na_filter=True,
    verbose=False,
    parse_dates=False,
    date_parser=None,
    date_format=None,
    thousands=None,
    decimal='.',
    comment=None,
    skipfooter=0,
    storage_options=None,
    _bodo_df_type=None,
):
    with bodo.ir.object_mode.no_warning_objmode(df="{t_name}"):
        df = pd.read_excel(
            io=io,
            sheet_name=sheet_name,
            header=header,
            names={list(df_type.columns)},
            index_col=index_col,
            usecols=usecols,
            dtype={{{pd_dtype_strs}}},
            engine=engine,
            converters=converters,
            true_values=true_values,
            false_values=false_values,
            skiprows=skiprows,
            nrows=nrows,
            na_values=na_values,
            keep_default_na=keep_default_na,
            na_filter=na_filter,
            verbose=verbose,
            parse_dates={parse_dates_const},
            date_parser=pd._libs.lib.no_default,
            date_format=date_format,
            thousands=thousands,
            decimal=decimal,
            comment=comment,
            skipfooter=skipfooter,
            storage_options=storage_options,
        )
    return df
"""
    loc_vars = {}
    exec(func_text, globals(), loc_vars)
    impl = loc_vars["impl"]
    return impl


def overload_dataframe_plot(
    df,
    x=None,
    y=None,
    kind="line",
    figsize=None,
    xlabel=None,
    ylabel=None,
    title=None,
    legend=True,
    fontsize=None,
    xticks=None,
    yticks=None,
    ax=None,
):
    try:
        import matplotlib.pyplot as plt
    except ImportError:
        raise BodoError("df.plot needs matplotlib which is not installed.")
    # Pandas behavior
    # This is based on testing. Nothing clear in the source code to indicate this.
    # When x is None, x is range(len(y))
    # When y is None, all number columns will be y (excluding x's column if x is not None)
    func_text = (
        "def impl(df, x=None, y=None, kind='line', figsize=None, xlabel=None, \n"
    )
    func_text += "    ylabel=None, title=None, legend=True, fontsize=None, \n"
    func_text += "    xticks=None, yticks=None, ax=None):\n"
    if is_overload_none(ax):
        func_text += "   fig, ax = plt.subplots()\n"
    else:
        func_text += "   fig = ax.get_figure()\n"
    if not is_overload_none(figsize):
        func_text += "   fig.set_figwidth(figsize[0])\n"
        func_text += "   fig.set_figheight(figsize[1])\n"
    if is_overload_none(xlabel):
        func_text += "   xlabel = x\n"
    func_text += "   ax.set_xlabel(xlabel)\n"
    if is_overload_none(ylabel):
        func_text += "   ylabel = y\n"
    else:
        func_text += "   ax.set_ylabel(ylabel)\n"
    if not is_overload_none(title):
        func_text += "   ax.set_title(title)\n"
    if not is_overload_none(fontsize):
        func_text += "   ax.tick_params(labelsize=fontsize)\n"
    kind = get_overload_const_str(kind)
    if kind == "line":
        if is_overload_none(x) and is_overload_none(y):
            for i in range(len(df.columns)):
                if isinstance(
                    df.data[i], (types.Array, IntegerArrayType, FloatingArrayType)
                ) and isinstance(df.data[i].dtype, (types.Integer, types.Float)):
                    func_text += f"   ax.plot(df.iloc[:, {i}], label=df.columns[{i}])\n"
        elif is_overload_none(x):
            func_text += "   ax.plot(df[y], label=y)\n"
        elif is_overload_none(y):
            x_val = get_overload_const_str(x)
            x_idx = df.columns.index(x_val)
            for i in range(len(df.columns)):
                if isinstance(
                    df.data[i], (types.Array, IntegerArrayType, FloatingArrayType)
                ) and isinstance(df.data[i].dtype, (types.Integer, types.Float)):
                    if x_idx != i:
                        func_text += f"   ax.plot(df[x], df.iloc[:, {i}], label=df.columns[{i}])\n"
        else:
            func_text += "   ax.plot(df[x], df[y], label=y)\n"
    elif kind == "scatter":
        legend = False
        # s=20 to match dot size with pandas
        func_text += "   ax.scatter(df[x], df[y], s=20)\n"
        # ax.scatter ignores label=y
        func_text += "   ax.set_ylabel(ylabel)\n"
    if not is_overload_none(xticks):
        func_text += "   ax.set_xticks(xticks)\n"
    if not is_overload_none(yticks):
        func_text += "   ax.set_yticks(yticks)\n"
    if is_overload_true(legend):
        func_text += "   ax.legend()\n"

    func_text += "   return ax\n"
    loc_vars = {}
    exec(func_text, {"bodo": bodo, "plt": plt}, loc_vars)
    impl = loc_vars["impl"]
    return impl


@lower_builtin("df.plot", DataFrameType, types.VarArg(types.Any))
def dataframe_plot_low(context, builder, sig, args):
    impl = overload_dataframe_plot(*sig.args)
    return context.compile_internal(builder, impl, sig, args)


def is_df_values_numpy_supported_dftyp(df_typ):
    """helper function that checks if the dataframe type contains only numeric/boolean/dt64/td64 values"""
    for col_typ in df_typ.data:
        if not (
            isinstance(col_typ, (IntegerArrayType, FloatingArrayType))
            or isinstance(col_typ.dtype, types.Number)
            or col_typ.dtype in (bodo.types.datetime64ns, bodo.types.timedelta64ns)
        ):
            return False
    return True


def typeref_to_type(v):
    """convert TypeRef and NumberClass to a regular data type"""
    if isinstance(v, types.BaseTuple):
        return types.BaseTuple.from_types(tuple(typeref_to_type(a) for a in v))

    return v.instance_type if isinstance(v, (types.TypeRef, types.NumberClass)) else v


def _install_typer_for_type(type_name, typ):
    """install typer for a bodo type call to be used inside jit
    e.g. bodo.types.DataFrameType()
    """

    @type_callable(typ)
    def type_call_type(context):
        def typer(*args, **kws):
            args = tuple(typeref_to_type(v) for v in args)
            kws = {name: typeref_to_type(v) for name, v in kws.items()}
            return types.TypeRef(typ(*args, **kws))

        return typer

    no_side_effect_call_tuples.add((type_name, "types", bodo))
    no_side_effect_call_tuples.add((typ,))
    # TODO(ehsan): make lower_builtin work in case the type calls is not removed by
    # dead code elimination for some reason
    # lower_builtin(typ, types.VarArg(types.Any), types.StarArgTuple)(lambda c, b, s, a: c.get_dummy_value())


def _install_type_call_typers():
    """install typers for all bodo type calls to be used inside jit
    e.g. bodo.types.DataFrameType()
    """
    for type_name in bodo_types_with_params:
        typ = getattr(bodo.types, type_name)
        _install_typer_for_type(type_name, typ)


_install_type_call_typers()


def set_df_col(df, cname, arr, inplace):  # pragma: no cover
    df[cname] = arr


@infer_global(set_df_col)
class SetDfColInfer(AbstractTemplate):
    def generic(self, args, kws):
        from bodo.hiframes.pd_dataframe_ext import DataFrameType

        assert not kws
        assert len(args) == 4
        assert isinstance(args[1], types.Literal)
        target = args[0]
        ind = args[1].literal_value
        val = args[2]
        assert val != types.unknown
        ret = target
        check_runtime_cols_unsupported(target, "set_df_col()")

        if isinstance(target, DataFrameType):
            index = target.index
            # empty df index is updated based on new column
            if len(target.columns) == 0:
                index = bodo.hiframes.pd_index_ext.RangeIndexType(types.none)
            if isinstance(val, SeriesType):
                if len(target.columns) == 0:
                    index = val.index
                val = val.data
            if is_pd_index_type(val):
                val = bodo.utils.typing.get_index_data_arr_types(val)[0]

            if isinstance(val, types.List):
                val = dtype_to_array_type(val.dtype)
            if is_overload_constant_str(val) or val == types.unicode_type:
                # String scalars are coerced to dictionary encoded arrays.
                val = bodo.types.dict_str_arr_type
            elif not is_array_typ(val):
                val = dtype_to_array_type(val)
            if ind in target.columns:
                # set existing column, with possibly a new array type
                new_cols = target.columns
                col_id = target.columns.index(ind)
                new_typs = list(target.data)
                new_typs[col_id] = val
                new_typs = tuple(new_typs)
            else:
                # set a new column
                new_cols = target.columns + (ind,)
                new_typs = target.data + (val,)
            ret = DataFrameType(
                new_typs, index, new_cols, target.dist, target.is_table_format
            )

        return ret(*args)


SetDfColInfer.prefer_literal = True


def __bodosql_replace_columns_dummy(
    df, col_names_to_replace, cols_to_replace_with
):  # pragma: no cover
    for i in range(len(col_names_to_replace)):
        df[col_names_to_replace[i]] = cols_to_replace_with[i]


@infer_global(__bodosql_replace_columns_dummy)
class BodoSQLReplaceColsInfer(AbstractTemplate):  # pragma: no cover
    """
    Type inferer/validator function for __bodosql_replace_columns_dummy.
    __bodosql_replace_columns_dummy is a function that should only be generated by BodoSQL. It is replaced in dataframe pass
    With a list of sequential set_table_value's.
    __bodosql_replace_columns_dummy takes three arguments, a table, a tuple of column names to replace, and a tuple of column values used to replace the columns found at the specified indices.
    """

    def generic(self, args, kws):
        from bodo.hiframes.pd_dataframe_ext import DataFrameType

        assert not kws
        assert len(args) == 3
        assert is_overload_constant_tuple(args[1])
        assert isinstance(args[2], types.BaseTuple)
        input_df_typ = args[0]

        assert (
            isinstance(input_df_typ, DataFrameType) and len(input_df_typ.columns) > 0
        ), (
            "Error while typechecking __bodosql_replace_columns_dummy: we should only generate a call __bodosql_replace_columns_dummy if the input dataframe"
        )

        col_names_to_replace = get_overload_const_tuple(args[1])
        replacement_columns = args[2]
        assert len(col_names_to_replace) == len(replacement_columns), (
            "Error while typechecking __bodosql_replace_columns_dummy: the tuple of column indicies to replace should be equal to the number of columns to replace them with"
        )
        assert len(col_names_to_replace) <= len(input_df_typ.columns), (
            "Error while typechecking __bodosql_replace_columns_dummy: The number of indicies provided should be less than or equal to the number of columns in the input dataframe"
        )

        for col_name in col_names_to_replace:
            assert col_name in input_df_typ.columns, (
                "Error while typechecking __bodosql_replace_columns_dummy: All columns specified to be replaced should already be present in input dataframe"
            )

        # I don't think this check should ever be needed, due to the way bodosql does
        # codegen, but better safe than sorry
        check_runtime_cols_unsupported(
            input_df_typ, "__bodosql_replace_columns_dummy()"
        )
        index = input_df_typ.index
        new_cols = input_df_typ.columns
        # convert to list so we can modify it in the for loop below
        new_typs = list(input_df_typ.data)

        for i in range(len(col_names_to_replace)):
            col_name = col_names_to_replace[i]
            replace_col = replacement_columns[i]
            # Currently, replace_col should always be series type
            # But, we may pass non series values to this function in the future
            assert isinstance(replace_col, SeriesType), (
                "Error while typechecking __bodosql_replace_columns_dummy: the values to replace the columns with are expected to be series"
            )
            if isinstance(replace_col, SeriesType):
                replace_col = replace_col.data

            # set existing column, with possibly a new array type
            col_idx = input_df_typ.column_index[col_name]
            new_typs[col_idx] = replace_col
        # convert list back to tuple
        new_typs = tuple(new_typs)

        ret = DataFrameType(
            new_typs, index, new_cols, input_df_typ.dist, input_df_typ.is_table_format
        )
        return ret(*args)


BodoSQLReplaceColsInfer.prefer_literal = True


def _parse_query_expr(
    expr: str,
    env,
    columns,
    cleaned_columns,
    index_name=None,
    join_cleaned_cols=(),
):
    """Parses expression string for DataFrame.query() call handling.
    Patches Pandas query expr parsing code to avoid evaluating values during parsing.

    join_cleaned_cols are used by general join condition to detect
    non-valid identifiers.
    """
    used_cols = {}

    # NOTE: all comments are Bodo specific
    # avoid rewrite of operations in Pandas such as early evaluation of string exprs
    def _rewrite_membership_op(self, node, left, right):
        op_instance = node.op
        op = self.visit(op_instance)
        return op, op_instance, left, right

    def _maybe_evaluate_binop(
        self,
        op,
        op_class,
        lhs,
        rhs,
        eval_in_python=("in", "not in"),
        maybe_eval_in_python=("==", "!=", "<", ">", "<=", ">="),
    ):
        res = op(lhs, rhs)
        return res

    # avoid early evaluation of getattr such as C.str.contains().
    # functions like C.str.contains are saved and handled similar to
    # intrinsic functions like sqrt instead of evaluation.
    new_funcs = []

    class NewFuncNode(pandas.core.computation.ops.FuncNode):
        def __init__(self, name):
            if name not in pandas.core.computation.ops.MATHOPS or (
                pandas.core.computation.check._NUMEXPR_INSTALLED
                and pandas.core.computation.check_NUMEXPR_VERSION
                < pandas.core.computation.ops.LooseVersion("2.6.9")
                and name in ("floor", "ceil")
            ):
                if name not in new_funcs:
                    raise BodoError(f'"{name}" is not a supported function')

            self.name = name
            if name in new_funcs:
                self.func = name
            else:
                self.func = getattr(np, name)

        def __call__(self, *args):
            return pandas.core.computation.ops.MathCall(self, args)

        # __repr__ is needed if this attr node is not called, e.g. A.dt.year
        def __repr__(self):
            # _replace_column_accesses expects column access to be wraped in parenthesis,
            # so we wrap everything in parenthesis when converting to string,
            # since we can't distinguish a column access from any other type of expression
            return pandas.io.formats.printing.pprint_thing("(" + self.name + ")")

    def visit_Attribute(self, node, **kwargs):
        """handles value.attr cases such as C.str.contains()
        functions are turned into NewFuncNode. Intermediate values like C.str
        are added to local scope as local variable to avoid evaluation.
        """
        attr = node.attr
        value = node.value
        sentinel = pandas.core.computation.ops.LOCAL_TAG

        if attr in ("str", "dt"):
            # check the case where df.column.str where column is not in df
            try:
                value_str = str(self.visit(value))
            except pandas.errors.UndefinedVariableError as e:
                col_name = e.args[0].split("'")[1]
                raise BodoError(
                    f"df.query(): column {col_name} is not found in dataframe columns {columns}"
                )
        else:
            value_str = str(self.visit(value))

        escape_key = (value_str, attr)

        # convert column names back the original string
        if escape_key in join_cleaned_cols:
            attr = join_cleaned_cols[escape_key]

        name = value_str + "." + attr
        if name.startswith(sentinel):
            name = name[len(sentinel) :]

        # make local variable in case of C.str
        if attr in ("str", "dt"):
            orig_col_name = columns[cleaned_columns.index(value_str)]
            used_cols[orig_col_name] = value_str
            self.env.scope[name] = 0
            return self.term_type(sentinel + name, self.env)

        # make function node
        new_funcs.append(name)
        return NewFuncNode(name)

    # make sure string literals are printed correctly in expression
    def __str__(self):
        if isinstance(self.value, list):
            return f"{self.value}"
        if isinstance(self.value, str):
            return f"'{self.value}'"
        return pandas.io.formats.printing.pprint_thing(self.name)

    # handle math calls
    def math__str__(self):
        """makes math calls compilable by adding "np." and Series functions"""
        # avoid change if it is a dummy attribute call
        if self.op in new_funcs:
            return pandas.io.formats.printing.pprint_thing(
                "{}({})".format(self.op, ",".join(map(str, self.operands)))
            )

        op = f"np.{self.op}"
        ind = f"bodo.hiframes.pd_index_ext.init_range_index(0, len({str(self.operands[0])}), 1, None)"
        return pandas.io.formats.printing.pprint_thing(
            "bodo.hiframes.pd_series_ext.init_series({}({}), {})".format(
                op,
                ",".join(
                    f"bodo.hiframes.pd_series_ext.get_series_data({str(a)})"
                    for a in self.operands
                ),
                ind,
            )
        )

    # replace 'in' operator with dummy function to convert to prange later
    def op__str__(self):
        parened = (
            f"({pandas.io.formats.printing.pprint_thing(opr)})" for opr in self.operands
        )
        if self.op == "in":
            return pandas.io.formats.printing.pprint_thing(
                "bodo.hiframes.pd_dataframe_ext.val_isin_dummy({})".format(
                    ", ".join(parened)
                )
            )
        if self.op == "not in":
            return pandas.io.formats.printing.pprint_thing(
                "bodo.hiframes.pd_dataframe_ext.val_notin_dummy({})".format(
                    ", ".join(parened)
                )
            )
        return pandas.io.formats.printing.pprint_thing(f" {self.op} ".join(parened))

    saved_rewrite_membership_op = (
        pandas.core.computation.expr.BaseExprVisitor._rewrite_membership_op  # type: ignore
    )
    saved_maybe_evaluate_binop = (
        pandas.core.computation.expr.BaseExprVisitor._maybe_evaluate_binop  # type: ignore
    )
    saved_visit_Attribute = pandas.core.computation.expr.BaseExprVisitor.visit_Attribute
    saved__maybe_downcast_constants = (
        pandas.core.computation.expr.BaseExprVisitor._maybe_downcast_constants  # type: ignore
    )
    saved__str__ = pandas.core.computation.ops.Term.__str__
    saved_math__str__ = pandas.core.computation.ops.MathCall.__str__
    saved_op__str__ = pandas.core.computation.ops.Op.__str__
    saved__disallow_scalar_only_bool_ops = (
        pandas.core.computation.ops.BinOp._disallow_scalar_only_bool_ops  # type: ignore
    )
    try:
        pandas.core.computation.expr.BaseExprVisitor._rewrite_membership_op = (  # type: ignore
            _rewrite_membership_op
        )
        pandas.core.computation.expr.BaseExprVisitor._maybe_evaluate_binop = (  # type: ignore
            _maybe_evaluate_binop
        )
        pandas.core.computation.expr.BaseExprVisitor.visit_Attribute = visit_Attribute
        # _maybe_downcast_constants accesses actual value which is not possible
        pandas.core.computation.expr.BaseExprVisitor._maybe_downcast_constants = (
            lambda self, left, right: (  # type: ignore
                left,
                right,
            )
        )
        pandas.core.computation.ops.Term.__str__ = __str__
        pandas.core.computation.ops.MathCall.__str__ = math__str__
        pandas.core.computation.ops.Op.__str__ = op__str__
        # _disallow_scalar_only_bool_ops accesses actual value which is not possible
        pandas.core.computation.ops.BinOp._disallow_scalar_only_bool_ops = (
            lambda self: None
        )  # type: ignore
        parsed_expr = pandas.core.computation.expr.Expr(expr, env=env)
        parsed_expr_str = str(parsed_expr)
    except pandas.errors.UndefinedVariableError as e:
        # catch undefined variable error

        if (
            not is_overload_none(index_name)
            and get_overload_const_str(index_name) == e.args[0].split("'")[1]
        ):
            # currently do not support named index appears in expr
            raise BodoError(
                "df.query(): Refering to named"
                f" index ('{get_overload_const_str(index_name)}') by name is not supported"
            )
        else:
            # throw other errors
            # this includes: columns does not exist in dataframe,
            #                undefined local variable using @
            raise BodoError(f"df.query(): undefined variable, {e}")
    finally:
        pandas.core.computation.expr.BaseExprVisitor._rewrite_membership_op = (  # type: ignore
            saved_rewrite_membership_op
        )
        pandas.core.computation.expr.BaseExprVisitor._maybe_evaluate_binop = (  # type: ignore
            saved_maybe_evaluate_binop
        )
        pandas.core.computation.expr.BaseExprVisitor.visit_Attribute = (
            saved_visit_Attribute
        )
        pandas.core.computation.expr.BaseExprVisitor._maybe_downcast_constants = (  # type: ignore
            saved__maybe_downcast_constants
        )
        pandas.core.computation.ops.Term.__str__ = saved__str__
        pandas.core.computation.ops.MathCall.__str__ = saved_math__str__
        pandas.core.computation.ops.Op.__str__ = saved_op__str__
        pandas.core.computation.ops.BinOp._disallow_scalar_only_bool_ops = (  # type: ignore
            saved__disallow_scalar_only_bool_ops
        )

    clean_name = pandas.core.computation.parsing.clean_column_name
    used_cols.update(
        {c: clean_name(c) for c in columns if clean_name(c) in parsed_expr.names}
    )
    return parsed_expr, parsed_expr_str, used_cols


class DataFrameTupleIterator(types.SimpleIteratorType):
    """
    Type class for itertuples of dataframes.
    """

    def __init__(self, col_names, arr_typs):
        self.array_types = arr_typs
        self.col_names = col_names
        name_args = [f"{col_names[i]}={arr_typs[i]}" for i in range(len(col_names))]
        name = "itertuples({})".format(",".join(name_args))
        py_ntup = namedtuple("Pandas", col_names)
        yield_type = types.NamedTuple([_get_series_dtype(a) for a in arr_typs], py_ntup)
        super().__init__(name, yield_type)

    @property
    def mangling_args(self):
        """
        Avoids long mangled function names in the generated LLVM, which slows down
        compilation time. See [BE-1726]
        https://github.com/numba/numba/blob/8e6fa5690fbe4138abf69263363be85987891e8b/numba/core/funcdesc.py#L67
        https://github.com/numba/numba/blob/8e6fa5690fbe4138abf69263363be85987891e8b/numba/core/itanium_mangler.py#L219
        """
        return self.__class__.__name__, (self._code,)


def _get_series_dtype(arr_typ):
    # values of datetimeindex are extracted as Timestamp
    if arr_typ == types.Array(types.NPDatetime("ns"), 1, "C"):
        return pd_timestamp_tz_naive_type
    return arr_typ.dtype


def get_itertuples():  # pragma: no cover
    pass


@infer_global(get_itertuples)
class TypeIterTuples(AbstractTemplate):
    def generic(self, args, kws):
        assert not kws
        assert len(args) % 2 == 0, "name and column pairs expected"
        col_names = [a.literal_value for a in args[: len(args) // 2]]
        arr_types = [if_series_to_array_type(a) for a in args[len(args) // 2 :]]
        # XXX index handling, assuming implicit index
        assert "Index" not in col_names[0]
        col_names = ["Index"] + col_names
        arr_types = [types.Array(types.int64, 1, "C")] + arr_types
        iter_typ = DataFrameTupleIterator(col_names, arr_types)
        return iter_typ(*args)


TypeIterTuples.prefer_literal = True


@register_model(DataFrameTupleIterator)
class DataFrameTupleIteratorModel(models.StructModel):
    def __init__(self, dmm, fe_type):
        # We use an unsigned index to avoid the cost of negative index tests.
        # XXX array_types[0] is implicit index
        members = [("index", types.EphemeralPointer(types.uintp))] + [
            (f"array{i}", arr) for i, arr in enumerate(fe_type.array_types[1:])
        ]
        super().__init__(dmm, fe_type, members)

    def from_return(self, builder, value):
        # dummy to avoid lowering error for itertuples_overload
        # TODO: remove when overload_method can avoid lowering or avoid cpython
        # wrapper
        return value


@lower_builtin(get_itertuples, types.VarArg(types.Any))
def get_itertuples_impl(context, builder, sig, args):
    arrays = args[len(args) // 2 :]
    array_types = sig.args[len(sig.args) // 2 :]

    iterobj = context.make_helper(builder, sig.return_type)

    zero = context.get_constant(types.intp, 0)
    indexptr = cgutils.alloca_once_value(builder, zero)

    iterobj.index = indexptr

    for i, arr in enumerate(arrays):
        setattr(iterobj, f"array{i}", arr)

    # Incref arrays
    for arr, arr_typ in zip(arrays, array_types):
        context.nrt.incref(builder, arr_typ, arr)

    res = iterobj._getvalue()

    # Note: a decref on the iterator will dereference all internal MemInfo*
    return impl_ret_new_ref(context, builder, sig.return_type, res)


@lower_builtin("getiter", DataFrameTupleIterator)
def getiter_itertuples(context, builder, sig, args):
    # simply return the iterator
    return impl_ret_borrowed(context, builder, sig.return_type, args[0])


# similar to iternext of ArrayIterator
@lower_builtin("iternext", DataFrameTupleIterator)
@iternext_impl(RefType.UNTRACKED)
def iternext_itertuples(context, builder, sig, args, result):
    # TODO: refcount issues?
    (iterty,) = sig.args
    (it,) = args

    # TODO: support string arrays
    iterobj = context.make_helper(builder, iterty, value=it)
    # first array type is implicit int index
    # use len() to support string arrays
    len_sig = signature(types.intp, iterty.array_types[1])
    nitems = context.compile_internal(
        builder, lambda a: len(a), len_sig, [iterobj.array0]
    )
    # ary = make_array(iterty.array_types[1])(context, builder, value=iterobj.array0)
    # nitems, = cgutils.unpack_tuple(builder, ary.shape, count=1)

    index = builder.load(iterobj.index)
    is_valid = builder.icmp_signed("<", index, nitems)
    result.set_valid(is_valid)

    with builder.if_then(is_valid):
        values = [index]  # XXX implicit int index
        for i, arr_typ in enumerate(iterty.array_types[1:]):
            arr_ptr = getattr(iterobj, f"array{i}")

            if arr_typ == types.Array(types.NPDatetime("ns"), 1, "C"):
                getitem_sig = signature(pd_timestamp_tz_naive_type, arr_typ, types.intp)
                val = context.compile_internal(
                    builder,
                    lambda a,
                    i: bodo.hiframes.pd_timestamp_ext.convert_datetime64_to_timestamp(
                        np.int64(a[i])
                    ),
                    getitem_sig,
                    [arr_ptr, index],
                )
            else:
                getitem_sig = signature(arr_typ.dtype, arr_typ, types.intp)
                val = context.compile_internal(
                    builder, lambda a, i: a[i], getitem_sig, [arr_ptr, index]
                )
            # arr = make_array(arr_typ)(context, builder, value=arr_ptr)
            # val = _getitem_array1d(context, builder, arr_typ, arr, index,
            #                      wraparound=False)
            values.append(val)

        value = context.make_tuple(builder, iterty.yield_type, values)
        result.yield_(value)
        nindex = cgutils.increment_index(builder, index)
        builder.store(nindex, iterobj.index)


# TODO: move this to array analysis
# the namedtuples created by get_itertuples-iternext-pair_first don't have
# shapes created in array analysis
# def _analyze_op_static_getitem(self, scope, equiv_set, expr):
#     var = expr.value
#     typ = self.typemap[var.name]
#     if not isinstance(typ, types.BaseTuple):
#         return self._index_to_shape(scope, equiv_set, expr.value, expr.index_var)
#     try:
#         shape = equiv_set._get_shape(var)
#         require(isinstance(expr.index, int) and expr.index < len(shape))
#         return shape[expr.index], []
#     except:
#         pass

#     return None

# numba.parfors.array_analysis.ArrayAnalysis._analyze_op_static_getitem = _analyze_op_static_getitem


# FIXME: fix array analysis for tuples in general
def _analyze_op_pair_first(self, scope, equiv_set, expr, lhs):
    # TODO(ehsan): Numba 0.53 adds lhs so this code should be refactored
    # make dummy lhs since we don't have access to lhs
    typ = self.typemap[expr.value.name].first_type
    if not isinstance(typ, types.NamedTuple):
        return None
    lhs = ir.Var(scope, mk_unique_var("tuple_var"), expr.loc)
    self.typemap[lhs.name] = typ
    rhs = ir.Expr.pair_first(expr.value, expr.loc)
    lhs_assign = ir.Assign(rhs, lhs, expr.loc)
    # (shape, post) = self._gen_shape_call(equiv_set, lhs, typ.count, )
    var = lhs
    out = []
    size_vars = []
    ndims = typ.count
    for i in range(ndims):
        # get size: Asize0 = A_sh_attr[0]
        size_var = ir.Var(var.scope, mk_unique_var(f"{var.name}_size{i}"), var.loc)
        getitem = ir.Expr.static_getitem(lhs, i, None, var.loc)
        self.calltypes[getitem] = None
        out.append(ir.Assign(getitem, size_var, var.loc))
        self._define(equiv_set, size_var, types.intp, getitem)
        size_vars.append(size_var)
    shape = tuple(size_vars)
    return numba.parfors.array_analysis.ArrayAnalysis.AnalyzeResult(
        shape=shape, pre=[lhs_assign] + out
    )


numba.parfors.array_analysis.ArrayAnalysis._analyze_op_pair_first = (
    _analyze_op_pair_first
)
