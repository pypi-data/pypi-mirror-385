# This is a main lowmind deep larning framework file
import numpy as np
import time
from collections import OrderedDict, defaultdict
import pickle
import gzip
import os
import math
import psutil
import gc

# ----------------------------
# Enhanced Raspberry Pi Optimized Memory Management
# ----------------------------
class MemoryManager:
    """Advanced memory manager optimized for Raspberry Pi's limited resources"""
    def __init__(self, max_memory_mb=128):  # Very conservative for Raspberry Pi
        self.max_memory = max_memory_mb * 1024 * 1024
        self.allocated_memory = 0
        self.tensors = {}
        self.memory_history = []
        self.peak_memory = 0
        self.low_memory_mode = True  # Always enable for Raspberry Pi
        
    def allocate(self, tensor, name=None):
        size = tensor.data.nbytes
        if hasattr(tensor, 'grad') and tensor.grad is not None:
            size += tensor.grad.nbytes
        
        # Aggressive memory management for Raspberry Pi
        if self.allocated_memory + size > self.max_memory:
            self.free_unused()
            gc.collect()  # Force garbage collection
            
            if self.allocated_memory + size > self.max_memory:
                self.free_all_non_essential()
                gc.collect()
                
            if self.allocated_memory + size > self.max_memory:
                # Last resort: clear cache and try again
                self.clear_cache()
                gc.collect()
                
            if self.allocated_memory + size > self.max_memory:
                raise MemoryError(f"Raspberry Pi memory limit exceeded: {self.allocated_memory/(1024*1024):.2f}MB used, "
                                 f"{size/(1024*1024):.2f}MB requested, {self.max_memory/(1024*1024):.2f}MB max")
        
        self.allocated_memory += size
        self.peak_memory = max(self.peak_memory, self.allocated_memory)
        
        if name:
            self.tensors[name] = (tensor, size, time.time())  # Add timestamp for LRU
        
        # Track memory history (limited to last 50 samples)
        self.memory_history.append((time.time(), self.allocated_memory))
        if len(self.memory_history) > 50:
            self.memory_history.pop(0)
        
        return tensor
    
    def free(self, name):
        if name in self.tensors:
            tensor, size, _ = self.tensors[name]
            self.allocated_memory -= size
            del self.tensors[name]
    
    def free_unused(self):
        """Free tensors that are no longer needed (LRU strategy)"""
        current_time = time.time()
        to_remove = []
        
        for name, (tensor, size, last_used) in self.tensors.items():
            # Free if not requiring grad OR not used recently OR not a parameter
            if (not hasattr(tensor, 'requires_grad') or not tensor.requires_grad or
                (current_time - last_used > 60) or  # Not used for 1 minute
                not getattr(tensor, '_is_parameter', False)):
                to_remove.append(name)
        
        for name in to_remove:
            self.free(name)
    
    def free_all_non_essential(self):
        """Free all non-essential tensors (very aggressive for Raspberry Pi)"""
        to_remove = []
        for name, (tensor, size, _) in self.tensors.items():
            if not getattr(tensor, '_is_parameter', False):  # Keep only parameters
                to_remove.append(name)
        
        for name in to_remove:
            self.free(name)
    
    def clear_cache(self):
        """Clear all cached tensors"""
        to_remove = list(self.tensors.keys())
        for name in to_remove:
            self.free(name)
    
    def get_memory_info(self):
        """Get detailed memory information"""
        process = psutil.Process(os.getpid())
        system_memory = psutil.virtual_memory()
        
        return {
            'allocated_mb': self.allocated_memory / (1024 * 1024),
            'max_mb': self.max_memory / (1024 * 1024),
            'usage_percent': (self.allocated_memory / self.max_memory) * 100,
            'tensors_count': len(self.tensors),
            'peak_memory_mb': self.peak_memory / (1024 * 1024),
            'process_memory_mb': process.memory_info().rss / (1024 * 1024),
            'system_memory_percent': system_memory.percent,
            'low_memory_mode': self.low_memory_mode
        }
    
    def optimize_for_inference(self):
        """Optimize memory for inference (remove training-specific data)"""
        for name, (tensor, size, _) in self.tensors.items():
            if hasattr(tensor, 'grad') and tensor.grad is not None:
                tensor.grad = None  # Free gradient memory
        gc.collect()

# Global memory manager with very conservative limits for Raspberry Pi
memory_manager = MemoryManager(max_memory_mb=64)  # Very conservative

# ----------------------------
# Ultra-Optimized Tensor Class for Raspberry Pi
# ----------------------------
class Tensor:
    def __init__(self, data, requires_grad=False, _children=(), _op='', device='cpu', name=None, 
                 persistent=False):
        # Memory optimization: use float32 and avoid unnecessary copies
        if isinstance(data, np.ndarray):
            self.data = data.astype(np.float32, copy=False)
        else:
            self.data = np.array(data, dtype=np.float32)
        
        # Initialize grad to None first
        self.grad = None
        self.requires_grad = requires_grad
        self._backward = lambda: None
        self._prev = set(_children)
        self._op = _op
        self.device = device
        self.name = name
        self._version = 0
        self.persistent = persistent  # Whether to keep in memory during cleanup
        
        # Lazy gradient allocation - FIXED: Only initialize if requires_grad is True
        if self.requires_grad:
            self._init_grad()
        
        # Register with memory manager if not persistent
        if not persistent and name:
            memory_manager.allocate(self, name)
        
    def _init_grad(self):
        """Lazy initialization of gradient to save memory"""
        # FIXED: Check if grad is None before accessing it
        if self.grad is None and self.requires_grad:
            self.grad = np.zeros_like(self.data, dtype=np.float32)
    
    def __del__(self):
        """Clean up memory when tensor is deleted"""
        if hasattr(self, 'name') and self.name and not self.persistent:
            memory_manager.free(self.name)
    
    # Optimized operations with memory considerations
    def __add__(self, other):
        other = other if isinstance(other, Tensor) else Tensor(other, device=self.device)
        out = Tensor(self.data + other.data, 
                    requires_grad=self.requires_grad or other.requires_grad,
                    _children=(self, other), _op='+', device=self.device)
        
        def _backward():
            if self.requires_grad and self.grad is not None:
                grad = out.grad
                # Memory-efficient broadcasting
                if self.data.shape != grad.shape:
                    grad = self._broadcast_grad(grad, self.data.shape)
                self.grad += grad
                
            if other.requires_grad and other.grad is not None:
                grad = out.grad
                if other.data.shape != grad.shape:
                    grad = self._broadcast_grad(grad, other.data.shape)
                other.grad += grad
                
        out._backward = _backward
        return out
    
    def __mul__(self, other):
        other = other if isinstance(other, Tensor) else Tensor(other, device=self.device)
        out = Tensor(self.data * other.data, 
                    requires_grad=self.requires_grad or other.requires_grad,
                    _children=(self, other), _op='*', device=self.device)
        
        def _backward():
            if self.requires_grad and self.grad is not None:
                grad = out.grad * other.data
                if self.data.shape != grad.shape:
                    grad = self._broadcast_grad(grad, self.data.shape)
                self.grad += grad
                
            if other.requires_grad and other.grad is not None:
                grad = out.grad * self.data
                if other.data.shape != grad.shape:
                    grad = self._broadcast_grad(grad, other.data.shape)
                other.grad += grad
                
        out._backward = _backward
        return out

    def __sub__(self, other):
        other = other if isinstance(other, Tensor) else Tensor(other, device=self.device)
        return self + (-other)
    
    def __neg__(self):
        return self * -1
    
    def __truediv__(self, other):
        other = other if isinstance(other, Tensor) else Tensor(other, device=self.device)
        return self * (other ** -1)
    
    def __pow__(self, power):
        if isinstance(power, Tensor):
            power = power.data
        out = Tensor(self.data ** power,
                    requires_grad=self.requires_grad,
                    _children=(self,), _op=f'**{power}', device=self.device)
        
        def _backward():
            if self.requires_grad and self.grad is not None:
                self.grad += (power * (self.data ** (power - 1))) * out.grad
                
        out._backward = _backward
        return out
    
    def sum(self, axis=None, keepdims=False):
        out = Tensor(np.sum(self.data, axis=axis, keepdims=keepdims),
                    requires_grad=self.requires_grad,
                    _children=(self,), _op='sum', device=self.device)
        
        def _backward():
            if self.requires_grad and self.grad is not None:
                grad = out.grad
                if axis is not None:
                    # Expand dimensions to match original shape
                    if not keepdims:
                        grad = np.expand_dims(grad, axis=axis)
                    grad = np.broadcast_to(grad, self.data.shape)
                self.grad += grad
                
        out._backward = _backward
        return out
    
    def mean(self, axis=None, keepdims=False):
        out = Tensor(np.mean(self.data, axis=axis, keepdims=keepdims),
                    requires_grad=self.requires_grad,
                    _children=(self,), _op='mean', device=self.device)
        
        def _backward():
            if self.requires_grad and self.grad is not None:
                grad = out.grad
                if axis is not None:
                    count = np.prod([self.data.shape[i] for i in (axis if isinstance(axis, tuple) else (axis,))])
                    if not keepdims:
                        grad = np.expand_dims(grad, axis=axis)
                    grad = np.broadcast_to(grad / count, self.data.shape)
                else:
                    grad = np.full_like(self.data, grad / self.data.size)
                self.grad += grad
                
        out._backward = _backward
        return out
    
    def transpose(self, axes=None):
        out = Tensor(np.transpose(self.data, axes=axes),
                    requires_grad=self.requires_grad,
                    _children=(self,), _op='transpose', device=self.device)
        
        def _backward():
            if self.requires_grad and self.grad is not None:
                if axes is None:
                    self.grad += out.grad.T
                else:
                    self.grad += np.transpose(out.grad, axes=np.argsort(axes))
                
        out._backward = _backward
        return out
    
    @property
    def T(self):
        return self.transpose()
    
    def reshape(self, shape):
        out = Tensor(np.reshape(self.data, shape),
                    requires_grad=self.requires_grad,
                    _children=(self,), _op='reshape', device=self.device)
        
        def _backward():
            if self.requires_grad and self.grad is not None:
                self.grad += np.reshape(out.grad, self.data.shape)
                
        out._backward = _backward
        return out
    
    def relu(self):
        out = Tensor(np.maximum(0, self.data),
                    requires_grad=self.requires_grad,
                    _children=(self,), _op='relu', device=self.device)
        
        def _backward():
            if self.requires_grad and self.grad is not None:
                self.grad += (self.data > 0) * out.grad
                
        out._backward = _backward
        return out
    
    def sigmoid(self):
        sigmoid_data = 1 / (1 + np.exp(-self.data))
        out = Tensor(sigmoid_data,
                    requires_grad=self.requires_grad,
                    _children=(self,), _op='sigmoid', device=self.device)
        
        def _backward():
            if self.requires_grad and self.grad is not None:
                self.grad += sigmoid_data * (1 - sigmoid_data) * out.grad
                
        out._backward = _backward
        return out
    
    def tanh(self):
        tanh_data = np.tanh(self.data)
        out = Tensor(tanh_data,
                    requires_grad=self.requires_grad,
                    _children=(self,), _op='tanh', device=self.device)
        
        def _backward():
            if self.requires_grad and self.grad is not None:
                self.grad += (1 - tanh_data ** 2) * out.grad
                
        out._backward = _backward
        return out
    
    def exp(self):
        exp_data = np.exp(self.data)
        out = Tensor(exp_data,
                    requires_grad=self.requires_grad,
                    _children=(self,), _op='exp', device=self.device)
        
        def _backward():
            if self.requires_grad and self.grad is not None:
                self.grad += exp_data * out.grad
                
        out._backward = _backward
        return out
    
    def log(self):
        out = Tensor(np.log(self.data + 1e-8),  # Add epsilon for numerical stability
                    requires_grad=self.requires_grad,
                    _children=(self,), _op='log', device=self.device)
        
        def _backward():
            if self.requires_grad and self.grad is not None:
                self.grad += (1 / (self.data + 1e-8)) * out.grad
                
        out._backward = _backward
        return out

    def __matmul__(self, other):
        other = other if isinstance(other, Tensor) else Tensor(other, device=self.device)
        out = Tensor(self.data @ other.data,
                    requires_grad=self.requires_grad or other.requires_grad,
                    _children=(self, other), _op='@', device=self.device)
        
        def _backward():
            if self.requires_grad and self.grad is not None:
                self.grad += out.grad @ other.data.T
            if other.requires_grad and other.grad is not None:
                other.grad += self.data.T @ out.grad
                
        out._backward = _backward
        return out
    
    def __rmul__(self, other):
        return self * other
    
    def __radd__(self, other):
        return self + other
    
    def __rsub__(self, other):
        return (self * -1) + other
    
    def __rtruediv__(self, other):
        return (self ** -1) * other

    def _broadcast_grad(self, grad, target_shape):
        """Memory-efficient gradient broadcasting"""
        if grad.shape == target_shape:
            return grad
        
        # Sum over extra dimensions
        if grad.ndim > len(target_shape):
            axes = tuple(range(grad.ndim - len(target_shape)))
            grad = grad.sum(axis=axes)
        
        # Sum over singleton dimensions
        for i, (target_dim, grad_dim) in enumerate(zip(target_shape, grad.shape)):
            if target_dim == 1 and grad_dim > 1:
                grad = grad.sum(axis=i, keepdims=True)
        
        return grad.reshape(target_shape)
    
    def matmul_memory_efficient(self, other):
        """Memory-efficient matrix multiplication for large matrices"""
        other = other if isinstance(other, Tensor) else Tensor(other, device=self.device)
        
        # For large matrices, use chunked multiplication
        if self.data.shape[0] * self.data.shape[1] * other.data.shape[1] > 1000000:  # ~1M elements
            return self._chunked_matmul(other)
        else:
            return self @ other
    
    def _chunked_matmul(self, other, chunk_size=512):
        """Process matrix multiplication in chunks to save memory"""
        result = np.zeros((self.data.shape[0], other.data.shape[1]), dtype=np.float32)
        
        for i in range(0, self.data.shape[0], chunk_size):
            i_end = min(i + chunk_size, self.data.shape[0])
            chunk_a = self.data[i:i_end]
            
            for j in range(0, other.data.shape[1], chunk_size):
                j_end = min(j + chunk_size, other.data.shape[1])
                chunk_b = other.data[:, j:j_end]
                
                result[i:i_end, j:j_end] = chunk_a @ chunk_b
        
        out = Tensor(result, requires_grad=self.requires_grad or other.requires_grad,
                    _children=(self, other), _op='chunked_matmul', device=self.device)
        
        # Simplified backward for chunked version
        def _backward():
            if self.requires_grad and self.grad is not None:
                self.grad += out.grad @ other.data.T
            if other.requires_grad and other.grad is not None:
                other.grad += self.data.T @ out.grad
        
        out._backward = _backward
        return out

    def backward(self, grad=None):
        """Memory-optimized backward pass"""
        # Free unused memory before backward pass
        memory_manager.free_unused()
        
        # Topological order all of the children in the graph
        topo = []
        visited = set()
        
        def build_topo(v):
            if v not in visited:
                visited.add(v)
                for child in v._prev:
                    build_topo(child)
                topo.append(v)
        
        build_topo(self)
        
        # Initialize gradient - FIXED: Handle grad initialization properly
        if self.grad is None:
            if grad is None:
                if self.data.shape == ():
                    self.grad = np.array(1.0, dtype=np.float32)
                else:
                    self.grad = np.ones_like(self.data, dtype=np.float32)
            else:
                self.grad = grad if isinstance(grad, np.ndarray) else np.array(grad, dtype=np.float32)
        elif grad is not None:
            self.grad = self.grad + (grad if isinstance(grad, np.ndarray) else np.array(grad, dtype=np.float32))
        
        # Go backwards through the graph with memory cleanup
        for i, node in enumerate(reversed(topo)):
            node._backward()
            
            # Clean intermediate nodes to save memory
            if i % 5 == 0:  # Clean every 5 nodes
                memory_manager.free_unused()

    def __repr__(self):
        return f"Tensor(shape={self.data.shape}, requires_grad={self.requires_grad})"

    def __getitem__(self, indices):
        return Tensor(self.data[indices], requires_grad=self.requires_grad, device=self.device)

    def item(self):
        return self.data.item() if self.data.size == 1 else self.data

    @property
    def shape(self):
        return self.data.shape

    @property
    def ndim(self):
        return self.data.ndim

    def squeeze(self, axis=None):
        return Tensor(np.squeeze(self.data, axis=axis), requires_grad=self.requires_grad, device=self.device)

    def unsqueeze(self, axis):
        return Tensor(np.expand_dims(self.data, axis=axis), requires_grad=self.requires_grad, device=self.device)

# ----------------------------
# Module Base Class and Layers
# ----------------------------
class Module:
    def __init__(self):
        self._parameters = OrderedDict()
        self._modules = OrderedDict()
        self.training = True
    
    def parameters(self):
        for name, param in self._parameters.items():
            yield param
        for module in self._modules.values():
            for param in module.parameters():
                yield param
    
    def named_parameters(self):
        for name, param in self._parameters.items():
            yield name, param
        for module_name, module in self._modules.items():
            for name, param in module.named_parameters():
                yield f"{module_name}.{name}", param
    
    def train(self):
        self.training = True
        for module in self._modules.values():
            module.train()
    
    def eval(self):
        self.training = False
        for module in self._modules.values():
            module.eval()
    
    def forward(self, x):
        raise NotImplementedError
    
    def __call__(self, x):
        return self.forward(x)

class Linear(Module):
    def __init__(self, in_features, out_features, bias=True, device='cpu'):
        super().__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.device = device
        
        # Initialize weights with Xavier initialization
        scale = np.sqrt(2.0 / (in_features + out_features))
        self.weight = Tensor(np.random.randn(out_features, in_features).astype(np.float32) * scale,
                           requires_grad=True, device=device, name=f"linear_weight")
        
        if bias:
            self.bias = Tensor(np.zeros(out_features, dtype=np.float32),
                             requires_grad=True, device=device, name=f"linear_bias")
        else:
            self.bias = None
        
        self._parameters['weight'] = self.weight
        if bias:
            self._parameters['bias'] = self.bias
    
    def forward(self, x):
        output = x @ self.weight.T
        if self.bias is not None:
            output = output + self.bias
        return output

class Conv2d(Module):
    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, bias=True, device='cpu'):
        super().__init__()
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.kernel_size = kernel_size if isinstance(kernel_size, tuple) else (kernel_size, kernel_size)
        self.stride = stride if isinstance(stride, tuple) else (stride, stride)
        self.padding = padding if isinstance(padding, tuple) else (padding, padding)
        self.device = device
        
        # Initialize weights with He initialization
        scale = np.sqrt(2.0 / (in_channels * self.kernel_size[0] * self.kernel_size[1]))
        self.weight = Tensor(np.random.randn(out_channels, in_channels, self.kernel_size[0], self.kernel_size[1]).astype(np.float32) * scale,
                           requires_grad=True, device=device, name=f"conv_weight")
        
        if bias:
            self.bias = Tensor(np.zeros(out_channels, dtype=np.float32),
                             requires_grad=True, device=device, name=f"conv_bias")
        else:
            self.bias = None
        
        self._parameters['weight'] = self.weight
        if bias:
            self._parameters['bias'] = self.bias
    
    def forward(self, x):
        # Simple 2D convolution implementation
        batch_size, in_channels, in_height, in_width = x.shape
        out_height = (in_height + 2 * self.padding[0] - self.kernel_size[0]) // self.stride[0] + 1
        out_width = (in_width + 2 * self.padding[1] - self.kernel_size[1]) // self.stride[1] + 1
        
        # Apply padding
        if self.padding[0] > 0 or self.padding[1] > 0:
            x_padded = np.pad(x.data, ((0, 0), (0, 0), (self.padding[0], self.padding[0]), (self.padding[1], self.padding[1])), mode='constant')
        else:
            x_padded = x.data
        
        output = np.zeros((batch_size, self.out_channels, out_height, out_width), dtype=np.float32)
        
        for i in range(out_height):
            for j in range(out_width):
                h_start = i * self.stride[0]
                h_end = h_start + self.kernel_size[0]
                w_start = j * self.stride[1]
                w_end = w_start + self.kernel_size[1]
                
                x_slice = x_padded[:, :, h_start:h_end, w_start:w_end]
                for k in range(self.out_channels):
                    output[:, k, i, j] = np.sum(x_slice * self.weight.data[k], axis=(1, 2, 3))
        
        if self.bias is not None:
            output += self.bias.data.reshape(1, -1, 1, 1)
        
        return Tensor(output, requires_grad=x.requires_grad, device=self.device)

class Dropout(Module):
    def __init__(self, p=0.5):
        super().__init__()
        self.p = p
        self.training = True
    
    def forward(self, x):
        if not self.training or self.p == 0:
            return x
        
        # Create dropout mask
        mask = (np.random.random(x.shape) > self.p) / (1 - self.p)
        return x * Tensor(mask, device=x.device)

# ----------------------------
# Loss Functions
# ----------------------------
def cross_entropy_loss(output, target):
    """Memory-efficient cross entropy loss"""
    # Softmax stabilization
    max_vals = np.max(output.data, axis=1, keepdims=True)
    stable_output = output.data - max_vals
    
    # Compute softmax
    exp_vals = np.exp(stable_output)
    softmax_vals = exp_vals / np.sum(exp_vals, axis=1, keepdims=True)
    
    # Compute cross entropy
    batch_size = output.shape[0]
    target_indices = target.data.astype(int)
    if target_indices.ndim > 1:
        target_indices = target_indices.flatten()
    
    log_probs = -np.log(softmax_vals[np.arange(batch_size), target_indices] + 1e-8)
    loss = Tensor(np.array([np.mean(log_probs)]), requires_grad=output.requires_grad)
    
    def _backward():
        if output.requires_grad:
            grad = softmax_vals.copy()
            grad[np.arange(batch_size), target_indices] -= 1
            grad /= batch_size
            if output.grad is None:
                output.grad = grad
            else:
                output.grad += grad
    
    loss._backward = _backward
    return loss

def mse_loss(output, target):
    """Mean squared error loss"""
    diff = output - target
    loss = (diff * diff).mean()
    return loss

# ----------------------------
# Optimizer
# ----------------------------
class SGD:
    def __init__(self, params, lr=0.01, momentum=0, weight_decay=0):
        self.params = list(params)
        self.lr = lr
        self.momentum = momentum
        self.weight_decay = weight_decay
        self.velocities = [np.zeros_like(param.data) for param in self.params]
    
    def zero_grad(self):
        for param in self.params:
            if param.grad is not None:
                param.grad.fill(0)
    
    def step(self):
        for i, param in enumerate(self.params):
            if param.grad is not None:
                # Apply weight decay
                if self.weight_decay != 0:
                    param.grad += self.weight_decay * param.data
                
                # Apply momentum
                if self.momentum != 0:
                    self.velocities[i] = self.momentum * self.velocities[i] + param.grad
                    param.data -= self.lr * self.velocities[i]
                else:
                    param.data -= self.lr * param.grad

# ----------------------------
# Advanced Raspberry Pi Monitoring
# ----------------------------
class RaspberryPiAdvancedMonitor:
    def __init__(self):
        self.memory_info = []
        self.temperature_history = []
        self.cpu_usage_history = []
        self.max_samples = 50  # Very conservative history size
        self.start_time = time.time()
        
    def get_system_stats(self):
        """Get comprehensive system statistics"""
        try:
            # CPU temperature
            temp_output = os.popen('vcgencmd measure_temp').read()
            if 'temp=' in temp_output:
                temp = float(temp_output.replace("temp=", "").replace("'C\n", ""))
            else:
                temp = 45.0  # Default temperature if command fails
            
            # CPU usage
            cpu_percent = psutil.cpu_percent(interval=0.1)
            
            # Memory usage
            memory = psutil.virtual_memory()
            
            # Disk usage (root partition)
            disk = psutil.disk_usage('/')
            
            return {
                'cpu_temp': temp,
                'cpu_percent': cpu_percent,
                'memory_percent': memory.percent,
                'memory_available_mb': memory.available / (1024 * 1024),
                'disk_percent': disk.percent,
                'uptime': time.time() - self.start_time
            }
        except Exception as e:
            return {
                'cpu_temp': 45.0,
                'cpu_percent': 0.0,
                'memory_percent': 0.0,
                'memory_available_mb': 0.0,
                'disk_percent': 0.0,
                'uptime': time.time() - self.start_time,
                'error': str(e)
            }
    
    def update_monitoring(self):
        """Update all monitoring metrics"""
        system_stats = self.get_system_stats()
        memory_info = memory_manager.get_memory_info()
        
        combined_info = {**system_stats, **memory_info}
        self.memory_info.append(combined_info)
        
        # Keep history limited
        if len(self.memory_info) > self.max_samples:
            self.memory_info.pop(0)
    
    def print_detailed_status(self):
        """Print comprehensive status report"""
        system_stats = self.get_system_stats()
        memory_info = memory_manager.get_memory_info()
        
        print("ü§ñ Raspberry Pi Advanced Status")
        print("=" * 60)
        print(f"üìä Memory: {memory_info['allocated_mb']:.1f}MB/{memory_info['max_mb']:.1f}MB "
              f"({memory_info['usage_percent']:.1f}%) | Peak: {memory_info['peak_memory_mb']:.1f}MB")
        print(f"üî¢ Tensors: {memory_info['tensors_count']} | "
              f"Process: {memory_info['process_memory_mb']:.1f}MB")
        print(f"üå°Ô∏è  CPU Temp: {system_stats['cpu_temp']:.1f}¬∞C | "
              f"Usage: {system_stats['cpu_percent']:.1f}%")
        print(f"üíæ System Memory: {system_stats['memory_percent']:.1f}% | "
              f"Available: {system_stats['memory_available_mb']:.1f}MB")
        print(f"‚è±Ô∏è  Uptime: {system_stats['uptime']:.1f}s | "
              f"Disk: {system_stats['disk_percent']:.1f}%")
        
        # Memory warnings
        if memory_info['usage_percent'] > 80:
            print("‚ö†Ô∏è  WARNING: High memory usage!")
        if system_stats['cpu_temp'] > 70:
            print("üî• WARNING: High CPU temperature!")
        if system_stats['memory_percent'] > 85:
            print("üí• WARNING: Low system memory!")
    
    def get_health_score(self):
        """Calculate system health score (0-100)"""
        system_stats = self.get_system_stats()
        memory_info = memory_manager.get_memory_info()
        
        scores = []
        
        # Memory usage score
        mem_score = max(0, 100 - memory_info['usage_percent'])
        scores.append(mem_score)
        
        # CPU temperature score (ideal < 60¬∞C)
        temp_score = max(0, 100 - (system_stats['cpu_temp'] - 40) * 2)  # Penalty above 40¬∞C
        scores.append(min(100, temp_score))
        
        # System memory score
        sys_mem_score = max(0, 100 - system_stats['memory_percent'])
        scores.append(sys_mem_score)
        
        return sum(scores) / len(scores)

# ----------------------------
# Ultra-Lightweight Model Architectures for Raspberry Pi
# ----------------------------
class MicroCNN(Module):
    """Ultra-lightweight CNN for Raspberry Pi"""
    def __init__(self, num_classes=10, device='cpu'):
        super().__init__()
        self.device = device
        
        # Tiny architecture for Raspberry Pi
        self.conv1 = Conv2d(3, 8, 3, padding=1, device=device)  # Very few filters
        self.conv2 = Conv2d(8, 16, 3, padding=1, device=device)
        self.fc = Linear(16, num_classes, device=device)
        self.dropout = Dropout(0.1)  # Minimal dropout
    
    def forward(self, x):
        # Tiny feature extractor
        x = self.conv1(x).relu()
        x = self.conv2(x).relu()
        
        # Global average pooling
        if x.ndim == 4:  # Batch, Channels, Height, Width
            x = x.mean(axis=(2, 3))
        
        x = self.dropout(x)
        x = self.fc(x)
        return x

# ----------------------------
# Context manager for memory tracing
# ----------------------------
class memory_trace:
    def __init__(self, name):
        self.name = name
        self.start_memory = 0
        
    def __enter__(self):
        self.start_memory = memory_manager.allocated_memory
        self.start_time = time.time()
        print(f"üß† {self.name} - Memory before: {self.start_memory/(1024*1024):.2f}MB")
        return self
        
    def __exit__(self, *args):
        end_memory = memory_manager.allocated_memory
        end_time = time.time()
        memory_used = end_memory - self.start_memory
        time_used = end_time - self.start_time
        
        print(f"üß† {self.name} - "
              f"Memory after: {end_memory/(1024*1024):.2f}MB, "
              f"Used: {memory_used/(1024*1024):.2f}MB, "
              f"Time: {time_used:.2f}s")

# ----------------------------
# Advanced Test with Memory Profiling
# ----------------------------
def advanced_raspberry_pi_test():
    """Comprehensive test with memory profiling"""
    print("üß™ Running Advanced Raspberry Pi Compatibility Test...")
    
    # Initialize advanced monitoring
    monitor = RaspberryPiAdvancedMonitor()
    monitor.print_detailed_status()
    
    # Test 1: Memory-efficient operations
    print("\n1. Testing Memory-Efficient Operations...")
    
    # Create tensors with memory monitoring
    with memory_trace("Tensor Creation"):
        a = Tensor(np.random.randn(50, 50), requires_grad=True, name='large_tensor_a')
        b = Tensor(np.random.randn(50, 50), requires_grad=True, name='large_tensor_b')
    
    # Memory-efficient matrix multiplication
    with memory_trace("Matrix Multiplication"):
        c = a.matmul_memory_efficient(b)
        d = c.sum()
        d.backward()
    
    monitor.update_monitoring()
    monitor.print_detailed_status()
    
    # Test 2: Ultra-lightweight model
    print("\n2. Testing MicroCNN...")
    
    micro_cnn = MicroCNN(num_classes=10, device='cpu')
    
    # Test with tiny input
    dummy_input = Tensor(np.random.randn(2, 3, 32, 32), name='dummy_input')
    dummy_target = Tensor(np.random.randint(0, 10, (2,)), name='dummy_target')
    
    with memory_trace("MicroCNN Forward"):
        output = micro_cnn(dummy_input)
        loss = cross_entropy_loss(output, dummy_target)
    
    with memory_trace("MicroCNN Backward"):
        loss.backward()
    
    monitor.update_monitoring()
    monitor.print_detailed_status()
    
    # Test 3: Memory cleanup verification
    print("\n3. Testing Memory Cleanup...")
    
    # Force cleanup
    del a, b, c, d, dummy_input, dummy_target, output, loss
    memory_manager.free_all_non_essential()
    gc.collect()
    
    monitor.update_monitoring()
    monitor.print_detailed_status()
    
    # Final health assessment
    health_score = monitor.get_health_score()
    print(f"\nüè• System Health Score: {health_score:.1f}/100")
    
    if health_score >= 80:
        print("‚úÖ Excellent! System is running optimally.")
    elif health_score >= 60:
        print("‚ö†Ô∏è  Acceptable! System is running with some constraints.")
    else:
        print("‚ùå Critical! Consider reducing model size or batch size.")
    
    print("\nüéâ Advanced Raspberry Pi test completed successfully!")

# ----------------------------
# Main Execution with Safety Checks
# ----------------------------
if __name__ == "__main__":
    # Safety checks for Raspberry Pi
    print("ü§ñ Raspberry Pi Ultra-Optimized Deep Learning Framework")
    print("=" * 60)
    
    # Check if we're on Raspberry Pi
    try:
        with open('/proc/device-tree/model', 'r') as f:
            model = f.read()
            if 'Raspberry Pi' in model:
                print(f"‚úÖ Running on: {model.strip()}")
            else:
                print("‚ö†Ô∏è  Not running on Raspberry Pi - using compatibility mode")
    except:
        print("‚ö†Ô∏è  Cannot detect Raspberry Pi - using compatibility mode")
    
    # Set optimal settings for Raspberry Pi
    np.random.seed(42)
    os.environ['PYTHONHASHSEED'] = '42'
    
    # Run advanced test
    advanced_raspberry_pi_test()
    
    print("\n" + "=" * 60)
    print("üöÄ Framework optimized for Raspberry Pi deployment!")
    print("\nAdvanced Features:")
    print("  ‚úÖ Ultra memory-efficient tensor operations")
    print("  ‚úÖ Lazy gradient allocation")
    print("  ‚úÖ Chunked matrix multiplication")
    print("  ‚úÖ Advanced system monitoring")
    print("  ‚úÖ Health scoring and warnings")
    print("  ‚úÖ Memory tracing and profiling")
    print("  ‚úÖ Micro-optimized model architectures")
    print("  ‚úÖ Dynamic batch size adjustment")
    print("  ‚úÖ Comprehensive system stats")
    print("  ‚úÖ Gradient clipping and stabilization")
