<div align="center">

# Coocan

**ğŸš€ è½»é‡çº§å¼‚æ­¥çˆ¬è™«æ¡†æ¶**

[![PyPI version](https://badge.fury.io/py/coocan.svg)](https://badge.fury.io/py/coocan)
[![Python Version](https://img.shields.io/pypi/pyversions/coocan.svg)](https://pypi.org/project/coocan/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

[å®‰è£…](#å®‰è£…) â€¢
[å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹) â€¢
[åŠŸèƒ½ç‰¹æ€§](#åŠŸèƒ½ç‰¹æ€§) â€¢
[ç¤ºä¾‹](#ç¤ºä¾‹) â€¢
[æ–‡æ¡£](#æ–‡æ¡£)

![Demo](demo.gif)

</div>

---

## ğŸ“– ç®€ä»‹

Coocan æ˜¯ä¸€ä¸ªç®€æ´ã€é«˜æ•ˆçš„ Python å¼‚æ­¥çˆ¬è™«æ¡†æ¶ï¼Œä¸“ä¸ºå¿«é€Ÿå¼€å‘è€Œè®¾è®¡ã€‚å®ƒåŸºäº `httpx` å’Œ `asyncio`ï¼Œæä¾›äº†ç®€å•æ˜“ç”¨çš„ APIï¼Œè®©ä½ èƒ½å¤Ÿå¿«é€Ÿæ„å»ºé«˜æ€§èƒ½çš„ç½‘ç»œçˆ¬è™«ã€‚

### âœ¨ ä¸ºä»€ä¹ˆé€‰æ‹© Coocanï¼Ÿ

- **ğŸª¶ è½»é‡çº§** - æ ¸å¿ƒä»£ç ç®€æ´ï¼Œä¾èµ–å°‘ï¼Œæ˜“äºç†è§£å’Œæ‰©å±•
- **âš¡ å¼‚æ­¥é«˜æ•ˆ** - åŸºäº asyncioï¼Œå……åˆ†åˆ©ç”¨å¼‚æ­¥ I/O æå‡çˆ¬å–æ•ˆç‡
- **ğŸ¯ ç®€å•æ˜“ç”¨** - ç±» Scrapy çš„ API è®¾è®¡ï¼Œä¸Šæ‰‹å³ç”¨
- **ğŸ”§ åŠŸèƒ½å®Œå–„** - å†…ç½®è¯·æ±‚é‡è¯•ã€ä¼˜å…ˆçº§é˜Ÿåˆ—ã€ä»£ç†æ”¯æŒã€æ•°æ®å¤„ç†ç­‰åŠŸèƒ½
- **ğŸ¨ å¼€ç®±å³ç”¨** - è‡ªå¸¦ XPath/CSS é€‰æ‹©å™¨ï¼Œéšæœº User-Agentï¼Œå‘½ä»¤è¡Œå·¥å…·ç­‰

---

## ğŸ“¦ å®‰è£…

### ä½¿ç”¨ pip å®‰è£…

```bash
pip install -U coocan
```

### è¦æ±‚

- Python >= 3.10

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. åˆ›å»ºçˆ¬è™«

ä½¿ç”¨å‘½ä»¤è¡Œå·¥å…·å¿«é€Ÿç”Ÿæˆçˆ¬è™«æ¨¡æ¿ï¼š

```bash
coocan new -s my_spider
```

![å‘½ä»¤è¡Œå·¥å…·](cmd.png)

### 2. ç¼–å†™çˆ¬è™«ä»£ç 

```python
from coocan import MiniSpider, Request
from loguru import logger


class MySpider(MiniSpider):
    # èµ·å§‹ URL åˆ—è¡¨
    start_urls = ["https://example.com"]

    # æœ€å¤§å¹¶å‘è¯·æ±‚æ•°
    max_requests = 10

    def parse(self, response):
        """è§£æå“åº”"""
        # ä½¿ç”¨ CSS é€‰æ‹©å™¨æå–æ•°æ®
        titles = response.css('h1::text').getall()

        # ä½¿ç”¨ XPath æå–æ•°æ®
        links = response.xpath('//a/@href').getall()

        for title, link in zip(titles, links):
            logger.info(f"Title: {title}, Link: {link}")

            # å‘èµ·æ–°è¯·æ±‚
            yield Request(link, callback=self.parse_detail)

    def parse_detail(self, response):
        """è§£æè¯¦æƒ…é¡µ"""
        content = response.css('.content::text').get()
        logger.success(f"Content: {content}")


if __name__ == '__main__':
    spider = MySpider()
    spider.go()
```

### 3. è¿è¡Œçˆ¬è™«

```bash
python my_spider.py
```

---

## ğŸ¯ åŠŸèƒ½ç‰¹æ€§

### æ ¸å¿ƒåŠŸèƒ½

| åŠŸèƒ½           | è¯´æ˜                            |
| -------------- | ------------------------------- |
| **å¼‚æ­¥è¯·æ±‚**   | åŸºäº httpx çš„å¼‚æ­¥ HTTP å®¢æˆ·ç«¯   |
| **æ™ºèƒ½é‡è¯•**   | è‡ªåŠ¨é‡è¯•å¤±è´¥çš„è¯·æ±‚              |
| **ä¼˜å…ˆçº§é˜Ÿåˆ—** | æ”¯æŒè¯·æ±‚ä¼˜å…ˆçº§æ§åˆ¶              |
| **ä»£ç†æ”¯æŒ**   | è½»æ¾é…ç½® HTTP/HTTPS ä»£ç†        |
| **è¯·æ±‚å»¶è¿Ÿ**   | å¯é…ç½®è¯·æ±‚é—´éš”ï¼Œé¿å…è¢«å°        |
| **éšæœº UA**    | è‡ªåŠ¨éšæœº User-Agent             |
| **ä¸­é—´ä»¶**     | æ”¯æŒè¯·æ±‚é¢„å¤„ç†                  |
| **æ•°æ®ç®¡é“**   | `process_item` æ–¹æ³•å¤„ç†çˆ¬å–æ•°æ® |
| **å¼‚å¸¸å¤„ç†**   | å®Œå–„çš„å¼‚å¸¸å¤„ç†æœºåˆ¶              |
| **é€‰æ‹©å™¨**     | å†…ç½® XPath å’Œ CSS é€‰æ‹©å™¨        |

### ç±»å±æ€§é…ç½®

```python
class MySpider(MiniSpider):
    start_urls = ["https://example.com"]  # èµ·å§‹ URL
    max_requests = 20                      # æœ€å¤§å¹¶å‘æ•°
    max_retry_times = 3                    # æœ€å¤§é‡è¯•æ¬¡æ•°
    delay = 0                              # è¯·æ±‚å»¶è¿Ÿï¼ˆç§’ï¼‰
    enable_random_ua = True                # å¯ç”¨éšæœº User-Agent
    timeout = 6                            # è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
```

---

## ğŸ“š ç¤ºä¾‹

### ç¤ºä¾‹ 1ï¼šåŸºç¡€çˆ¬è™«

```python
from coocan import MiniSpider
from loguru import logger


class BasicSpider(MiniSpider):
    start_urls = ["https://httpbin.org/get"]

    def parse(self, response):
        data = response.json()
        logger.info(f"Your IP: {data.get('origin')}")


if __name__ == '__main__':
    BasicSpider().go()
```

### ç¤ºä¾‹ 2ï¼šä½¿ç”¨ä»£ç†

```python
from coocan import MiniSpider, Request


class ProxySpider(MiniSpider):
    def start_requests(self):
        yield Request(
            url="https://httpbin.org/ip",
            callback=self.parse,
            proxy="http://proxy.example.com:8080"
        )

    def parse(self, response):
        print(response.text)
```

æ›´å¤šç¤ºä¾‹è¯·æŸ¥çœ‹ [`coocan/_examples/`](coocan/_examples/) ç›®å½•ï¼š

- `crawl_csdn_list.py` - çˆ¬å– CSDN æ–‡ç« åˆ—è¡¨
- `crawl_csdn_detail.py` - çˆ¬å– CSDN æ–‡ç« è¯¦æƒ…
- `recv_item.py` - æ•°æ®å¤„ç†ç¤ºä¾‹
- `use_proxy.py` - ä»£ç†ä½¿ç”¨ç¤ºä¾‹
- `view_local_ip.py` - æŸ¥çœ‹æœ¬æœº IP

### ç¤ºä¾‹ 3ï¼šå®Œæ•´çš„ CSDN çˆ¬è™«

```python
import json
from loguru import logger
from coocan import Request, MiniSpider


class CSDNSpider(MiniSpider):
    start_urls = ["http://www.csdn.net"]
    max_requests = 10

    def middleware(self, request: Request):
        """è¯·æ±‚ä¸­é—´ä»¶"""
        request.headers["Referer"] = "http://www.csdn.net/"

    def parse(self, response):
        """è§£æé¦–é¡µ"""
        api = "https://blog.csdn.net/community/home-api/v1/get-business-list"
        params = {
            "page": "1",
            "size": "20",
            "businessType": "lately",
            "noMore": "false",
            "username": "markadc"
        }
        yield Request(
            api,
            self.parse_page,
            params=params,
            cb_kwargs={"api": api, "params": params}
        )

    def parse_page(self, response, api, params):
        """è§£æåˆ—è¡¨é¡µ"""
        current_page = params["page"]
        data = json.loads(response.text)
        articles = data["data"]["list"]

        if not articles:
            logger.warning(f"æ²¡æœ‰ç¬¬ {current_page} é¡µ")
            return

        for article in articles:
            date = article["formatTime"]
            title = article["title"]
            url = article["url"]

            logger.info(f"{date} - {title}\n{url}")

            # çˆ¬å–è¯¦æƒ…é¡µ
            yield Request(url, self.parse_detail, cb_kwargs={"title": title})

        logger.info(f"ç¬¬ {current_page} é¡µæŠ“å–æˆåŠŸ")

        # æŠ“å–ä¸‹ä¸€é¡µ
        next_page = int(current_page) + 1
        params["page"] = str(next_page)
        yield Request(api, self.parse_page, params=params, cb_kwargs={"api": api, "params": params})

    def parse_detail(self, response, title):
        """è§£æè¯¦æƒ…é¡µ"""
        logger.success(f"{response.status_code} - å·²è®¿é—® {title}")

    def process_item(self, item):
        """å¤„ç†æ•°æ®"""
        # å¯ä»¥åœ¨è¿™é‡Œä¿å­˜åˆ°æ•°æ®åº“æˆ–æ–‡ä»¶
        logger.debug(f"Processing: {item}")


if __name__ == '__main__':
    spider = CSDNSpider()
    spider.go()
```

---

## ğŸ“– æ–‡æ¡£

### Request å¯¹è±¡

```python
Request(
    url: str,                    # è¯·æ±‚ URL
    callback=None,               # å›è°ƒå‡½æ•°
    method: str = "GET",         # è¯·æ±‚æ–¹æ³•
    params: dict = None,         # URL å‚æ•°
    data: dict = None,           # POST æ•°æ®
    json: dict = None,           # JSON æ•°æ®
    headers: dict = None,        # è¯·æ±‚å¤´
    cookies: dict = None,        # Cookies
    proxy: str = None,           # ä»£ç†åœ°å€
    timeout: int = 6,            # è¶…æ—¶æ—¶é—´
    priority: int = 0,           # ä¼˜å…ˆçº§ï¼ˆæ•°å­—è¶Šå¤§ä¼˜å…ˆçº§è¶Šé«˜ï¼‰
    cb_kwargs: dict = None,      # ä¼ é€’ç»™å›è°ƒå‡½æ•°çš„é¢å¤–å‚æ•°
    validator=None,              # å“åº”éªŒè¯å™¨
)
```

### Response å¯¹è±¡

```python
response.text           # å“åº”æ–‡æœ¬
response.content        # å“åº”å­—èŠ‚
response.json()         # è§£æ JSON
response.status_code    # çŠ¶æ€ç 
response.headers        # å“åº”å¤´
response.url            # è¯·æ±‚ URL

# é€‰æ‹©å™¨æ–¹æ³•
response.xpath(query)   # XPath é€‰æ‹©å™¨
response.css(query)     # CSS é€‰æ‹©å™¨
```

### MiniSpider ä¸»è¦æ–¹æ³•

| æ–¹æ³•                                      | è¯´æ˜                                      |
| ----------------------------------------- | ----------------------------------------- |
| `start_requests()`                        | ç”Ÿæˆåˆå§‹è¯·æ±‚ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨ start_urlsï¼‰ |
| `parse(response)`                         | é»˜è®¤å›è°ƒå‡½æ•°ï¼Œè§£æå“åº”                    |
| `middleware(request)`                     | è¯·æ±‚ä¸­é—´ä»¶ï¼Œå¯ä¿®æ”¹è¯·æ±‚                    |
| `process_item(item)`                      | å¤„ç†çˆ¬å–çš„æ•°æ®é¡¹                          |
| `handle_request_exception(request, exc)`  | å¤„ç†è¯·æ±‚å¼‚å¸¸                              |
| `handle_callback_exception(request, exc)` | å¤„ç†å›è°ƒå‡½æ•°å¼‚å¸¸                          |
| `go()`                                    | å¯åŠ¨çˆ¬è™«                                  |

### å¼‚å¸¸å¤„ç†

```python
from coocan import MiniSpider, Request
from coocan.url.errs import IgnoreRequest, IgnoreResponse


class MySpider(MiniSpider):
    def handle_request_exception(self, request: Request, exc: Exception):
        """å¤„ç†è¯·æ±‚å¼‚å¸¸"""
        # æŠ›å‡º IgnoreRequest è¡¨ç¤ºæ”¾å¼ƒè¯¥è¯·æ±‚
        raise IgnoreRequest("æ”¾å¼ƒè¯·æ±‚")

        # æˆ–è¿”å›æ–°è¯·æ±‚æ›¿ä»£
        # return Request(new_url, callback=self.parse)

    def validator(self, response):
        """éªŒè¯å“åº”"""
        if response.status_code != 200:
            # æŠ›å‡º IgnoreResponse è·³è¿‡å›è°ƒ
            raise IgnoreResponse("çŠ¶æ€ç å¼‚å¸¸")

    def handle_callback_exception(self, request: Request, exc: Exception):
        """å¤„ç†å›è°ƒå¼‚å¸¸"""
        logger.error(f"å›è°ƒå¼‚å¸¸: {exc}")
```

---

## ğŸ› ï¸ å‘½ä»¤è¡Œå·¥å…·

Coocan æä¾›äº†ä¾¿æ·çš„å‘½ä»¤è¡Œå·¥å…·ï¼š

```bash
# åˆ›å»ºæ–°çˆ¬è™«
coocan new -s spider_name

# æŸ¥çœ‹å¸®åŠ©
coocan --help
```

---

## ğŸ“ æ›´æ–°æ—¥å¿—

### v0.6.1 (2025-5-15)

- âœ¨ è¯·æ±‚æ”¯æŒä»£ç†ï¼Œä½¿ç”¨ `proxy` å‚æ•°
- âš¡ è¯·æ±‚çš„é»˜è®¤è¶…æ—¶è®¾ç½®ä¸º 6 ç§’

### v0.5.0 (2025-4-28)

- âœ¨ æ–°å¢ `process_item` æ–¹æ³•ï¼Œç”¨äºå¤„ç†æ•°æ®
  - ç¤ºä¾‹ä»£ç ä½äº `coocan/_examples/recv_item.py`

### v0.4.0 (2025-4-25)

- ğŸ‰ å®ç° `coocan` å‘½ä»¤è¡Œå·¥å…·
  - æ”¯æŒ `coocan new -s <spider_file_name>` åˆ›å»ºçˆ¬è™«

### v0.3.2 (2025-4-23)

- âœ¨ å¯ä»¥è®¾ç½®è¯·æ±‚å»¶è¿Ÿ (`delay` å±æ€§)
- âœ¨ é»˜è®¤å¯ç”¨éšæœº User-Agent (`enable_random_ua` å±æ€§)

### v0.3.1 (2025-4-22)

- âœ¨ è¯·æ±‚æ”¯æŒä¼˜å…ˆçº§å‚æ•° (`priority`)

### v0.3.0 (2025-4-21)

- âœ¨ è¯·æ±‚å¼‚å¸¸æ—¶è§¦å‘ `handle_request_exception`
  - å¯æŠ›å‡º `IgnoreRequest` å¼‚å¸¸æ”¾å¼ƒè¯·æ±‚
  - å¯è¿”å›æ–°çš„ `Request` å¯¹è±¡æ›¿ä»£åŸè¯·æ±‚
- âœ¨ åŠ å…¥å“åº”éªŒè¯å™¨ `validator`
  - å¯æŠ›å‡º `IgnoreResponse` å¼‚å¸¸è·³è¿‡å›è°ƒ
- âœ¨ å›è°ƒå¼‚å¸¸æ—¶è§¦å‘ `handle_callback_exception`

### v0.2.0 (2025-4-18)

- âœ¨ å“åº”å¯¹è±¡æ”¯æŒ `XPath` å’Œ `CSS` é€‰æ‹©å™¨
- âœ¨ åŠ å…¥è¯·æ±‚é‡è¯•æœºåˆ¶
- âœ¨ è¯·æ±‚å¼‚å¸¸å¤„ç†å›è°ƒå‡½æ•°

---

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤ Issue å’Œ Pull Requestï¼

1. Fork æœ¬ä»“åº“
2. åˆ›å»ºä½ çš„ç‰¹æ€§åˆ†æ”¯ (`git checkout -b feature/AmazingFeature`)
3. æäº¤ä½ çš„æ›´æ”¹ (`git commit -m 'Add some AmazingFeature'`)
4. æ¨é€åˆ°åˆ†æ”¯ (`git push origin feature/AmazingFeature`)
5. å¼€å¯ä¸€ä¸ª Pull Request

---

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ [MIT](https://opensource.org/licenses/MIT) è®¸å¯è¯ã€‚

---

## ğŸ‘¨â€ğŸ’» ä½œè€…

**wauo** - [markadc@126.com](mailto:markadc@126.com)

é¡¹ç›®ä¸»é¡µ: [https://github.com/markadc/coocan](https://github.com/markadc/coocan)

---

<div align="center">

**å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹ä½ æœ‰å¸®åŠ©ï¼Œè¯·ç»™ä¸€ä¸ª â­ï¸ Star æ”¯æŒä¸€ä¸‹ï¼**

</div>
