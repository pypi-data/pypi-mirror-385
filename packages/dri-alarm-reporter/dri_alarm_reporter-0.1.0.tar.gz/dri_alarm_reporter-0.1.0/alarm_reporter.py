#!/usr/bin/env python3
"""
CloudWatch Alarm Daily Reporter
æ¯æ—¥åˆ†æ CloudWatch Alarms ä¸¦è¿½è¹¤åˆ°å°æ‡‰çš„ Lambda logs
"""

import boto3
from datetime import datetime, timedelta
from zoneinfo import ZoneInfo
from typing import List, Dict, Optional
import json
from collections import defaultdict

# å˜—è©¦è¼‰å…¥ Claude Analyzer (å¯é¸)
try:
    from claude_analyzer import ClaudeAnalyzer, load_claude_settings
    CLAUDE_AVAILABLE = True
except ImportError:
    CLAUDE_AVAILABLE = False

class AlarmReporter:
    def __init__(self, regions=None):
        if regions is None:
            # é è¨­æŸ¥è©¢æ‰€æœ‰ production rings çš„ regions
            # Ring1: ap-southeast-2, ap-southeast-1
            # Ring2: us-east-1, ap-south-1
            # Ring3: ap-northeast-1, eu-central-1, me-central-1
            self.regions = [
                'ap-southeast-2', 'ap-southeast-1',  # Ring1
                'us-east-1', 'ap-south-1',           # Ring2
                'ap-northeast-1', 'eu-central-1', 'me-central-1'  # Ring3
            ]
        elif isinstance(regions, str):
            self.regions = [regions]
        else:
            self.regions = regions

        # ç‚ºæ¯å€‹ region å»ºç«‹ clients
        self.clients = {}
        for region in self.regions:
            self.clients[region] = {
                'cloudwatch': boto3.client('cloudwatch', region_name=region),
                'logs': boto3.client('logs', region_name=region),
                'lambda': boto3.client('lambda', region_name=region),
                'apigateway': boto3.client('apigateway', region_name=region)
            }

        # å˜—è©¦åˆå§‹åŒ– Claude AI åˆ†æå™¨
        self.ai_analyzer = None
        self.ai_enabled = False

        if CLAUDE_AVAILABLE:
            try:
                claude_settings = load_claude_settings()
                if claude_settings:
                    self.ai_analyzer = ClaudeAnalyzer(claude_settings)
                    self.ai_enabled = True
                    print("âœ¨ AI åˆ†æåŠŸèƒ½å·²å•Ÿç”¨")
            except Exception as e:
                # éœé»˜å¤±æ•—ï¼Œä¸å½±éŸ¿åŸæœ‰åŠŸèƒ½
                pass

    def get_tw_time_range(self, date: Optional[datetime] = None):
        """
        ç²å–å°ç£æ™‚é–“çš„å·¥ä½œæ™‚æ®µ (9:00 - 18:00)
        è¿”å› UTC æ™‚é–“ç¯„åœ
        """
        if date is None:
            date = datetime.now(ZoneInfo('Asia/Taipei'))

        # è¨­å®šå°ç£æ™‚é–“çš„å·¥ä½œæ™‚æ®µ
        tw_tz = ZoneInfo('Asia/Taipei')
        start_tw = date.replace(hour=9, minute=0, second=0, microsecond=0)
        end_tw = date.replace(hour=18, minute=0, second=0, microsecond=0)

        # è½‰æ›ç‚º UTC (å°ç£ UTC+8)
        start_utc = start_tw.astimezone(ZoneInfo('UTC'))
        end_utc = end_tw.astimezone(ZoneInfo('UTC'))

        return start_utc, end_utc

    def get_alarm_history(self, start_time: datetime, end_time: datetime) -> List[Dict]:
        """
        ç²å–æŒ‡å®šæ™‚é–“ç¯„åœå…§çš„ alarm history (è·¨æ‰€æœ‰ regions)
        """
        alarms_in_period = []

        for i, region in enumerate(self.regions, 1):
            print(f"  æŸ¥è©¢ {region} ({i}/{len(self.regions)})...", end=' ', flush=True)
            try:
                cloudwatch = self.clients[region]['cloudwatch']
                paginator = cloudwatch.get_paginator('describe_alarm_history')

                page_iterator = paginator.paginate(
                    StartDate=start_time,
                    EndDate=end_time,
                    HistoryItemType='StateUpdate',
                    MaxRecords=100
                )

                region_alarms = 0
                for page in page_iterator:
                    for item in page['AlarmHistoryItems']:
                        history_data = json.loads(item['HistoryData'])
                        # åªè¨˜éŒ„é€²å…¥ ALARM ç‹€æ…‹çš„äº‹ä»¶
                        if history_data.get('newState', {}).get('stateValue') == 'ALARM':
                            alarms_in_period.append({
                                'alarm_name': item['AlarmName'],
                                'timestamp': item['Timestamp'],
                                'summary': item.get('HistorySummary', ''),
                                'new_state': history_data.get('newState', {}),
                                'region': region  # è¨˜éŒ„ region
                            })
                            region_alarms += 1

                print(f"æ‰¾åˆ° {region_alarms} å€‹ alarms")
            except Exception as e:
                print(f"éŒ¯èª¤: {e}")

        return alarms_in_period

    def get_alarm_details(self, alarm_name: str, region: str) -> Optional[Dict]:
        """
        ç²å– alarm çš„è©³ç´°è¨­å®š
        """
        try:
            cloudwatch = self.clients[region]['cloudwatch']
            response = cloudwatch.describe_alarms(AlarmNames=[alarm_name])
            if response['MetricAlarms']:
                alarm = response['MetricAlarms'][0]
                return {
                    'alarm_name': alarm['AlarmName'],
                    'namespace': alarm.get('Namespace'),
                    'metric_name': alarm.get('MetricName'),
                    'dimensions': alarm.get('Dimensions', []),
                    'state_reason': alarm.get('StateReason', ''),
                    'region': region
                }
        except Exception as e:
            print(f"Error getting alarm details for {alarm_name} in {region}: {e}")
        return None

    def extract_lambda_function(self, alarm_details: Dict) -> Optional[str]:
        """
        å¾ alarm dimensions ä¸­æå– Lambda function name
        """
        if alarm_details['namespace'] != 'AWS/Lambda':
            return None

        for dim in alarm_details['dimensions']:
            if dim['Name'] == 'FunctionName':
                return dim['Value']
        return None

    def extract_lambda_log_group_from_text(self, text: str) -> Optional[str]:
        """
        å¾æ–‡å­—ä¸­æå– Lambda log group è·¯å¾‘

        Args:
            text: å¯èƒ½åŒ…å« log group è·¯å¾‘çš„æ–‡å­—

        Returns:
            Lambda log group è·¯å¾‘ï¼Œä¾‹å¦‚: /aws/lambda/production-get-credit-usage
        """
        import re

        # å°‹æ‰¾ /aws/lambda/* æ ¼å¼çš„è·¯å¾‘
        pattern = r'/aws/lambda/[\w\-]+'
        match = re.search(pattern, text)

        if match:
            return match.group(0)

        return None

    def get_api_gateway_log_group(self, api_name: str, region: str) -> Optional[str]:
        """
        å¾ API Gateway åç¨±æŸ¥è©¢å°æ‡‰çš„ execution log group

        Args:
            api_name: API Gateway åç¨± (ä¾‹å¦‚: production-vcs-inventory)
            region: AWS region

        Returns:
            Log group åç¨±ï¼Œä¾‹å¦‚: API-Gateway-Execution-Logs_{api_id}/production
        """
        try:
            apigateway_client = self.clients[region]['apigateway']

            # æŸ¥è©¢æ‰€æœ‰ REST APIs
            response = apigateway_client.get_rest_apis()

            # æ‰¾åˆ°åŒ¹é…çš„ API
            for api in response.get('items', []):
                if api['name'] == api_name:
                    api_id = api['id']
                    # å‡è¨­ä½¿ç”¨ production stage
                    return f'API-Gateway-Execution-Logs_{api_id}/production'

            return None
        except Exception as e:
            print(f"    Error querying API Gateway: {e}")
            return None

    def infer_lambda_from_api_gateway_alarm(self, alarm_name: str) -> Optional[str]:
        """
        å¾ API Gateway alarm åç¨±æ¨æ¸¬å¾Œç«¯ Lambda function name
        ä¾‹å¦‚: production-vcs-inventory-5XX-Error -> /aws/lambda/production-vcs-inventory
        """
        import re
        # åŒ¹é… production-{service-name}-5XX-Error æˆ–é¡ä¼¼æ¨¡å¼
        match = re.match(r'production-([a-z0-9\-]+?)(?:-5XX-Error|-4XX-Error|-error)', alarm_name, re.IGNORECASE)
        if match:
            service_name = match.group(1)
            return f'/aws/lambda/production-{service_name}'
        return None

    def extract_log_group_from_custom_metric(self, alarm_details: Dict, region: str) -> Optional[str]:
        """
        å¾ custom metric alarm ä¸­å˜—è©¦æå– log group name
        æ”¯æ´:
        - Lambda alarms
        - API Gateway alarms
        - Custom metric alarms
        """
        alarm_name = alarm_details['alarm_name']
        namespace = alarm_details['namespace']
        metric_name = alarm_details['metric_name']
        dimensions = alarm_details.get('dimensions', [])

        # è™•ç† API Gateway alarms
        if namespace == 'AWS/ApiGateway':
            # å¾ dimensions ä¸­æå– ApiName
            for dim in dimensions:
                if dim['Name'] == 'ApiName':
                    api_name = dim['Value']
                    return self.get_api_gateway_log_group(api_name, region)
            return None

        # è™•ç† vcs-inventory-production-* pattern
        if 'vcs-inventory-production' in alarm_name and 'logErrors' in alarm_name:
            # æå– lambda function name
            # vcs-inventory-production-k8s-inventory-clusters-logErrors
            # -> k8s-inventory-clusters
            parts = alarm_name.replace('vcs-inventory-production-', '').replace('-logErrors', '')
            return f'/aws/lambda/production-{parts}'

        # è™•ç† container-security namespace pattern (é€™äº›ä¹Ÿæ˜¯ Lambda!)
        if namespace == 'container-security/production':
            # LogErrorCount/evaluator -> evaluator
            # LogErrorCount/healthInfoForwarder -> health-info-forwarder
            # LogErrorCount/startScan -> start-scan
            if 'LogErrorCount/' in metric_name:
                service_name = metric_name.split('/')[-1]
                # è½‰æ› camelCase ç‚º kebab-case
                import re
                kebab_name = re.sub(r'(?<!^)(?=[A-Z])', '-', service_name).lower()
                return f'/aws/lambda/production-{kebab_name}'

        # è™•ç† vcs-us-east-1-production-* pattern
        if alarm_name.startswith('vcs-us-east-1-production-'):
            # vcs-us-east-1-production-evaluator-ErrorCountGreaterThanZero
            # -> production-evaluator
            parts = alarm_name.replace('vcs-us-east-1-production-', '').split('-ErrorCount')[0]
            return f'/aws/lambda/production-{parts}'

        return None

    def get_error_logs(self, log_group_name: str, region: str, start_time: datetime,
                      end_time: datetime, max_logs: int = 10, include_all: bool = False) -> List[Dict]:
        """
        ç²å–æŒ‡å®š log group çš„éŒ¯èª¤æ—¥èªŒ
        æ”¯æ´å¤šç¨®æ—¥èªŒæ ¼å¼ï¼šJSON å’Œç´”æ–‡å­—
        """
        errors = []

        try:
            logs_client = self.clients[region]['logs']

            # æ ¹æ“š log group é¡å‹é¸æ“‡ filter pattern
            is_api_gateway = log_group_name.startswith('API-Gateway-Execution-Logs_')

            if include_all:
                # å¦‚æœéœ€è¦åŒ…å«æ‰€æœ‰æ—¥èªŒ,ä¸ä½¿ç”¨ filter pattern
                response = logs_client.filter_log_events(
                    logGroupName=log_group_name,
                    startTime=int(start_time.timestamp() * 1000),
                    endTime=int(end_time.timestamp() * 1000),
                    limit=max_logs
                )

                for event in response.get('events', []):
                    errors.append({
                        'timestamp': datetime.fromtimestamp(event['timestamp'] / 1000,
                                                           ZoneInfo('Asia/Taipei')),
                        'message': event['message'],
                        'log_stream': event['logStreamName']
                    })
            else:
                # å˜—è©¦å¤šç¨® filter patterns ä¾†æ•ç²ä¸åŒæ ¼å¼çš„éŒ¯èª¤æ—¥èªŒ
                filter_patterns = []

                if is_api_gateway:
                    # API Gateway åŸ·è¡Œæ—¥èªŒæ ¼å¼
                    filter_patterns = ['?"Status: 5" ?500 ?502 ?503 ?504 ?statusCode']
                else:
                    # Lambda æ—¥èªŒæ”¯æ´å¤šç¨®æ ¼å¼
                    filter_patterns = [
                        # JSON æ ¼å¼: {"level": "error"}
                        '{$.level = "error"}',
                        # JSON æ ¼å¼: {"severity": "error"}
                        '{$.severity = "error"}',
                        # ç´”æ–‡å­—æ ¼å¼
                        '?ERROR ?Error ?error ?Exception ?exception ?Task timed out ?WARN ?Warning'
                    ]

                # ä½¿ç”¨æ¯å€‹ pattern æŸ¥è©¢ï¼Œç›´åˆ°æ‰¾åˆ°è¶³å¤ çš„ logs
                seen_messages = set()  # ç”¨æ–¼å»é‡

                for pattern in filter_patterns:
                    try:
                        response = logs_client.filter_log_events(
                            logGroupName=log_group_name,
                            startTime=int(start_time.timestamp() * 1000),
                            endTime=int(end_time.timestamp() * 1000),
                            filterPattern=pattern,
                            limit=max_logs
                        )

                        for event in response.get('events', []):
                            # å»é‡ï¼šåŒä¸€æ¢æ—¥èªŒå¯èƒ½è¢«å¤šå€‹ pattern åŒ¹é…
                            msg_key = (event['timestamp'], event['message'])
                            if msg_key not in seen_messages:
                                seen_messages.add(msg_key)
                                errors.append({
                                    'timestamp': datetime.fromtimestamp(event['timestamp'] / 1000,
                                                                       ZoneInfo('Asia/Taipei')),
                                    'message': event['message'],
                                    'log_stream': event['logStreamName']
                                })

                        # å¦‚æœå·²ç¶“æ‰¾åˆ°è¶³å¤ çš„ logsï¼Œä¸éœ€è¦ç¹¼çºŒå˜—è©¦å…¶ä»– patterns
                        if len(errors) >= max_logs:
                            break

                    except Exception as pattern_error:
                        # å¦‚æœæŸå€‹ pattern å¤±æ•—ï¼Œç¹¼çºŒå˜—è©¦ä¸‹ä¸€å€‹
                        continue

        except logs_client.exceptions.ResourceNotFoundException:
            print(f"Log group not found: {log_group_name}")
        except Exception as e:
            print(f"Error fetching logs for {log_group_name} in {region}: {e}")

        return errors

    def query_api_gateway_5xx_errors_with_insights(
        self,
        log_group_name: str,
        region: str,
        start_time: datetime,
        end_time: datetime,
        max_results: int = 20
    ) -> List[Dict]:
        """
        ä½¿ç”¨ CloudWatch Logs Insights æŸ¥è©¢ API Gateway 5XX éŒ¯èª¤

        è¿”å›: [
            {
                'timestamp': datetime,
                'request_id': str,
                'status_code': str,
                'error_message': str,
                'lambda_arn': str (å¦‚æœæœ‰)
            }
        ]
        """
        errors = []

        try:
            logs_client = self.clients[region]['logs']

            # Insights æŸ¥è©¢èªå¥
            query_string = f'''fields @timestamp, @message
| filter @message like /Status: 5/ or @message like /500/ or @message like /502/ or @message like /503/ or @message like /504/
| sort @timestamp desc
| limit {max_results}'''

            # å•Ÿå‹•æŸ¥è©¢
            start_query_response = logs_client.start_query(
                logGroupName=log_group_name,
                startTime=int(start_time.timestamp()),
                endTime=int(end_time.timestamp()),
                queryString=query_string
            )

            query_id = start_query_response['queryId']

            # è¼ªè©¢æŸ¥è©¢çµæœ (æœ€å¤šç­‰å¾… 30 ç§’)
            import time
            max_wait = 30
            waited = 0

            while waited < max_wait:
                time.sleep(2)
                waited += 2

                result = logs_client.get_query_results(queryId=query_id)

                if result['status'] == 'Complete':
                    # è§£æçµæœ
                    for record in result.get('results', []):
                        message = ''
                        timestamp_str = ''

                        for field in record:
                            if field['field'] == '@message':
                                message = field['value']
                            elif field['field'] == '@timestamp':
                                timestamp_str = field['value']

                        if message and timestamp_str:
                            # æå– request ID
                            import re
                            request_id_match = re.search(r'\(([0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12})\)', message)
                            request_id = request_id_match.group(1) if request_id_match else None

                            # æå– status code
                            status_code = None
                            if 'Status: 5' in message:
                                status_match = re.search(r'Status: (\d+)', message)
                                if status_match:
                                    status_code = status_match.group(1)
                            elif 'statusCode' in message:
                                status_match = re.search(r'statusCode["\']?:\s*(\d+)', message)
                                if status_match:
                                    status_code = status_match.group(1)

                            # è§£ææ™‚é–“æˆ³
                            try:
                                # CloudWatch Insights è¿”å›æ ¼å¼: "2025-10-07 01:29:14.230"
                                dt = datetime.strptime(timestamp_str, '%Y-%m-%d %H:%M:%S.%f')
                                dt = dt.replace(tzinfo=ZoneInfo('UTC')).astimezone(ZoneInfo('Asia/Taipei'))
                            except:
                                dt = datetime.now(ZoneInfo('Asia/Taipei'))

                            errors.append({
                                'timestamp': dt,
                                'request_id': request_id,
                                'status_code': status_code or '5XX',
                                'error_message': message[:500],  # é™åˆ¶é•·åº¦
                                'full_message': message
                            })

                    break
                elif result['status'] == 'Failed':
                    print(f"    Insights æŸ¥è©¢å¤±æ•—: {result.get('statusMessage', 'Unknown error')}")
                    break

            if waited >= max_wait:
                print(f"    Insights æŸ¥è©¢è¶…æ™‚")

        except Exception as e:
            print(f"    Insights æŸ¥è©¢éŒ¯èª¤: {e}")

        return errors

    def extract_lambda_from_request_id(
        self,
        log_group_name: str,
        region: str,
        request_id: str,
        error_timestamp: datetime,
        time_window_seconds: int = 10
    ) -> Optional[str]:
        """
        å¾ API Gateway logs ä¸­æ‰¾å‡ºç‰¹å®š request ID å°æ‡‰çš„ Lambda function

        æŸ¥è©¢ç­–ç•¥ï¼š
        1. ç”¨ request_id éæ¿¾æ—¥èªŒ
        2. æ‰¾åˆ° "Endpoint request URI" åŒ…å« Lambda ARN çš„è¡Œ
        3. æå– function name

        è¿”å›: "/aws/lambda/production-xxx" æˆ– None
        """
        try:
            logs_client = self.clients[region]['logs']

            # åœ¨éŒ¯èª¤æ™‚é–“å‰å¾ŒæŸ¥è©¢
            start_time = error_timestamp - timedelta(seconds=time_window_seconds)
            end_time = error_timestamp + timedelta(seconds=time_window_seconds)

            # ä½¿ç”¨ Insights æŸ¥è©¢
            query_string = f'''fields @timestamp, @message
| filter @message like "{request_id}"
| filter @message like /Endpoint request URI.*lambda/
| sort @timestamp asc
| limit 1'''

            start_query_response = logs_client.start_query(
                logGroupName=log_group_name,
                startTime=int(start_time.timestamp()),
                endTime=int(end_time.timestamp()),
                queryString=query_string
            )

            query_id = start_query_response['queryId']

            # è¼ªè©¢çµæœ
            import time
            for _ in range(10):  # æœ€å¤šç­‰ 20 ç§’
                time.sleep(2)
                result = logs_client.get_query_results(queryId=query_id)

                if result['status'] == 'Complete':
                    if result.get('results'):
                        for field in result['results'][0]:
                            if field['field'] == '@message':
                                message = field['value']
                                # æå– Lambda function name
                                # æ ¼å¼: https://lambda.{region}.amazonaws.com/.../function:arn:aws:lambda:{region}:{account}:function:{name}/invocations
                                import re
                                lambda_match = re.search(r'function[:/]arn:aws:lambda:[^:]+:\d+:function:([^/]+)', message)
                                if lambda_match:
                                    function_name = lambda_match.group(1)
                                    return f'/aws/lambda/{function_name}'

                                # å¦ä¸€ç¨®æ ¼å¼: .../functions/arn:aws:lambda:...:function:{name}/invocations
                                lambda_match2 = re.search(r'functions/arn:aws:lambda:[^:]+:\d+:function:([^/]+)', message)
                                if lambda_match2:
                                    function_name = lambda_match2.group(1)
                                    return f'/aws/lambda/{function_name}'
                    break
                elif result['status'] == 'Failed':
                    break

        except Exception as e:
            print(f"    æå– Lambda function éŒ¯èª¤: {e}")

        return None

    def find_related_lambda_logs(self, alarm_name: str, region: str) -> List[str]:
        """
        æ ¹æ“š alarm åç¨±æ‰¾å‡ºç›¸é—œçš„ Lambda log groups
        ä¾‹å¦‚: production-vcs-inventory-5XX-Error
        -> åˆ—å‡ºæ‰€æœ‰ /aws/lambda/production-*inventory* çš„ log groups
        """
        import re

        # å¾ alarm name æå–é—œéµå­—
        keywords = []
        alarm_lower = alarm_name.lower()

        # æå–å¯èƒ½çš„æœå‹™åç¨±é—œéµå­—
        if 'inventory' in alarm_lower:
            keywords.append('inventory')
        if 'vcs' in alarm_lower:
            keywords.append('vcs')
        if 'public-api' in alarm_lower:
            keywords.append('public')

        # å¦‚æœæ²’æœ‰æ˜é¡¯é—œéµå­—ï¼Œå˜—è©¦å¾ production-{service}-5XX æ ¼å¼æå–
        match = re.match(r'production-([a-z0-9\-]+?)-5XX', alarm_lower)
        if match:
            service_name = match.group(1)
            # å°‡ kebab-case æ‹†åˆ†ç‚ºå–®è©
            words = service_name.split('-')
            keywords.extend(words)

        if not keywords:
            return []

        # æ§‹å»º log group prefix
        prefix = '/aws/lambda/production-'

        # åˆ—å‡ºæ‰€æœ‰ç›¸é—œ log groups
        logs_client = self.clients[region]['logs']
        log_groups = []

        try:
            # åˆ†é æŸ¥è©¢æ‰€æœ‰ log groups
            paginator = logs_client.get_paginator('describe_log_groups')
            pages = paginator.paginate(
                logGroupNamePrefix=prefix,
                PaginationConfig={'MaxItems': 100}
            )

            for page in pages:
                for log_group in page['logGroups']:
                    log_group_name = log_group['logGroupName']
                    log_group_lower = log_group_name.lower()

                    # æª¢æŸ¥æ˜¯å¦åŒ…å«ä»»ä¸€é—œéµå­—
                    if any(keyword in log_group_lower for keyword in keywords):
                        # è¨ˆç®—åŒ¹é…åˆ†æ•¸ï¼šåŒ¹é…çš„é—œéµå­—è¶Šå¤šï¼Œåˆ†æ•¸è¶Šé«˜
                        score = sum(1 for keyword in keywords if keyword in log_group_lower)
                        log_groups.append((log_group_name, score))

        except Exception as e:
            print(f"    æŸ¥è©¢ Lambda log groups éŒ¯èª¤: {e}")

        # æŒ‰åŒ¹é…åˆ†æ•¸é™åºæ’åˆ—ï¼Œåˆ†æ•¸ç›¸åŒå‰‡æŒ‰åç¨±æ’åº
        log_groups.sort(key=lambda x: (-x[1], x[0]))

        return [lg[0] for lg in log_groups]

    def query_lambda_errors_with_insights(
        self,
        log_group_name: str,
        region: str,
        start_time: datetime,
        end_time: datetime,
        max_results: int = 20
    ) -> List[Dict]:
        """
        ä½¿ç”¨ CloudWatch Logs Insights æŸ¥è©¢ Lambda éŒ¯èª¤æ—¥èªŒ
        å°ˆé–€é‡å° JSON æ ¼å¼çš„éŒ¯èª¤æ—¥èªŒ
        """
        errors = []

        try:
            logs_client = self.clients[region]['logs']

            # Insights æŸ¥è©¢èªå¥ - æŸ¥æ‰¾å„ç¨®éŒ¯èª¤æ ¼å¼
            query_string = f'''fields @timestamp, @message
| filter @message like /"level":"error"/ or @message like /"severity":"error"/ or @message like /ERROR/ or @message like /Exception/ or @message like /Failed/
| sort @timestamp desc
| limit {max_results}'''

            start_query_response = logs_client.start_query(
                logGroupName=log_group_name,
                startTime=int(start_time.timestamp()),
                endTime=int(end_time.timestamp()),
                queryString=query_string
            )

            query_id = start_query_response['queryId']

            # è¼ªè©¢çµæœ (æœ€å¤šç­‰å¾… 30 ç§’)
            import time
            for _ in range(15):
                time.sleep(2)
                result = logs_client.get_query_results(queryId=query_id)

                if result['status'] == 'Complete':
                    for record in result.get('results', []):
                        message = ''
                        timestamp_str = ''

                        for field in record:
                            if field['field'] == '@message':
                                message = field['value']
                            elif field['field'] == '@timestamp':
                                timestamp_str = field['value']

                        if message and timestamp_str:
                            try:
                                # CloudWatch Insights è¿”å›æ ¼å¼: "2025-10-17 09:36:39.829"
                                dt = datetime.strptime(timestamp_str, '%Y-%m-%d %H:%M:%S.%f')
                                dt = dt.replace(tzinfo=ZoneInfo('UTC')).astimezone(ZoneInfo('Asia/Taipei'))
                            except:
                                try:
                                    dt = datetime.strptime(timestamp_str, '%Y-%m-%d %H:%M:%S')
                                    dt = dt.replace(tzinfo=ZoneInfo('UTC')).astimezone(ZoneInfo('Asia/Taipei'))
                                except:
                                    dt = datetime.now(ZoneInfo('Asia/Taipei'))

                            errors.append({
                                'timestamp': dt,
                                'message': message,
                                'log_stream': log_group_name
                            })
                    break
                elif result['status'] == 'Failed':
                    break

        except Exception as e:
            print(f"    Lambda Insights æŸ¥è©¢éŒ¯èª¤: {e}")

        return errors

    def get_complete_execution_logs(self, log_group_name: str, region: str,
                                   error_logs: List[Dict], max_logs: int = 200,
                                   is_timeout: bool = False) -> List[Dict]:
        """
        ç²å–å®Œæ•´çš„åŸ·è¡Œæ—¥èªŒï¼ˆæ”¯æ´ Lambda å’Œ API Gatewayï¼‰
        å°æ–¼æ¯å€‹éŒ¯èª¤æ—¥èªŒ,æ”¶é›†è©²åŸ·è¡Œçš„å®Œæ•´æ—¥èªŒ

        å„ªåŒ–ç­–ç•¥:
        - åªè™•ç†å‰ 10 æ¢ error logs
        - Timeout éŒ¯èª¤æ”¶é›† 35 ç§’ï¼Œä¸€èˆ¬éŒ¯èª¤æ”¶é›† 10 ç§’
        - åˆä½µç›¸åŒ log stream çš„æŸ¥è©¢
        - é”åˆ° max_logs å³åœæ­¢
        """
        complete_logs = []

        try:
            logs_client = self.clients[region]['logs']
            import re
            from collections import defaultdict

            # åµæ¸¬ log group é¡å‹
            is_api_gateway = log_group_name.startswith('API-Gateway-Execution-Logs_')

            # 1. é™åˆ¶è™•ç†æ•¸é‡ï¼šåªè™•ç†å‰ 10 æ¢
            logs_to_process = error_logs[:10]

            # 2. ä¾éŒ¯èª¤é¡å‹æ±ºå®šæ™‚é–“ç¯„åœ
            time_before = 35 if is_timeout else 10  # timeout éœ€è¦æ›´é•·æ™‚é–“è¿½è¹¤
            time_after = 5

            # 3. æŒ‰ log stream åˆ†çµ„ï¼Œé¿å…é‡è¤‡æŸ¥è©¢
            stream_groups = defaultdict(list)
            for error_log in logs_to_process:
                log_stream = error_log['log_stream']
                stream_groups[log_stream].append(error_log)

            # 4. å°æ¯å€‹ log stream åªæŸ¥è©¢ä¸€æ¬¡
            for log_stream, errors in stream_groups.items():
                # 4. ææ—©çµ‚æ­¢ï¼šå¦‚æœå·²ç¶“æ”¶é›†è¶³å¤ çš„æ—¥èªŒï¼Œåœæ­¢è™•ç†
                if len(complete_logs) >= max_logs:
                    break

                # æ‰¾åˆ°è©² stream ä¸­æœ€æ—©å’Œæœ€æ™šçš„éŒ¯èª¤æ™‚é–“
                earliest = min(errors, key=lambda x: x['timestamp'])
                latest = max(errors, key=lambda x: x['timestamp'])

                # è¨ˆç®—æŸ¥è©¢æ™‚é–“ç¯„åœï¼ˆæ¶µè“‹æ‰€æœ‰éŒ¯èª¤ï¼‰
                execution_start = earliest['timestamp'] - timedelta(seconds=time_before)
                execution_end = latest['timestamp'] + timedelta(seconds=time_after)

                # æå– request IDsï¼ˆæ ¹æ“š log group é¡å‹ä½¿ç”¨ä¸åŒçš„ patternï¼‰
                request_ids = set()
                for error in errors:
                    if is_api_gateway:
                        # API Gateway format: (uuid) at beginning of log lines
                        request_id_match = re.search(r'\(([0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12})\)',
                                                    error['message'])
                    else:
                        # Lambda format: uuid in various places
                        request_id_match = re.search(r'([0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12})',
                                                    error['message'])

                    if request_id_match:
                        request_ids.add(request_id_match.group(1))

                # å–®æ¬¡ API å‘¼å«å–å¾—è©² stream çš„æ‰€æœ‰ç›¸é—œæ—¥èªŒ
                response = logs_client.filter_log_events(
                    logGroupName=log_group_name,
                    logStreamNames=[log_stream],
                    startTime=int(execution_start.timestamp() * 1000),
                    endTime=int(execution_end.timestamp() * 1000),
                    limit=max_logs - len(complete_logs)  # åªå–å‰©é¤˜éœ€è¦çš„æ•¸é‡
                )

                # æ”¶é›†æ—¥èªŒ
                for event in response.get('events', []):
                    # å˜—è©¦å¾æ—¥èªŒä¸­æ‰¾åˆ° request ID
                    event_request_id = 'unknown'
                    for req_id in request_ids:
                        if req_id in event['message']:
                            event_request_id = req_id
                            break

                    complete_logs.append({
                        'timestamp': datetime.fromtimestamp(event['timestamp'] / 1000,
                                                          ZoneInfo('Asia/Taipei')),
                        'message': event['message'],
                        'log_stream': event['logStreamName'],
                        'request_id': event_request_id
                    })

        except Exception as e:
            print(f"Error fetching complete execution logs: {e}")

        # æŒ‰æ™‚é–“æ’åºä¸¦å»é‡
        seen = set()
        unique_logs = []
        for log in sorted(complete_logs, key=lambda x: x['timestamp']):
            log_key = (log['timestamp'], log['message'][:100])  # ç”¨æ™‚é–“æˆ³å’Œè¨Šæ¯å‰ 100 å­—å…ƒä½œç‚º key
            if log_key not in seen:
                seen.add(log_key)
                unique_logs.append(log)

        return unique_logs

    def get_cloudwatch_alarm_url(self, alarm_name: str, region: str, account_id: str = "484917860638") -> str:
        """ç”Ÿæˆ CloudWatch Alarm çš„ URL"""
        return f"https://{region}.console.aws.amazon.com/cloudwatch/home?region={region}#alarm:name={alarm_name}"

    def get_grafana_dashboard_url(self, service_name: str, start_time: datetime, end_time: datetime) -> Optional[str]:
        """
        ç”Ÿæˆ Grafana Dashboard æ·±åº¦é€£çµ

        Args:
            service_name: æœå‹™åç¨± (ä¾‹å¦‚: vcs-inventory, event-forwarder)
            start_time: é–‹å§‹æ™‚é–“ï¼ˆdatetime with timezoneï¼‰
            end_time: çµæŸæ™‚é–“ï¼ˆdatetime with timezoneï¼‰

        Returns:
            Grafana Dashboard URLï¼Œè‹¥ç„¡å°æ‡‰ dashboard å‰‡è¿”å› None
        """
        from config.constants import GRAFANA_DASHBOARD_URLS

        # æŸ¥æ‰¾å°æ‡‰çš„ dashboard URL
        dashboard_base_url = GRAFANA_DASHBOARD_URLS.get(service_name)
        if not dashboard_base_url:
            return None

        # è½‰æ›æ™‚é–“ç‚ºæ¯«ç§’æ™‚é–“æˆ³
        from_ms = int(start_time.timestamp() * 1000)
        to_ms = int(end_time.timestamp() * 1000)

        # æ§‹å»ºå¸¶æ™‚é–“ç¯„åœçš„ Grafana URL
        return f"{dashboard_base_url}?from={from_ms}&to={to_ms}"

    def get_cloudwatch_logs_url(
        self,
        log_group_name: str,
        region: str,
        start_time: datetime,
        end_time: datetime,
        filter_pattern: str = ""
    ) -> str:
        """
        ç”Ÿæˆ CloudWatch Logs Console æ·±åº¦é€£çµ

        Args:
            log_group_name: Log group åç¨±
            region: AWS region
            start_time: é–‹å§‹æ™‚é–“ï¼ˆdatetime with timezoneï¼‰
            end_time: çµæŸæ™‚é–“ï¼ˆdatetime with timezoneï¼‰
            filter_pattern: æ—¥èªŒéæ¿¾æ¨¡å¼ï¼ˆå¯é¸ï¼‰

        Returns:
            CloudWatch Logs Console URLï¼ŒåŒ…å«é å¡«çš„æ™‚é–“ç¯„åœå’Œéæ¿¾æ¢ä»¶
        """
        from urllib.parse import quote

        # å°‡ log group name ç·¨ç¢¼ï¼ˆéœ€è¦ç‰¹æ®Šè™•ç†æ–œç·šï¼‰
        # /aws/lambda/production-xxx -> $252Faws$252Flambda$252Fproduction-xxx
        encoded_log_group = log_group_name.replace('/', '$252F')

        # ç·¨ç¢¼éæ¿¾æ¨¡å¼
        encoded_filter = quote(filter_pattern) if filter_pattern else ""

        # è½‰æ›æ™‚é–“ç‚ºæ¯«ç§’æ™‚é–“æˆ³
        start_time_ms = int(start_time.timestamp() * 1000)
        end_time_ms = int(end_time.timestamp() * 1000)

        # ä½¿ç”¨é…ç½®ä¸­çš„ URL æ¨¡æ¿
        from config.constants import CLOUDWATCH_LOGS_URL_TEMPLATE

        url = CLOUDWATCH_LOGS_URL_TEMPLATE.format(
            region=region,
            encoded_log_group=encoded_log_group,
            encoded_filter=encoded_filter,
            start_time_ms=start_time_ms,
            end_time_ms=end_time_ms
        )

        return url

    def check_alarm_recovery(
        self,
        alarm_name: str,
        region: str,
        alarm_trigger_time: datetime,
        check_window_hours: int = 2
    ) -> Optional[Dict]:
        """
        æª¢æŸ¥ Synthetics alarm æ˜¯å¦å·²æ¢å¾©

        Args:
            alarm_name: Alarm åç¨±
            region: AWS region
            alarm_trigger_time: Alarm è§¸ç™¼æ™‚é–“
            check_window_hours: æª¢æŸ¥æ™‚é–“çª—å£ï¼ˆå°æ™‚ï¼‰

        Returns:
            è‹¥å·²æ¢å¾©ï¼Œè¿”å› {'recovered': True, 'recovery_time': datetime}
            è‹¥æœªæ¢å¾©ï¼Œè¿”å› {'recovered': False}
            è‹¥ç„¡æ³•åˆ¤æ–·ï¼Œè¿”å› None
        """
        try:
            cloudwatch = self.clients[region]['cloudwatch']

            # æª¢æŸ¥è§¸ç™¼æ™‚é–“ä¹‹å¾Œçš„ alarm history
            check_end_time = alarm_trigger_time + timedelta(hours=check_window_hours)
            now = datetime.now(ZoneInfo('UTC'))
            check_end_time = min(check_end_time, now)  # ä¸è¶…éç¾åœ¨æ™‚é–“

            # æŸ¥è©¢ alarm history
            response = cloudwatch.describe_alarm_history(
                AlarmName=alarm_name,
                StartDate=alarm_trigger_time,
                EndDate=check_end_time,
                HistoryItemType='StateUpdate',
                MaxRecords=50
            )

            # å°‹æ‰¾å¾ ALARM -> OK çš„ç‹€æ…‹è½‰æ›
            for item in response.get('AlarmHistoryItems', []):
                history_data = json.loads(item['HistoryData'])
                new_state = history_data.get('newState', {}).get('stateValue')

                if new_state == 'OK':
                    return {
                        'recovered': True,
                        'recovery_time': item['Timestamp']
                    }

            # æœªæ‰¾åˆ°æ¢å¾©è¨˜éŒ„
            return {'recovered': False}

        except Exception as e:
            print(f"    æª¢æŸ¥ alarm æ¢å¾©ç‹€æ…‹æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return None

    def format_log_for_display(self, log_message: str, max_fields: int = 20, max_length: int = 1000) -> str:
        """
        æ ¼å¼åŒ–æ—¥èªŒè¨Šæ¯ä»¥ä¾¿é¡¯ç¤º

        å˜—è©¦è§£æ JSON æ ¼å¼çš„æ—¥èªŒä¸¦é¡¯ç¤ºæ‰€æœ‰æ¬„ä½
        è‹¥é JSON æ ¼å¼ï¼Œå‰‡é¡¯ç¤ºåŸå§‹è¨Šæ¯

        Args:
            log_message: åŸå§‹æ—¥èªŒè¨Šæ¯
            max_fields: æœ€å¤šé¡¯ç¤ºå¤šå°‘å€‹æ¬„ä½
            max_length: å–®ä¸€æ¬„ä½å€¼çš„æœ€å¤§é¡¯ç¤ºé•·åº¦

        Returns:
            æ ¼å¼åŒ–å¾Œçš„æ—¥èªŒå­—ä¸²ï¼ˆMarkdown æ ¼å¼ï¼‰
        """
        try:
            # å˜—è©¦è§£æç‚º JSON
            log_data = json.loads(log_message)

            if not isinstance(log_data, dict):
                # éå­—å…¸æ ¼å¼ï¼Œé¡¯ç¤ºåŸå§‹è¨Šæ¯
                return log_message[:max_length]

            # æ ¼å¼åŒ– JSON æ¬„ä½
            formatted_lines = []
            field_count = 0

            for key, value in log_data.items():
                if field_count >= max_fields:
                    formatted_lines.append(f"... (é‚„æœ‰ {len(log_data) - max_fields} å€‹æ¬„ä½)")
                    break

                # å°‡å€¼è½‰æ›ç‚ºå­—ä¸²
                if isinstance(value, (dict, list)):
                    value_str = json.dumps(value, ensure_ascii=False)
                else:
                    value_str = str(value)

                # é™åˆ¶é•·åº¦
                if len(value_str) > max_length:
                    value_str = value_str[:max_length] + "..."

                # æ ¼å¼åŒ–ç‚ºéµå€¼å°
                formatted_lines.append(f"**{key}**: {value_str}")
                field_count += 1

            return "<br>".join(formatted_lines)

        except json.JSONDecodeError:
            # é JSON æ ¼å¼ï¼Œé¡¯ç¤ºåŸå§‹è¨Šæ¯ï¼ˆé™åˆ¶é•·åº¦ï¼‰
            if len(log_message) > max_length:
                return log_message[:max_length] + "..."
            return log_message

    def extract_service_name(self, alarm_details: Dict) -> str:
        """å¾ alarm è³‡è¨Šä¸­æå–æœå‹™åç¨±"""
        alarm_name = alarm_details['alarm_name']
        namespace = alarm_details.get('namespace', '')

        # å˜—è©¦å¾ Lambda function name æå–
        function_name = self.extract_lambda_function(alarm_details)
        if function_name:
            # production-list-k8s-sensor-events -> list-k8s-sensor-events
            return function_name.replace('production-', '')

        # å¾ alarm name æå–
        if 'vcs-inventory' in alarm_name:
            return 'vcs-inventory'
        elif 'vcs-public-api' in alarm_name:
            return 'vcs-public-api'
        elif 'container-security' in namespace:
            return 'container-security'
        elif 'Synthetics' in alarm_name:
            return 'monitor'

        return 'unknown'

    def classify_alarm_status(self, alarm_details: Dict, error_logs: List[Dict],
                            recovery_info: Optional[Dict] = None) -> tuple[str, str]:
        """
        æ™ºèƒ½åˆ¤æ–· alarm ç‹€æ…‹

        Returns:
            (status_emoji, status_reason) - ä¾‹å¦‚ ('âœ… recovered', 'å·²è‡ªå‹•æ¢å¾©')
        """
        alarm_name = alarm_details.get('alarm_name', '')

        # 1. æª¢æŸ¥æ˜¯å¦å·²æ¢å¾©
        if recovery_info and recovery_info.get('recovered'):
            recovery_time = recovery_info['recovery_time'].astimezone(ZoneInfo('Asia/Taipei'))
            return 'âœ… recovered', f"å·²æ–¼ {recovery_time.strftime('%H:%M')} æ¢å¾©"

        # 2. æª¢æŸ¥æ˜¯å¦ç‚ºå¯å¿½ç•¥çš„å®¢æˆ¶ç’°å¢ƒå•é¡Œ
        if error_logs:
            log_text = ' '.join([log['message'] for log in error_logs[:20]])

            # å®¢æˆ¶ç’°å¢ƒå•é¡Œæ¨¡å¼
            ignore_patterns = [
                ('Stack.*does not exist', 'å®¢æˆ¶æœªå®‰è£è©²åŠŸèƒ½'),
                ('Stack with id.*does not exist', 'å®¢æˆ¶ CloudFormation Stack ä¸å­˜åœ¨'),
                ('AWS account is in failed state', 'å®¢æˆ¶å¸³è™Ÿç‹€æ…‹ç•°å¸¸'),
                ('Container protection.*not enabled', 'å®¢æˆ¶æœªå•Ÿç”¨å®¹å™¨é˜²è­·'),
                ('No accounts found to scan', 'ç„¡å¯æƒæå¸³è™Ÿ')
            ]

            import re
            for pattern, reason in ignore_patterns:
                if re.search(pattern, log_text, re.IGNORECASE):
                    return 'ğŸŸ¡ ignore', reason

        # 3. æª¢æŸ¥åš´é‡æ€§ - Timeout + å¤–éƒ¨æœå‹™
        if error_logs:
            log_text = ' '.join([log['message'] for log in error_logs[:50]])

            # Critical: Lambda timeout + å¤–éƒ¨ API/æœå‹™
            if 'timed out' in log_text.lower() or 'timeout' in log_text.lower():
                if any(keyword in log_text.lower() for keyword in ['xlogr', 'api', 'http', 'request']):
                    return 'ğŸ”´ critical', 'Lambda timeout å‘¼å«å¤–éƒ¨æœå‹™'
                return 'ğŸŸ  high', 'Lambda execution timeout'

            # High: Exception/Error
            if any(keyword in log_text for keyword in ['Exception', 'ERROR', 'Failed', 'error']):
                return 'ğŸŸ  high', 'åŸ·è¡ŒéŒ¯èª¤'

        # 4. é è¨­: éœ€è¦ç›£æ§
        return 'âš ï¸ monitor', 'éœ€è¦äººå·¥ç¢ºèª'

    def extract_key_errors(self, error_logs: List[Dict], max_errors: int = 5) -> List[str]:
        """
        å¾å¤§é‡æ—¥èªŒä¸­æå–é—œéµéŒ¯èª¤è¨Šæ¯

        Args:
            error_logs: å®Œæ•´çš„éŒ¯èª¤æ—¥èªŒåˆ—è¡¨
            max_errors: æœ€å¤šè¿”å›å¹¾æ¢é—œéµéŒ¯èª¤

        Returns:
            é—œéµéŒ¯èª¤è¨Šæ¯åˆ—è¡¨
        """
        if not error_logs:
            return []

        key_errors = []
        seen_errors = set()

        # éŒ¯èª¤é—œéµå­—å„ªå…ˆç´š
        error_keywords = [
            ('timed out', 'â±ï¸'),
            ('Exception', 'âŒ'),
            ('ERROR', 'âŒ'),
            ('Failed', 'âŒ'),
            ('does not exist', 'âš ï¸'),
            ('error', 'âš ï¸'),
            ('warning', 'âš ï¸')
        ]

        import re

        for log in error_logs:
            message = log.get('message', '')

            # è·³éå¤ªé•·æˆ–å¤ªçŸ­çš„è¨Šæ¯
            if len(message) < 20 or len(message) > 500:
                continue

            # æª¢æŸ¥æ˜¯å¦åŒ…å«éŒ¯èª¤é—œéµå­—
            for keyword, emoji in error_keywords:
                if keyword.lower() in message.lower():
                    # æå–é—œéµéƒ¨åˆ† (å‰ 200 å­—å…ƒ)
                    error_snippet = message[:200].strip()

                    # å»é‡ï¼šé¿å…é‡è¤‡çš„éŒ¯èª¤
                    error_hash = hash(error_snippet[:100])
                    if error_hash not in seen_errors:
                        seen_errors.add(error_hash)
                        key_errors.append(f"{emoji} `{error_snippet}`")

                        if len(key_errors) >= max_errors:
                            return key_errors
                    break

        # å¦‚æœæ²’æœ‰æ‰¾åˆ°é—œéµéŒ¯èª¤ï¼Œè¿”å›å‰å¹¾æ¢æ—¥èªŒ
        if not key_errors and error_logs:
            for log in error_logs[:max_errors]:
                message = log.get('message', '')[:200].strip()
                if message:
                    key_errors.append(f"ğŸ“ `{message}`")

        return key_errors

    def identify_backend_service(self, error_logs: List[Dict]) -> Optional[str]:
        """
        å¾éŒ¯èª¤æ—¥èªŒæ¨æ–·å¾Œç«¯æœå‹™

        Args:
            error_logs: éŒ¯èª¤æ—¥èªŒåˆ—è¡¨

        Returns:
            å¾Œç«¯æœå‹™åç¨±ï¼Œè‹¥ç„¡æ³•è­˜åˆ¥å‰‡è¿”å› None
        """
        if not error_logs:
            return None

        log_text = ' '.join([log.get('message', '') for log in error_logs[:50]])

        # å¾Œç«¯æœå‹™è­˜åˆ¥æ¨¡å¼ (æŒ‰å„ªå…ˆé †åºæ’åˆ—ï¼Œæ ¹æ“šå¯¦éš›äººå·¥æ—¥èªŒèª¿æ•´)
        backend_patterns = [
            # BigTable (Google Cloud Bigtable) - äººå·¥æ—¥èªŒä¸­å¸¸è¦‹ "BT timeout", "Bigtable timeout"
            ('BigTable', ['bigtable', 'BT ', 'gcp.bigtable', 'google.bigtable', 'cloud.google.com/bigtable',
                         'query mgmt scope', 'mgmtscope']),  # mgmt scope queries é€šå¸¸æŒ‡å‘ BigTable

            # xdl/xlogr (Log forwarder service) - äººå·¥æ—¥èªŒ: "xdl timed out"
            ('xdl', ['xlogr', 'xdl', 'log-forwarder', 'xlogr-ase1', 'xlogr-ec1', 'xlogr-use1',
                    'xlogr-sg']),

            # SP (Service Provider / internal-api) - äººå·¥æ—¥èªŒ: "SP issue"
            ('SP (internal-api)', ['internal-api.xdr', 'internal-api-eu', 'internal-api-ap',
                                   'internal-api-sg', '/fbt/internal', 'SP API', 'SP issue']),

            # CAM API (Cloud Account Management)
            ('CAM API', ['/cam/external', 'cam/api', '/external/cam', 'GetAWSTemporaryCredential']),

            # Synthetics/Canary monitoring
            ('Synthetics', ['synthetics', 'canary', 'APIHealth']),

            # XDR API endpoints
            ('XDR API', ['api.xdr.trendmicro.com', 'api.eu.xdr', 'api.ap.xdr']),
        ]

        for service_name, patterns in backend_patterns:
            if any(pattern.lower() in log_text.lower() for pattern in patterns):
                return service_name

        return None

    def correlate_alarms(self, alarms: List[Dict]) -> List[Dict]:
        """
        é—œè¯ä¸¦åˆ†çµ„ç›¸é—œçš„ alarms

        Args:
            alarms: Alarm åˆ—è¡¨ï¼Œæ¯å€‹åŒ…å« alarm_name, timestamp, service, region ç­‰

        Returns:
            åˆ†çµ„å¾Œçš„ alarm åˆ—è¡¨ï¼Œç›¸é—œçš„ alarms æœƒè¢«åˆä½µ
        """
        if not alarms:
            return []

        # å»ºç«‹é—œè¯ç¾¤çµ„
        groups = []
        used_indices = set()

        for i, alarm1 in enumerate(alarms):
            if i in used_indices:
                continue

            # å»ºç«‹æ–°ç¾¤çµ„
            group = {
                'primary_alarm': alarm1,
                'related_alarms': [],
                'correlation_reason': []
            }

            # å°‹æ‰¾ç›¸é—œçš„ alarms
            for j, alarm2 in enumerate(alarms):
                if i == j or j in used_indices:
                    continue

                # æª¢æŸ¥é—œè¯æ¢ä»¶
                reasons = self._check_alarm_correlation(alarm1, alarm2)
                if reasons:
                    group['related_alarms'].append(alarm2)
                    group['correlation_reason'].extend(reasons)
                    used_indices.add(j)

            used_indices.add(i)
            groups.append(group)

        return groups

    def _check_alarm_correlation(self, alarm1: Dict, alarm2: Dict) -> List[str]:
        """
        æª¢æŸ¥å…©å€‹ alarms æ˜¯å¦ç›¸é—œ

        Returns:
            ç›¸é—œåŸå› åˆ—è¡¨ï¼Œè‹¥ä¸ç›¸é—œå‰‡è¿”å›ç©ºåˆ—è¡¨
        """
        reasons = []

        # æ¢ä»¶ 1: æ™‚é–“æ¥è¿‘ (Â±5åˆ†é˜å…§)
        time1 = alarm1.get('first_occurrence', {}).get('timestamp')
        time2 = alarm2.get('first_occurrence', {}).get('timestamp')

        if time1 and time2:
            time_diff = abs((time1 - time2).total_seconds())
            if time_diff <= 300:  # 5åˆ†é˜
                reasons.append('åŒæ™‚è§¸ç™¼')

        # æ¢ä»¶ 2: ç›¸åŒæœå‹™
        service1 = alarm1.get('service_name', '')
        service2 = alarm2.get('service_name', '')

        # æå–æœå‹™åŸºç¤åç¨± (ç§»é™¤ region-specific éƒ¨åˆ†)
        base_service1 = service1.split('-')[0] if service1 else ''
        base_service2 = service2.split('-')[0] if service2 else ''

        if base_service1 and base_service1 == base_service2:
            reasons.append('ç›¸åŒæœå‹™')

        # æ¢ä»¶ 3: ç›¸åŒå€åŸŸ
        if alarm1.get('region') == alarm2.get('region'):
            reasons.append('ç›¸åŒå€åŸŸ')

        # æ¢ä»¶ 4: ç›¸ä¼¼çš„éŒ¯èª¤æ¨¡å¼
        desc1 = alarm1.get('description', '').lower()
        desc2 = alarm2.get('description', '').lower()

        # æª¢æŸ¥æ˜¯å¦éƒ½æ˜¯ timeout éŒ¯èª¤
        if 'timeout' in desc1 and 'timeout' in desc2:
            reasons.append('ç›¸åŒéŒ¯èª¤é¡å‹(timeout)')

        # æª¢æŸ¥æ˜¯å¦éƒ½æ˜¯ 5XX éŒ¯èª¤
        if '5xx' in desc1 and '5xx' in desc2:
            reasons.append('ç›¸åŒéŒ¯èª¤é¡å‹(5XX)')

        # åªæœ‰åœ¨è‡³å°‘æœ‰ 2 å€‹é—œè¯æ¢ä»¶æ™‚æ‰èªç‚ºç›¸é—œ
        if len(reasons) >= 2:
            return reasons
        return []

    def extract_customer_impact(self, error_logs: List[Dict]) -> Dict:
        """
        å¾éŒ¯èª¤æ—¥èªŒä¸­æå–å—å½±éŸ¿çš„å®¢æˆ¶è³‡è¨Š

        Args:
            error_logs: éŒ¯èª¤æ—¥èªŒåˆ—è¡¨

        Returns:
            {
                'customer_ids': List[str],  # å—å½±éŸ¿çš„å®¢æˆ¶ ID åˆ—è¡¨
                'count': int,               # å—å½±éŸ¿å®¢æˆ¶æ•¸é‡
                'summary': str              # æ‘˜è¦æ–‡å­—
            }
        """
        import re

        if not error_logs:
            return {'customer_ids': [], 'count': 0, 'summary': ''}

        customer_ids = set()

        # Customer ID è­˜åˆ¥æ¨¡å¼ (UUID format) - æ ¹æ“šå¯¦éš›æ—¥èªŒæ ¼å¼å„ªåŒ–
        customer_id_patterns = [
            # JSON æ ¼å¼: "customerId": "uuid" æˆ– "customerId":"uuid"
            r'["\']?customerId["\']?\s*[:=]\s*["\']?([0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12})["\']?',
            r'["\']?customerID["\']?\s*[:=]\s*["\']?([0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12})["\']?',
            r'["\']?customer_id["\']?\s*[:=]\s*["\']?([0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12})["\']?',

            # v1BusinessID format
            r'v1BusinessID["\']?\s*[:=]\s*["\']?([0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12})["\']?',

            # ç›´æ¥åœ¨æ—¥èªŒä¸­çš„ UUID (æ›´å¯¬é¬†çš„æ¨¡å¼ï¼Œç”¨æ–¼äººå·¥æ—¥èªŒä¸­ç›´æ¥è²¼ä¸Šçš„ ID)
            # ä¾‹å¦‚: customerId: d61a1eb5-a61d-4978-b7c0-17112f545f17
            r'customer[A-Za-z]*\s*[:\s]+([0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12})',
        ]

        for log in error_logs:
            message = log.get('message', '')
            for pattern in customer_id_patterns:
                matches = re.findall(pattern, message, re.IGNORECASE)
                customer_ids.update(matches)

        customer_list = sorted(list(customer_ids))
        count = len(customer_list)

        # ç”Ÿæˆæ‘˜è¦
        if count == 0:
            summary = ''
        elif count == 1:
            summary = f"å½±éŸ¿ 1 ä½å®¢æˆ¶ ({customer_list[0][:8]}...)"
        elif count <= 3:
            preview = ', '.join([cid[:8] + '...' for cid in customer_list])
            summary = f"å½±éŸ¿ {count} ä½å®¢æˆ¶ ({preview})"
        else:
            preview = ', '.join([cid[:8] + '...' for cid in customer_list[:3]])
            summary = f"å½±éŸ¿ {count} ä½å®¢æˆ¶ ({preview}, ...)"

        return {
            'customer_ids': customer_list,
            'count': count,
            'summary': summary
        }

    def extract_error_context(self, error_logs: List[Dict], status: str,
                            alarm_details: Dict) -> str:
        """
        å¾ error logs ä¸­æ™ºèƒ½æå– DRI éœ€è¦çš„é—œéµè³‡è¨Š

        Args:
            error_logs: å®Œæ•´çš„éŒ¯èª¤æ—¥èªŒåˆ—è¡¨
            status: Alarm ç‹€æ…‹ï¼ˆåŒ…å« emoji å’ŒåŸå› ï¼‰
            alarm_details: Alarm è©³ç´°è³‡è¨Š

        Returns:
            æ›´å…·é«”çš„éŒ¯èª¤æè¿°
        """
        import re

        if not error_logs:
            return "ç„¡æ³•å–å¾—éŒ¯èª¤æ—¥èªŒ"

        # åˆä½µæ‰€æœ‰ log messages ç”¨æ–¼åˆ†æ
        log_text = ' '.join([log.get('message', '') for log in error_logs[:50]])
        alarm_name = alarm_details.get('alarm_name', '')

        # 1. Timeout éŒ¯èª¤ - æå–å‘¼å«çš„ API/Service
        if 'timeout' in status.lower() or 'timed out' in log_text.lower():
            # é¦–å…ˆè­˜åˆ¥å¾Œç«¯æœå‹™
            backend_service = self.identify_backend_service(error_logs)

            # æ‰¾å‡º timeout å‰çš„ HTTP request
            http_patterns = [
                r'https?://([a-zA-Z0-9\.-]+(?:\.[a-zA-Z]{2,})?)',  # URL
                r'calling\s+([a-zA-Z0-9\.-]+)',                    # calling <service>
                r'request\s+to\s+([a-zA-Z0-9\.-]+)',               # request to <service>
                r'fetch(?:ing)?\s+from\s+([a-zA-Z0-9\.-]+)',       # fetch from <service>
                r'connect(?:ing)?\s+to\s+([a-zA-Z0-9\.-]+)'        # connect to <service>
            ]

            api_endpoint = None
            for pattern in http_patterns:
                matches = re.findall(pattern, log_text, re.IGNORECASE)
                if matches:
                    # å–ç¬¬ä¸€å€‹åŒ¹é…çš„æœå‹™
                    service = matches[0]
                    # éæ¿¾æ‰å…§éƒ¨æœå‹™ (IP, localhost, etc.)
                    if not re.match(r'^\d+\.\d+\.\d+\.\d+', service) and 'localhost' not in service:
                        api_endpoint = service
                        break

            # çµ„åˆæè¿°ï¼šå„ªå…ˆä½¿ç”¨å¾Œç«¯æœå‹™ï¼Œå†è£œå…… API endpoint
            if backend_service and api_endpoint:
                return f"{backend_service} timeout (via {api_endpoint})"
            elif backend_service:
                return f"{backend_service} timeout"
            elif api_endpoint:
                return f"Lambda timeout å‘¼å« {api_endpoint}"

            # æª¢æŸ¥æ˜¯å¦ç‚ºè³‡æ–™åº« timeout
            if 'database' in log_text.lower() or 'db' in log_text.lower():
                db_info = self._extract_database_info(log_text)
                if db_info:
                    return f"Lambda timeout æŸ¥è©¢ {db_info}"

            # é è¨­
            return "Lambda timeout (ç„¡æ³•è­˜åˆ¥ç›®æ¨™æœå‹™)"

        # 2. Database éŒ¯èª¤ - æå–è³‡æ–™åº«è³‡è¨Š
        if any(keyword in log_text.lower() for keyword in ['database', 'db', 'connection', 'postgres', 'mysql']):
            db_info = self._extract_database_info(log_text)
            if db_info:
                # åˆ¤æ–·éŒ¯èª¤é¡å‹
                if 'failed to connect' in log_text.lower() or 'connection refused' in log_text.lower():
                    return f"è³‡æ–™åº«é€£ç·šå¤±æ•—: {db_info}"
                elif 'too many connections' in log_text.lower():
                    return f"è³‡æ–™åº«é€£ç·šæ•¸è¶…é™: {db_info}"
                elif 'timeout' in log_text.lower():
                    return f"è³‡æ–™åº«æŸ¥è©¢ timeout: {db_info}"
                else:
                    return f"è³‡æ–™åº«éŒ¯èª¤: {db_info}"

        # 3. API Gateway éŒ¯èª¤ - æå– request path
        if 'API-Gateway' in alarm_name or 'apigateway' in alarm_name.lower():
            # å˜—è©¦å¾ logs æå– request path
            path_match = re.search(r'(?:Method Request path|resource path):\s*([^\s,]+)', log_text, re.IGNORECASE)
            if path_match:
                path = path_match.group(1)

                # æ‰¾å‡º status code
                status_match = re.search(r'Status:\s*(\d{3})', log_text)
                if status_match:
                    status_code = status_match.group(1)
                    return f"API {status_code} éŒ¯èª¤: {path}"

                return f"API Gateway éŒ¯èª¤: {path}"

            # æª¢æŸ¥æ˜¯å¦ç‚º authorizer éŒ¯èª¤
            if 'authorizer' in log_text.lower():
                if 'timeout' in log_text.lower():
                    return "Lambda Authorizer timeout"
                elif 'failed' in log_text.lower():
                    return "Lambda Authorizer é©—è­‰å¤±æ•—"
                return "Lambda Authorizer éŒ¯èª¤"

        # 4. å¤–éƒ¨ API éŒ¯èª¤ (401, 502, etc.)
        status_code_match = re.search(r'(\d{3})\s+(\d{3}\s+)?([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)', log_text)
        if status_code_match:
            status_code = status_code_match.group(1)
            error_name = status_code_match.group(3) if status_code_match.group(3) else ''

            # å˜—è©¦æ‰¾å‡ºæœå‹™åç¨±
            service_patterns = [
                r'([a-zA-Z0-9-]+)\s+(?:API|api|service)',
                r'request.*?to\s+([a-zA-Z0-9-]+)',
            ]

            service_name = None
            for pattern in service_patterns:
                match = re.search(pattern, log_text, re.IGNORECASE)
                if match:
                    service_name = match.group(1)
                    break

            # æ¨æ¸¬åŸå› 
            reason = self._guess_error_reason(status_code, error_name)

            if service_name:
                return f"API {status_code} {error_name}: {service_name} ({reason})"
            else:
                return f"API {status_code} {error_name} ({reason})"

        # 5. CloudFormation éŒ¯èª¤ - æå– stack name
        if 'cloudformation' in log_text.lower() or 'stack' in log_text.lower():
            stack_match = re.search(r'Stack.*?([A-Za-z0-9_-]{10,})', log_text, re.IGNORECASE)
            if stack_match:
                stack_name = stack_match.group(1)
                if 'does not exist' in log_text.lower():
                    return f"CloudFormation Stack ä¸å­˜åœ¨: {stack_name}"
                elif 'access denied' in log_text.lower() or 'permission' in log_text.lower():
                    return f"CloudFormation æ¬Šé™ä¸è¶³: {stack_name}"
                return f"CloudFormation éŒ¯èª¤: {stack_name}"

        # 6. SQS éŒ¯èª¤
        if 'sqs' in log_text.lower():
            # æå– operation
            operation_match = re.search(r'SQS:\s*([A-Za-z]+)', log_text)
            operation = operation_match.group(1) if operation_match else 'Operation'

            # å˜—è©¦æ‰¾å‡º queue name
            queue_match = re.search(r'queue[:/\s]+([a-zA-Z0-9_-]+)', log_text, re.IGNORECASE)
            if queue_match:
                queue_name = queue_match.group(1)
                return f"SQS {operation} å¤±æ•—: {queue_name}"

            return f"SQS {operation} å¤±æ•—"

        # 7. å…¶ä»–åŸ·è¡ŒéŒ¯èª¤ - å˜—è©¦æå–é—œéµéŒ¯èª¤è¨Šæ¯
        error_patterns = [
            r'error:\s*(.{20,100})',
            r'failed to\s+(.{20,100})',
            r'exception:\s*(.{20,100})'
        ]

        for pattern in error_patterns:
            match = re.search(pattern, log_text, re.IGNORECASE)
            if match:
                error_msg = match.group(1).strip()
                # æ¸…ç†éé•·çš„éŒ¯èª¤è¨Šæ¯
                if len(error_msg) > 80:
                    error_msg = error_msg[:77] + '...'
                return error_msg

        # é è¨­ï¼šè¿”å›ç‹€æ…‹åŸå› 
        if 'Lambda timeout' in status or 'timeout' in status.lower():
            return "Lambda timeout"
        elif 'error' in status.lower() or 'failed' in status.lower():
            return "åŸ·è¡ŒéŒ¯èª¤"

        return "æœªçŸ¥éŒ¯èª¤"

    def _extract_database_info(self, log_text: str) -> Optional[str]:
        """
        å¾ log ä¸­æå–è³‡æ–™åº«è³‡è¨Š

        Returns:
            æ ¼å¼: "database_name @ endpoint" æˆ– "database_name"
        """
        import re

        # æå– database name
        db_name_match = re.search(r'database[=:\s]+[`"\']?([a-zA-Z0-9_-]+)[`"\']?', log_text, re.IGNORECASE)
        db_name = db_name_match.group(1) if db_name_match else None

        # æå– endpoint (IP:port æˆ– hostname)
        endpoint_patterns = [
            r'(\d+\.\d+\.\d+\.\d+:\d+)',  # IP:port
            r'([a-zA-Z0-9-]+\.proxy[a-zA-Z0-9-]*)',  # proxy hostname
            r'([a-zA-Z0-9-]+-proxy-[a-zA-Z0-9-]+)'  # proxy pattern
        ]

        endpoint = None
        for pattern in endpoint_patterns:
            match = re.search(pattern, log_text)
            if match:
                endpoint = match.group(1)
                break

        # çµ„åˆçµæœ
        if db_name and endpoint:
            return f"{db_name} @ {endpoint}"
        elif db_name:
            return db_name
        elif endpoint:
            return f"database @ {endpoint}"

        return None

    def _guess_error_reason(self, status_code: str, error_name: str) -> str:
        """æ ¹æ“š status code æ¨æ¸¬å¯èƒ½çš„åŸå› """
        reason_map = {
            '400': 'è«‹æ±‚æ ¼å¼éŒ¯èª¤',
            '401': 'èªè­‰å¤±æ•—',
            '403': 'æ¬Šé™ä¸è¶³',
            '404': 'è³‡æºä¸å­˜åœ¨',
            '408': 'è«‹æ±‚ timeout',
            '429': 'Rate limit è¶…é™',
            '500': 'ä¼ºæœå™¨å…§éƒ¨éŒ¯èª¤',
            '502': 'å¾Œç«¯æœå‹™ç•°å¸¸',
            '503': 'æœå‹™ä¸å¯ç”¨',
            '504': 'Gateway timeout'
        }

        return reason_map.get(status_code, 'æœªçŸ¥åŸå› ')

    def get_alarm_trend(self, alarm_name: str, region: str, days: int = 7) -> Dict:
        """
        ç²å– alarm çš„è¶¨å‹¢åˆ†æï¼ˆéå» N å¤©çš„è§¸ç™¼æ¬¡æ•¸ï¼‰

        Args:
            alarm_name: Alarm åç¨±
            region: AWS region
            days: æŸ¥è©¢å¤©æ•¸ï¼ˆé è¨­ 7 å¤©ï¼‰

        Returns:
            {
                'trigger_count': int,  # è§¸ç™¼æ¬¡æ•¸
                'trend_emoji': str,    # ğŸ“ˆ ğŸ“Š ğŸ“‰
                'trend_text': str,     # "12 æ¬¡/é€±"
                'severity': str        # 'critical' | 'high' | 'medium' | 'low'
            }
        """
        try:
            cloudwatch = self.clients[region]['cloudwatch']
            now = datetime.now(ZoneInfo('UTC'))
            start_date = now - timedelta(days=days)

            # æŸ¥è©¢éå» N å¤©çš„ alarm history
            response = cloudwatch.describe_alarm_history(
                AlarmName=alarm_name,
                StartDate=start_date,
                EndDate=now,
                HistoryItemType='StateUpdate',
                MaxRecords=100  # æœ€å¤šæŸ¥ 100 æ¬¡
            )

            # çµ±è¨ˆé€²å…¥ ALARM ç‹€æ…‹çš„æ¬¡æ•¸
            trigger_count = 0
            for item in response.get('AlarmHistoryItems', []):
                history_data = json.loads(item['HistoryData'])
                if history_data.get('newState', {}).get('stateValue') == 'ALARM':
                    trigger_count += 1

            # è¨ˆç®—è¶¨å‹¢
            if trigger_count >= 15:
                trend_emoji = 'ğŸ“ˆ'
                severity = 'critical'
            elif trigger_count >= 5:
                trend_emoji = 'ğŸ“Š'
                severity = 'high'
            elif trigger_count >= 1:
                trend_emoji = 'ğŸ“‰'
                severity = 'medium'
            else:
                trend_emoji = 'âœ…'
                severity = 'low'

            trend_text = f"{trigger_count} æ¬¡/{days}å¤©"

            return {
                'trigger_count': trigger_count,
                'trend_emoji': trend_emoji,
                'trend_text': trend_text,
                'severity': severity
            }

        except Exception as e:
            print(f"    æŸ¥è©¢è¶¨å‹¢æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return {
                'trigger_count': 0,
                'trend_emoji': 'â“',
                'trend_text': 'ç„¡æ³•å–å¾—',
                'severity': 'unknown'
            }

    def generate_action_items(self, alarm_details: Dict, status: str, description: str,
                             note: str, error_logs: List[Dict]) -> List[str]:
        """
        æ ¹æ“š alarm è³‡è¨Šè‡ªå‹•ç”Ÿæˆå¯åŸ·è¡Œçš„ Action Items

        Args:
            alarm_details: Alarm è©³ç´°è³‡è¨Š
            status: Alarm ç‹€æ…‹ï¼ˆğŸ”´ critical, ğŸŸ  high, etc.ï¼‰
            description: æè¿°æ–‡å­—
            note: AI å»ºè­°
            error_logs: éŒ¯èª¤æ—¥èªŒ

        Returns:
            Action items åˆ—è¡¨
        """
        actions = []
        alarm_name = alarm_details.get('alarm_name', '')
        log_text = ' '.join([log['message'] for log in error_logs[:20]]) if error_logs else ''

        # æ ¹æ“šç‹€æ…‹ç”Ÿæˆ actions
        if 'ğŸ”´ critical' in status or 'Lambda timeout å‘¼å«å¤–éƒ¨æœå‹™' in status:
            # Critical: Timeout + å¤–éƒ¨æœå‹™
            if 'xlogr' in log_text.lower():
                actions.append("æª¢æŸ¥ xlogr API æœå‹™å¥åº·åº¦å’Œå›æ‡‰æ™‚é–“")
                actions.append("é™ä½ Lambda HTTP client timeout è¨­å®šç‚º 10 ç§’")
                actions.append("å¯¦ä½œ circuit breaker æ©Ÿåˆ¶é¿å…ç´šè¯æ•…éšœ")
            elif 'timeout' in description.lower():
                actions.append(f"æª¢æŸ¥ {alarm_name} Lambda çš„ timeout è¨­å®š")
                actions.append("åˆ†æ Lambda CloudWatch Logs ç¢ºèªæ…¢æŸ¥è©¢æˆ–å¤–éƒ¨æœå‹™å»¶é²")
                actions.append("è€ƒæ…®å¢åŠ  timeout æˆ–å„ªåŒ–ç¨‹å¼é‚è¼¯")

        elif 'ğŸŸ¡ ignore' in status:
            # Ignore: å®¢æˆ¶ç’°å¢ƒå•é¡Œ
            if 'å®¢æˆ¶æœªå®‰è£' in status or 'Stack.*does not exist' in log_text:
                actions.append("ç¢ºèªæ­¤ alarm æ˜¯å¦ç‚ºæ­£å¸¸çš„å®¢æˆ¶ç’°å¢ƒå·®ç•°")
                actions.append("è€ƒæ…®èª¿æ•´ alarm è§¸ç™¼æ¢ä»¶ï¼Œæ’é™¤æœªå®‰è£æœå‹™çš„å¸³è™Ÿ")

        elif 'ğŸŸ  high' in status:
            # High: åŸ·è¡ŒéŒ¯èª¤
            if 'database' in log_text.lower() or 'db' in log_text.lower():
                actions.append("æª¢æŸ¥è³‡æ–™åº«é€£ç·šæ± ç‹€æ…‹å’Œé€£ç·šæ•¸")
                actions.append("ç¢ºèªè³‡æ–™åº« proxy å’Œ read-only endpoint å¥åº·åº¦")
            elif 'sqs' in log_text.lower():
                actions.append("æª¢æŸ¥ SQS queue å¥åº·åº¦å’Œè¨Šæ¯å †ç©æƒ…æ³")
                actions.append("ç¢ºèª SQS permissions å’Œ IAM role è¨­å®š")
            elif 'unauthorized' in log_text.lower() or '401' in log_text:
                actions.append("æª¢æŸ¥ API authentication token æˆ–æ†‘è­‰æœ‰æ•ˆæ€§")
                actions.append("ç¢ºèª service account æ¬Šé™è¨­å®š")

        # é€šç”¨ actions
        if not actions:
            actions.append(f"æŸ¥çœ‹ {alarm_name} çš„ CloudWatch Logs ç¢ºèªæ ¹æœ¬åŸå› ")
            actions.append("æª¢æŸ¥ç›¸é—œæœå‹™çš„å¥åº·åº¦å’Œä¾è³´é …ç‹€æ…‹")

        # æ·»åŠ è¶¨å‹¢ç›¸é—œçš„ action
        if status in ['ğŸ”´ critical', 'ğŸŸ  high']:
            actions.append("ç›£æ§æ­¤ alarm çš„è§¸ç™¼é »ç‡ï¼Œè©•ä¼°æ˜¯å¦éœ€è¦é€²ä¸€æ­¥è™•ç†")

        return actions

    def generate_report(self, start_time: datetime, end_time: datetime) -> str:
        """
        ç”Ÿæˆæ¯æ—¥å ±å‘Šï¼ˆConfluence è¡¨æ ¼æ ¼å¼ï¼‰
        """
        print(f"æ­£åœ¨åˆ†æ {start_time.astimezone(ZoneInfo('Asia/Taipei')).strftime('%Y-%m-%d %H:%M')} "
              f"åˆ° {end_time.astimezone(ZoneInfo('Asia/Taipei')).strftime('%Y-%m-%d %H:%M')} (TW) çš„ alarms...")

        # 1. ç²å– alarm history
        alarm_history = self.get_alarm_history(start_time, end_time)

        if not alarm_history:
            return f"âœ… åœ¨ {start_time.astimezone(ZoneInfo('Asia/Taipei')).strftime('%Y-%m-%d')} å·¥ä½œæ™‚æ®µå…§æ²’æœ‰è§¸ç™¼ä»»ä½• alarm"

        # 2. æŒ‰ alarm name åˆ†çµ„
        alarms_grouped = defaultdict(list)
        for item in alarm_history:
            alarms_grouped[item['alarm_name']].append(item)

        # 3. æ”¶é›†æ‰€æœ‰ alarm è³‡æ–™ï¼ˆç”¨æ–¼è¡¨æ ¼ç”Ÿæˆï¼‰
        table_rows = []

        print(f"\né–‹å§‹è™•ç† {len(alarms_grouped)} å€‹ alarms...")

        for alarm_name, occurrences in sorted(alarms_grouped.items(), key=lambda x: x[1][0]['timestamp']):
            print(f"\nè™•ç†: {alarm_name}")

            # ç²å– region (å¾ç¬¬ä¸€å€‹ occurrence)
            region = occurrences[0].get('region', 'us-east-1')

            # ç²å– alarm è©³ç´°è³‡è¨Š
            alarm_details = self.get_alarm_details(alarm_name, region)
            if not alarm_details:
                continue

            # ç”Ÿæˆ CloudWatch alarm é€£çµ
            alarm_url = self.get_cloudwatch_alarm_url(alarm_name, region)

            # æå–æœå‹™åç¨±
            service_name = self.extract_service_name(alarm_details)

            # å˜—è©¦ç²å– log group (Lambda æˆ– custom metrics)
            log_group_name = None
            function_name = self.extract_lambda_function(alarm_details)

            if function_name:
                log_group_name = f'/aws/lambda/{function_name}'
            else:
                # å˜—è©¦å¾ custom metric æå– log groupï¼ˆåŒ…å« API Gatewayï¼‰
                log_group_name = self.extract_log_group_from_custom_metric(alarm_details, region)

            # ç²å–ç¬¬ä¸€æ¬¡å’Œæœ€å¾Œä¸€æ¬¡è§¸ç™¼æ™‚é–“
            first_occurrence = min(occurrences, key=lambda x: x['timestamp'])
            last_occurrence = max(occurrences, key=lambda x: x['timestamp'])

            # ç”Ÿæˆè§¸ç™¼æ™‚é–“åˆ—è¡¨
            alarm_times = []
            for occurrence in sorted(occurrences, key=lambda x: x['timestamp']):
                tw_time = occurrence['timestamp'].astimezone(ZoneInfo('Asia/Taipei'))
                alarm_times.append(tw_time.strftime('%H:%M'))

            # åˆå§‹åŒ–è®Šæ•¸
            description = ""
            note = ""
            error_logs = []
            complete_execution_logs = []
            recovery_info = None
            log_start = None
            log_end = None

            # ç‰¹æ®Šè™•ç†: Synthetics alarm æª¢æŸ¥æ¢å¾©ç‹€æ…‹
            is_synthetics_alarm = 'Synthetics' in alarm_name
            if is_synthetics_alarm:
                print(f"  æª¢æŸ¥ Synthetics alarm æ¢å¾©ç‹€æ…‹...", end=' ', flush=True)
                recovery_info = self.check_alarm_recovery(
                    alarm_name,
                    region,
                    first_occurrence['timestamp'],
                    check_window_hours=2
                )

                if recovery_info and recovery_info.get('recovered'):
                    recovery_time = recovery_info['recovery_time'].astimezone(ZoneInfo('Asia/Taipei'))
                    print(f"å·²æ¢å¾©æ–¼ {recovery_time.strftime('%H:%M')}")
                    description = f"å·²æ–¼ {recovery_time.strftime('%H:%M')} æ¢å¾©"
                else:
                    print("å°šæœªæ¢å¾©")

            # è™•ç†æ—¥èªŒåˆ†æï¼ˆå¦‚æœæœªæ¢å¾©æˆ–ä¸æ˜¯ Syntheticsï¼‰
            if log_group_name and not (recovery_info and recovery_info.get('recovered')):
                # æŸ¥è©¢éŒ¯èª¤æ—¥èªŒï¼ˆå‰å¾Œ 3 åˆ†é˜ï¼‰
                log_start = first_occurrence['timestamp'] - timedelta(minutes=3)
                log_end = last_occurrence['timestamp'] + timedelta(minutes=3)
                error_logs = self.get_error_logs(log_group_name, region, log_start, log_end, max_logs=100)

                # æª¢æŸ¥æ˜¯å¦ç‚º timeout éŒ¯èª¤
                is_timeout = any('timed out' in log['message'].lower() for log in error_logs)

                # æ”¶é›†å®Œæ•´çš„åŸ·è¡Œæ—¥èªŒ
                logs_to_collect = error_logs if error_logs else []

                if not logs_to_collect:
                    sample_start = first_occurrence['timestamp'] - timedelta(minutes=3)
                    sample_end = first_occurrence['timestamp'] + timedelta(minutes=3)
                    logs_to_collect = self.get_error_logs(log_group_name, region, sample_start, sample_end, max_logs=100, include_all=True)

                if logs_to_collect:
                    print(f"  æ­£åœ¨æ”¶é›†å®Œæ•´åŸ·è¡Œæ—¥èªŒ...", end=' ', flush=True)
                    complete_execution_logs = self.get_complete_execution_logs(
                        log_group_name, region, logs_to_collect, max_logs=500, is_timeout=is_timeout
                    )
                    print(f"æ”¶é›†åˆ° {len(complete_execution_logs)} æ¢")

                # AI åˆ†æ
                if self.ai_enabled and complete_execution_logs:
                    try:
                        print(f"  æ­£åœ¨é€²è¡Œ AI åˆ†æ...", end=' ', flush=True)
                        ai_analysis = self.ai_analyzer.analyze_alarm(
                            alarm_details,
                            complete_execution_logs,
                            region,
                            is_timeout=is_timeout if error_logs else False
                        )

                        if ai_analysis:
                            print("å®Œæˆ")
                            description = ai_analysis['root_cause']
                            note = ai_analysis['recommendation']
                        else:
                            print("å¤±æ•—")
                            description = "AI åˆ†æå¤±æ•—"
                    except Exception as e:
                        print(f"éŒ¯èª¤: {e}")
                        description = f"AI åˆ†æç•°å¸¸: {str(e)}"
                else:
                    # æ²’æœ‰ AI åˆ†æï¼Œä½¿ç”¨åŸºæœ¬æè¿°
                    if is_timeout:
                        description = "Lambda timeout"
                    elif error_logs:
                        description = "åŸ·è¡ŒéŒ¯èª¤"
            elif not log_group_name:
                description = "ç„¡æ³•è­˜åˆ¥ log group"

            # æ™ºèƒ½ç‹€æ…‹åˆ¤æ–·
            status_emoji, status_reason = self.classify_alarm_status(
                alarm_details,
                complete_execution_logs if complete_execution_logs else error_logs,
                recovery_info
            )

            # æ™ºèƒ½æå–éŒ¯èª¤ä¸Šä¸‹æ–‡ - ç‚º description æ·»åŠ æ›´å¤šç´°ç¯€
            if not description or description in ["Lambda timeout", "åŸ·è¡ŒéŒ¯èª¤", "AI åˆ†æå¤±æ•—"]:
                # ä½¿ç”¨æ™ºèƒ½æå–ä¾†æ”¹é€²æè¿°
                context_description = self.extract_error_context(
                    complete_execution_logs if complete_execution_logs else error_logs,
                    status_reason,
                    alarm_details
                )
                # å¦‚æœæœ‰ AI åˆ†æçµæœï¼Œä¿ç•™å®ƒï¼›å¦å‰‡ä½¿ç”¨æ™ºèƒ½æå–çš„çµæœ
                if not description or "AI åˆ†æå¤±æ•—" not in description:
                    description = context_description

            # æå–é—œéµéŒ¯èª¤
            key_errors = self.extract_key_errors(
                complete_execution_logs if complete_execution_logs else error_logs,
                max_errors=3
            )

            # æå–å®¢æˆ¶å½±éŸ¿åˆ†æ
            customer_impact = self.extract_customer_impact(
                complete_execution_logs if complete_execution_logs else error_logs
            )

            # ç”Ÿæˆ CloudWatch Logs é€£çµ
            logs_url = ""
            if log_group_name and log_start and log_end:
                logs_url = self.get_cloudwatch_logs_url(
                    log_group_name=log_group_name,
                    region=region,
                    start_time=log_start,
                    end_time=log_end,
                    filter_pattern="ERROR"
                )

            # ç”Ÿæˆ Grafana Dashboard é€£çµ
            grafana_url = None
            if log_start and log_end:
                grafana_url = self.get_grafana_dashboard_url(
                    service_name=service_name,
                    start_time=log_start,
                    end_time=log_end
                )

            # ç²å–è¶¨å‹¢åˆ†æ
            print(f"  æŸ¥è©¢è¶¨å‹¢...", end=' ', flush=True)
            trend_info = self.get_alarm_trend(alarm_name, region, days=7)
            print(f"{trend_info['trend_text']}")

            # ç”Ÿæˆ Action Items
            action_items = self.generate_action_items(
                alarm_details,
                status_emoji,
                description,
                note,
                complete_execution_logs if complete_execution_logs else error_logs
            )

            # æ§‹å»ºè¡¨æ ¼è¡Œè³‡æ–™
            table_row = {
                'alarm_name': alarm_name,
                'alarm_times': alarm_times,
                'alarm_url': alarm_url,
                'service': service_name,
                'status': status_emoji,
                'status_reason': status_reason,
                'description': description,
                'note': note,
                'key_errors': key_errors,
                'customer_impact': customer_impact,
                'logs_url': logs_url,
                'grafana_url': grafana_url,
                'region': region,
                'log_count': len(complete_execution_logs) if complete_execution_logs else len(error_logs),
                'trend_info': trend_info,
                'action_items': action_items
            }

            table_rows.append(table_row)

        # 4. ç”Ÿæˆ Markdown è¡¨æ ¼
        report_date = start_time.astimezone(ZoneInfo('Asia/Taipei')).strftime('%Y%m%d')
        report_lines = []

        # æ¨™é¡Œ
        report_lines.append(f"# DRI Diary - {report_date}")
        report_lines.append("")

        # çµ±è¨ˆæ‘˜è¦ï¼ˆå³æ™‚è¨ˆç®—ï¼‰
        total_alarms = len(table_rows)
        critical_count = sum(1 for row in table_rows if 'ğŸ”´ critical' in row['status'])
        recovered_count = sum(1 for row in table_rows if 'âœ… recovered' in row['status'])
        ignore_count = sum(1 for row in table_rows if 'ğŸŸ¡ ignore' in row['status'])

        report_lines.append("## ğŸ“Š æ‘˜è¦")
        report_lines.append(f"- **Total Alarms**: {total_alarms}")
        report_lines.append(f"- **Critical**: {critical_count}")
        report_lines.append(f"- **Recovered**: {recovered_count}")
        report_lines.append(f"- **Ignore** (å®¢æˆ¶ç’°å¢ƒ): {ignore_count}")
        report_lines.append("")

        # è¡¨æ ¼ header (å¢åŠ è¶¨å‹¢æ¬„ä½)
        report_lines.append("| Alarm | Service | Trend | Status | Description | Note |")
        report_lines.append("|-------|---------|-------|--------|-------------|------|")

        # è¡¨æ ¼å…§å®¹
        for row in table_rows:
            # Alarm æ¬„ä½: æ™‚é–“ + é€£çµ
            times_str = ' '.join(row['alarm_times'])
            alarm_cell = f"{times_str} [{row['alarm_name']}]({row['alarm_url']})"

            # Service æ¬„ä½
            service_cell = row['service']

            # Trend æ¬„ä½: emoji + æ–‡å­—
            trend_info = row.get('trend_info', {})
            trend_cell = f"{trend_info.get('trend_emoji', 'â“')}<br>{trend_info.get('trend_text', 'N/A')}"

            # Status æ¬„ä½: emoji + åŸå› 
            status_cell = f"{row['status']}<br>{row['status_reason']}"

            # Description æ¬„ä½: AI åˆ†æçµæœ æˆ–åŸºæœ¬æè¿°
            desc_cell = row['description'] if row['description'] else "-"

            # Note æ¬„ä½: AI å»ºè­° + å®¢æˆ¶å½±éŸ¿ + é—œéµéŒ¯èª¤ + Action Items + Logs é€£çµ
            note_parts = []

            # AI å»ºè­°
            if row['note']:
                note_parts.append(f"**å»ºè­°**: {row['note']}")

            # å®¢æˆ¶å½±éŸ¿ (å¦‚æœæœ‰)
            customer_impact = row.get('customer_impact', {})
            if customer_impact and customer_impact.get('count', 0) > 0:
                note_parts.append(f"<br>**å®¢æˆ¶å½±éŸ¿**: {customer_impact['summary']}")

            # é—œéµéŒ¯èª¤ (ç²¾ç°¡é¡¯ç¤º)
            if row['key_errors']:
                note_parts.append(f"<br>**é—œéµéŒ¯èª¤**:<br>{'<br>'.join(row['key_errors'])}")

            # Action Items
            if row.get('action_items'):
                action_list = '<br>'.join([f"- {item}" for item in row['action_items'][:3]])  # åªé¡¯ç¤ºå‰ 3 å€‹
                note_parts.append(f"<br>**Action Items**:<br>{action_list}")

            # é€£çµå€å¡Š
            links = []
            if row['logs_url']:
                links.append(f"[æŸ¥çœ‹æ—¥èªŒ ({row['log_count']} æ¢)]({row['logs_url']})")
            if row.get('grafana_url'):
                links.append(f"[Grafana Dashboard]({row['grafana_url']})")

            if links:
                note_parts.append(f"<br>{' | '.join(links)}")

            note_cell = ''.join(note_parts) if note_parts else "-"

            # ç”Ÿæˆè¡¨æ ¼è¡Œ
            report_lines.append(f"| {alarm_cell} | {service_cell} | {trend_cell} | {status_cell} | {desc_cell} | {note_cell} |")

        return "\n".join(report_lines)
    def run_daily_report(self, date: Optional[datetime] = None, save_to_file: bool = True):
        """
        åŸ·è¡Œæ¯æ—¥å ±å‘Š

        Args:
            date: æŒ‡å®šçš„æ—¥æœŸï¼ˆé è¨­ç‚ºä»Šå¤©ï¼‰
            save_to_file: æ˜¯å¦å°‡å ±å‘Šå„²å­˜åˆ°æª”æ¡ˆï¼ˆé è¨­ Trueï¼‰
        """
        import os

        start_time, end_time = self.get_tw_time_range(date)
        report = self.generate_report(start_time, end_time)
        print(report)

        # å„²å­˜åˆ°æª”æ¡ˆ
        if save_to_file:
            # å»ºç«‹ report ç›®éŒ„
            report_dir = 'report'
            os.makedirs(report_dir, exist_ok=True)

            # ä½¿ç”¨å°ç£æ™‚é–“çš„æ—¥æœŸä½œç‚ºæª”å
            report_date = start_time.astimezone(ZoneInfo('Asia/Taipei')).strftime('%Y%m%d')
            report_file = os.path.join(report_dir, f'{report_date}.md')

            with open(report_file, 'w', encoding='utf-8') as f:
                f.write(report)

            print(f"\nâœ… å ±å‘Šå·²å„²å­˜è‡³: {report_file}")

        return report


def main():
    """
    ä¸»ç¨‹å¼å…¥å£
    """
    import sys

    # é è¨­æŸ¥è©¢æ‰€æœ‰ production rings çš„ regions
    reporter = AlarmReporter()

    # æª¢æŸ¥æ˜¯å¦æœ‰å‘½ä»¤åˆ—åƒæ•¸æŒ‡å®šæ—¥æœŸ
    if len(sys.argv) > 1:
        # æ”¯æ´æ ¼å¼: python alarm_reporter.py 2025-10-17
        # æˆ–: python alarm_reporter.py 2025-10-16
        date_str = sys.argv[1]
        try:
            # è§£ææ—¥æœŸå­—ä¸² (æ ¼å¼: YYYY-MM-DD)
            from datetime import datetime
            parsed_date = datetime.strptime(date_str, '%Y-%m-%d')
            # è¨­å®šç‚ºå°ç£æ™‚å€
            specific_date = parsed_date.replace(tzinfo=ZoneInfo('Asia/Taipei'))
            print(f"æŸ¥è©¢æ—¥æœŸ: {date_str}\n")
            reporter.run_daily_report(specific_date)
        except ValueError:
            print(f"éŒ¯èª¤: æ—¥æœŸæ ¼å¼ä¸æ­£ç¢ºã€‚è«‹ä½¿ç”¨ YYYY-MM-DD æ ¼å¼ï¼Œä¾‹å¦‚: 2025-10-17")
            print(f"ç”¨æ³•: python alarm_reporter.py [YYYY-MM-DD]")
            print(f"ç¯„ä¾‹: python alarm_reporter.py 2025-10-16")
            sys.exit(1)
    else:
        # æ²’æœ‰åƒæ•¸ï¼ŒåŸ·è¡Œä»Šå¤©çš„å ±å‘Š
        print("æŸ¥è©¢æ—¥æœŸ: ä»Šå¤©\n")
        reporter.run_daily_report()


if __name__ == '__main__':
    main()
