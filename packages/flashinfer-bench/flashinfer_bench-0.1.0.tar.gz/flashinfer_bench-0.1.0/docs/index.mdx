---
title: FlashInfer Bench
description: Benchmarking and infrastructure for GPU kernel optimization.
---

# FlashInfer Bench

FlashInfer-Bench is a comprehensive benchmark and infrastructure designed to create a "virtuous cycle" where AI can automatically optimize and improve the core GPU kernels of the AI systems it runs on. It provides a systematic framework to identify performance bottlenecks, generate solutions, and deploy them immediately into production.

- Standardized Schema: Introduces "FlashInfer Trace," a standardized format to describe GPU kernel workloads, solutions, and results.
- Real-World Benchmarks: Datasets are curated from production-grade LLM serving traffic.
- Seamless Deployment: Enables immediate integration of high-performance kernels into live LLM engines.
- Performance Tracking: A public leaderboard visualizes and ranks kernel performance.

Useful links:

- FlashInfer Blog: https://flashinfer.ai/
- GitHub: https://github.com/flashinfer-ai/flashinfer-bench/
- Join Slack: https://join.slack.com/t/flashinfer/shared_invite/zt-379wct3hc-D5jR~1ZKQcU00WHsXhgvtA

## Next Steps

- Get Started: ./start/quick_start
- Installation: ./start/installation
- Schema (FlashInfer Trace): ./flashinfer_trace/flashinfer_trace
- Tutorials: ./tutorials/bring_your_own_kernel
- API Reference: ./api/reference
