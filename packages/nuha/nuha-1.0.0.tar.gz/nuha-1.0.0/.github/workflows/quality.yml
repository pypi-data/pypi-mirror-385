name: Code Quality & Performance

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main, develop]
  schedule:
    # Run weekly on Sundays at 3 AM UTC
    - cron: "0 3 * * 0"
  workflow_dispatch:

jobs:
  # Quick change detection
  quick-checks:
    name: Quick Checks
    runs-on: ubuntu-latest
    outputs:
      should-run: ${{ steps.changes.outputs.should-run }}
    steps:
      - uses: actions/checkout@v4

      - name: Detect changes
        uses: dorny/paths-filter@v2
        id: changes
        with:
          base: main
          filters: |
            core:
              - 'nuha/**'
              - 'tests/**'
              - 'pyproject.toml'
              - 'Makefile'
              - '.pre-commit-config.yaml'
            docs:
              - '**.md'
              - 'docs/**'
              - 'README.md'
              - 'CONTRIBUTING.md'
              - 'LICENSE'
            ci:
              - '.github/**'
              - '.github/workflows/**'

      - name: Set output
        run: echo "should-run=${{ steps.changes.outputs.core == 'true' || steps.changes.outputs.ci == 'true' }}" >> $GITHUB_OUTPUT

  # Code quality metrics
  code-quality-metrics:
    name: Code Quality Metrics
    runs-on: ubuntu-latest
    needs: quick-checks
    if: needs.quick-checks.outputs.should-run == 'true'
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.11"

      - name: Install UV
        run: curl -LsSf https://astral.sh/uv/install.sh | sh

      - name: Install dependencies
        run: uv sync

      - name: Install dev dependencies
        run: uv pip install -e ".[dev]"

      - name: Calculate code metrics
        run: |
          uv pip install radon

          # Calculate cyclomatic complexity
          echo "## Cyclomatic Complexity" >> $GITHUB_STEP_SUMMARY
          uv run radon cc nuha/ --average >> $GITHUB_STEP_SUMMARY

          # Calculate maintainability index
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## Maintainability Index" >> $GITHUB_STEP_SUMMARY
          uv run radon mi nuha/ >> $GITHUB_STEP_SUMMARY

          # Calculate raw metrics
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## Raw Metrics" >> $GITHUB_STEP_SUMMARY
          uv run radon raw nuha/ >> $GITHUB_STEP_SUMMARY

      - name: Code coverage analysis
        run: |
          uv run pytest --cov=nuha --cov-report=term-missing --cov-report=html

          # Extract coverage percentage
          COVERAGE=$(uv run python -c "
          import coverage
          cov = coverage.Coverage()
          cov.load()
          total = cov.report()
          print(f'{total:.1f}')
          ")

          echo "Coverage: ${COVERAGE}%" >> $GITHUB_STEP_SUMMARY

          # Fail if coverage is below threshold
          if python -c "import sys; exit(0 if float('$COVERAGE') >= 20 else 1)"; then
            echo "Coverage meets minimum threshold of 20%"
          else
            echo "Coverage is below 20% threshold"
            exit 1
          fi

      - name: Upload coverage reports
        uses: actions/upload-artifact@v4
        with:
          name: coverage-reports
          path: htmlcov/
          retention-days: 30

  # Performance benchmarking
  performance-benchmark:
    name: Performance Benchmarking
    runs-on: ubuntu-latest
    needs: quick-checks
    if: needs.quick-checks.outputs.should-run == 'true'
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.11"

      - name: Install UV
        run: curl -LsSf https://astral.sh/uv/install.sh | sh

      - name: Install dependencies
        run: uv sync

      - name: Install dev dependencies
        run: uv pip install -e ".[dev]"

      - name: Install performance tools
        run: uv pip install pytest-benchmark memory-profiler

      - name: Run performance benchmarks
        run: |
          # Run benchmarks if they exist
          if [ -d "tests/benchmarks" ]; then
            uv run pytest tests/benchmarks/ --benchmark-only --benchmark-json=benchmark-results.json
          else
            echo "No benchmark tests found, creating basic performance test..."
            mkdir -p tests/benchmarks

            # Create basic benchmark test
            cat > tests/benchmarks/test_basic.py << 'EOF'
            import pytest
            from nuha.core.command_parser import CommandParser

            def test_parse_performance(benchmark):
                parser = CommandParser()
                command = "ls -la /usr/local/bin && grep -r 'pattern' /etc/"
                result = benchmark(parser.parse, command)
                assert result.command == "ls -la /usr/local/bin"
            EOF

            uv run pytest tests/benchmarks/ --benchmark-only --benchmark-json=benchmark-results.json
          fi

      - name: Analyze performance results
        run: |
          if [ -f benchmark-results.json ]; then
            echo "## Performance Results" >> $GITHUB_STEP_SUMMARY

            # Extract key metrics
            uv run python << 'EOF'
            import json
            import sys

            try:
                with open('benchmark-results.json', 'r') as f:
                    data = json.load(f)

                for bench in data.get('benchmarks', []):
                    name = bench.get('name', 'Unknown')
                    mean = bench.get('stats', {}).get('mean', 0)
                    std = bench.get('stats', {}).get('stddev', 0)
                    print(f"- **{name}**: {mean:.6f}s Â± {std:.6f}s")

            except Exception as e:
                print(f"Error analyzing benchmarks: {e}")
                sys.exit(1)
            EOF >> $GITHUB_STEP_SUMMARY
          fi

      - name: Memory profiling
        run: |
          # Profile memory usage of key functions
          uv run python << 'EOF'
          from memory_profiler import profile
          from nuha.core.command_parser import CommandParser

          @profile
          def test_parser_memory():
              parser = CommandParser()
              for i in range(100):
                  parser.parse(f"ls -la /path/{i}")

          if __name__ == "__main__":
              test_parser_memory()
          EOF > memory_profile.txt 2>&1 || true

          echo "## Memory Profile" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          cat memory_profile.txt >> $GITHUB_STEP_SUMMARY || echo "Memory profiling completed"
          echo '```' >> $GITHUB_STEP_SUMMARY

      - name: Upload performance reports
        uses: actions/upload-artifact@v4
        with:
          name: performance-reports
          path: |
            benchmark-results.json
            memory_profile.txt
          retention-days: 30

  # Code complexity analysis
  complexity-analysis:
    name: Code Complexity Analysis
    runs-on: ubuntu-latest
    needs: quick-checks
    if: needs.quick-checks.outputs.should-run == 'true'
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.11"

      - name: Install UV
        run: curl -LsSf https://astral.sh/uv/install.sh | sh

      - name: Install dependencies
        run: uv sync

      - name: Install analysis tools
        run: uv pip install lizard xenon

      - name: Run complexity analysis
        run: |
          echo "## Code Complexity Analysis" >> $GITHUB_STEP_SUMMARY

          # Lizard analysis
          echo "### Lizard Analysis" >> $GITHUB_STEP_SUMMARY
          uv run lizard nuha/ --languages "Python" --extensions ".py" >> $GITHUB_STEP_SUMMARY

          # Xenon analysis (maintainability)
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Xenon Analysis" >> $GITHUB_STEP_SUMMARY

          # Check absolute complexity
          echo "#### Absolute Complexity" >> $GITHUB_STEP_SUMMARY
          uv run xenon --max-absolute A nuha/ || echo "Absolute complexity check completed" >> $GITHUB_STEP_SUMMARY

          # Check modules complexity
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "#### Module Complexity" >> $GITHUB_STEP_SUMMARY
          uv run xenon --max-modules A nuha/ || echo "Module complexity check completed" >> $GITHUB_STEP_SUMMARY

          # Check average complexity
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "#### Average Complexity" >> $GITHUB_STEP_SUMMARY
          uv run xenon --max-average A nuha/ || echo "Average complexity check completed" >> $GITHUB_STEP_SUMMARY

      - name: Check for code smells
        run: |
          uv pip install flake8-flake-tidy-imports flake8-docstrings

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Code Smells Analysis" >> $GITHUB_STEP_SUMMARY

          # Check import organization
          echo "#### Import Organization" >> $GITHUB_STEP_SUMMARY
          uv run flake8 nuha/ --select TID --extend-ignore=TID003 >> $GITHUB_STEP_SUMMARY || echo "Import check completed"

          # Check docstrings
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "#### Docstring Coverage" >> $GITHUB_STEP_SUMMARY
          uv run flake8 nuha/ --select D --extend-ignore=D100,D104,D105,D107 >> $GITHUB_STEP_SUMMARY || echo "Docstring check completed"

  # Security quality checks
  security-quality:
    name: Security Quality Checks
    runs-on: ubuntu-latest
    needs: quick-checks
    if: needs.quick-checks.outputs.should-run == 'true'
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.11"

      - name: Install UV
        run: curl -LsSf https://astral.sh/uv/install.sh | sh

      - name: Install dependencies
        run: uv sync

      - name: Install security tools
        run: uv pip install bandit semgrep

      - name: Run bandit security analysis
        run: |
          echo "## Security Analysis" >> $GITHUB_STEP_SUMMARY

          # Run bandit with confidence level
          uv run bandit -r nuha/ -f json -o bandit-report.json || true

          if [ -f bandit-report.json ]; then
            echo "### Bandit Security Issues" >> $GITHUB_STEP_SUMMARY
            uv run python << 'EOF'
            import json
            try:
                with open('bandit-report.json', 'r') as f:
                    data = json.load(f)

                issues = data.get('results', [])
                if issues:
                    high_issues = [i for i in issues if i.get('issue_severity') == 'HIGH']
                    medium_issues = [i for i in issues if i.get('issue_severity') == 'MEDIUM']
                    low_issues = [i for i in issues if i.get('issue_severity') == 'LOW']

                    print(f"- **High Severity**: {len(high_issues)}")
                    print(f"- **Medium Severity**: {len(medium_issues)}")
                    print(f"- **Low Severity**: {len(low_issues)}")

                    if high_issues:
                        print("\n#### High Severity Issues:")
                        for issue in high_issues[:5]:  # Show first 5
                            print(f"- {issue.get('test_name', 'Unknown')}: {issue.get('issue_text', 'No description')}")
                else:
                    print("âœ… No security issues found by Bandit")
            except Exception as e:
                print(f"Error processing bandit report: {e}")
            EOF >> $GITHUB_STEP_SUMMARY
          fi

      - name: Run semgrep analysis
        run: |
          # Run semgrep if available (requires API token for full features)
          if command -v semgrep &> /dev/null; then
            uv run semgrep --config=auto --json --output=semgrep-report.json nuha/ || true

            if [ -f semgrep-report.json ]; then
              echo "" >> $GITHUB_STEP_SUMMARY
              echo "### Semgrep Analysis" >> $GITHUB_STEP_SUMMARY
              uv run python << 'EOF'
              import json
              try:
                  with open('semgrep-report.json', 'r') as f:
                      data = json.load(f)

                  findings = data.get('results', [])
                  print(f"Found {len(findings)} findings with Semgrep")

                  # Group by severity
                  by_severity = {}
                  for finding in findings:
                      severity = finding.get('metadata', {}).get('severity', 'INFO')
                      by_severity[severity] = by_severity.get(severity, 0) + 1

                  for severity, count in by_severity.items():
                      print(f"- **{severity}**: {count}")
              except Exception as e:
                  print(f"Error processing semgrep report: {e}")
              EOF >> $GITHUB_STEP_SUMMARY
            fi
          else
            echo "Semgrep not available, skipping advanced security analysis" >> $GITHUB_STEP_SUMMARY
          fi

      - name: Upload security reports
        uses: actions/upload-artifact@v4
        with:
          name: security-reports
          path: |
            bandit-report.json
            semgrep-report.json
          retention-days: 30

  # Documentation quality
  docs-quality:
    name: Documentation Quality
    runs-on: ubuntu-latest
    needs: quick-checks
    if: needs.quick-checks.outputs.should-run == 'true' || needs.quick-checks.outputs.docs == 'true'
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.11"

      - name: Install UV
        run: curl -LsSf https://astral.sh/uv/install.sh | sh

      - name: Install dependencies
        run: uv sync

      - name: Install documentation tools
        run: uv pip install pydocstyle

      - name: Check docstring quality
        run: |
          echo "## Documentation Quality" >> $GITHUB_STEP_SUMMARY

          # Run pydocstyle
          echo "### Docstring Quality Analysis" >> $GITHUB_STEP_SUMMARY
          uv run pydocstyle nuha/ >> $GITHUB_STEP_SUMMARY || echo "Docstyle check completed with issues"

          # Check README quality
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### README Analysis" >> $GITHUB_STEP_SUMMARY

          README_SIZE=$(wc -l < README.md)
          echo "- **Lines in README**: $README_SIZE" >> $GITHUB_STEP_SUMMARY

          if grep -q "## ðŸš€ Quick Start" README.md; then
            echo "- âœ… Installation section found" >> $GITHUB_STEP_SUMMARY
          else
            echo "- âŒ Installation section missing" >> $GITHUB_STEP_SUMMARY
          fi

          if grep -q "## ðŸ’¡ Usage Examples" README.md; then
            echo "- âœ… Usage section found" >> $GITHUB_STEP_SUMMARY
          else
            echo "- âŒ Usage section missing" >> $GITHUB_STEP_SUMMARY
          fi

          if grep -q "## ðŸ¤ Contributing" README.md; then
            echo "- âœ… Contributing section found" >> $GITHUB_STEP_SUMMARY
          else
            echo "- âš ï¸ Contributing section missing" >> $GITHUB_STEP_SUMMARY
          fi

  # Quality gate summary
  quality-gate:
    name: Quality Gate Summary
    runs-on: ubuntu-latest
    needs:
      [
        quick-checks,
        code-quality-metrics,
        performance-benchmark,
        complexity-analysis,
        security-quality,
        docs-quality,
      ]
    if: always()
    steps:
      - name: Quality Gate Summary
        run: |
          echo "## ðŸ† Quality Gate Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Check if core files were changed
          if [[ "${{ needs.quick-checks.outputs.should-run }}" == "true" ]]; then
            # Core files changed - expect all checks to pass
            echo "Core files changed - checking all quality metrics..." >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY

            # Collect results
            echo "| Check | Status | Details |" >> $GITHUB_STEP_SUMMARY
            echo "|-------|--------|---------|" >> $GITHUB_STEP_SUMMARY
            echo "| Code Quality Metrics | ${{ needs.code-quality-metrics.result }} | Coverage, complexity, maintainability |" >> $GITHUB_STEP_SUMMARY
            echo "| Performance Benchmark | ${{ needs.performance-benchmark.result }} | Performance and memory profiling |" >> $GITHUB_STEP_SUMMARY
            echo "| Complexity Analysis | ${{ needs.complexity-analysis.result }} | Code complexity and smells |" >> $GITHUB_STEP_SUMMARY
            echo "| Security Quality | ${{ needs.security-quality.result }} | Security analysis and vulnerabilities |" >> $GITHUB_STEP_SUMMARY
            echo "| Documentation Quality | ${{ needs.docs-quality.result }} | Docstring and README quality |" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY

            # Overall assessment
            FAILED_CHECKS=0
            if [[ "${{ needs.code-quality-metrics.result }}" != "success" ]]; then ((FAILED_CHECKS++)); fi
            if [[ "${{ needs.performance-benchmark.result }}" != "success" ]]; then ((FAILED_CHECKS++)); fi
            if [[ "${{ needs.complexity-analysis.result }}" != "success" ]]; then ((FAILED_CHECKS++)); fi
            if [[ "${{ needs.security-quality.result }}" != "success" ]]; then ((FAILED_CHECKS++)); fi
            if [[ "${{ needs.docs-quality.result }}" != "success" ]]; then ((FAILED_CHECKS++)); fi

            if [ $FAILED_CHECKS -eq 0 ]; then
              echo "ðŸŽ‰ **All quality checks passed!**" >> $GITHUB_STEP_SUMMARY
              echo "" >> $GITHUB_STEP_SUMMARY
              echo "The codebase meets all quality standards and is ready for production." >> $GITHUB_STEP_SUMMARY
            else
              echo "âš ï¸ **$FAILED_CHECKS quality check(s) failed**" >> $GITHUB_STEP_SUMMARY
              echo "" >> $GITHUB_STEP_SUMMARY
              echo "Please review the failed checks and address the issues before merging." >> $GITHUB_STEP_SUMMARY
              exit 1
            fi
          else
            # Only CI/docs files changed - skip detailed checks
            echo "âœ… CI/docs files updated - no core changes detected" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "Quality checks skipped as no core code changes were detected." >> $GITHUB_STEP_SUMMARY
          fi
