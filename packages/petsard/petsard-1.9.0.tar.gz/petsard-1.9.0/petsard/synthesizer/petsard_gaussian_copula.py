import numpy as np
import pandas as pd
import torch
from scipy import stats

from petsard.metadater import Schema
from petsard.synthesizer.synthesizer_base import BaseSynthesizer

# Try to import numba for JIT compilation
try:
    from numba import jit

    NUMBA_AVAILABLE = True
except ImportError:
    NUMBA_AVAILABLE = False

    # Fallback decorator that does nothing
    def jit(*args, **kwargs):
        def decorator(func):
            return func

        return decorator


# Fast rank calculation using NumPy (2-3x faster than scipy.stats.rankdata)
@jit(nopython=True, cache=True) if NUMBA_AVAILABLE else lambda f: f
def fast_rankdata_average(arr):
    """
    Fast rank calculation with average method using NumPy.
    ~2-3x faster than scipy.stats.rankdata with method='average'.

    Args:
        arr: 1D numpy array

    Returns:
        ranks: 1D array of ranks (1-based, with averaging for ties)
    """
    n = len(arr)
    sorter = np.argsort(arr)
    inv = np.empty(n, dtype=np.int64)
    inv[sorter] = np.arange(n)

    arr_sorted = arr[sorter]
    ranks = np.empty(n, dtype=np.float64)

    i = 0
    while i < n:
        j = i
        # Find end of tied group
        while j < n - 1 and arr_sorted[j] == arr_sorted[j + 1]:
            j += 1
        # Average rank for tied values
        avg_rank = (i + j + 2) / 2.0  # +2 because ranks are 1-based
        for k in range(i, j + 1):
            ranks[sorter[k]] = avg_rank
        i = j + 1

    return ranks


@jit(nopython=True, cache=True) if NUMBA_AVAILABLE else lambda f: f
def fast_transform_column(values, eps):
    """
    JIT-compiled transform of a single column to Gaussian space.

    Args:
        values: 1D array of valid (non-NaN) values
        eps: Small epsilon for numerical stability

    Returns:
        normal_values: Transformed values in standard normal space
    """
    n = len(values)

    # Check for constant column
    if np.min(values) == np.max(values):
        return np.zeros(n, dtype=np.float64)

    # Fast rank calculation
    ranks = fast_rankdata_average(values)

    # Transform ranks to uniform [0, 1]
    uniform_values = (ranks - 0.5) / n

    # Clip to avoid issues at boundaries
    uniform_values = np.clip(uniform_values, eps, 1 - eps)

    return uniform_values


@jit(nopython=True, cache=True) if NUMBA_AVAILABLE else lambda f: f
def fast_interp_1d(x, xp, fp):
    """
    JIT-compiled 1D linear interpolation (faster than np.interp for repeated calls).

    Args:
        x: Points to evaluate at
        xp: X coordinates of data points (must be sorted)
        fp: Y coordinates of data points

    Returns:
        Interpolated values
    """
    n = len(x)
    result = np.empty(n, dtype=np.float64)

    for i in range(n):
        xi = x[i]

        # Handle boundary cases
        if xi <= xp[0]:
            result[i] = fp[0]
            continue
        if xi >= xp[-1]:
            result[i] = fp[-1]
            continue

        # Binary search for insertion point
        left = 0
        right = len(xp) - 1
        while right - left > 1:
            mid = (left + right) // 2
            if xp[mid] <= xi:
                left = mid
            else:
                right = mid

        # Linear interpolation
        t = (xi - xp[left]) / (xp[right] - xp[left])
        result[i] = fp[left] + t * (fp[right] - fp[left])

    return result


class PetsardGaussianCopulaSynthesizer(BaseSynthesizer):
    """
    A Gaussian Copula synthesizer for preprocessed data using PyTorch.

    **é‡è¦å‡è¨­**ï¼š
    1. è¼¸å…¥æ•¸æ“šå·²é è™•ç†ï¼šé¡žåˆ¥è®Šæ•¸å·²ç·¨ç¢¼ç‚ºæ•¸å€¼ï¼Œæ•¸å€¼è®Šæ•¸å·²æ¨™æº–åŒ–
    2. ä¸æŽ¥å— string/object é¡žåž‹çš„æ¬„ä½
    3. æ‰€æœ‰æ¬„ä½çµ±ä¸€è™•ç†ç‚º floatï¼Œä¿ç•™åŽŸå§‹ dtype ç”¨æ–¼é‚„åŽŸ

    The Gaussian Copula approach:
    1. Validate data types (reject string/object)
    2. Record original dtypes for later conversion
    3. Transform to Gaussian space using empirical CDF
    4. Learn correlation structure
    5. Sample from multivariate Gaussian
    6. Inverse transform and restore original dtypes
    """

    def __init__(self, config: dict, metadata: Schema = None):
        """
        Initialize the PETsARD Gaussian Copula Synthesizer.

        Args:
            config: Configuration dictionary with syn_method and sample_num_rows
            metadata: Optional Schema metadata object
        """
        super().__init__(config, metadata)

        # Log Numba availability
        if NUMBA_AVAILABLE:
            self._logger.info(
                "âœ“ Numba JIT compilation enabled - expect 10-100x speedup after first run"
            )
        else:
            self._logger.info(
                "âš  Numba not installed - using NumPy fallback (still 2-3x faster than scipy)"
            )
            self._logger.info(
                "  Install numba for maximum performance: pip install numba"
            )

        # Device selection for PyTorch with smart threshold
        # Use GPU only for large datasets (>50K rows) to avoid transfer overhead
        self.gpu_threshold = config.get("gpu_threshold", 50000)
        self.use_gpu = config.get("use_gpu", "auto")  # auto, True, False
        self.device = None  # Will be set in fit based on data size

        # Storage for fitted parameters
        self.marginals: dict[str, dict] = {}
        self.correlation_matrix: torch.Tensor | None = None
        self.column_names: list[str] = []
        self.column_dtypes: dict[str, np.dtype] = {}  # Record original dtypes

        # Numerical stability parameters
        self.eps = 1e-8
        self.correlation_regularization = 1e-3

        self._logger.info(
            "PETsARD Gaussian Copula initialized with optimizations enabled"
        )

    def _select_device(self, n_rows: int) -> None:
        """
        Smart device selection based on data size and availability.

        Args:
            n_rows: Number of rows in the dataset
        """
        if self.use_gpu == "auto":
            # Auto mode: use GPU only for large datasets
            if torch.cuda.is_available() and n_rows >= self.gpu_threshold:
                self.device = torch.device("cuda")
                self._logger.info(
                    f"Auto-selected GPU device (data size: {n_rows} >= threshold: {self.gpu_threshold})"
                )
            else:
                self.device = torch.device("cpu")
                reason = (
                    "below threshold"
                    if n_rows < self.gpu_threshold
                    else "CUDA not available"
                )
                self._logger.info(
                    f"Auto-selected CPU device (data size: {n_rows}, {reason})"
                )
        elif self.use_gpu is True:
            # Force GPU mode - raise error if not available
            if not torch.cuda.is_available():
                error_msg = (
                    "GPU requested via use_gpu=true but CUDA is not available. "
                    "Please install CUDA-enabled PyTorch or set use_gpu='auto' or use_gpu=false"
                )
                self._logger.error(error_msg)
                raise RuntimeError(error_msg)
            self.device = torch.device("cuda")
            self._logger.info("Using GPU device (forced by config)")
        else:
            # Force CPU mode
            self.device = torch.device("cpu")
            self._logger.info("Using CPU device (forced by config)")

    def _validate_data_types(self, data: pd.DataFrame) -> None:
        """
        Validate that all columns are numeric (no string/object types).

        Args:
            data: Input dataframe

        Raises:
            ValueError: If any column contains string/object type
        """
        invalid_columns = []

        for col in data.columns:
            dtype = data[col].dtype
            # Check for string/object types
            if dtype is object or pd.api.types.is_string_dtype(dtype):
                invalid_columns.append(col)

        if invalid_columns:
            raise ValueError(
                f"Found non-numeric columns: {invalid_columns}. "
                f"PETsARD Gaussian Copula requires all data to be preprocessed. "
                f"Please encode categorical variables as integers before using this synthesizer."
            )

        self._logger.info("Data type validation passed - all columns are numeric")

    def _record_dtypes(self, data: pd.DataFrame) -> None:
        """
        Record original data types for each column to restore after synthesis.

        Args:
            data: Input dataframe
        """
        self.column_dtypes = {}
        for col in data.columns:
            self.column_dtypes[col] = data[col].dtype

        self._logger.debug(f"Recorded dtypes: {self.column_dtypes}")

    def _fit_marginal(self, series: pd.Series, col_name: str) -> dict:
        """
        Fit marginal distribution for a single column.
        Uses empirical CDF (rank-based) and precomputed quantiles for inverse transform.

        Args:
            series: Data series to fit
            col_name: Column name

        Returns:
            Dictionary containing fitted parameters
        """
        marginal_info = {}

        # Handle missing values
        valid_data = series.dropna()
        marginal_info["null_rate"] = (len(series) - len(valid_data)) / len(series)

        values = valid_data.values.astype(np.float64)

        # Store quantile values for inverse transform using numpy interp
        # Limit to 1000 points for memory efficiency
        n_quantiles = min(1000, len(values))
        quantile_levels = np.linspace(0, 1, n_quantiles)
        quantile_values = np.quantile(values, quantile_levels)

        marginal_info["quantile_levels"] = quantile_levels
        marginal_info["quantile_values"] = quantile_values

        # Store basic statistics
        marginal_info["min"] = float(values.min())
        marginal_info["max"] = float(values.max())
        marginal_info["n_samples"] = len(values)

        return marginal_info

    def _transform_to_gaussian(self, data: pd.DataFrame) -> np.ndarray:
        """
        Fully optimized transform to Gaussian space using:
        1. Fast NumPy-based rank calculation (2-3x faster than scipy)
        2. Numba JIT compilation when available
        3. Vectorized operations

        Args:
            data: Preprocessed data (all numeric)

        Returns:
            Transformed data as NumPy array
        """
        import time

        n_rows = len(data)
        n_cols = len(self.column_names)

        # Pre-allocate output array
        transformed_array = np.full((n_rows, n_cols), np.nan, dtype=np.float64)

        # Convert all data to float64 at once
        data_array = data[self.column_names].astype(np.float64).values

        self._logger.info(
            f"     Transforming {n_cols} columns with {n_rows:,} rows each..."
        )

        if NUMBA_AVAILABLE:
            self._logger.info(
                "     âš¡ Numba JIT enabled - first column may be slow (compiling fast_transform_column)"
            )

        # Track time for first few columns to estimate
        first_col_time = None
        total_transform_time = 0
        total_ppf_time = 0

        # Process all columns with optimized methods
        for i, col in enumerate(self.column_names):
            col_start = time.time()

            col_data = data_array[:, i]
            valid_mask = ~np.isnan(col_data)
            valid_data = col_data[valid_mask]

            if len(valid_data) == 0:
                continue  # Keep as NaN

            # Use optimized transform (JIT-compiled if Numba available)
            transform_start = time.time()
            uniform_values = fast_transform_column(valid_data, self.eps)
            transform_time = time.time() - transform_start
            total_transform_time += transform_time

            # Transform to normal space (scipy is still fastest for ppf)
            ppf_start = time.time()
            normal_values = stats.norm.ppf(uniform_values)
            ppf_time = time.time() - ppf_start
            total_ppf_time += ppf_time

            transformed_array[valid_mask, i] = normal_values

            col_time = time.time() - col_start

            # Log progress
            if i == 0:
                first_col_time = col_time
                self._logger.info(f"       [1/{n_cols}] '{col}': {col_time:.3f}s")
            elif i == 1 and first_col_time:
                if NUMBA_AVAILABLE and col_time > 0:
                    speedup = first_col_time / col_time
                    self._logger.info(
                        f"       [2/{n_cols}] '{col}': {col_time:.3f}s (ðŸš€ {speedup:.1f}x speedup after JIT)"
                    )
                else:
                    self._logger.info(f"       [2/{n_cols}] '{col}': {col_time:.3f}s")
                estimated_remaining = col_time * (n_cols - 2)
                self._logger.info(
                    f"       â†’ Est. remaining: ~{estimated_remaining:.1f}s"
                )
            elif (i + 1) % max(1, n_cols // 4) == 0:  # Report every 25%
                avg_time = (
                    time.time() - col_start - col_time + sum([transform_time, ppf_time])
                ) / max(1, i)
                remaining = avg_time * (n_cols - i - 1)
                self._logger.info(
                    f"       [{i + 1}/{n_cols}] Progress: {(i + 1) / n_cols * 100:.0f}% | Current: '{col}' ({col_time:.3f}s) | Est. remaining: ~{remaining:.1f}s"
                )

        self._logger.info(
            f"     âœ“ Transform breakdown: rank+uniform={total_transform_time:.2f}s, norm.ppf={total_ppf_time:.2f}s"
        )

        return transformed_array

    def _fit(self, data: pd.DataFrame) -> None:
        """
        Fit the Gaussian Copula model to preprocessed data.

        Args:
            data: Training data (must be preprocessed - all numeric)

        Raises:
            ValueError: If data contains non-numeric columns
        """
        import time

        total_start = time.time()

        # Store training data size for default sample_num_rows
        self.training_data_rows = len(data)

        self._logger.info("=" * 60)
        self._logger.info("Starting Gaussian Copula Fit")
        self._logger.info(f"Data: {data.shape[0]:,} rows Ã— {data.shape[1]} columns")
        self._logger.info("=" * 60)

        # Smart device selection based on data size
        step_start = time.time()
        self._select_device(len(data))
        self._logger.info(f"[1/6] Device selection: {time.time() - step_start:.3f}s")

        # Validate data types (reject string/object)
        step_start = time.time()
        self._validate_data_types(data)
        self._record_dtypes(data)
        self.column_names = list(data.columns)
        self._logger.info(f"[2/6] Data validation: {time.time() - step_start:.3f}s")

        # Fit marginal distributions
        step_start = time.time()
        self._logger.info(
            f"[3/6] Fitting {len(self.column_names)} marginal distributions..."
        )
        for i, col in enumerate(self.column_names):
            self.marginals[col] = self._fit_marginal(data[col], col)
            if (i + 1) % 5 == 0 or (i + 1) == len(self.column_names):
                self._logger.info(
                    f"     Progress: {i + 1}/{len(self.column_names)} columns fitted"
                )
        fit_marginals_time = time.time() - step_start
        self._logger.info(
            f"[3/6] Marginals fitted: {fit_marginals_time:.3f}s ({fit_marginals_time / len(self.column_names):.4f}s per column)"
        )

        # Transform data to Gaussian space
        step_start = time.time()
        self._logger.info("[4/6] Transforming to Gaussian space...")
        if NUMBA_AVAILABLE:
            self._logger.info(
                "     âš¡ Using JIT compilation - first column will be slowest (compiling), please wait..."
            )
        gaussian_data = self._transform_to_gaussian(data)
        transform_time = time.time() - step_start
        self._logger.info(f"[4/6] Transform completed: {transform_time:.3f}s")

        # Remove rows with NaN values for correlation calculation
        self._logger.info("     Checking for NaN values in transformed data...")
        prep_start = time.time()
        valid_mask = ~np.isnan(gaussian_data).any(axis=1)
        gaussian_data_clean = gaussian_data[valid_mask]
        prep_time = time.time() - prep_start
        self._logger.info(
            f"     âœ“ Found {gaussian_data_clean.shape[0]:,}/{gaussian_data.shape[0]:,} complete cases ({prep_time:.3f}s)"
        )

        # Calculate correlation matrix using NumPy (faster and more stable)
        step_start = time.time()
        self._logger.info(
            f"[5/6] Calculating {len(self.column_names)}Ã—{len(self.column_names)} correlation matrix..."
        )
        if gaussian_data_clean.shape[0] < 2:
            self._logger.warning(
                "Insufficient complete cases for correlation, using identity matrix"
            )
            n = len(self.column_names)
            correlation_matrix = np.eye(n)
        else:
            self._logger.info(
                f"     Computing np.corrcoef on {gaussian_data_clean.shape[0]:,} rows..."
            )
            correlation_matrix = np.corrcoef(gaussian_data_clean.T)
        corr_time = time.time() - step_start
        self._logger.info(f"[5/6] Correlation calculated: {corr_time:.3f}s")

        # Regularization
        step_start = time.time()
        self._logger.info("[6/6] Regularizing correlation matrix...")
        correlation_matrix = self._regularize_correlation_matrix(correlation_matrix)
        # Convert to torch tensor only after regularization
        self.correlation_matrix = torch.tensor(
            correlation_matrix, dtype=torch.float32, device=self.device
        )
        reg_time = time.time() - step_start
        self._logger.info(f"[6/6] Regularization completed: {reg_time:.3f}s")

        # Mark as fitted
        self._impl = True

        total_time = time.time() - total_start
        self._logger.info("=" * 60)
        self._logger.info(f"âœ“ Fit completed successfully in {total_time:.3f}s")
        self._logger.info(
            f"  Breakdown: Marginals={fit_marginals_time:.2f}s, Transform={transform_time:.2f}s, Corr={corr_time:.2f}s, Reg={reg_time:.2f}s"
        )
        self._logger.info("=" * 60)

    def _regularize_correlation_matrix(self, corr_matrix: np.ndarray) -> np.ndarray:
        """
        Fast correlation matrix regularization using Ledoit-Wolf shrinkage.
        Only does expensive eigendecomposition when necessary.

        Args:
            corr_matrix: Input correlation matrix (NumPy array)

        Returns:
            Regularized correlation matrix (NumPy array)
        """
        # Handle edge cases
        if corr_matrix.ndim == 0:
            return np.array([[1.0]])
        elif corr_matrix.ndim == 1:
            return corr_matrix.reshape(1, 1)

        n = corr_matrix.shape[0]

        # Quick check for NaN/Inf
        if np.isnan(corr_matrix).any() or np.isinf(corr_matrix).any():
            self._logger.warning("Correlation matrix contains NaN/Inf, using identity")
            return np.eye(n)

        # Ensure symmetry
        corr_matrix = (corr_matrix + corr_matrix.T) / 2

        # Simple shrinkage towards identity (fast Ledoit-Wolf approximation)
        shrinkage_factor = self.correlation_regularization
        reg_matrix = (1 - shrinkage_factor) * corr_matrix + shrinkage_factor * np.eye(n)

        # Quick positive definiteness check
        try:
            np.linalg.cholesky(reg_matrix)
            return reg_matrix
        except np.linalg.LinAlgError:
            # Only do expensive eigendecomposition if shrinkage failed
            self._logger.debug("Shrinkage failed, using eigendecomposition")
            try:
                eigenvalues, eigenvectors = np.linalg.eigh(reg_matrix)
                eigenvalues = np.maximum(eigenvalues, self.correlation_regularization)
                reg_matrix = eigenvectors @ np.diag(eigenvalues) @ eigenvectors.T

                # Normalize diagonal to 1
                d = np.sqrt(np.diag(reg_matrix))
                d = np.maximum(d, self.eps)
                reg_matrix = reg_matrix / np.outer(d, d)

                return reg_matrix
            except (np.linalg.LinAlgError, RuntimeError):
                self._logger.warning("Regularization failed, using identity matrix")
                return np.eye(n)

    def _sample_gaussian(self, n_samples: int) -> torch.Tensor:
        """
        Sample from multivariate Gaussian with learned correlation.
        Uses NumPy for fast CPU sampling, then converts to torch if needed.

        Args:
            n_samples: Number of samples to generate

        Returns:
            Samples from multivariate Gaussian as torch.Tensor
        """
        n_features = len(self.column_names)

        # Convert correlation matrix to NumPy for sampling
        corr_np = self.correlation_matrix.cpu().numpy()

        # Fast path: if correlation is identity (independent variables), use randn
        if np.allclose(corr_np, np.eye(n_features)):
            self._logger.debug(
                "Using fast path for independent variables (identity matrix)"
            )
            samples_np = np.random.randn(n_samples, n_features).astype(np.float32)
        else:
            # Use NumPy's multivariate_normal (faster than PyTorch on CPU)
            mean = np.zeros(n_features)
            samples_np = np.random.multivariate_normal(
                mean, corr_np, size=n_samples
            ).astype(np.float32)

        # Convert to torch tensor and move to device if needed
        samples = torch.from_numpy(samples_np)
        if self.device.type != "cpu":
            samples = samples.to(self.device)

        return samples

    def _inverse_transform_gaussian(
        self, gaussian_samples: torch.Tensor
    ) -> pd.DataFrame:
        """
        Fully optimized inverse transform with:
        1. JIT-compiled interpolation (faster than np.interp)
        2. Vectorized operations
        3. Batch processing

        Args:
            gaussian_samples: Samples from multivariate Gaussian

        Returns:
            DataFrame with samples in original space and dtypes
        """
        n_samples = gaussian_samples.shape[0]

        # Move to CPU once for all columns
        gaussian_array = gaussian_samples.cpu().numpy()

        # Pre-allocate output array
        synthetic_array = np.empty(
            (n_samples, len(self.column_names)), dtype=np.float64
        )

        # Fully vectorized transformation
        for i, col in enumerate(self.column_names):
            marginal = self.marginals[col]
            gaussian_col = gaussian_array[:, i]

            # Handle invalid values vectorized
            invalid_mask = np.isnan(gaussian_col) | np.isinf(gaussian_col)
            if invalid_mask.any():
                gaussian_col = gaussian_col.copy()
                gaussian_col[invalid_mask] = 0.0

            # Vectorized: Gaussian -> Uniform
            uniform_samples = stats.norm.cdf(gaussian_col)
            uniform_samples = np.clip(uniform_samples, self.eps, 1 - self.eps)

            # Use optimized interpolation (JIT-compiled if Numba available)
            if NUMBA_AVAILABLE:
                synthetic_values = fast_interp_1d(
                    uniform_samples,
                    marginal["quantile_levels"],
                    marginal["quantile_values"],
                )
            else:
                # Fallback to numpy interp
                synthetic_values = np.interp(
                    uniform_samples,
                    marginal["quantile_levels"],
                    marginal["quantile_values"],
                )

            # Vectorized null injection
            if marginal["null_rate"] > 0:
                null_mask = np.random.random(n_samples) < marginal["null_rate"]
                synthetic_values[null_mask] = np.nan

            synthetic_array[:, i] = synthetic_values

        # Create DataFrame from array (faster than dict)
        df = pd.DataFrame(synthetic_array, columns=self.column_names)

        # Batch dtype restoration
        self._restore_dtypes_batch(df)

        return df

    def _restore_dtypes_batch(self, df: pd.DataFrame) -> None:
        """
        Restore original dtypes in-place for better performance.

        Args:
            df: DataFrame to restore dtypes in-place
        """
        for col in self.column_names:
            original_dtype = self.column_dtypes[col]
            dtype_str = str(original_dtype)

            try:
                if dtype_str in [
                    "Int8",
                    "Int16",
                    "Int32",
                    "Int64",
                    "UInt8",
                    "UInt16",
                    "UInt32",
                    "UInt64",
                ]:
                    df[col] = df[col].round().astype(original_dtype)
                elif pd.api.types.is_integer_dtype(original_dtype):
                    df[col] = df[col].round().astype(original_dtype)
                elif pd.api.types.is_datetime64_any_dtype(original_dtype):
                    df[col] = pd.to_datetime(df[col].round().astype("int64"), unit="ns")
                elif pd.api.types.is_timedelta64_dtype(original_dtype):
                    df[col] = pd.to_timedelta(
                        df[col].round().astype("int64"), unit="ns"
                    )
                else:
                    df[col] = df[col].astype(original_dtype)
            except Exception as e:
                self._logger.warning(f"Failed to restore dtype for {col}: {e}")

    def _sample(self) -> pd.DataFrame:
        """
        Generate synthetic data using the fitted Gaussian Copula model.

        Returns:
            Synthetic data as DataFrame with original dtypes restored
        """
        import time

        total_start = time.time()

        # Use sample_num_rows if specified, otherwise use training data size
        n_samples = self.config.get("sample_num_rows")
        if n_samples is None:
            n_samples = self.training_data_rows
            self._logger.info(
                f"sample_num_rows not specified, using training data size: {n_samples:,}"
            )

        self._logger.info("=" * 60)
        self._logger.info("Starting Gaussian Copula Sampling")
        self._logger.info(f"Target: {n_samples:,} synthetic rows")
        self._logger.info("=" * 60)

        # Sample from multivariate Gaussian
        step_start = time.time()
        self._logger.info("[1/2] Sampling from multivariate Gaussian...")
        gaussian_samples = self._sample_gaussian(n_samples)
        sample_time = time.time() - step_start
        self._logger.info(f"[1/2] Gaussian sampling completed: {sample_time:.3f}s")

        # Transform back to original space and restore dtypes
        step_start = time.time()
        self._logger.info("[2/2] Inverse transforming to original space...")
        if NUMBA_AVAILABLE:
            self._logger.info("     Using JIT-compiled interpolation")
        synthetic_data = self._inverse_transform_gaussian(gaussian_samples)
        inverse_time = time.time() - step_start
        self._logger.info(f"[2/2] Inverse transform completed: {inverse_time:.3f}s")

        # Ensure column order matches original
        synthetic_data = synthetic_data[self.column_names]

        total_time = time.time() - total_start
        self._logger.info("=" * 60)
        self._logger.info(f"âœ“ Sampling completed successfully in {total_time:.3f}s")
        self._logger.info(f"  Generated {len(synthetic_data):,} rows")
        self._logger.info(
            f"  Breakdown: Gaussian={sample_time:.2f}s, Inverse={inverse_time:.2f}s"
        )
        self._logger.info("=" * 60)

        return synthetic_data

    def get_correlation_matrix(self) -> pd.DataFrame | None:
        """
        Get the learned correlation matrix as a DataFrame.

        Returns:
            Correlation matrix with column names as index and columns
        """
        if self.correlation_matrix is None:
            return None

        corr_np = self.correlation_matrix.cpu().numpy()
        return pd.DataFrame(corr_np, index=self.column_names, columns=self.column_names)

    def get_marginal_info(self, column: str) -> dict | None:
        """
        Get marginal distribution information for a specific column.

        Args:
            column: Column name

        Returns:
            Dictionary with marginal distribution information
        """
        if column not in self.marginals:
            return None

        marginal = self.marginals[column]

        # Return basic statistics (no mean/std as we use empirical CDF)
        info = {
            "null_rate": marginal["null_rate"],
            "min": marginal["min"],
            "max": marginal["max"],
            "n_samples": marginal["n_samples"],
            "n_quantiles": len(marginal["quantile_levels"]),
            "original_dtype": str(self.column_dtypes.get(column, "unknown")),
        }

        return info
