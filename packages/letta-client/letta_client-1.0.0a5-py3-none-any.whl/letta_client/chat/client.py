# This file was auto-generated by Fern from our API Definition.

import typing

from ..core.client_wrapper import AsyncClientWrapper, SyncClientWrapper
from ..core.request_options import RequestOptions
from ..types.chat_completion import ChatCompletion
from .raw_client import AsyncRawChatClient, RawChatClient
from .types.chat_completion_request_messages_item import ChatCompletionRequestMessagesItem
from .types.chat_completion_request_stop import ChatCompletionRequestStop

# this is used as the default value for optional parameters
OMIT = typing.cast(typing.Any, ...)


class ChatClient:
    def __init__(self, *, client_wrapper: SyncClientWrapper):
        self._raw_client = RawChatClient(client_wrapper=client_wrapper)

    @property
    def with_raw_response(self) -> RawChatClient:
        """
        Retrieves a raw implementation of this client that returns raw responses.

        Returns
        -------
        RawChatClient
        """
        return self._raw_client

    def create_chat_completion(
        self,
        *,
        model: str,
        messages: typing.Sequence[ChatCompletionRequestMessagesItem],
        temperature: typing.Optional[float] = OMIT,
        top_p: typing.Optional[float] = OMIT,
        n: typing.Optional[int] = OMIT,
        stream: typing.Optional[bool] = OMIT,
        stop: typing.Optional[ChatCompletionRequestStop] = OMIT,
        max_tokens: typing.Optional[int] = OMIT,
        presence_penalty: typing.Optional[float] = OMIT,
        frequency_penalty: typing.Optional[float] = OMIT,
        user: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> ChatCompletion:
        """
        Create a chat completion using a Letta agent (OpenAI-compatible).

        This endpoint provides full OpenAI API compatibility. The agent is selected based on:
        - The 'model' parameter in the request (should contain an agent ID in format 'agent-...')

        When streaming is enabled (stream=true), the response will be Server-Sent Events
        with ChatCompletionChunk objects.

        Parameters
        ----------
        model : str
            ID of the model to use

        messages : typing.Sequence[ChatCompletionRequestMessagesItem]
            Messages comprising the conversation so far

        temperature : typing.Optional[float]
            Sampling temperature

        top_p : typing.Optional[float]
            Nucleus sampling parameter

        n : typing.Optional[int]
            Number of chat completion choices to generate

        stream : typing.Optional[bool]
            Whether to stream back partial progress

        stop : typing.Optional[ChatCompletionRequestStop]
            Sequences where the API will stop generating

        max_tokens : typing.Optional[int]
            Maximum number of tokens to generate

        presence_penalty : typing.Optional[float]
            Presence penalty

        frequency_penalty : typing.Optional[float]
            Frequency penalty

        user : typing.Optional[str]
            A unique identifier representing your end-user

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ChatCompletion
            Successful response

        Examples
        --------
        from letta_client import ChatCompletionDeveloperMessageParam, Letta

        client = Letta(
            project="YOUR_PROJECT",
            token="YOUR_TOKEN",
        )
        client.chat.create_chat_completion(
            model="model",
            messages=[
                ChatCompletionDeveloperMessageParam(
                    content="content",
                )
            ],
        )
        """
        _response = self._raw_client.create_chat_completion(
            model=model,
            messages=messages,
            temperature=temperature,
            top_p=top_p,
            n=n,
            stream=stream,
            stop=stop,
            max_tokens=max_tokens,
            presence_penalty=presence_penalty,
            frequency_penalty=frequency_penalty,
            user=user,
            request_options=request_options,
        )
        return _response.data


class AsyncChatClient:
    def __init__(self, *, client_wrapper: AsyncClientWrapper):
        self._raw_client = AsyncRawChatClient(client_wrapper=client_wrapper)

    @property
    def with_raw_response(self) -> AsyncRawChatClient:
        """
        Retrieves a raw implementation of this client that returns raw responses.

        Returns
        -------
        AsyncRawChatClient
        """
        return self._raw_client

    async def create_chat_completion(
        self,
        *,
        model: str,
        messages: typing.Sequence[ChatCompletionRequestMessagesItem],
        temperature: typing.Optional[float] = OMIT,
        top_p: typing.Optional[float] = OMIT,
        n: typing.Optional[int] = OMIT,
        stream: typing.Optional[bool] = OMIT,
        stop: typing.Optional[ChatCompletionRequestStop] = OMIT,
        max_tokens: typing.Optional[int] = OMIT,
        presence_penalty: typing.Optional[float] = OMIT,
        frequency_penalty: typing.Optional[float] = OMIT,
        user: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> ChatCompletion:
        """
        Create a chat completion using a Letta agent (OpenAI-compatible).

        This endpoint provides full OpenAI API compatibility. The agent is selected based on:
        - The 'model' parameter in the request (should contain an agent ID in format 'agent-...')

        When streaming is enabled (stream=true), the response will be Server-Sent Events
        with ChatCompletionChunk objects.

        Parameters
        ----------
        model : str
            ID of the model to use

        messages : typing.Sequence[ChatCompletionRequestMessagesItem]
            Messages comprising the conversation so far

        temperature : typing.Optional[float]
            Sampling temperature

        top_p : typing.Optional[float]
            Nucleus sampling parameter

        n : typing.Optional[int]
            Number of chat completion choices to generate

        stream : typing.Optional[bool]
            Whether to stream back partial progress

        stop : typing.Optional[ChatCompletionRequestStop]
            Sequences where the API will stop generating

        max_tokens : typing.Optional[int]
            Maximum number of tokens to generate

        presence_penalty : typing.Optional[float]
            Presence penalty

        frequency_penalty : typing.Optional[float]
            Frequency penalty

        user : typing.Optional[str]
            A unique identifier representing your end-user

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ChatCompletion
            Successful response

        Examples
        --------
        import asyncio

        from letta_client import AsyncLetta, ChatCompletionDeveloperMessageParam

        client = AsyncLetta(
            project="YOUR_PROJECT",
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.chat.create_chat_completion(
                model="model",
                messages=[
                    ChatCompletionDeveloperMessageParam(
                        content="content",
                    )
                ],
            )


        asyncio.run(main())
        """
        _response = await self._raw_client.create_chat_completion(
            model=model,
            messages=messages,
            temperature=temperature,
            top_p=top_p,
            n=n,
            stream=stream,
            stop=stop,
            max_tokens=max_tokens,
            presence_penalty=presence_penalty,
            frequency_penalty=frequency_penalty,
            user=user,
            request_options=request_options,
        )
        return _response.data
