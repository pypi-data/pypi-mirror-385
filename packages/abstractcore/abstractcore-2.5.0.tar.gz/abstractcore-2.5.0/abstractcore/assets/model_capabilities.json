{
  "models": {
    "gpt-4": {
      "max_output_tokens": 4096,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": true,
      "max_tools": -1,
      "vision_support": false,
      "audio_support": false,
      "source": "OpenAI official docs",
      "canonical_name": "gpt-4",
      "aliases": [],
      "max_tokens": 128000
    },
    "gpt-4-turbo-with-vision": {
      "max_output_tokens": 4096,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": true,
      "max_tools": -1,
      "vision_support": true,
      "audio_support": false,
      "image_resolutions": [
        "variable"
      ],
      "notes": "GPT-4 Turbo with vision capabilities",
      "source": "OpenAI official docs 2025",
      "canonical_name": "gpt-4-turbo-with-vision",
      "aliases": [
        "gpt-4-turbo-vision",
        "gpt-4-vision-preview"
      ],
      "max_tokens": 128000
    },
    "gpt-4o": {
      "max_output_tokens": 16384,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": true,
      "max_tools": -1,
      "vision_support": true,
      "audio_support": true,
      "video_support": true,
      "image_resolutions": [
        "variable"
      ],
      "notes": "Multimodal omni model, 2x faster, half price, 5x higher rate limits (updated Nov 2024)",
      "source": "OpenAI official docs 2025",
      "canonical_name": "gpt-4o",
      "aliases": [],
      "max_tokens": 128000
    },
    "gpt-4o-long-output": {
      "max_output_tokens": 64000,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": true,
      "max_tools": -1,
      "vision_support": true,
      "audio_support": true,
      "notes": "16x output capacity variant",
      "source": "OpenAI official docs",
      "canonical_name": "gpt-4o-long-output",
      "aliases": [],
      "max_tokens": 128000
    },
    "gpt-4o-mini": {
      "max_output_tokens": 16000,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": true,
      "max_tools": -1,
      "vision_support": true,
      "audio_support": true,
      "source": "OpenAI official docs",
      "canonical_name": "gpt-4o-mini",
      "aliases": [],
      "max_tokens": 128000
    },
    "gpt-3.5-turbo": {
      "max_output_tokens": 4096,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": true,
      "max_tools": -1,
      "vision_support": false,
      "audio_support": false,
      "source": "OpenAI official docs",
      "canonical_name": "gpt-3.5-turbo",
      "aliases": [],
      "max_tokens": 16385
    },
    "o1": {
      "max_output_tokens": 32768,
      "tool_support": "none",
      "structured_output": "prompted",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "notes": "Reasoning model, no native tool support",
      "source": "OpenAI official docs",
      "canonical_name": "o1",
      "aliases": [],
      "max_tokens": 128000
    },
    "o1-mini": {
      "max_output_tokens": 65536,
      "tool_support": "none",
      "structured_output": "prompted",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "source": "OpenAI official docs",
      "canonical_name": "o1-mini",
      "aliases": [],
      "max_tokens": 128000
    },
    "o3": {
      "max_output_tokens": 32768,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": true,
      "max_tools": -1,
      "vision_support": false,
      "audio_support": false,
      "notes": "May hallucinate tools with complex sets",
      "source": "OpenAI official docs",
      "canonical_name": "o3",
      "aliases": [],
      "max_tokens": 128000
    },
    "o3-mini": {
      "max_output_tokens": 32768,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": true,
      "max_tools": -1,
      "vision_support": false,
      "audio_support": false,
      "notes": "Known issues in Assistants API",
      "source": "OpenAI official docs",
      "canonical_name": "o3-mini",
      "aliases": [],
      "max_tokens": 128000
    },
    "claude-3.5-sonnet": {
      "max_output_tokens": 8192,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": true,
      "max_tools": -1,
      "vision_support": true,
      "image_resolutions": [
        "up to 1568x1568"
      ],
      "audio_support": false,
      "notes": "disable_parallel_tool_use option available",
      "source": "Anthropic official docs",
      "canonical_name": "claude-3.5-sonnet",
      "aliases": [],
      "max_tokens": 200000
    },
    "claude-3.7-sonnet": {
      "max_output_tokens": 128000,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": true,
      "max_tools": -1,
      "vision_support": true,
      "image_resolutions": [
        "up to 1568x1568"
      ],
      "audio_support": false,
      "notes": "Extended output with beta header",
      "source": "Anthropic official docs",
      "canonical_name": "claude-3.7-sonnet",
      "aliases": [],
      "max_tokens": 200000
    },
    "claude-3.5-haiku": {
      "max_output_tokens": 8192,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": true,
      "max_tools": -1,
      "vision_support": true,
      "image_resolutions": [
        "up to 1568x1568"
      ],
      "audio_support": false,
      "notes": "More likely to call unnecessary tools",
      "source": "Anthropic official docs",
      "canonical_name": "claude-3.5-haiku",
      "aliases": [
        "claude-3-5-haiku-20241022"
      ],
      "max_tokens": 200000
    },
    "claude-3-opus": {
      "max_output_tokens": 4096,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": false,
      "max_tools": 1,
      "vision_support": true,
      "image_resolutions": [
        "up to 1568x1568"
      ],
      "audio_support": false,
      "source": "Anthropic official docs",
      "canonical_name": "claude-3-opus",
      "aliases": [],
      "max_tokens": 200000
    },
    "claude-3-sonnet": {
      "max_output_tokens": 4096,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": false,
      "max_tools": 1,
      "vision_support": true,
      "image_resolutions": [
        "up to 1568x1568"
      ],
      "audio_support": false,
      "source": "Anthropic official docs",
      "canonical_name": "claude-3-sonnet",
      "aliases": [],
      "max_tokens": 200000
    },
    "claude-3-haiku": {
      "max_output_tokens": 4096,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": false,
      "max_tools": 1,
      "vision_support": true,
      "image_resolutions": [
        "up to 1568x1568"
      ],
      "audio_support": false,
      "source": "Anthropic official docs",
      "canonical_name": "claude-3-haiku",
      "aliases": [],
      "max_tokens": 200000
    },
    "claude-4-opus": {
      "max_output_tokens": 4096,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": true,
      "max_tools": -1,
      "vision_support": true,
      "image_resolutions": [
        "up to 1568x1568"
      ],
      "audio_support": false,
      "notes": "Claude 4 Opus with enhanced capabilities",
      "source": "Anthropic official docs",
      "canonical_name": "claude-4-opus",
      "aliases": [],
      "max_tokens": 200000
    },
    "claude-4.1-opus": {
      "max_output_tokens": 4096,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": true,
      "max_tools": -1,
      "vision_support": true,
      "image_resolutions": [
        "up to 1568x1568"
      ],
      "audio_support": false,
      "notes": "Claude 4.1 Opus with improved performance",
      "source": "Anthropic official docs",
      "canonical_name": "claude-4.1-opus",
      "aliases": [],
      "max_tokens": 200000
    },
    "claude-4-sonnet": {
      "max_output_tokens": 8192,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": true,
      "max_tools": -1,
      "vision_support": true,
      "image_resolutions": [
        "up to 1568x1568"
      ],
      "audio_support": false,
      "notes": "Claude 4 Sonnet with balanced performance",
      "source": "Anthropic official docs",
      "canonical_name": "claude-4-sonnet",
      "aliases": [],
      "max_tokens": 200000
    },
    "claude-4.5-sonnet": {
      "max_output_tokens": 8192,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": true,
      "max_tools": -1,
      "vision_support": true,
      "image_resolutions": [
        "up to 1568x1568"
      ],
      "audio_support": false,
      "notes": "Claude 4.5 Sonnet with enhanced reasoning",
      "source": "Anthropic official docs",
      "canonical_name": "claude-4.5-sonnet",
      "aliases": [],
      "max_tokens": 200000
    },
    "llama-3.2-1b": {
      "max_output_tokens": 2048,
      "tool_support": "prompted",
      "structured_output": "prompted",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "notes": "Text only, optimized for edge",
      "source": "Meta official docs",
      "canonical_name": "llama-3.2-1b",
      "aliases": [],
      "max_tokens": 8192
    },
    "llama-3.2-3b": {
      "max_output_tokens": 2048,
      "tool_support": "prompted",
      "structured_output": "prompted",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "notes": "Text only, optimized for edge",
      "source": "Meta official docs",
      "canonical_name": "llama-3.2-3b",
      "aliases": [],
      "max_tokens": 8192
    },
    "llama-3.2-11b-vision": {
      "max_output_tokens": 2048,
      "tool_support": "prompted",
      "structured_output": "prompted",
      "parallel_tools": false,
      "vision_support": true,
      "image_resolutions": [
        "variable"
      ],
      "audio_support": false,
      "notes": "Vision model, may be limited by deployment platform",
      "source": "Meta official docs",
      "canonical_name": "llama-3.2-11b-vision",
      "aliases": [],
      "max_tokens": 128000
    },
    "llama-3.3-70b": {
      "max_output_tokens": 8192,
      "tool_support": "prompted",
      "structured_output": "prompted",
      "parallel_tools": true,
      "vision_support": false,
      "audio_support": false,
      "notes": "Improved multilingual and tool use",
      "source": "Meta official docs",
      "canonical_name": "llama-3.3-70b",
      "aliases": [],
      "max_tokens": 128000
    },
    "llama-3.1-8b": {
      "max_output_tokens": 8192,
      "tool_support": "native",
      "structured_output": "prompted",
      "parallel_tools": true,
      "vision_support": false,
      "audio_support": false,
      "notes": "Fine-tuned on tool calling",
      "source": "Meta official docs",
      "canonical_name": "llama-3.1-8b",
      "aliases": [],
      "max_tokens": 128000
    },
    "llama-3.1-70b": {
      "max_output_tokens": 8192,
      "tool_support": "native",
      "structured_output": "prompted",
      "parallel_tools": true,
      "vision_support": false,
      "audio_support": false,
      "notes": "Fine-tuned on tool calling",
      "source": "Meta official docs",
      "canonical_name": "llama-3.1-70b",
      "aliases": [],
      "max_tokens": 128000
    },
    "llama-3.1-405b": {
      "max_output_tokens": 8192,
      "tool_support": "native",
      "structured_output": "prompted",
      "parallel_tools": true,
      "vision_support": false,
      "audio_support": false,
      "notes": "Largest Llama model",
      "source": "Meta official docs",
      "canonical_name": "llama-3.1-405b",
      "aliases": [],
      "max_tokens": 128000
    },
    "llama-4": {
      "max_output_tokens": 8192,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": true,
      "vision_support": true,
      "audio_support": true,
      "notes": "Multimodal with early fusion, 109B total params (MoE)",
      "source": "Meta announcement",
      "canonical_name": "llama-4",
      "aliases": [
        "llama4-17b-scout-16e-instruct",
        "llama-4-17b-scout-16e-instruct"
      ],
      "max_tokens": 10000000
    },
    "qwen2.5-0.5b": {
      "max_output_tokens": 8192,
      "tool_support": "prompted",
      "structured_output": "prompted",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "notes": "Qwen2.5 0.5B model",
      "source": "Alibaba official docs",
      "canonical_name": "qwen2.5-0.5b",
      "aliases": [],
      "max_tokens": 32768
    },
    "qwen2.5-1.5b": {
      "max_output_tokens": 8192,
      "tool_support": "prompted",
      "structured_output": "prompted",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "notes": "Qwen2.5 1.5B model",
      "source": "Alibaba official docs",
      "canonical_name": "qwen2.5-1.5b",
      "aliases": [],
      "max_tokens": 32768
    },
    "qwen2.5-3b": {
      "max_output_tokens": 8192,
      "tool_support": "prompted",
      "structured_output": "prompted",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "notes": "Qwen2.5 3B model",
      "source": "Alibaba official docs",
      "canonical_name": "qwen2.5-3b",
      "aliases": [],
      "max_tokens": 32768
    },
    "qwen2.5-7b": {
      "max_output_tokens": 8192,
      "tool_support": "prompted",
      "structured_output": "prompted",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "notes": "Qwen2.5 7B model with MCP support",
      "source": "Alibaba official docs",
      "canonical_name": "qwen2.5-7b",
      "aliases": [],
      "max_tokens": 131072
    },
    "qwen2.5-14b": {
      "max_output_tokens": 8192,
      "tool_support": "prompted",
      "structured_output": "prompted",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "notes": "Qwen2.5 14B model",
      "source": "Alibaba official docs",
      "canonical_name": "qwen2.5-14b",
      "aliases": [],
      "max_tokens": 131072
    },
    "qwen2.5-32b": {
      "max_output_tokens": 8192,
      "tool_support": "prompted",
      "structured_output": "prompted",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "notes": "Qwen2.5 32B model",
      "source": "Alibaba official docs",
      "canonical_name": "qwen2.5-32b",
      "aliases": [],
      "max_tokens": 131072
    },
    "qwen2.5-72b": {
      "max_output_tokens": 8192,
      "tool_support": "prompted",
      "structured_output": "prompted",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "notes": "Qwen2.5 72B model",
      "source": "Alibaba official docs",
      "canonical_name": "qwen2.5-72b",
      "aliases": [],
      "max_tokens": 131072
    },
    "qwen3-0.6b": {
      "max_output_tokens": 8192,
      "tool_support": "prompted",
      "structured_output": "prompted",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "thinking_support": true,
      "notes": "Qwen3 base model with thinking capabilities",
      "source": "Alibaba Qwen3 technical report",
      "canonical_name": "qwen3-0.6b",
      "aliases": [],
      "max_tokens": 32768
    },
    "qwen3-1.7b": {
      "max_output_tokens": 8192,
      "tool_support": "prompted",
      "structured_output": "prompted",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "thinking_support": true,
      "notes": "Qwen3 1.7B model with thinking capabilities",
      "source": "Alibaba Qwen3 technical report",
      "canonical_name": "qwen3-1.7b",
      "aliases": [],
      "max_tokens": 32768
    },
    "qwen3-4b": {
      "max_output_tokens": 8192,
      "tool_support": "prompted",
      "structured_output": "prompted",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "thinking_support": true,
      "notes": "Qwen3 4B model with extended context via YaRN scaling",
      "source": "Alibaba Qwen3 technical report",
      "canonical_name": "qwen3-4b",
      "aliases": [],
      "max_tokens": 131072
    },
    "qwen3-32b": {
      "max_output_tokens": 8192,
      "tool_support": "prompted",
      "structured_output": "prompted",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "thinking_support": true,
      "notes": "Qwen3 32B model with advanced thinking capabilities",
      "source": "Alibaba Qwen3 technical report",
      "canonical_name": "qwen3-32b",
      "aliases": [],
      "max_tokens": 131072
    },
    "qwen3-30b-a3b": {
      "max_output_tokens": 8192,
      "tool_support": "prompted",
      "structured_output": "prompted",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "thinking_support": true,
      "notes": "Qwen3 MoE model with 4-bit precision, 30B total/3B active parameters",
      "source": "Alibaba Qwen3 technical report",
      "canonical_name": "qwen3-30b-a3b",
      "aliases": [],
      "max_tokens": 40960
    },
    "qwen3-coder-30b": {
      "max_output_tokens": 8192,
      "tool_support": "native",
      "structured_output": "prompted",
      "parallel_tools": true,
      "vision_support": false,
      "audio_support": false,
      "notes": "Code-focused model with native tool support via chatml-function-calling format",
      "source": "Alibaba official docs",
      "canonical_name": "qwen3-coder-30b",
      "aliases": [],
      "max_tokens": 32768
    },
    "qwen2-vl": {
      "max_output_tokens": 8192,
      "tool_support": "prompted",
      "structured_output": "prompted",
      "parallel_tools": false,
      "vision_support": true,
      "image_resolutions": [
        "variable"
      ],
      "audio_support": false,
      "source": "Alibaba official docs",
      "canonical_name": "qwen2-vl",
      "aliases": [],
      "max_tokens": 32768
    },
    "qwen2.5-vl": {
      "max_output_tokens": 8192,
      "tool_support": "prompted",
      "structured_output": "native",
      "parallel_tools": false,
      "vision_support": true,
      "image_resolutions": [
        "variable"
      ],
      "audio_support": false,
      "notes": "Qwen2.5-VL multimodal model",
      "source": "Alibaba official docs",
      "canonical_name": "qwen2.5-vl",
      "aliases": [],
      "max_tokens": 128000
    },
    "phi-2": {
      "max_output_tokens": 2048,
      "tool_support": "none",
      "structured_output": "none",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "source": "Microsoft official docs",
      "canonical_name": "phi-2",
      "aliases": [],
      "max_tokens": 2048
    },
    "phi-3-mini": {
      "max_output_tokens": 4096,
      "tool_support": "prompted",
      "structured_output": "prompted",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "notes": "Poor JSON, use XML format",
      "source": "Microsoft official docs",
      "canonical_name": "phi-3-mini",
      "aliases": [],
      "max_tokens": 4096
    },
    "phi-3-small": {
      "max_output_tokens": 8192,
      "tool_support": "prompted",
      "structured_output": "prompted",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "source": "Microsoft official docs",
      "canonical_name": "phi-3-small",
      "aliases": [],
      "max_tokens": 8192
    },
    "phi-3-medium": {
      "max_output_tokens": 4096,
      "tool_support": "prompted",
      "structured_output": "prompted",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "source": "Microsoft official docs",
      "canonical_name": "phi-3-medium",
      "aliases": [],
      "max_tokens": 128000
    },
    "phi-3.5-mini": {
      "max_output_tokens": 4096,
      "tool_support": "prompted",
      "structured_output": "prompted",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "source": "Microsoft official docs",
      "canonical_name": "phi-3.5-mini",
      "aliases": [],
      "max_tokens": 128000
    },
    "phi-3.5-moe": {
      "max_output_tokens": 4096,
      "tool_support": "prompted",
      "structured_output": "prompted",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "notes": "Mixture of Experts",
      "source": "Microsoft official docs",
      "canonical_name": "phi-3.5-moe",
      "aliases": [],
      "max_tokens": 128000
    },
    "phi-3-vision": {
      "max_output_tokens": 4096,
      "tool_support": "prompted",
      "structured_output": "prompted",
      "parallel_tools": false,
      "vision_support": true,
      "image_resolutions": [
        "variable"
      ],
      "audio_support": false,
      "source": "Microsoft official docs",
      "canonical_name": "phi-3-vision",
      "aliases": [],
      "max_tokens": 128000
    },
    "phi-4": {
      "max_output_tokens": 16000,
      "tool_support": "prompted",
      "structured_output": "prompted",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "notes": "Extended to 16K during mid-training",
      "source": "Microsoft official docs",
      "canonical_name": "phi-4",
      "aliases": [],
      "max_tokens": 16000
    },
    "mistral-7b": {
      "max_output_tokens": 8192,
      "tool_support": "prompted",
      "structured_output": "prompted",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "source": "Mistral AI docs",
      "canonical_name": "mistral-7b",
      "aliases": [],
      "max_tokens": 8192
    },
    "mixtral-8x7b": {
      "max_output_tokens": 32768,
      "tool_support": "prompted",
      "structured_output": "native",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "notes": "MoE architecture",
      "source": "Mistral AI docs",
      "canonical_name": "mixtral-8x7b",
      "aliases": [],
      "max_tokens": 32768
    },
    "mixtral-8x22b": {
      "max_output_tokens": 65536,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "source": "Mistral AI docs",
      "canonical_name": "mixtral-8x22b",
      "aliases": [],
      "max_tokens": 65536
    },
    "mistral-small": {
      "max_output_tokens": 32768,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "source": "Mistral AI docs",
      "canonical_name": "mistral-small",
      "aliases": [],
      "max_tokens": 32768
    },
    "mistral-medium": {
      "max_output_tokens": 32768,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "source": "Mistral AI docs",
      "canonical_name": "mistral-medium",
      "aliases": [],
      "max_tokens": 32768
    },
    "mistral-large": {
      "max_output_tokens": 128000,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": true,
      "vision_support": false,
      "audio_support": false,
      "source": "Mistral AI docs",
      "canonical_name": "mistral-large",
      "aliases": [],
      "max_tokens": 128000
    },
    "codestral": {
      "max_output_tokens": 32768,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "notes": "Code-specialized",
      "source": "Mistral AI docs",
      "canonical_name": "codestral",
      "aliases": [],
      "max_tokens": 32768
    },
    "magistral-small-2509": {
      "max_output_tokens": 8192,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": true,
      "vision_support": true,
      "audio_support": false,
      "video_support": false,
      "image_resolutions": [
        "variable"
      ],
      "max_image_resolution": "variable",
      "notes": "Mistral vision model optimized for multimodal tasks",
      "source": "Mistral AI 2025 release",
      "canonical_name": "magistral-small-2509",
      "aliases": [
        "mistralai/magistral-small-2509"
      ],
      "max_tokens": 128000
    },
    "Qwen/Qwen3-VL-8B-Instruct-FP8": {
      "max_output_tokens": 8192,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": true,
      "vision_support": true,
      "audio_support": false,
      "video_support": false,
      "image_resolutions": [
        "variable"
      ],
      "max_image_resolution": "variable",
      "image_patch_size": 16,
      "max_image_tokens": 24576,
      "pixel_grouping": "32x32",
      "notes": "Qwen3-VL 8B model with FP8 quantization for HuggingFace, optimized for efficient inference",
      "source": "Qwen team 2025 HuggingFace release",
      "canonical_name": "Qwen/Qwen3-VL-8B-Instruct-FP8",
      "aliases": [
        "qwen3-vl-8b-fp8",
        "qwen3-vl-8b-instruct-fp8"
      ],
      "max_tokens": 262144
    },
    "llama3.2-vision:11b": {
      "max_output_tokens": 4096,
      "tool_support": "prompted",
      "structured_output": "prompted",
      "parallel_tools": false,
      "vision_support": true,
      "audio_support": false,
      "video_support": false,
      "image_resolutions": [
        "560x560",
        "1120x560",
        "560x1120",
        "1120x1120"
      ],
      "max_image_resolution": "1120x1120",
      "image_patch_size": 14,
      "max_image_tokens": 6400,
      "notes": "Llama 3.2 Vision 11B model with multimodal capabilities for visual recognition and reasoning",
      "source": "Meta AI Llama 3.2 release",
      "canonical_name": "llama3.2-vision:11b",
      "aliases": [
        "llama3.2-vision-11b",
        "llama-3.2-vision:11b"
      ],
      "max_tokens": 131072
    },
    "llama3.2-vision:70b": {
      "max_output_tokens": 4096,
      "tool_support": "prompted",
      "structured_output": "prompted",
      "parallel_tools": false,
      "vision_support": true,
      "audio_support": false,
      "video_support": false,
      "image_resolutions": [
        "560x560",
        "1120x560",
        "560x1120",
        "1120x1120"
      ],
      "max_image_resolution": "1120x1120",
      "image_patch_size": 14,
      "max_image_tokens": 6400,
      "notes": "Llama 3.2 Vision 70B model with advanced multimodal capabilities for complex visual reasoning",
      "source": "Meta AI Llama 3.2 release",
      "canonical_name": "llama3.2-vision:70b",
      "aliases": [
        "llama3.2-vision-70b",
        "llama-3.2-vision:70b"
      ],
      "max_tokens": 131072
    },
    "llama3.2-vision:90b": {
      "max_output_tokens": 4096,
      "tool_support": "prompted",
      "structured_output": "prompted",
      "parallel_tools": false,
      "vision_support": true,
      "audio_support": false,
      "video_support": false,
      "image_resolutions": [
        "560x560",
        "1120x560",
        "560x1120",
        "1120x1120"
      ],
      "max_image_resolution": "1120x1120",
      "image_patch_size": 14,
      "max_image_tokens": 6400,
      "notes": "Llama 3.2 Vision 90B model with top-tier multimodal capabilities for advanced visual understanding",
      "source": "Meta AI Llama 3.2 release",
      "canonical_name": "llama3.2-vision:90b",
      "aliases": [
        "llama3.2-vision-90b",
        "llama-3.2-vision:90b"
      ],
      "max_tokens": 131072
    },
    "gemma-2b": {
      "max_output_tokens": 8192,
      "tool_support": "none",
      "structured_output": "prompted",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "source": "Google docs",
      "canonical_name": "gemma-2b",
      "aliases": [],
      "max_tokens": 8192
    },
    "gemma-7b": {
      "max_output_tokens": 8192,
      "tool_support": "none",
      "structured_output": "prompted",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "source": "Google docs",
      "canonical_name": "gemma-7b",
      "aliases": [],
      "max_tokens": 8192
    },
    "gemma2-9b": {
      "max_output_tokens": 8192,
      "tool_support": "prompted",
      "structured_output": "native",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "source": "Google docs",
      "canonical_name": "gemma2-9b",
      "aliases": [],
      "max_tokens": 8192
    },
    "gemma2-27b": {
      "max_output_tokens": 8192,
      "tool_support": "prompted",
      "structured_output": "native",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "source": "Google docs",
      "canonical_name": "gemma2-27b",
      "aliases": [],
      "max_tokens": 8192
    },
    "gemma3": {
      "max_output_tokens": 8192,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": true,
      "vision_support": false,
      "audio_support": false,
      "notes": "Native function calling support introduced in Gemma 3",
      "source": "Google docs",
      "canonical_name": "gemma3",
      "aliases": [],
      "max_tokens": 128000
    },
    "codegemma": {
      "max_output_tokens": 8192,
      "tool_support": "none",
      "structured_output": "prompted",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "notes": "Code-specialized",
      "source": "Google docs",
      "canonical_name": "codegemma",
      "aliases": [],
      "max_tokens": 8192
    },
    "paligemma": {
      "max_output_tokens": 1024,
      "tool_support": "none",
      "structured_output": "none",
      "parallel_tools": false,
      "vision_support": true,
      "image_resolutions": [
        "224x224",
        "448x448",
        "896x896"
      ],
      "audio_support": false,
      "notes": "Vision-language model",
      "source": "Google docs",
      "canonical_name": "paligemma",
      "aliases": [],
      "max_tokens": 8192
    },
    "glm-4": {
      "max_output_tokens": 4096,
      "tool_support": "prompted",
      "structured_output": "prompted",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "notes": "ChatGLM4 from Zhipu AI",
      "source": "Model documentation",
      "canonical_name": "glm-4",
      "aliases": [],
      "max_tokens": 128000
    },
    "glm-4-9b": {
      "max_output_tokens": 4096,
      "tool_support": "prompted",
      "structured_output": "prompted",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "notes": "9B parameter version",
      "source": "Model documentation",
      "canonical_name": "glm-4-9b",
      "aliases": [],
      "max_tokens": 128000
    },
    "glm-4-9b-0414-4bit": {
      "max_output_tokens": 4096,
      "tool_support": "prompted",
      "structured_output": "prompted",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "notes": "MLX quantized version",
      "source": "Model documentation",
      "canonical_name": "glm-4-9b-0414-4bit",
      "aliases": [],
      "max_tokens": 128000
    },
    "deepseek-r1": {
      "max_output_tokens": 8192,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": true,
      "vision_support": false,
      "audio_support": false,
      "notes": "Reasoning model with native tool calling capability",
      "source": "MLX community",
      "canonical_name": "deepseek-r1",
      "aliases": [],
      "max_tokens": 32768
    },
    "qwen3": {
      "max_output_tokens": 8192,
      "tool_support": "prompted",
      "structured_output": "prompted",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "notes": "Instruct model with good reasoning. Use prompted tool support when running via MLX.",
      "source": "MLX community",
      "canonical_name": "qwen3",
      "aliases": [],
      "max_tokens": 32768
    },
    "qwen3-14b": {
      "max_output_tokens": 8192,
      "tool_support": "prompted",
      "structured_output": "prompted",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "thinking_support": true,
      "notes": "Qwen3 14B model with thinking capabilities",
      "source": "Alibaba Qwen3 technical report",
      "canonical_name": "qwen3-14b",
      "aliases": [],
      "max_tokens": 131072
    },
    "qwen3-next-80b-a3b": {
      "max_output_tokens": 16384,
      "tool_support": "prompted",
      "structured_output": "prompted",
      "parallel_tools": true,
      "vision_support": false,
      "audio_support": false,
      "thinking_support": true,
      "notes": "Qwen3-Next hybrid MoE with 512 experts/10 activated, extensible to 1M tokens with YaRN. Uses <|tool_call|> format for tool calls",
      "source": "Alibaba Qwen3-Next technical report",
      "canonical_name": "qwen3-next-80b-a3b",
      "aliases": [
        "qwen/qwen3-next-80b"
      ],
      "max_tokens": 262144
    },
    "gpt-5": {
      "max_output_tokens": 8192,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": true,
      "max_tools": -1,
      "vision_support": true,
      "audio_support": true,
      "notes": "GPT-5 base model with multimodal capabilities",
      "source": "OpenAI official docs",
      "canonical_name": "gpt-5",
      "aliases": [],
      "max_tokens": 200000
    },
    "gpt-5-turbo": {
      "max_output_tokens": 4096,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": true,
      "max_tools": -1,
      "vision_support": true,
      "audio_support": true,
      "notes": "GPT-5 Turbo with faster inference",
      "source": "OpenAI official docs",
      "canonical_name": "gpt-5-turbo",
      "aliases": [],
      "max_tokens": 200000
    },
    "gpt-5-pro": {
      "max_output_tokens": 16384,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": true,
      "max_tools": -1,
      "vision_support": true,
      "audio_support": true,
      "notes": "GPT-5 Pro with extended output capabilities",
      "source": "OpenAI official docs",
      "canonical_name": "gpt-5-pro",
      "aliases": [],
      "max_tokens": 200000
    },
    "gpt-5-mini": {
      "max_output_tokens": 8192,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": true,
      "max_tools": -1,
      "vision_support": true,
      "audio_support": true,
      "notes": "GPT-5 Mini with cost-optimized performance",
      "source": "OpenAI official docs",
      "canonical_name": "gpt-5-mini",
      "aliases": [],
      "max_tokens": 200000
    },
    "gpt-5-vision": {
      "max_output_tokens": 8192,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": true,
      "max_tools": -1,
      "vision_support": true,
      "image_resolutions": [
        "up to 2048x2048"
      ],
      "audio_support": true,
      "notes": "GPT-5 Vision with enhanced multimodal capabilities",
      "source": "OpenAI official docs",
      "canonical_name": "gpt-5-vision",
      "aliases": [],
      "max_tokens": 200000
    },
    "qwen3-8b": {
      "max_output_tokens": 8192,
      "tool_support": "prompted",
      "structured_output": "prompted",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "thinking_support": true,
      "notes": "Qwen3 8B model with thinking capabilities",
      "source": "Alibaba Qwen3 technical report",
      "canonical_name": "qwen3-8b",
      "aliases": [],
      "max_tokens": 131072
    },
    "qwen3-235b-a22b": {
      "max_output_tokens": 8192,
      "tool_support": "prompted",
      "structured_output": "prompted",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "thinking_support": true,
      "notes": "Qwen3 MoE model with 4-bit precision, 235B total/22B active parameters",
      "source": "Alibaba Qwen3 technical report",
      "canonical_name": "qwen3-235b-a22b",
      "aliases": [],
      "max_tokens": 40960
    },
    "qwen3-vl": {
      "max_output_tokens": 8192,
      "tool_support": "prompted",
      "structured_output": "prompted",
      "parallel_tools": false,
      "vision_support": true,
      "video_support": true,
      "audio_support": false,
      "image_resolutions": [
        "variable"
      ],
      "image_patch_size": 16,
      "max_image_tokens": 24576,
      "pixel_grouping": "32x32",
      "notes": "Qwen3-VL multimodal model with vision and video support, 32x32 pixel patches",
      "source": "Alibaba Qwen3-VL technical report",
      "canonical_name": "qwen3-vl",
      "aliases": [],
      "max_tokens": 131072
    },
    "qwen3-vl-4b": {
      "max_output_tokens": 8192,
      "tool_support": "prompted",
      "structured_output": "prompted",
      "parallel_tools": false,
      "vision_support": true,
      "video_support": true,
      "audio_support": false,
      "image_resolutions": [
        "variable"
      ],
      "max_image_resolution": "variable",
      "image_patch_size": 16,
      "max_image_tokens": 24576,
      "pixel_grouping": "32x32",
      "notes": "Qwen3-VL 4B dense model with 256K context, optimized for LMStudio",
      "source": "Alibaba Qwen3-VL technical report 2025",
      "canonical_name": "qwen3-vl-4b",
      "aliases": [
        "qwen/qwen3-vl-4b"
      ],
      "max_tokens": 262144
    },
    "qwen3-vl-8b": {
      "max_output_tokens": 8192,
      "tool_support": "prompted",
      "structured_output": "prompted",
      "parallel_tools": false,
      "vision_support": true,
      "video_support": true,
      "audio_support": false,
      "image_resolutions": [
        "variable"
      ],
      "max_image_resolution": "variable",
      "image_patch_size": 16,
      "max_image_tokens": 24576,
      "pixel_grouping": "32x32",
      "notes": "Qwen3-VL 8B dense model with 256K context, optimized for LMStudio",
      "source": "Alibaba Qwen3-VL technical report 2025",
      "canonical_name": "qwen3-vl-8b",
      "aliases": [
        "qwen/qwen3-vl-8b"
      ],
      "max_tokens": 262144
    },
    "qwen3-vl-30b": {
      "max_output_tokens": 8192,
      "tool_support": "prompted",
      "structured_output": "prompted",
      "parallel_tools": false,
      "vision_support": true,
      "video_support": true,
      "audio_support": false,
      "image_resolutions": [
        "variable"
      ],
      "max_image_resolution": "variable",
      "image_patch_size": 16,
      "max_image_tokens": 24576,
      "pixel_grouping": "32x32",
      "notes": "Qwen3-VL 30B MoE model (30.5B total/3.3B active), best performing vision model, 256K context",
      "source": "Alibaba Qwen3-VL technical report 2025",
      "canonical_name": "qwen3-vl-30b",
      "aliases": [
        "qwen/qwen3-vl-30b"
      ],
      "max_tokens": 262144
    },
    "qwen2.5-vl-7b": {
      "max_output_tokens": 8192,
      "tool_support": "prompted",
      "structured_output": "native",
      "parallel_tools": false,
      "vision_support": true,
      "audio_support": false,
      "image_resolutions": [
        "56x56 to 3584x3584"
      ],
      "max_image_resolution": "3584x3584",
      "image_patch_size": 14,
      "max_image_tokens": 16384,
      "pixel_grouping": "28x28",
      "notes": "Qwen2.5-VL 7B parameter vision model, 28x28 pixel patches, max 3584x3584 resolution",
      "source": "Alibaba official docs",
      "canonical_name": "qwen2.5-vl-7b",
      "aliases": [
        "qwen/qwen2.5-vl-7b",
        "unsloth/Qwen2.5-VL-7B-Instruct-GGUF"
      ],
      "max_tokens": 128000
    },    
    "gemma3-4b": {
      "max_output_tokens": 8192,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": true,
      "vision_support": true,
      "audio_support": false,
      "video_support": false,
      "image_resolutions": [
        "896x896"
      ],
      "max_image_resolution": "896x896",
      "vision_encoder": "SigLIP-400M",
      "image_tokens_per_image": 256,
      "adaptive_windowing": true,
      "notes": "Gemma3 4B parameter model with vision support, 896x896 fixed resolution with adaptive windowing",
      "source": "Google Gemma3 documentation 2025",
      "canonical_name": "gemma3-4b",
      "aliases": [
        "gemma3:4b"
      ],
      "max_tokens": 128000
    },
    "qwen2.5vl:7b": {
      "max_output_tokens": 8192,
      "tool_support": "prompted",
      "structured_output": "native",
      "parallel_tools": false,
      "vision_support": true,
      "audio_support": false,
      "image_resolutions": [
        "56x56 to 3584x3584"
      ],
      "max_image_resolution": "3584x3584",
      "image_patch_size": 14,
      "max_image_tokens": 16384,
      "pixel_grouping": "28x28",
      "notes": "Qwen2.5-VL 7B Ollama variant, 28x28 pixel patches, max 3584x3584 resolution",
      "source": "Ollama model library",
      "canonical_name": "qwen2.5vl:7b",
      "aliases": [
        "qwen2.5vl"
      ],
      "max_tokens": 128000
    },
    "gemma3:4b-it-qat": {
      "max_output_tokens": 8192,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": true,
      "vision_support": true,
      "audio_support": false,
      "video_support": false,
      "image_resolutions": [
        "896x896"
      ],
      "max_image_resolution": "896x896",
      "vision_encoder": "SigLIP-400M",
      "image_tokens_per_image": 256,
      "adaptive_windowing": true,
      "notes": "Gemma3 4B instruct-tuned quantized model for Ollama, 896x896 fixed resolution",
      "source": "Ollama model library",
      "canonical_name": "gemma3:4b-it-qat",
      "aliases": [],
      "max_tokens": 128000
    },
    "gemma3n:e4b": {
      "max_output_tokens": 8192,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": true,
      "vision_support": true,
      "audio_support": true,
      "video_support": true,
      "image_resolutions": [
        "896x896"
      ],
      "max_image_resolution": "896x896",
      "vision_encoder": "SigLIP-400M",
      "image_tokens_per_image": 256,
      "adaptive_windowing": true,
      "memory_footprint": "3GB",
      "notes": "Gemma3n device-optimized multimodal model, 896x896 fixed resolution",
      "source": "Google Gemma3n documentation 2025",
      "canonical_name": "gemma3n:e4b",
      "aliases": [
        "gemma3n:e4b:latest",
        "gemma-3n-e4b",
        "google/gemma-3n-e4b",
        "gemma3n",
        "gemma3n:e2b:latest",
        "gemma3n:e2b"
      ],
      "max_tokens": 32768
    },
    "seed-oss": {
      "max_output_tokens": 8192,
      "tool_support": "prompted",
      "structured_output": "prompted",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "thinking_support": true,
      "thinking_budget": true,
      "notes": "SEED-OSS 36B parameter model with 512K context and thinking budget control",
      "source": "ByteDance SEED-OSS documentation",
      "canonical_name": "seed-oss",
      "aliases": [],
      "max_tokens": 524288
    },
    "glm-4.5": {
      "max_output_tokens": 4096,
      "tool_support": "prompted",
      "structured_output": "prompted",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "thinking_support": true,
      "notes": "GLM-4.5 MoE model with 355B total/32B active parameters",
      "source": "Zhipu AI GLM-4.5 announcement",
      "canonical_name": "glm-4.5",
      "aliases": [],
      "max_tokens": 128000
    },
    "glm-4.6": {
      "max_output_tokens": 4096,
      "tool_support": "prompted",
      "structured_output": "prompted",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "thinking_support": true,
      "notes": "GLM-4.6 MoE model with enhanced capabilities",
      "source": "Zhipu AI GLM-4.6 announcement",
      "canonical_name": "glm-4.6",
      "aliases": [],
      "max_tokens": 128000
    },
    "glm-4.5-air": {
      "max_output_tokens": 4096,
      "tool_support": "prompted",
      "structured_output": "prompted",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "thinking_support": true,
      "notes": "GLM-4.5-Air lightweight model optimized for efficiency",
      "source": "Zhipu AI GLM-4.5-Air announcement",
      "canonical_name": "glm-4.5-air",
      "aliases": [],
      "max_tokens": 128000
    },
    "llama-4-109b": {
      "max_output_tokens": 8192,
      "tool_support": "native",
      "structured_output": "native",
      "parallel_tools": true,
      "vision_support": true,
      "audio_support": true,
      "notes": "LLaMA 4 109B total parameters (MoE), multimodal with early fusion",
      "source": "Meta LLaMA 4 announcement",
      "canonical_name": "llama-4-109b",
      "aliases": [],
      "max_tokens": 10000000
    },
    "granite3.2:2b": {
      "max_output_tokens": 8192,
      "tool_support": "prompted",
      "structured_output": "prompted",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "notes": "IBM Granite 3.2 2B text-only model with reasoning capabilities",
      "source": "IBM Granite 3.2 technical report",
      "canonical_name": "granite3.2:2b",
      "aliases": [
        "granite3.2-2b"
      ],
      "max_tokens": 32768
    },
    "granite3.2:8b": {
      "max_output_tokens": 8192,
      "tool_support": "prompted",
      "structured_output": "prompted",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "notes": "IBM Granite 3.2 8B text-only model with reasoning capabilities",
      "source": "IBM Granite 3.2 technical report",
      "canonical_name": "granite3.2:8b",
      "aliases": [
        "granite3.2-8b"
      ],
      "max_tokens": 32768
    },
    "granite3.2-vision:2b": {
      "max_output_tokens": 8192,
      "tool_support": "prompted",
      "structured_output": "prompted",
      "parallel_tools": false,
      "vision_support": true,
      "audio_support": false,
      "video_support": false,
      "image_resolutions": [
        "768x768"
      ],
      "max_image_resolution": "768x768",
      "vision_encoder": "SigLIP2-so400m-patch14-384",
      "image_patch_size": 14,
      "notes": "IBM Granite 3.2-Vision 2B model with SigLIP2 encoder, optimized for visual document understanding",
      "source": "IBM Granite 3.2 technical report arXiv:2502.09927",
      "canonical_name": "granite3.2-vision:2b",
      "aliases": [
        "granite3.2-vision:latest",
        "granite3.2-vision",
        "granite-vision",
        "ibm-granite-vision"
      ],
      "max_tokens": 32768
    },
    "granite3.3:2b": {
      "max_output_tokens": 8192,
      "tool_support": "prompted",
      "structured_output": "prompted",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "notes": "IBM Granite 3.3 2B text-only model with enhanced reasoning capabilities",
      "source": "IBM Granite 3.3 release announcement",
      "canonical_name": "granite3.3:2b",
      "aliases": [
        "granite3.3-2b"
      ],
      "max_tokens": 32768
    },
    "granite3.3:8b": {
      "max_output_tokens": 8192,
      "tool_support": "prompted",
      "structured_output": "prompted",
      "parallel_tools": false,
      "vision_support": false,
      "audio_support": false,
      "notes": "IBM Granite 3.3 8B text-only model with enhanced reasoning capabilities",
      "source": "IBM Granite 3.3 release announcement",
      "canonical_name": "granite3.3:8b",
      "aliases": [
        "granite3.3-8b"
      ],
      "max_tokens": 32768
    }
  },
  "tool_support_levels": {
    "native": "Full native API support with structured tool calling",
    "prompted": "Works with careful prompt engineering",
    "none": "No tool support capabilities"
  },
  "structured_output_levels": {
    "native": "Native JSON mode or structured output support",
    "prompted": "Can output JSON with prompting",
    "none": "Poor structured output capability"
  },
  "capability_types": {
    "thinking_support": "Chain-of-thought reasoning capabilities",
    "thinking_budget": "Configurable reasoning length control",
    "video_support": "Video processing capabilities",
    "fim_support": "Fill-in-the-middle code completion"
  },
  "default_capabilities": {
    "max_output_tokens": 4096,
    "tool_support": "none",
    "structured_output": "none",
    "parallel_tools": false,
    "vision_support": false,
    "audio_support": false,
    "thinking_support": false,
    "thinking_budget": false,
    "video_support": false,
    "fim_support": false,
    "max_tokens": 16384
  }
}