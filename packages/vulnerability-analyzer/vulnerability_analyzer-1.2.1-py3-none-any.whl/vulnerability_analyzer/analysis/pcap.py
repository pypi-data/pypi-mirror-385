#!/usr/bin/env python3
"""
PySploit PCAP Analyzer
Network traffic analysis with vulnerability detection capabilities.
"""

import subprocess
import pandas as pd
import os
import re
from typing import Optional, List, Dict, Any
import json
from datetime import datetime
from pathlib import Path


class PcapAnalyzer:
    def save_results(self, results: Dict[str, Any], output_file: str, format: str = 'csv'):
        """Save analysis results to file in standardized format."""
        if format.lower() == 'json':
            import json
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, default=str)
        elif format.lower() == 'csv':
            self._save_csv(results, output_file)
        else:
            raise ValueError(f"Unsupported format: {format}")

    def _save_csv(self, results: Dict[str, Any], output_file: str):
        """Save results in CSV format matching NmapAnalyzer."""
        import csv
        with open(output_file, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            # Write header
            writer.writerow([
                'ip_address', 'mac_address', 'protocol', 'port', 'state',
                'service_name', 'product', 'version', 'vulnerability_id'
            ])
            # Write data
            for host in results.get('hosts', []):
                ip_address = host.get('ip', 'unknown')
                mac_address = host.get('mac', 'unknown')
                for port in host.get('ports', []):
                    port_id = port.get('portid', '')
                    protocol = port.get('protocol', '')
                    state = port.get('state', {}).get('state', '')
                    service = port.get('service', {})
                    service_name = service.get('name', '')
                    product = service.get('product', '')
                    version = service.get('version', '')
                    vulns = port.get('vulnerabilities', [])
                    if vulns:
                        for vuln in vulns:
                            vuln_id = vuln.get('id', '')
                            writer.writerow([
                                ip_address, mac_address, protocol, port_id, state,
                                service_name, product, version, vuln_id
                            ])
                    else:
                        writer.writerow([
                            ip_address, mac_address, protocol, port_id, state,
                            service_name, product, version, ''
                        ])
    """
    Analyze PCAP files for vulnerability indicators and network patterns.
    """
    
    def __init__(self):
        self.tshark_available = self._check_tshark()
        
    def _check_tshark(self) -> bool:
        """Check if tshark is available in system PATH."""
        try:
            subprocess.run(["tshark", "--version"], 
                         capture_output=True, check=True)
            return True
        except (subprocess.CalledProcessError, FileNotFoundError):
            return False
    
    def analyze(self, pcap_path: str, filter_type: str = "all") -> Dict[str, Any]:
        """
        Analyze PCAP file and extract vulnerability-relevant data.
        Produces standardized output matching Nmap analyzer format.
        
        Args:
            pcap_path (str): Path to PCAP file
            filter_type (str): Analysis filter ('all', 'router', 'web', 'iot', 'suspicious')
        
        Returns:
            dict: Standardized analysis results with scan_info, hosts, summary, analysis_metadata
        """
        if not self.tshark_available:
            raise RuntimeError("tshark is not available. Please install Wireshark/tshark.")
        
        if not os.path.exists(pcap_path):
            raise FileNotFoundError(f"PCAP file not found: {pcap_path}")
        
        # Initialize analysis timestamp
        analysis_start = datetime.now()
        
        # Extract data using tshark
        csv_data = self._extract_to_dataframe(pcap_path, filter_type)
        
        # Parse hosts and vulnerabilities in standardized format
        hosts = self._parse_hosts_standardized(csv_data, filter_type)
        scan_info = self._generate_scan_info(pcap_path, analysis_start)
        summary = self._generate_summary(hosts)
        
        # Return standardized format matching Nmap analyzer
        return {
            'scan_info': scan_info,
            'hosts': hosts,
            'summary': summary,
            'analysis_metadata': {
                'analyzer_version': '2.0',
                'analysis_time': datetime.now().isoformat(),
                'pcap_file': str(pcap_path),
                'filter_type': filter_type,
                'github_references': {
                    'wireshark_project': 'https://github.com/wireshark/wireshark',
                    'vulnerability_analyzer': 'Local vulnerability database integration'
                }
            }
        }
        
        # Analyze the extracted data
        analysis_results = self._analyze_traffic(csv_data, filter_type)
        
        return {
            'pcap_file': pcap_path,
            'filter_type': filter_type,
            'packet_count': len(csv_data) if csv_data is not None else 0,
            'analysis_results': analysis_results,
            'raw_data': csv_data.to_dict('records') if csv_data is not None else []
        }
    
    def _extract_to_dataframe(self, pcap_path: str, filter_type: str) -> Optional[pd.DataFrame]:
        """Extract PCAP data to pandas DataFrame using tshark."""
        
        # Build tshark command based on filter type
        fields = self._get_fields_for_filter(filter_type)
        
        tshark_command = [
            "tshark", "-r", pcap_path, "-T", "fields"
        ]
        
        # Add field extraction parameters
        for field in fields:
            tshark_command.extend(["-e", field])
        
        # Add formatting options
        tshark_command.extend([
            "-E", "header=y",
            "-E", "separator=,", 
            "-E", "quote=d"
        ])
        
        try:
            # Run tshark and capture output
            result = subprocess.run(
                tshark_command, 
                capture_output=True, 
                text=True, 
                check=True
            )
            
            # Convert output to DataFrame
            if result.stdout:
                from io import StringIO
                df = pd.read_csv(StringIO(result.stdout))
                return df
            
        except subprocess.CalledProcessError as e:
            print(f"tshark error: {e}")
            return None
        except Exception as e:
            print(f"Error processing PCAP: {e}")
            return None
        
        return None
    
    def _get_fields_for_filter(self, filter_type: str) -> List[str]:
        """Get tshark fields based on analysis filter type."""
        
        # Base fields always included
        base_fields = [
            "frame.time_epoch", "frame.number", "frame.len",
            "ip.src", "ip.dst", "tcp.srcport", "tcp.dstport", 
            "udp.srcport", "udp.dstport", "_ws.col.Protocol", "_ws.col.Info"
        ]
        
        # Additional fields by filter type
        filter_fields = {
            "router": [
                "http.host", "http.request.uri", "http.user_agent", 
                "http.authorization", "telnet.data", "ssh.protocol",
                "snmp.community", "dhcp.option.hostname"
            ],
            "web": [
                "http.request.method", "http.request.uri", "http.host",
                "http.user_agent", "http.cookie", "http.response.code",
                "http.file_data", "tls.handshake.extensions.server_name"
            ],
            "iot": [
                "http.host", "dns.qry.name", "dhcp.option.hostname",
                "mdns.queries.name", "upnp.method", "coap.code"
            ],
            "weak_auth": [
                "ftp.request.command", "ftp.response.code", "telnet.data",
                "http.authorization", "snmp.community", "pop.request.command",
                "imap.request.command", "smtp.command"
            ],
            "suspicious": [
                "dns.qry.name", "http.request.uri", "tls.alert.description",
                "icmp.type", "tcp.analysis", "smb.cmd", "nbns.name"
            ]
        }
        
        if filter_type == "all":
            # Include all field types
            all_fields = base_fields[:]
            for fields in filter_fields.values():
                all_fields.extend(fields)
            # Remove duplicates while preserving order
            return list(dict.fromkeys(all_fields))
        
        return base_fields + filter_fields.get(filter_type, [])
    
    def _analyze_traffic(self, df: pd.DataFrame, filter_type: str) -> Dict[str, Any]:
        """Analyze extracted traffic data for vulnerability indicators."""
        
        if df is None or df.empty:
            return {"error": "No data to analyze"}
        
        analysis = {
            "summary": self._get_traffic_summary(df),
            "protocols": self._analyze_protocols(df),
            "hosts": self._analyze_hosts(df),
            "vulnerabilities": self._detect_vulnerabilities(df, filter_type)
        }
        
        return analysis
    
    def _get_traffic_summary(self, df: pd.DataFrame) -> Dict[str, Any]:
        """Generate traffic summary statistics."""
        return {
            "total_packets": len(df),
            "unique_src_ips": df['ip.src'].nunique() if 'ip.src' in df.columns else 0,
            "unique_dst_ips": df['ip.dst'].nunique() if 'ip.dst' in df.columns else 0,
            "protocols": df['_ws.col.Protocol'].value_counts().to_dict() if '_ws.col.Protocol' in df.columns else {},
            "time_span": {
                "start": df['frame.time_epoch'].min() if 'frame.time_epoch' in df.columns else None,
                "end": df['frame.time_epoch'].max() if 'frame.time_epoch' in df.columns else None
            }
        }
    
    def _analyze_protocols(self, df: pd.DataFrame) -> Dict[str, Any]:
        """Analyze protocol usage and patterns."""
        protocols = {}
        
        if '_ws.col.Protocol' in df.columns:
            protocol_counts = df['_ws.col.Protocol'].value_counts()
            protocols['distribution'] = protocol_counts.to_dict()
            
            # Identify potentially risky protocols
            risky_protocols = ['TELNET', 'FTP', 'HTTP', 'SNMP', 'SMB']
            detected_risky = [p for p in risky_protocols if p in protocol_counts.index]
            protocols['risky_protocols'] = detected_risky
        
        return protocols
    
    def _analyze_hosts(self, df: pd.DataFrame) -> Dict[str, Any]:
        """Analyze host communication patterns."""
        hosts = {}
        
        if 'ip.src' in df.columns and 'ip.dst' in df.columns:
            # Most active source IPs
            src_counts = df['ip.src'].value_counts().head(10)
            hosts['top_sources'] = src_counts.to_dict()
            
            # Most contacted destinations
            dst_counts = df['ip.dst'].value_counts().head(10)
            hosts['top_destinations'] = dst_counts.to_dict()
            
            # Communication pairs
            pairs = df.groupby(['ip.src', 'ip.dst']).size().sort_values(ascending=False).head(10)
            hosts['top_communications'] = pairs.to_dict()
        
        return hosts
    
    def _detect_vulnerabilities(self, df: pd.DataFrame, filter_type: str) -> List[Dict[str, Any]]:
        """Detect potential vulnerability indicators in traffic."""
        vulnerabilities = []
        
        # HTTP-based vulnerability indicators
        if 'http.request.uri' in df.columns:
            http_df = df[df['http.request.uri'].notna()]
            
            for _, row in http_df.iterrows():
                uri = str(row['http.request.uri'])
                
                # Check for common attack patterns
                vuln_indicators = self._check_http_vulnerabilities(uri, row)
                vulnerabilities.extend(vuln_indicators)
        
        # DNS-based indicators
        if 'dns.qry.name' in df.columns:
            dns_df = df[df['dns.qry.name'].notna()]
            
            for _, row in dns_df.iterrows():
                dns_name = str(row['dns.qry.name'])
                
                # Check for suspicious DNS queries
                dns_indicators = self._check_dns_vulnerabilities(dns_name, row)
                vulnerabilities.extend(dns_indicators)
        
        # Authentication-based indicators
        auth_indicators = self._check_auth_vulnerabilities(df)
        vulnerabilities.extend(auth_indicators)
        
        return vulnerabilities
    
    def _check_http_vulnerabilities(self, uri: str, row: pd.Series) -> List[Dict[str, Any]]:
        """Check HTTP requests for vulnerability patterns."""
        indicators = []
        
        # SQL Injection patterns
        sql_patterns = [
            r"('|(\\')|(;)|(\\;))", r"(\\x27)|(\\x2D)|(%27)|(%2D)",
            r"(union.*select)|(select.*from)|(insert.*into)",
            r"(drop.*table)|(update.*set)|(delete.*from)"
        ]
        
        for pattern in sql_patterns:
            if re.search(pattern, uri, re.IGNORECASE):
                indicators.append({
                    'type': 'sql_injection',
                    'severity': 'HIGH',
                    'description': f'Potential SQL injection in URI: {uri}',
                    'source_ip': row.get('ip.src', 'Unknown'),
                    'destination_ip': row.get('ip.dst', 'Unknown'),
                    'timestamp': row.get('frame.time_epoch', 'Unknown')
                })
                break
        
        # XSS patterns
        xss_patterns = [
            r"<script", r"javascript:", r"onload=", r"onerror=",
            r"alert\\(", r"document\\.cookie", r"<iframe"
        ]
        
        for pattern in xss_patterns:
            if re.search(pattern, uri, re.IGNORECASE):
                indicators.append({
                    'type': 'xss',
                    'severity': 'MEDIUM', 
                    'description': f'Potential XSS in URI: {uri}',
                    'source_ip': row.get('ip.src', 'Unknown'),
                    'destination_ip': row.get('ip.dst', 'Unknown'),
                    'timestamp': row.get('frame.time_epoch', 'Unknown')
                })
                break
        
        # Path traversal
        if '../' in uri or '..\\\\' in uri:
            indicators.append({
                'type': 'path_traversal',
                'severity': 'HIGH',
                'description': f'Potential path traversal in URI: {uri}',
                'source_ip': row.get('ip.src', 'Unknown'),
                'destination_ip': row.get('ip.dst', 'Unknown'),
                'timestamp': row.get('frame.time_epoch', 'Unknown')
            })
        
        return indicators
    
    def _check_dns_vulnerabilities(self, dns_name: str, row: pd.Series) -> List[Dict[str, Any]]:
        """Check DNS queries for suspicious patterns."""
        indicators = []
        
        # DNS tunneling indicators
        if len(dns_name) > 63:  # Unusually long DNS names
            indicators.append({
                'type': 'dns_tunneling',
                'severity': 'MEDIUM',
                'description': f'Unusually long DNS query (potential tunneling): {dns_name}',
                'source_ip': row.get('ip.src', 'Unknown'),
                'timestamp': row.get('frame.time_epoch', 'Unknown')
            })
        
        # Suspicious TLDs or patterns
        suspicious_patterns = [
            r'\\.[a-z0-9]{10,}$',  # Long random TLD
            r'\\.tk$', r'\\.ml$', r'\\.ga$',  # Common malware TLDs
            r'[0-9]{1,3}-[0-9]{1,3}-[0-9]{1,3}-[0-9]{1,3}'  # IP-like patterns
        ]
        
        for pattern in suspicious_patterns:
            if re.search(pattern, dns_name, re.IGNORECASE):
                indicators.append({
                    'type': 'suspicious_dns',
                    'severity': 'LOW',
                    'description': f'Suspicious DNS pattern: {dns_name}',
                    'source_ip': row.get('ip.src', 'Unknown'),
                    'timestamp': row.get('frame.time_epoch', 'Unknown')
                })
                break
        
        return indicators
    
    def _check_auth_vulnerabilities(self, df: pd.DataFrame) -> List[Dict[str, Any]]:
        """Check for authentication-related vulnerabilities."""
        indicators = []
        
        # Clear text authentication protocols
        if 'telnet.data' in df.columns:
            telnet_df = df[df['telnet.data'].notna()]
            if not telnet_df.empty:
                indicators.append({
                    'type': 'cleartext_protocol',
                    'severity': 'HIGH',
                    'description': 'Telnet traffic detected (cleartext authentication)',
                    'count': len(telnet_df)
                })
        
        if 'ftp.request.command' in df.columns:
            ftp_df = df[df['ftp.request.command'].notna()]
            if not ftp_df.empty:
                indicators.append({
                    'type': 'cleartext_protocol',
                    'severity': 'MEDIUM',
                    'description': 'FTP traffic detected (potential cleartext authentication)',
                    'count': len(ftp_df)
                })
        
        # HTTP Basic Auth
        if 'http.authorization' in df.columns:
            auth_df = df[df['http.authorization'].notna()]
            basic_auth = auth_df[auth_df['http.authorization'].str.startswith('Basic', na=False)]
            if not basic_auth.empty:
                indicators.append({
                    'type': 'weak_authentication',
                    'severity': 'MEDIUM',
                    'description': 'HTTP Basic Authentication detected',
                    'count': len(basic_auth)
                })
        
        return indicators
    
    def extract_http(self, pcap_path: str) -> Dict[str, Any]:
        """Extract only HTTP traffic from PCAP."""
        return self.analyze(pcap_path, filter_type="web")
    
    def extract_dns(self, pcap_path: str) -> Dict[str, Any]:
        """Extract only DNS traffic from PCAP.""" 
        return self.analyze(pcap_path, filter_type="suspicious")
    
    def filter_router_traffic(self, pcap_path: str) -> Dict[str, Any]:
        """Extract router-related traffic patterns."""
        return self.analyze(pcap_path, filter_type="router")
    
    def detect_suspicious_patterns(self, analysis_results: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Extract suspicious patterns from analysis results."""
        if 'analysis_results' in analysis_results:
            return analysis_results['analysis_results'].get('vulnerabilities', [])
        return []
    
    def _parse_hosts_standardized(self, df: pd.DataFrame, filter_type: str) -> List[Dict[str, Any]]:
        """Parse network traffic into standardized host format matching Nmap analyzer."""
        if df is None or df.empty:
            return []
        
        hosts = []
        unique_ips = set()
        
        # Collect unique IP addresses
        if 'ip.src' in df.columns:
            unique_ips.update(df['ip.src'].dropna().unique())
        if 'ip.dst' in df.columns:
            unique_ips.update(df['ip.dst'].dropna().unique())
        
        for ip in unique_ips:
            if ip and str(ip) != 'nan':
                host_data = self._analyze_host_from_traffic(ip, df)
                if host_data:
                    hosts.append(host_data)
        
        return hosts
    
    def _analyze_host_from_traffic(self, ip: str, df: pd.DataFrame) -> Dict[str, Any]:
        """Analyze individual host from network traffic in standardized format."""
        host_data = {
            'addresses': [{'addr': ip, 'addrtype': 'ipv4'}],
            'hostnames': [],
            'status': {'state': 'up', 'reason': 'traffic_observed'},
            'ports': [],
            'services': set(),
            'vulnerabilities': [],
            'cves': set(),
            'risk_score': 0
        }
        
        # Analyze ports and services from traffic
        host_traffic = df[(df['ip.src'] == ip) | (df['ip.dst'] == ip)]
        
        # Extract TCP ports
        tcp_ports = set()
        if 'tcp.srcport' in host_traffic.columns:
            tcp_ports.update(host_traffic['tcp.srcport'].dropna().unique())
        if 'tcp.dstport' in host_traffic.columns:
            tcp_ports.update(host_traffic['tcp.dstport'].dropna().unique())
        
        # Extract UDP ports
        udp_ports = set()
        if 'udp.srcport' in host_traffic.columns:
            udp_ports.update(host_traffic['udp.srcport'].dropna().unique())
        if 'udp.dstport' in host_traffic.columns:
            udp_ports.update(host_traffic['udp.dstport'].dropna().unique())
        
        # Create port entries with vulnerability analysis
        for port in tcp_ports:
            if port and str(port) != 'nan':
                port_data = self._analyze_port_from_traffic(int(float(port)), 'tcp', host_traffic, ip)
                host_data['ports'].append(port_data)
                host_data['services'].update(port_data.get('services', []))
                host_data['vulnerabilities'].extend(port_data.get('vulnerabilities', []))
                host_data['cves'].update(port_data.get('cves', []))
        
        for port in udp_ports:
            if port and str(port) != 'nan':
                port_data = self._analyze_port_from_traffic(int(float(port)), 'udp', host_traffic, ip)
                host_data['ports'].append(port_data)
                host_data['services'].update(port_data.get('services', []))
                host_data['vulnerabilities'].extend(port_data.get('vulnerabilities', []))
                host_data['cves'].update(port_data.get('cves', []))
        
        # Calculate risk score
        host_data['risk_score'] = len(host_data['vulnerabilities']) * 10
        
        # Convert sets to lists for JSON serialization
        host_data['services'] = list(host_data['services'])
        host_data['cves'] = list(host_data['cves'])
        
        return host_data
    
    def _analyze_port_from_traffic(self, port: int, protocol: str, traffic_df: pd.DataFrame, ip: str) -> Dict[str, Any]:
        """Analyze port from network traffic and detect vulnerabilities."""
        port_data = {
            'portid': port,
            'protocol': protocol,
            'state': {'state': 'open', 'reason': 'traffic_observed'},
            'service': {'name': self._identify_service(port, protocol), 'method': 'traffic_analysis'},
            'services': [],
            'vulnerabilities': [],
            'cves': [],
            'scripts': []
        }
        
        # Service identification
        service_name = self._identify_service(port, protocol)
        port_data['services'].append(service_name)
        
        # Vulnerability detection based on traffic patterns
        vulnerabilities = self._detect_port_vulnerabilities(port, protocol, service_name, traffic_df, ip)
        port_data['vulnerabilities'] = vulnerabilities
        
        # Extract CVEs from vulnerabilities
        cves = []
        for vuln in vulnerabilities:
            if vuln.get('cve_id'):
                cves.append(vuln['cve_id'])
        port_data['cves'] = cves
        
        return port_data
    
    def _identify_service(self, port: int, protocol: str) -> str:
        """Identify service based on port number and protocol."""
        well_known_ports = {
            20: 'ftp-data', 21: 'ftp', 22: 'ssh', 23: 'telnet', 25: 'smtp',
            53: 'dns', 67: 'dhcp', 68: 'dhcp', 69: 'tftp', 80: 'http',
            110: 'pop3', 111: 'rpc', 119: 'nntp', 123: 'ntp', 135: 'rpc',
            139: 'netbios-ssn', 143: 'imap', 161: 'snmp', 162: 'snmp-trap',
            389: 'ldap', 443: 'https', 445: 'smb', 465: 'smtps', 514: 'syslog',
            587: 'submission', 993: 'imaps', 995: 'pop3s', 1433: 'mssql',
            1521: 'oracle', 3306: 'mysql', 3389: 'rdp', 5432: 'postgresql',
            5060: 'sip', 5061: 'sips', 6379: 'redis', 8080: 'http-proxy',
            8443: 'https-alt', 9200: 'elasticsearch'
        }
        
        return well_known_ports.get(port, f'unknown-{protocol}-{port}')
    
    def _detect_port_vulnerabilities(self, port: int, protocol: str, service: str, traffic_df: pd.DataFrame, ip: str) -> List[Dict[str, Any]]:
        """Detect vulnerabilities for specific port based on traffic analysis."""
        vulnerabilities = []
        
        # Service-specific vulnerability detection
        if service == 'http' or service == 'https' or port in [80, 443, 8080, 8443]:
            http_vulns = self._detect_http_vulnerabilities(traffic_df, ip, port)
            vulnerabilities.extend(http_vulns)
        
        elif service == 'ssh' or port == 22:
            ssh_vulns = self._detect_ssh_vulnerabilities(traffic_df, ip, port)
            vulnerabilities.extend(ssh_vulns)
        
        elif service == 'ftp' or port in [20, 21]:
            ftp_vulns = self._detect_ftp_vulnerabilities(traffic_df, ip, port)
            vulnerabilities.extend(ftp_vulns)
        
        elif service == 'smb' or port in [139, 445]:
            smb_vulns = self._detect_smb_vulnerabilities(traffic_df, ip, port)
            vulnerabilities.extend(smb_vulns)
        
        # Generic vulnerability patterns
        generic_vulns = self._detect_generic_vulnerabilities(traffic_df, ip, port, protocol)
        vulnerabilities.extend(generic_vulns)
        
        return vulnerabilities
    
    def _detect_http_vulnerabilities(self, df: pd.DataFrame, ip: str, port: int) -> List[Dict[str, Any]]:
        """Detect HTTP-specific vulnerabilities from traffic."""
        vulnerabilities = []
        
        # Check for SQL injection patterns
        if 'http.request.uri' in df.columns:
            http_traffic = df[df['http.request.uri'].notna()]
            for _, row in http_traffic.iterrows():
                uri = str(row['http.request.uri'])
                if any(pattern in uri.lower() for pattern in ['union select', 'drop table', "' or 1=1"]):
                    vulnerabilities.append({
                        'cve_id': 'CVE-2021-44228',  # Example Log4j
                        'service': 'http',
                        'port': port,
                        'risk_level': 'high',
                        'cvss_score': 8.5,
                        'description': 'SQL Injection vulnerability detected in HTTP traffic',
                        'source': 'TrafficAnalysis',
                        'detection_method': 'pattern_matching'
                    })
        
        # Check for XSS patterns
        if 'http.request.uri' in df.columns:
            xss_patterns = ['<script>', 'javascript:', 'onerror=', 'onload=']
            http_traffic = df[df['http.request.uri'].notna()]
            for _, row in http_traffic.iterrows():
                uri = str(row['http.request.uri'])
                if any(pattern in uri.lower() for pattern in xss_patterns):
                    vulnerabilities.append({
                        'cve_id': 'CVE-2022-0847',  # Example XSS CVE
                        'service': 'http',
                        'port': port,
                        'risk_level': 'medium',
                        'cvss_score': 6.2,
                        'description': 'Cross-Site Scripting (XSS) vulnerability detected',
                        'source': 'TrafficAnalysis',
                        'detection_method': 'pattern_matching'
                    })
        
        return vulnerabilities
    
    def _detect_ssh_vulnerabilities(self, df: pd.DataFrame, ip: str, port: int) -> List[Dict[str, Any]]:
        """Detect SSH-specific vulnerabilities from traffic."""
        vulnerabilities = []
        
        # SSH brute force detection
        ssh_traffic = df[(df['tcp.dstport'] == port) | (df['tcp.srcport'] == port)]
        connection_count = len(ssh_traffic)
        
        if connection_count > 10:  # Threshold for potential brute force
            vulnerabilities.append({
                'cve_id': 'CVE-2020-15778',  # Example SSH CVE
                'service': 'ssh',
                'port': port,
                'risk_level': 'high',
                'cvss_score': 7.8,
                'description': f'Potential SSH brute force attack detected ({connection_count} connections)',
                'source': 'TrafficAnalysis',
                'detection_method': 'connection_analysis'
            })
        
        return vulnerabilities
    
    def _detect_ftp_vulnerabilities(self, df: pd.DataFrame, ip: str, port: int) -> List[Dict[str, Any]]:
        """Detect FTP-specific vulnerabilities from traffic."""
        vulnerabilities = []
        
        # Detect plain text FTP
        ftp_traffic = df[(df['tcp.dstport'] == port) | (df['tcp.srcport'] == port)]
        if not ftp_traffic.empty:
            vulnerabilities.append({
                'cve_id': 'CVE-2019-6977',  # Example FTP CVE
                'service': 'ftp',
                'port': port,
                'risk_level': 'medium',
                'cvss_score': 5.3,
                'description': 'Plain text FTP communication detected',
                'source': 'TrafficAnalysis',
                'detection_method': 'protocol_analysis'
            })
        
        return vulnerabilities
    
    def _detect_smb_vulnerabilities(self, df: pd.DataFrame, ip: str, port: int) -> List[Dict[str, Any]]:
        """Detect SMB-specific vulnerabilities from traffic."""
        vulnerabilities = []
        
        # Detect SMB traffic (potential EternalBlue)
        smb_traffic = df[(df['tcp.dstport'] == port) | (df['tcp.srcport'] == port)]
        if not smb_traffic.empty and port == 445:
            vulnerabilities.append({
                'cve_id': 'CVE-2017-0144',  # EternalBlue
                'service': 'smb',
                'port': port,
                'risk_level': 'critical',
                'cvss_score': 9.3,
                'description': 'SMBv1 EternalBlue vulnerability potential',
                'source': 'TrafficAnalysis',
                'detection_method': 'protocol_analysis'
            })
        
        return vulnerabilities
    
    def _detect_generic_vulnerabilities(self, df: pd.DataFrame, ip: str, port: int, protocol: str) -> List[Dict[str, Any]]:
        """Detect generic vulnerabilities from traffic patterns."""
        vulnerabilities = []
        
        # High port usage (potential backdoor)
        if port > 49152:  # Dynamic/private port range
            vulnerabilities.append({
                'cve_id': f'TRAFFIC-{port}',
                'service': f'unknown-{protocol}',
                'port': port,
                'risk_level': 'low',
                'cvss_score': 3.1,
                'description': f'Suspicious high port activity detected on {protocol}/{port}',
                'source': 'TrafficAnalysis',
                'detection_method': 'port_analysis'
            })
        
        return vulnerabilities
    
    def _generate_scan_info(self, pcap_path: str, analysis_start: datetime) -> Dict[str, Any]:
        """Generate scan info matching Nmap analyzer format."""
        return {
            'type': 'pcap_analysis',
            'start_time': analysis_start.strftime('%Y-%m-%d %H:%M:%S'),
            'scanner': 'pcap_analyzer',
            'version': '2.0',
            'file_analyzed': str(pcap_path),
            'analysis_method': 'traffic_analysis'
        }
    
    def _generate_summary(self, hosts: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Generate summary statistics matching Nmap analyzer format."""
        total_ports = sum(len(host.get('ports', [])) for host in hosts)
        total_vulns = sum(len(host.get('vulnerabilities', [])) for host in hosts)
        
        all_services = set()
        all_cves = set()
        
        for host in hosts:
            all_services.update(host.get('services', []))
            all_cves.update(host.get('cves', []))
        
        # Calculate vulnerability statistics
        vuln_stats = {'critical': 0, 'high': 0, 'medium': 0, 'low': 0}
        for host in hosts:
            for vuln in host.get('vulnerabilities', []):
                risk_level = vuln.get('risk_level', 'low').lower()
                if risk_level in vuln_stats:
                    vuln_stats[risk_level] += 1
        
        return {
            'scan_info': {
                'total_hosts': len(hosts),
                'hosts_analyzed': len([h for h in hosts if h.get('ports')])
            },
            'statistics': {
                'total_hosts': len(hosts),
                'hosts_up': len(hosts),
                'total_ports': total_ports,
                'open_ports': total_ports,  # All observed ports are considered "open"
                'unique_services': len(all_services),
                'total_vulnerabilities': total_vulns,
                'unique_cves': len(all_cves)
            },
            'vulnerability_stats': vuln_stats,
            'top_services': list(all_services),
            'discovered_cves': list(all_cves)
        }