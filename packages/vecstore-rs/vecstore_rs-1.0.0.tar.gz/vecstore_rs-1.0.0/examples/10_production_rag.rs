//! Production RAG System
//!
//! Demonstrates a production-ready RAG setup with:
//! - Error handling
//! - Logging
//! - Performance monitoring
//! - Best practices
//!
//! Run with: cargo run --example 10_production_rag

use anyhow::{Context, Result};
use std::collections::HashMap;
use std::time::Instant;
use vecstore::{
    text_splitter::{RecursiveCharacterTextSplitter, TextSplitter},
    Metadata, Query, VecDatabase,
};

fn main() -> Result<()> {
    println!("ðŸ­ Production RAG System\n");

    // Use Collections API for better organization
    println!("Step 1: Setting up database with collections...");
    let mut db =
        VecDatabase::open("./data/10_production_rag").context("Failed to open database")?;

    let mut docs_collection = db
        .create_collection("documents")
        .context("Failed to create documents collection")?;

    println!("   âœ“ Database ready: {:?}", db.list_collections()?);

    // Load and process documents with error handling
    println!("\nStep 2: Loading documents with error handling...");
    let documents = load_documents()?;
    println!("   âœ“ Loaded {} documents", documents.len());

    // Split and store with performance monitoring
    println!("\nStep 3: Processing and storing documents...");
    let start = Instant::now();

    let splitter = RecursiveCharacterTextSplitter::new(500, 50);
    let mut total_chunks = 0;

    for (doc_id, content) in documents {
        let chunks = splitter
            .split_text(&content)
            .context(format!("Failed to split document: {}", doc_id))?;

        for (i, chunk) in chunks.into_iter().enumerate() {
            let chunk_id = format!("{}_{}", doc_id, i);
            let embedding = create_embedding(&chunk)?;

            let mut metadata = Metadata {
                fields: HashMap::new(),
            };
            metadata
                .fields
                .insert("text".to_string(), serde_json::json!(chunk));
            metadata
                .fields
                .insert("doc_id".to_string(), serde_json::json!(doc_id));
            metadata
                .fields
                .insert("chunk_index".to_string(), serde_json::json!(i));

            docs_collection
                .upsert(chunk_id, embedding, metadata)
                .context("Failed to upsert chunk")?;

            total_chunks += 1;
        }
    }

    let elapsed = start.elapsed();
    println!(
        "   âœ“ Stored {} chunks in {:.2}s",
        total_chunks,
        elapsed.as_secs_f32()
    );
    println!(
        "   âš¡ Throughput: {:.0} chunks/sec",
        total_chunks as f32 / elapsed.as_secs_f32()
    );

    // Query with monitoring
    println!("\nStep 4: Querying with performance monitoring...");
    let queries = vec![
        "What is VecStore?",
        "How does HNSW work?",
        "What are the key features?",
    ];

    for query_text in &queries {
        let start = Instant::now();

        let query_embedding = create_embedding(query_text)?;
        let results = docs_collection.query(Query {
            vector: query_embedding,
            k: 5,
            filter: None,
        })?;

        let elapsed = start.elapsed();

        println!("\nâ“ Query: {}", query_text);
        println!("   â±ï¸  Latency: {:.2}ms", elapsed.as_millis());
        println!("   ðŸ“„ Results: {}", results.len());

        if let Some(top) = results.first() {
            let text = top
                .metadata
                .fields
                .get("text")
                .and_then(|v| v.as_str())
                .unwrap_or("N/A");
            println!("   ðŸŽ¯ Top result (score: {:.3}):", top.score);
            println!("      {}", &text[..text.len().min(100)]);
        }
    }

    // Show stats
    println!("\n\nStep 5: Collection statistics...");
    let stats = docs_collection.stats()?;
    println!("   Total vectors: {}", stats.vector_count);
    println!("   Active vectors: {}", stats.active_count);
    println!("   Deleted vectors: {}", stats.deleted_count);
    println!(
        "   Quota utilization: {:.1}%",
        stats.quota_utilization * 100.0
    );

    println!("\nâœ… Production RAG System Complete!");
    println!("\nðŸŽ¯ Production Best Practices Demonstrated:");
    println!("   â€¢ Use Collections API for organization");
    println!("   â€¢ Comprehensive error handling with context");
    println!("   â€¢ Performance monitoring and logging");
    println!("   â€¢ Batch operations for efficiency");
    println!("   â€¢ Metadata for provenance tracking");
    println!("   â€¢ Stats monitoring for capacity planning");

    Ok(())
}

fn load_documents() -> Result<Vec<(String, String)>> {
    Ok(vec![
        ("doc1".to_string(), "VecStore is a high-performance vector database built in Rust for production RAG applications.".to_string()),
        ("doc2".to_string(), "HNSW (Hierarchical Navigable Small World) is an efficient algorithm for approximate nearest neighbor search.".to_string()),
        ("doc3".to_string(), "Key features include SIMD acceleration, product quantization, hybrid search, and multi-tenant namespaces.".to_string()),
    ])
}

fn create_embedding(text: &str) -> Result<Vec<f32>> {
    // In production: Use real embedding model
    let words: Vec<&str> = text.split_whitespace().collect();
    let mut embedding = vec![0.0; 384];
    for (i, word) in words.iter().enumerate() {
        embedding[(word.len() * (i + 1)) % 384] += 1.0;
    }
    let mag: f32 = embedding.iter().map(|x| x * x).sum::<f32>().sqrt();
    if mag > 0.0 {
        for val in &mut embedding {
            *val /= mag;
        }
    }
    Ok(embedding)
}
