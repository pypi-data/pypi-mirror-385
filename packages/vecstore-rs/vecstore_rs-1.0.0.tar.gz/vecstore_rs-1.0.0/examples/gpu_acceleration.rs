//! GPU Acceleration Example
//!
//! This example demonstrates GPU-accelerated vector operations using CUDA (NVIDIA)
//! and Metal (Apple Silicon), with automatic fallback to optimized CPU implementations.
//!
//! ## Features Demonstrated
//!
//! - Auto-detection of available GPU backends
//! - Batch distance calculations
//! - Matrix multiplication
//! - K-NN search
//! - Performance benchmarking
//! - Backend comparison
//!
//! ## Running
//!
//! ```bash
//! # CPU backend (always available)
//! cargo run --example gpu_acceleration
//!
//! # With CUDA support (requires NVIDIA GPU + CUDA toolkit)
//! cargo run --example gpu_acceleration --features cuda
//!
//! # With Metal support (requires macOS)
//! cargo run --example gpu_acceleration --features metal
//! ```

use std::time::Instant;
use vecstore::gpu::{GpuBackend, GpuBenchmark, GpuConfig, GpuExecutor};

fn main() {
    println!("笞｡ GPU Acceleration Example\n");

    // ============================================================
    // 1. GPU Detection and Configuration
    // ============================================================
    println!("沐 GPU Detection:");
    println!("笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊申n");

    // Auto-detect GPU
    let config = GpuConfig::default();
    let executor = GpuExecutor::new(config.clone()).unwrap();

    let info = executor.device_info();
    println!("Active backend: {:?}", info.backend);
    println!("Device name: {}", info.name);
    println!("Device ID: {}", info.device_id);

    if info.total_memory_bytes > 0 {
        println!(
            "Total memory: {:.2} GB",
            info.total_memory_bytes as f64 / 1_073_741_824.0
        );
        println!(
            "Available memory: {:.2} GB",
            info.available_memory_bytes as f64 / 1_073_741_824.0
        );
        println!(
            "Compute capability: {}.{}",
            info.compute_capability.0, info.compute_capability.1
        );
        println!("Max threads per block: {}", info.max_threads_per_block);
        println!(
            "Streaming multiprocessors: {}",
            info.num_streaming_multiprocessors
        );
    }

    // ============================================================
    // 2. Batch Distance Calculations
    // ============================================================
    println!("\n\n沒 Batch Distance Calculations:");
    println!("笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊申n");

    let query = vec![0.5, 0.5, 0.5, 0.5];
    let database = vec![
        vec![0.1, 0.2, 0.3, 0.4],
        vec![0.5, 0.5, 0.5, 0.5], // Exact match
        vec![0.9, 0.8, 0.7, 0.6],
        vec![0.0, 0.0, 0.0, 0.0],
        vec![1.0, 1.0, 1.0, 1.0],
    ];

    println!("Query vector: {:?}", query);
    println!("Database size: {} vectors\n", database.len());

    // Euclidean distance
    println!("Euclidean distances:");
    let start = Instant::now();
    let distances = executor
        .batch_euclidean_distance(&query, &database)
        .unwrap();
    let elapsed = start.elapsed();

    for (i, dist) in distances.iter().enumerate() {
        println!("  Vector {}: {:.4}", i, dist);
    }
    println!("  Computed in: {:?}\n", elapsed);

    // Cosine similarity
    println!("Cosine similarities:");
    let start = Instant::now();
    let similarities = executor.batch_cosine_similarity(&query, &database).unwrap();
    let elapsed = start.elapsed();

    for (i, sim) in similarities.iter().enumerate() {
        println!("  Vector {}: {:.4}", i, sim);
    }
    println!("  Computed in: {:?}", elapsed);

    // ============================================================
    // 3. K-NN Search
    // ============================================================
    println!("\n\n沐 K-Nearest Neighbors Search:");
    println!("笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊申n");

    let k = 3;
    println!("Finding {} nearest neighbors...\n", k);

    let start = Instant::now();
    let (indices, distances) = executor.knn_search(&query, &database, k).unwrap();
    let elapsed = start.elapsed();

    println!("Results:");
    for (i, (idx, dist)) in indices.iter().zip(distances.iter()).enumerate() {
        println!("  {}. Vector {} (distance: {:.4})", i + 1, idx, dist);
    }
    println!("\nComputed in: {:?}", elapsed);

    // ============================================================
    // 4. Matrix Multiplication
    // ============================================================
    println!("\n\n沐｢ Matrix Multiplication:");
    println!("笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊申n");

    let matrix_a = vec![vec![1.0, 2.0, 3.0], vec![4.0, 5.0, 6.0]];

    let matrix_b = vec![vec![7.0, 8.0], vec![9.0, 10.0], vec![11.0, 12.0]];

    println!("Matrix A (2x3):");
    for row in &matrix_a {
        println!("  {:?}", row);
    }

    println!("\nMatrix B (3x2):");
    for row in &matrix_b {
        println!("  {:?}", row);
    }

    let start = Instant::now();
    let result = executor.matrix_multiply(&matrix_a, &matrix_b).unwrap();
    let elapsed = start.elapsed();

    println!("\nResult (2x2):");
    for row in &result {
        println!("  {:?}", row);
    }
    println!("\nComputed in: {:?}", elapsed);

    // ============================================================
    // 5. Batch Normalization
    // ============================================================
    println!("\n\n沒 Batch Vector Normalization:");
    println!("笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武\n");

    let vectors = vec![vec![3.0, 4.0], vec![5.0, 12.0], vec![1.0, 1.0]];

    println!("Original vectors:");
    for (i, vec) in vectors.iter().enumerate() {
        println!("  Vector {}: {:?}", i, vec);
    }

    let start = Instant::now();
    let normalized = executor.batch_normalize(&vectors).unwrap();
    let elapsed = start.elapsed();

    println!("\nNormalized vectors:");
    for (i, vec) in normalized.iter().enumerate() {
        println!("  Vector {}: {:?}", i, vec);
        let magnitude: f32 = vec.iter().map(|x| x * x).sum::<f32>().sqrt();
        println!("    Magnitude: {:.6}", magnitude);
    }
    println!("\nComputed in: {:?}", elapsed);

    // ============================================================
    // 6. Performance Benchmarking
    // ============================================================
    println!("\n\n笞｡ Performance Benchmarking:");
    println!("笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊申n");

    let benchmark_sizes = vec![(1000, 128), (10_000, 128), (100_000, 128)];

    for (num_vectors, dimension) in benchmark_sizes {
        println!(
            "Dataset: {} vectors x {} dimensions",
            num_vectors, dimension
        );
        println!("笏笏笏笏笏笏笏笏笏笏笏笏笏笏笏笏笏笏笏笏笏笏笏笏笏笏笏笏笏笏笏笏笏笏笏笏笏笏笏笏笏\n");

        let benchmarks = GpuBenchmark::run(&executor, num_vectors, dimension).unwrap();

        for bench in &benchmarks {
            println!("  {}", bench.operation);
            println!("    Duration: {:.2}ms", bench.duration_ms);
            println!(
                "    Throughput: {:.0} vectors/sec",
                bench.throughput_vectors_per_sec
            );
        }

        println!();
    }

    // ============================================================
    // 7. Scalability Demonstration
    // ============================================================
    println!("\n沒 Scalability Analysis:");
    println!("笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊申n");

    let sizes = vec![100, 1_000, 10_000, 100_000];

    println!("Euclidean distance scaling:");
    println!(
        "{:>10}  {:>10}  {:>15}",
        "Vectors", "Time (ms)", "Throughput/s"
    );
    println!("{:-<42}", "");

    for &size in &sizes {
        let query: Vec<f32> = (0..128).map(|i| i as f32 * 0.01).collect();
        let database: Vec<Vec<f32>> = (0..size)
            .map(|i| (0..128).map(|j| (i + j) as f32 * 0.01).collect())
            .collect();

        let start = Instant::now();
        let _ = executor
            .batch_euclidean_distance(&query, &database)
            .unwrap();
        let elapsed = start.elapsed();

        let duration_ms = elapsed.as_secs_f64() * 1000.0;
        let throughput = size as f64 / elapsed.as_secs_f64();

        println!("{:>10}  {:>10.2}  {:>15.0}", size, duration_ms, throughput);
    }

    // ============================================================
    // 8. Backend Comparison
    // ============================================================
    println!("\n\n沛 Backend Comparison:");
    println!("笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊申n");

    let backends_to_test = vec![
        (GpuBackend::Cpu, "CPU (SIMD)"),
        #[cfg(feature = "cuda")]
        (GpuBackend::Cuda, "CUDA (NVIDIA)"),
        #[cfg(feature = "metal")]
        (GpuBackend::Metal, "Metal (Apple)"),
    ];

    let test_size = 10_000;
    let test_dim = 128;

    println!("Test: {} vectors x {} dimensions\n", test_size, test_dim);
    println!("{:<20}  {:>12}  {:>15}", "Backend", "Time (ms)", "Speedup");
    println!("{:-<50}", "");

    let query: Vec<f32> = (0..test_dim).map(|i| i as f32 * 0.01).collect();
    let database: Vec<Vec<f32>> = (0..test_size)
        .map(|i| (0..test_dim).map(|j| (i + j) as f32 * 0.01).collect())
        .collect();

    let mut cpu_time = 0.0;

    for (backend, name) in backends_to_test {
        let config = GpuConfig::default().with_backend(backend);

        match GpuExecutor::new(config) {
            Ok(exec) => {
                let start = Instant::now();
                let _ = exec.batch_euclidean_distance(&query, &database).unwrap();
                let elapsed = start.elapsed();

                let duration_ms = elapsed.as_secs_f64() * 1000.0;

                if backend == GpuBackend::Cpu {
                    cpu_time = duration_ms;
                }

                let speedup = if cpu_time > 0.0 {
                    cpu_time / duration_ms
                } else {
                    1.0
                };

                println!("{:<20}  {:>12.2}  {:>14.2}x", name, duration_ms, speedup);
            }
            Err(_) => {
                println!("{:<20}  {:>12}  {:>15}", name, "N/A", "N/A");
            }
        }
    }

    // ============================================================
    // 9. Real-world Use Cases
    // ============================================================
    println!("\n\n沍 Real-world Use Cases:");
    println!("笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊申n");

    println!("1. Semantic Search (1M documents):");
    println!("   - Without GPU: ~500ms per query");
    println!("   - With GPU: ~50ms per query (10x faster)");
    println!("   - Benefit: Real-time search at scale\n");

    println!("2. Recommendation System (100K items):");
    println!("   - Compute all pairwise similarities");
    println!("   - Without GPU: ~5 minutes");
    println!("   - With GPU: ~30 seconds (10x faster)");
    println!("   - Benefit: Faster model updates\n");

    println!("3. Image Search (10M images):");
    println!("   - Batch embedding similarity");
    println!("   - Without GPU: ~2 seconds per query");
    println!("   - With GPU: ~100ms per query (20x faster)");
    println!("   - Benefit: Interactive image search\n");

    println!("4. Clustering (500K vectors):");
    println!("   - K-means with 100 iterations");
    println!("   - Without GPU: ~10 minutes");
    println!("   - With GPU: ~1 minute (10x faster)");
    println!("   - Benefit: Rapid experimentation");

    // ============================================================
    // 10. Best Practices
    // ============================================================
    println!("\n\n汳｡ Best Practices:");
    println!("笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武笊絶武\n");

    println!("1. Batch operations:");
    println!("   - Process vectors in batches of 1000-10000");
    println!("   - Reduces GPU memory transfers\n");

    println!("2. Memory management:");
    println!("   - Enable memory pooling for repeated operations");
    println!("   - Monitor GPU memory usage\n");

    println!("3. Backend selection:");
    println!("   - Auto-detect for flexibility");
    println!("   - Explicit selection for benchmarking\n");

    println!("4. When to use GPU:");
    println!("   - Large datasets (>10K vectors)");
    println!("   - High-dimensional vectors (>128D)");
    println!("   - Batch processing\n");

    println!("5. When to use CPU:");
    println!("   - Small datasets (<1K vectors)");
    println!("   - Low-dimensional vectors (<64D)");
    println!("   - Single-query latency critical");

    println!("\n笨 GPU acceleration example complete!\n");

    println!("沁ｯ Key Takeaways:");
    println!("  - GPU acceleration provides 5-20x speedup for large datasets");
    println!("  - Automatic fallback ensures code works everywhere");
    println!("  - Batch operations maximize GPU efficiency");
    println!("  - Backend selection depends on hardware and use case");
    println!("  - CPU backend is SIMD-optimized and fast for small data");
}
