# -*- mode: python -*-
# =============================================================================
#  @@-COPYRIGHT-START-@@
#
#  Copyright (c) 2018, Qualcomm Innovation Center, Inc. All rights reserved.
#
#  Redistribution and use in source and binary forms, with or without
#  modification, are permitted provided that the following conditions are met:
#
#  1. Redistributions of source code must retain the above copyright notice,
#     this list of conditions and the following disclaimer.
#
#  2. Redistributions in binary form must reproduce the above copyright notice,
#     this list of conditions and the following disclaimer in the documentation
#     and/or other materials provided with the distribution.
#
#  3. Neither the name of the copyright holder nor the names of its contributors
#     may be used to endorse or promote products derived from this software
#     without specific prior written permission.
#
#  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
#  AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
#  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
#  ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE
#  LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
#  CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
#  SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
#  INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
#  CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
#  ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
#  POSSIBILITY OF SUCH DAMAGE.
#
#  SPDX-License-Identifier: BSD-3-Clause
#
#  @@-COPYRIGHT-END-@@
# =============================================================================

"""Prunes layers using Channel Pruning scheme"""

from typing import Iterator, List
import copy

import torch
import numpy as np

# Import AIMET specific modules
from aimet_common.defs import CostMetric, LayerCompRatioPair
from aimet_common.cost_calculator import CostCalculator, Cost
from aimet_common.pruner import Pruner
from aimet_common.channel_pruner import select_channels_to_prune
from aimet_torch.layer_database import LayerDatabase, Layer
from aimet_torch.data_subsampler import DataSubSampler
from aimet_torch.channel_pruning.weight_reconstruction import WeightReconstructor
from aimet_torch import utils
from aimet_torch.winnow.winnow import winnow_model


class InputChannelPruner(Pruner):
    """
    Pruner for Channel Pruning method
    """

    def __init__(
        self,
        data_loader: Iterator,
        input_shape,
        num_reconstruction_samples: int,
        allow_custom_downsample_ops: bool,
    ):
        """
        Input Channel Pruner with given data_loader, input shape, number of batches and samples per image.

        :param data_loader: data loader
        :param input_shape: input shape
        :param num_reconstruction_samples: number of reconstruction samples
        """
        self._data_loader = data_loader
        self._input_shape = input_shape
        self._num_reconstruction_samples = num_reconstruction_samples
        self._allow_custom_downsample_ops = allow_custom_downsample_ops

    @staticmethod
    def _select_inp_channels(layer: torch.nn.Module, comp_ratio: float) -> list:
        """
        L2 magnitude filter selection

        :param layer: torch.nn.Conv2d
        :param comp_ratio: the ratio of costs after pruning has taken place
                           0 < comp_ratio <= 1.
        :return:
            prune_channels_indices: list of input channels indices to prune.
        """

        assert isinstance(layer, torch.nn.Conv2d)

        # weight data is of shape [Noc, Nic, k_h, k_w]
        weight_data = layer.weight.data

        if weight_data.is_cuda:
            weight_data = weight_data.cpu().detach().numpy()
        else:
            weight_data = np.array(layer.weight.data)

        num_in_channels = layer.in_channels

        prune_indices = select_channels_to_prune(
            weight_data, comp_ratio, num_in_channels
        )

        return prune_indices

    def _data_subsample_and_reconstruction(
        self,
        orig_layer: torch.nn.Conv2d,
        pruned_layer: torch.nn.Conv2d,
        orig_model: torch.nn.Module,
        comp_model: torch.nn.Module,
    ):
        """
        Collect and sub sampled output data from original layer and input from pruned layer and set
        reconstructed weight and bias to pruned layer in pruned model

        :param orig_layer: layer from original model
        :param pruned_layer: layer from potentially compressed model
        :param orig_model: original model without any compression
        :param comp_model: compressed model
        :return: Nothing
        """
        inp_data, out_data = DataSubSampler.get_sub_sampled_data(
            orig_layer,
            pruned_layer,
            orig_model,
            comp_model,
            self._data_loader,
            self._num_reconstruction_samples,
        )

        WeightReconstructor.reconstruct_params_for_conv2d(
            pruned_layer, inp_data, out_data
        )

    def _sort_on_occurrence(
        self, model: torch.nn.Module, layer_comp_ratio_list: List[LayerCompRatioPair]
    ) -> List[LayerCompRatioPair]:
        """
        Function takes model and list of conv layer-comp ratio to sort, and sorts them based on
        occurrence in the model.

        :param model: model
        :param layer_comp_ratio_list: layer compression ratio list
        :return: sorted_layer_comp_ratio_List
        """

        sorted_layer_comp_ratio_list = []
        input_data = torch.randn(self._input_shape)

        # check if the layer weight is on GPU
        if layer_comp_ratio_list[0].layer.module.weight.is_cuda:
            input_data = input_data.cuda()

        def sorting_hook(module, _inp, _out):
            """
            hook to sort modules based on occurrence
            """
            for pair in layer_comp_ratio_list:
                if pair.layer.module == module:
                    sorted_layer_comp_ratio_list.append(
                        LayerCompRatioPair(pair.layer, pair.comp_ratio)
                    )

        handles = []

        for pair in layer_comp_ratio_list:
            handles.append(pair.layer.module.register_forward_hook(sorting_hook))

        # run one forward pass with hooks
        with utils.in_eval_mode(model), torch.no_grad():
            _ = model(input_data)

        # remove hooks
        for handle in handles:
            handle.remove()

        return sorted_layer_comp_ratio_list

    def _winnow_and_reconstruct_layer(
        self,
        orig_layer_db: LayerDatabase,
        comp_layer_db: LayerDatabase,
        layer: Layer,
        comp_ratio: float,
        perform_reconstruction: bool,
    ):
        """
        Replaces a given layer within the comp_layer_db with a pruned version of the layer

        :param orig_layer_db: original Layer database
        :param comp_layer_db: Layer database, will be modified
        :param layer: Layer to prune
        :param comp_ratio: compression - ratio
        :return:
        """
        # 1) channel selection
        prune_indices = self._select_inp_channels(layer.module, comp_ratio)

        # 2) winnow - in place API
        _, module_list = winnow_model(
            comp_layer_db.model,
            self._input_shape,
            [(layer.module, prune_indices)],
            reshape=self._allow_custom_downsample_ops,
            in_place=True,
        )

        # 3) data sub sampling and reconstruction
        if perform_reconstruction:
            # get original layer reference
            orig_layer = orig_layer_db.find_layer_by_name(layer.name)
            self._data_subsample_and_reconstruction(
                orig_layer.module,
                layer.module,
                orig_layer_db.model,
                comp_layer_db.model,
            )

        # 4) update layer database
        if module_list:
            self._update_layer_database_after_winnowing(comp_layer_db, module_list)

    @staticmethod
    def _update_layer_database_after_winnowing(comp_layer_db, updated_module_list):
        # Get reference to the new module to update in the layer database
        for old_module_name, new_module in updated_module_list:
            try:
                old_layer = comp_layer_db.find_layer_by_name(old_module_name)
            except KeyError:
                # Nothing to update, LayerDatabase was not tracking this layer
                continue

            if isinstance(new_module, torch.nn.Sequential):
                comp_layer_db.update_layer_with_module_in_sequential(
                    old_layer, new_module
                )

            else:
                # Determine new output shape
                new_output_shape = [
                    old_layer.output_shape[0],
                    new_module.out_channels,
                    old_layer.output_shape[2],
                    old_layer.output_shape[3],
                ]

                new_layer = Layer(new_module, old_layer.name, new_output_shape)
                comp_layer_db.replace_layer(old_layer, new_layer)

    def _prune_layer(
        self,
        orig_layer_db: LayerDatabase,
        comp_layer_db: LayerDatabase,
        layer: Layer,
        comp_ratio: float,
        cost_metric: CostMetric,
    ):
        self._winnow_and_reconstruct_layer(
            orig_layer_db, comp_layer_db, layer, comp_ratio, True
        )

    def calculate_compressed_cost(
        self, layer_db: LayerDatabase, layer_comp_ratio_list: List[LayerCompRatioPair]
    ) -> Cost:
        """
        Calculate cost of a compressed model given a set of layers and corresponding comp-ratios
        :param layer_db: Layer database for original model
        :param layer_comp_ratio_list: List of (layer + comp-ratio) pairs
        :return: Estimated cost of the compressed model
        """
        # Copy the db
        comp_layer_db = copy.deepcopy(layer_db)

        # create a compressed model
        for layer_comp_ratio in layer_comp_ratio_list:
            if layer_comp_ratio.comp_ratio is not None:
                layer = comp_layer_db.find_layer_by_name(layer_comp_ratio.layer.name)
                comp_ratio = layer_comp_ratio.comp_ratio
                if comp_ratio == 1.0:
                    continue
                self._winnow_and_reconstruct_layer(
                    layer_db, comp_layer_db, layer, comp_ratio, False
                )

        # calculate and return the cost of this model
        return CostCalculator.compute_model_cost(comp_layer_db)

    def prune_model(
        self,
        layer_db: LayerDatabase,
        layer_comp_ratio_list: List[LayerCompRatioPair],
        cost_metric: CostMetric,
        trainer,
    ):
        # sort all the layers in layer_comp_ratio_list based on occurrence
        layer_comp_ratio_list = self._sort_on_occurrence(
            layer_db.model, layer_comp_ratio_list
        )
        # call the base class method
        comp_layer_db = Pruner.prune_model(
            self, layer_db, layer_comp_ratio_list, cost_metric, trainer
        )

        return comp_layer_db


class ChannelPruningCostCalculator(CostCalculator):
    """Cost calculation utilities for Channel Pruning"""

    def __init__(self, pruner: InputChannelPruner):
        self._pruner = pruner

    # pylint: disable=arguments-differ
    def calculate_compressed_cost(
        self,
        layer_db: LayerDatabase,
        layer_ratio_list: List[LayerCompRatioPair],
        cost_metric: CostMetric,
    ) -> Cost:
        """
        Calculate compressed cost of a model given a list of layer-compression-ratio pairs
        :param layer_db: Layer database for the original model
        :param layer_ratio_list: List of layer, compression-ratio
        :param cost_metric: Cost metric to use for compression (mac or memory)
        :return: Compressed cost
        """

        # Special logic for channel pruning - we first actually prune the model and then determine its cost
        # Because it is not easy to estimate it otherwise
        compressed_cost = self._pruner.calculate_compressed_cost(
            layer_db, layer_ratio_list
        )

        return compressed_cost
