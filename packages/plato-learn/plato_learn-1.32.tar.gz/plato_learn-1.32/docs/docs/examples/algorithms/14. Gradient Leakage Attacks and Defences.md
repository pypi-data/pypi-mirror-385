### Gradient leakage attacks and defenses

Gradient leakage attacks and their defenses have been extensively studied in the research literature on federated learning. In `examples/gradient_leakage_attacks/`, several attacks, including `DLG`, `iDLG`, and `csDLG`, have been implemented, as well as several defense mechanisms, including `Soteria`, `GradDefense`, `Differential Privacy`, `Gradient Compression`, and `Outpost`. A variety of methods in the trainer API has been used in their implementations. Refer to `examples/dlg/README.md` for more details.

```bash
cd examples/gradient_leakage_attacks
uv run dlg.py -c untrained_eval_delta.yml
```
