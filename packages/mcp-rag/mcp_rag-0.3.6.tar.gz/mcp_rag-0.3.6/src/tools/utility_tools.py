"""
MCP å®ç”¨å·¥å…·
============

æ­¤æ¨¡å—åŒ…å«å®ç”¨å·¥å…·å’Œç»´æŠ¤åŠŸèƒ½ã€‚
ä» rag_server.py è¿ç§»è€Œæ¥ï¼Œç”¨äºæ¨¡å—åŒ–æ¶æ„ã€‚

æ³¨æ„ï¼šè¿™äº›å‡½æ•°è¢«è®¾è®¡ä¸ºåœ¨ä¸»æœåŠ¡å™¨ä¸­ä½¿ç”¨ @mcp.tool() è£…é¥°å™¨ã€‚
"""

from rag_core_openai import (
    get_document_statistics,
    get_cache_stats,
    clear_embedding_cache,
    optimize_vector_store,
    get_vector_store_stats,
    reindex_vector_store
)
from utils.logger import log

# å¯¼å…¥ç»“æ„åŒ–æ¨¡å‹
try:
    from models import MetadataModel
except ImportError as e:
    log(f"è­¦å‘Šï¼šæ— æ³•å¯¼å…¥ç»“æ„åŒ–æ¨¡å‹ï¼š{e}")
    MetadataModel = None

# å¿…é¡»åœ¨æœåŠ¡å™¨ä¸­å¯ç”¨çš„å…¨å±€å˜é‡
rag_state = {}
initialize_rag_func = None

def set_rag_state(state):
    """è®¾ç½®å…¨å±€ RAG çŠ¶æ€ã€‚"""
    global rag_state
    rag_state = state

def set_initialize_rag_func(func):
    """è®¾ç½® RAG åˆå§‹åŒ–å‡½æ•°ã€‚"""
    global initialize_rag_func
    initialize_rag_func = func

def initialize_rag():
    """åˆå§‹åŒ– RAG ç³»ç»Ÿã€‚"""
    if initialize_rag_func:
        initialize_rag_func()
    elif "initialized" in rag_state:
        return
    # æ­¤å‡½æ•°å¿…é¡»åœ¨ä¸»æœåŠ¡å™¨ä¸­å®ç°
    pass

def analyze_documents_with_models(vector_store) -> dict:
    """
    ä½¿ç”¨ç»“æ„åŒ–æ¨¡å‹åˆ†ææ–‡æ¡£ä»¥è·å–æ›´è¯¦ç»†çš„ä¿¡æ¯ã€‚
    
    å‚æ•°ï¼š
        vector_store: å‘é‡æ•°æ®åº“
        
    Returns:
        ä½¿ç”¨æ¨¡å‹çš„è¯¦ç»†åˆ†æå­—å…¸
    """
    if MetadataModel is None:
        return {"error": "MetadataModel ä¸å¯ç”¨"}
    
    try:
        # è·å–æ‰€æœ‰æ–‡æ¡£
        all_docs = vector_store.get()
        
        if not all_docs or not all_docs['documents']:
            return {"total_documents": 0, "message": "æ•°æ®åº“ä¸ºç©º"}
        
        documents = all_docs['documents']
        metadatas = all_docs.get('metadatas', [])
        
        # è½¬æ¢ä¸ºç»“æ„åŒ–æ¨¡å‹
        metadata_models = []
        for metadata in metadatas:
            if metadata:
                try:
                    metadata_model = MetadataModel.from_dict(metadata)
                    metadata_models.append(metadata_model)
                except Exception as e:
                    log(f"MCP æœåŠ¡å™¨è­¦å‘Š: å°†å…ƒæ•°æ®è½¬æ¢ä¸ºæ¨¡å‹æ—¶å‡ºé”™: {e}")
        
        # ä½¿ç”¨ç»“æ„åŒ–æ¨¡å‹è¿›è¡Œåˆ†æ
        analysis = {
            "total_documents": len(documents),
            "structured_models": len(metadata_models),
            "file_types": {},
            "processing_methods": {},
            "chunking_methods": {},
            "content_quality": {
                "rich_content": 0,
                "standard_content": 0,
                "poor_content": 0
            },
            "structural_analysis": {
                "documents_with_tables": 0,
                "documents_with_titles": 0,
                "documents_with_lists": 0,
                "avg_tables_per_doc": 0,
                "avg_titles_per_doc": 0,
                "avg_lists_per_doc": 0,
                "avg_chunk_size": 0
            },
            "processing_quality": {
                "unstructured_enhanced": 0,
                "manual_input": 0,
                "markitdown": 0,
                "other": 0
            }
        }
        
        total_tables = 0
        total_titles = 0
        total_lists = 0
        total_chunk_sizes = 0
        
        for model in metadata_models:
            # æ–‡ä»¶ç±»å‹
            file_type = model.file_type or "unknown"
            analysis["file_types"][file_type] = analysis["file_types"].get(file_type, 0) + 1
            
            # å¤„ç†æ–¹æ³•
            processing_method = model.processing_method or "unknown"
            analysis["processing_methods"][processing_method] = analysis["processing_methods"].get(processing_method, 0) + 1
            
            # åˆ†å—æ–¹æ³•
            chunking_method = model.chunking_method or "unknown"
            analysis["chunking_methods"][chunking_method] = analysis["chunking_methods"].get(chunking_method, 0) + 1
            
            # å†…å®¹è´¨é‡
            if model.is_rich_content():
                analysis["content_quality"]["rich_content"] += 1
            elif model.total_elements > 1:
                analysis["content_quality"]["standard_content"] += 1
            else:
                analysis["content_quality"]["poor_content"] += 1
            
            # ç»“æ„åˆ†æ
            if model.tables_count > 0:
                analysis["structural_analysis"]["documents_with_tables"] += 1
                total_tables += model.tables_count
            
            if model.titles_count > 0:
                analysis["structural_analysis"]["documents_with_titles"] += 1
                total_titles += model.titles_count
            
            if model.lists_count > 0:
                analysis["structural_analysis"]["documents_with_lists"] += 1
                total_lists += model.lists_count
            
            # å—å¤§å°
            if model.avg_chunk_size > 0:
                total_chunk_sizes += model.avg_chunk_size
            
            # å¤„ç†è´¨é‡
            if processing_method == "unstructured_enhanced":
                analysis["processing_quality"]["unstructured_enhanced"] += 1
            elif processing_method == "manual_input":
                analysis["processing_quality"]["manual_input"] += 1
            elif processing_method == "markitdown":
                analysis["processing_quality"]["markitdown"] += 1
            else:
                analysis["processing_quality"]["other"] += 1
        
        # è®¡ç®—å¹³å‡å€¼
        if len(metadata_models) > 0:
            analysis["structural_analysis"]["avg_tables_per_doc"] = total_tables / len(metadata_models)
            analysis["structural_analysis"]["avg_titles_per_doc"] = total_titles / len(metadata_models)
            analysis["structural_analysis"]["avg_lists_per_doc"] = total_lists / len(metadata_models)
            analysis["structural_analysis"]["avg_chunk_size"] = total_chunk_sizes / len(metadata_models)
        
        return analysis
        
    except Exception as e:
        log(f"MCP æœåŠ¡å™¨é”™è¯¯: ä½¿ç”¨æ¨¡å‹åˆ†ææ—¶å‡ºé”™: {e}")
        return {"error": str(e)}

def get_knowledge_base_stats() -> str:
    """
    è·å–çŸ¥è¯†åº“çš„ç»¼åˆç»Ÿè®¡ä¿¡æ¯ï¼ŒåŒ…æ‹¬æ–‡æ¡£ç±»å‹ã€å¤„ç†æ–¹æ³•å’Œç»“æ„ä¿¡æ¯ã€‚
    ä½¿ç”¨æ­¤åŠŸèƒ½äº†è§£çŸ¥è¯†åº“ä¸­å¯ç”¨çš„ä¿¡æ¯ä»¥åŠå¦‚ä½•å¤„ç†è¿™äº›ä¿¡æ¯ã€‚
    
    ä½¿ç”¨åœºæ™¯ç¤ºä¾‹ï¼š
    - æ£€æŸ¥çŸ¥è¯†åº“ä¸­æœ‰å¤šå°‘æ–‡æ¡£
    - äº†è§£æ–‡ä»¶ç±»å‹çš„åˆ†å¸ƒ
    - æŸ¥çœ‹ä½¿ç”¨äº†å“ªäº›å¤„ç†æ–¹æ³•
    - åˆ†æå­˜å‚¨æ–‡æ¡£çš„ç»“æ„å¤æ‚æ€§
    
    è¿™æœ‰åŠ©äºæ‚¨å¯¹æœç´¢å†…å®¹å’Œè¿‡æ»¤æŸ¥è¯¢åšå‡ºæ˜æ™ºå†³ç­–ã€‚

    è¿”å›ï¼š
        å…³äºçŸ¥è¯†åº“å†…å®¹çš„è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯ã€‚
    """
    log(f"MCPæœåŠ¡å™¨ï¼šæ­£åœ¨è·å–çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯...")
    initialize_rag()
    
    try:
        # è·å–åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
        basic_stats = get_document_statistics(rag_state["vector_store"])
        
        if "error" in basic_stats:
            return f"âŒ è·å–ç»Ÿè®¡ä¿¡æ¯æ—¶å‡ºé”™ï¼š {basic_stats['error']}"
        
        if basic_stats.get("total_documents", 0) == 0:
            return "ğŸ“Š çŸ¥è¯†åº“ä¸ºç©º\n\nçŸ¥è¯†åº“ä¸­æ²¡æœ‰å­˜å‚¨ä»»ä½•æ–‡æ¡£ã€‚"
        
        # è·å–ç»“æ„åŒ–æ¨¡å‹åˆ†æ
        model_analysis = analyze_documents_with_models(rag_state["vector_store"])
        
        # æ„å»ºè¯¦ç»†å“åº”
        response = f"ğŸ“Š çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯\n\n"
        response += f"ğŸ“š æ–‡æ¡£æ€»æ•°: {basic_stats['total_documents']}\n"
        
        # å…³äºç»“æ„åŒ–æ¨¡å‹çš„ä¿¡æ¯ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if "error" not in model_analysis and model_analysis.get("structured_models", 0) > 0:
            response += f"ğŸ§  ç»“æ„åŒ–æ¨¡å‹æ–‡æ¡£: {model_analysis['structured_models']}\n"
            response += f"ğŸ“ˆ é«˜çº§åˆ†æå¯ç”¨: âœ…\n"
        else:
            response += f"ğŸ“ˆ é«˜çº§åˆ†æå¯ç”¨: âŒ (ä½¿ç”¨åŸºç¡€åˆ†æ)\n"
        
        response += "\n"
        
        # æ–‡ä»¶ç±»å‹
        if basic_stats["file_types"]:
            response += "ğŸ“„ æ–‡ä»¶ç±»å‹:\n"
            for file_type, count in sorted(basic_stats["file_types"].items(), key=lambda x: x[1], reverse=True):
                percentage = (count / basic_stats["total_documents"]) * 100
                display_ft = (file_type.upper() if isinstance(file_type, str) else "UNKNOWN")
                response += f"   â€¢ {display_ft}: {count} ({percentage:.1f}%)\n"
            response += "\n"
        
        # å¤„ç†æ–¹æ³•
        if basic_stats["processing_methods"]:
            response += "ğŸ”§ å¤„ç†æ–¹æ³•:\n"
            for method, count in sorted(basic_stats["processing_methods"].items(), key=lambda x: x[1], reverse=True):
                percentage = (count / basic_stats["total_documents"]) * 100
                method_display = method.replace('_', ' ').title()
                response += f"   â€¢ {method_display}: {count} ({percentage:.1f}%)\n"
            response += "\n"
        
        # åˆ†å—æ–¹æ³•ï¼ˆä»…å½“æœ‰æ¨¡å‹åˆ†ææ—¶ï¼‰
        if "error" not in model_analysis and model_analysis.get("chunking_methods"):
            response += "ğŸ§© åˆ†å—æ–¹æ³•:\n"
            for method, count in sorted(model_analysis["chunking_methods"].items(), key=lambda x: x[1], reverse=True):
                percentage = (count / model_analysis["structured_models"]) * 100
                method_display = method.replace('_', ' ').title()
                response += f"   â€¢ {method_display}: {count} ({percentage:.1f}%)\n"
            response += "\n"
        
        # å†…å®¹è´¨é‡ï¼ˆä»…å½“æœ‰æ¨¡å‹åˆ†ææ—¶ï¼‰
        if "error" not in model_analysis and model_analysis.get("content_quality"):
            response += "ğŸ“Š å†…å®¹è´¨é‡:\n"
            quality = model_analysis["content_quality"]
            total_analyzed = quality["rich_content"] + quality["standard_content"] + quality["poor_content"]
            
            if total_analyzed > 0:
                rich_pct = (quality["rich_content"] / total_analyzed) * 100
                standard_pct = (quality["standard_content"] / total_analyzed) * 100
                poor_pct = (quality["poor_content"] / total_analyzed) * 100
                
                response += f"   â€¢ ğŸŸ¢ ç»“æ„ä¸°å¯Œçš„å†…å®¹: {quality['rich_content']} ({rich_pct:.1f}%)\n"
                response += f"   â€¢ ğŸŸ¡ æ ‡å‡†å†…å®¹: {quality['standard_content']} ({standard_pct:.1f}%)\n"
                response += f"   â€¢ ğŸ”´ åŸºç¡€å†…å®¹: {quality['poor_content']} ({poor_pct:.1f}%)\n"
            response += "\n"
        
        # ç»“æ„ç»Ÿè®¡ä¿¡æ¯
        structural = basic_stats["structural_stats"]
        response += "ğŸ—ï¸ ç»“æ„ä¿¡æ¯:\n"
        response += f"   â€¢ åŒ…å«è¡¨æ ¼çš„æ–‡æ¡£: {structural['documents_with_tables']}\n"
        response += f"   â€¢ åŒ…å«æ ‡é¢˜çš„æ–‡æ¡£: {structural['documents_with_titles']}\n"
        response += f"   â€¢ åŒ…å«åˆ—è¡¨çš„æ–‡æ¡£: {structural['documents_with_lists']}\n"
        response += f"   â€¢ æ¯ä¸ªæ–‡æ¡£çš„å¹³å‡è¡¨æ ¼æ•°: {structural['avg_tables_per_doc']:.1f}\n"
        response += f"   â€¢ æ¯ä¸ªæ–‡æ¡£çš„å¹³å‡æ ‡é¢˜æ•°: {structural['avg_titles_per_doc']:.1f}\n"
        response += f"   â€¢ æ¯ä¸ªæ–‡æ¡£çš„å¹³å‡åˆ—è¡¨æ•°: {structural['avg_lists_per_doc']:.1f}\n"
        
        # æ¨¡å‹çš„é¢å¤–ä¿¡æ¯ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if "error" not in model_analysis and model_analysis.get("structural_analysis"):
            model_structural = model_analysis["structural_analysis"]
            response += f"   â€¢ å¹³å‡å—å¤§å°: {model_structural['avg_chunk_size']:.0f} ä¸ªå­—ç¬¦\n"
        
        response += "\n"
        
        # å¢å¼ºçš„æœç´¢å»ºè®®
        response += "ğŸ’¡ æœç´¢å»ºè®®:\n"
        if structural['documents_with_tables'] > 0:
            response += f"   â€¢ ä½¿ç”¨ `ask_rag_filtered` åŠ ä¸Š `min_tables=1` åœ¨åŒ…å«è¡¨æ ¼çš„æ–‡æ¡£ä¸­æœç´¢ä¿¡æ¯\n"
        if structural['documents_with_titles'] > 5:
            response += f"   â€¢ ä½¿ç”¨ `ask_rag_filtered` åŠ ä¸Š `min_titles=5` åœ¨ç»“æ„è‰¯å¥½çš„æ–‡æ¡£ä¸­æœç´¢\n"
        if ".pdf" in basic_stats["file_types"]:
            response += f"   â€¢ ä½¿ç”¨ `ask_rag_filtered` åŠ ä¸Š `file_type=\".pdf\"` ä»…åœ¨PDFæ–‡æ¡£ä¸­æœç´¢\n"
        
        # åŸºäºæ¨¡å‹åˆ†æçš„é¢å¤–å»ºè®®
        if "error" not in model_analysis:
            if model_analysis["content_quality"]["rich_content"] > 0:
                response += f"   â€¢ æ‚¨æœ‰ {model_analysis['content_quality']['rich_content']} ä¸ªç»“æ„ä¸°å¯Œçš„æ–‡æ¡£ - åˆ©ç”¨è¯­ä¹‰åˆ†å—\n"
            if model_analysis["processing_quality"]["unstructured_enhanced"] > 0:
                response += f"   â€¢ {model_analysis['processing_quality']['unstructured_enhanced']} ä¸ªæ–‡æ¡£ä½¿ç”¨å¢å¼ºçš„Unstructuredå¤„ç†\n"
        
        log(f"MCP æœåŠ¡å™¨: ç»Ÿè®¡ä¿¡æ¯è·å–æˆåŠŸ")
        return response
        
    except Exception as e:
        log(f"MCP æœåŠ¡å™¨: è·å–ç»Ÿè®¡ä¿¡æ¯æ—¶å‡ºé”™: {e}")
        return f"âŒ è·å–ç»Ÿè®¡ä¿¡æ¯æ—¶å‡ºé”™: {e}"

def get_embedding_cache_stats() -> str:
    """
    è·å–åµŒå…¥ç¼“å­˜æ€§èƒ½çš„è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯ã€‚
    ä½¿ç”¨æ­¤åŠŸèƒ½ç›‘æ§ç¼“å­˜æ•ˆç‡å¹¶äº†è§£ç³»ç»Ÿæ€§èƒ½ã€‚
    
    ä½¿ç”¨åœºæ™¯ç¤ºä¾‹ï¼š
    - æ£€æŸ¥ç¼“å­˜å‘½ä¸­ç‡ä»¥æŸ¥çœ‹ç³»ç»Ÿæ˜¯å¦é«˜æ•ˆå·¥ä½œ
    - ç›‘æ§ç¼“å­˜çš„å†…å­˜ä½¿ç”¨æƒ…å†µ
    - äº†è§£åµŒå…¥è¢«é‡ç”¨çš„é¢‘ç‡
    - è°ƒè¯•æ€§èƒ½é—®é¢˜
    
    è¿™æœ‰åŠ©äºæ‚¨ä¼˜åŒ–ç³»ç»Ÿå¹¶äº†è§£å…¶è¡Œä¸ºã€‚

    è¿”å›ï¼š
        å…³äºåµŒå…¥ç¼“å­˜æ€§èƒ½çš„è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯ã€‚
    """
    log(f"MCPæœåŠ¡å™¨ï¼šæ­£åœ¨è·å–åµŒå…¥ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯...")
    
    try:
        stats = get_cache_stats()
        
        if not stats:
            return "ğŸ“Š åµŒå…¥ç¼“å­˜ä¸å¯ç”¨\n\nåµŒå…¥ç¼“å­˜æœªåˆå§‹åŒ–ã€‚"
        
        # æ„å»ºè¯¦ç»†å“åº”
        response = f"ğŸ“Š åµŒå…¥ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯\n\n"
        
        # ä¸»è¦æŒ‡æ ‡
        response += f"ğŸ”„ ç¼“å­˜æ´»åŠ¨:\n"
        response += f"   â€¢ æ€»è¯·æ±‚æ•°: {stats['total_requests']}\n"
        response += f"   â€¢ å†…å­˜å‘½ä¸­æ•°: {stats['memory_hits']}\n"
        response += f"   â€¢ ç£ç›˜å‘½ä¸­æ•°: {stats['disk_hits']}\n"
        response += f"   â€¢ æœªå‘½ä¸­æ•°ï¼ˆæœªæ‰¾åˆ°ï¼‰: {stats['misses']}\n\n"
        
        # æˆåŠŸç‡
        response += f"ğŸ“ˆ æˆåŠŸç‡:\n"
        response += f"   â€¢ å†…å­˜å‘½ä¸­ç‡: {stats['memory_hit_rate']}\n"
        response += f"   â€¢ ç£ç›˜å‘½ä¸­ç‡: {stats['disk_hit_rate']}\n"
        response += f"   â€¢ æ€»å‘½ä¸­ç‡: {stats['overall_hit_rate']}\n\n"
        
        # å†…å­˜ä½¿ç”¨
        response += f"ğŸ’¾ å†…å­˜ä½¿ç”¨:\n"
        response += f"   â€¢ å†…å­˜ä¸­çš„åµŒå…¥: {stats['memory_cache_size']}\n"
        response += f"   â€¢ æœ€å¤§å¤§å°: {stats['max_memory_size']}\n"
        response += f"   â€¢ ç¼“å­˜ç›®å½•: {stats['cache_directory']}\n\n"
        
        # æ€§èƒ½åˆ†æ
        total_requests = stats['total_requests']
        if total_requests > 0:
            memory_hit_rate = float(stats['memory_hit_rate'].rstrip('%'))
            overall_hit_rate = float(stats['overall_hit_rate'].rstrip('%'))
            
            response += f"ğŸ¯ æ€§èƒ½åˆ†æ:\n"
            
            if overall_hit_rate > 70:
                response += f"   â€¢ âœ… æ€§èƒ½ä¼˜ç§€: {overall_hit_rate:.1f}% å‘½ä¸­ç‡\n"
            elif overall_hit_rate > 50:
                response += f"   â€¢ âš ï¸ æ€§èƒ½ä¸€èˆ¬: {overall_hit_rate:.1f}% å‘½ä¸­ç‡\n"
            else:
                response += f"   â€¢ âŒ æ€§èƒ½è¾ƒä½: {overall_hit_rate:.1f}% å‘½ä¸­ç‡\n"
            
            if memory_hit_rate > 50:
                response += f"   â€¢ ğŸš€ å†…å­˜ç¼“å­˜æœ‰æ•ˆ: {memory_hit_rate:.1f}% å†…å­˜å‘½ä¸­ç‡\n"
            else:
                response += f"   â€¢ ğŸ’¾ ä¾èµ–ç£ç›˜è®¿é—®: {memory_hit_rate:.1f}% å†…å­˜å‘½ä¸­ç‡\n"
            
            # ä¼˜åŒ–å»ºè®®
            response += f"\nğŸ’¡ ä¼˜åŒ–å»ºè®®:\n"
            if overall_hit_rate < 30:
                response += f"   â€¢ è€ƒè™‘å°†ç±»ä¼¼æ–‡æ¡£ä¸€èµ·å¤„ç†\n"
                response += f"   â€¢ æ£€æŸ¥æ˜¯å¦æœ‰å¤ªå¤šä¸é‡å¤çš„å”¯ä¸€æ–‡æœ¬\n"
            
            if memory_hit_rate < 30 and total_requests > 100:
                response += f"   â€¢ è€ƒè™‘å¢åŠ å†…å­˜ç¼“å­˜å¤§å°\n"
                response += f"   â€¢ ç£ç›˜å‘½ä¸­æ¯”å†…å­˜å‘½ä¸­æ…¢\n"
            
            if stats['memory_cache_size'] >= stats['max_memory_size'] * 0.9:
                response += f"   â€¢ å†…å­˜ç¼“å­˜å‡ ä¹å·²æ»¡\n"
                response += f"   â€¢ å¦‚æœ‰å¯ç”¨RAMï¼Œè€ƒè™‘å¢åŠ  max_memory_size\n"
        
        log(f"MCP Server: æˆåŠŸè·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯")
        return response
        
    except Exception as e:
        log(f"MCP Server: è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯æ—¶å‡ºé”™: {e}")
        return f"âŒ è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯æ—¶å‡ºé”™: {e}"

def clear_embedding_cache_tool() -> str:
    """
    æ¸…é™¤åµŒå…¥ç¼“å­˜ä»¥é‡Šæ”¾å†…å­˜å’Œç£ç›˜ç©ºé—´ã€‚
    åœ¨éœ€è¦é‡ç½®ç¼“å­˜æˆ–é‡Šæ”¾èµ„æºæ—¶ä½¿ç”¨æ­¤åŠŸèƒ½ã€‚
    
    ä½¿ç”¨åœºæ™¯ç¤ºä¾‹ï¼š
    - ç³»ç»ŸRAMä¸è¶³æ—¶é‡Šæ”¾å†…å­˜
    - æ›´æ”¹åµŒå…¥æ¨¡å‹åé‡ç½®ç¼“å­˜
    - æ¸…é™¤ä¸å†éœ€è¦çš„æ—§ç¼“å­˜åµŒå…¥
    - è§£å†³ç¼“å­˜ç›¸å…³é—®é¢˜
    
    è­¦å‘Šï¼šè¿™å°†åˆ é™¤æ‰€æœ‰ç¼“å­˜çš„åµŒå…¥ï¼Œéœ€è¦é‡æ–°è®¡ç®—ã€‚

    Returns:
        ç¼“å­˜æ¸…ç†æ“ä½œçš„ç¡®è®¤æ¶ˆæ¯
    """
    log(f"MCP Server: æ­£åœ¨æ¸…ç†åµŒå…¥ç¼“å­˜...")
    
    try:
        clear_embedding_cache()
        
        response = "ğŸ§¹ åµŒå…¥ç¼“å­˜æ¸…ç†æˆåŠŸ\n\n"
        response += "âœ… å·²åˆ é™¤æ‰€æœ‰å­˜å‚¨åœ¨ç¼“å­˜ä¸­çš„åµŒå…¥ã€‚\n"
        response += "ğŸ“ ä¸‹ä¸€æ¬¡åµŒå…¥å°†ä»å¤´å¼€å§‹è®¡ç®—ã€‚\n"
        response += "ğŸ’¾ å·²é‡Šæ”¾å†…å­˜å’Œç£ç›˜ç©ºé—´ã€‚\n\n"
        response += "âš ï¸ æ³¨æ„: åµŒå…¥å°†åœ¨éœ€è¦æ—¶è‡ªåŠ¨é‡æ–°è®¡ç®—ã€‚"
        
        log(f"MCP Server: åµŒå…¥ç¼“å­˜æ¸…ç†æˆåŠŸ")
        return response
        
    except Exception as e:
        log(f"MCP Server: æ¸…ç†ç¼“å­˜æ—¶å‡ºé”™: {e}")
        return f"âŒ æ¸…ç†ç¼“å­˜æ—¶å‡ºé”™: {e}"

def optimize_vector_database() -> str:
    """
    ä¼˜åŒ–å‘é‡æ•°æ®åº“ä»¥æé«˜æœç´¢æ€§èƒ½ã€‚
    æ­¤å·¥å…·é‡æ–°ç»„ç»‡å†…éƒ¨ç´¢å¼•ä»¥å®ç°æ›´å¿«çš„æœç´¢ã€‚
    
    ä½¿ç”¨æ­¤å·¥å…·çš„æƒ…å†µï¼š
    - æœç´¢é€Ÿåº¦ç¼“æ…¢
    - å·²æ·»åŠ å¤§é‡æ–°æ–‡æ¡£
    - å¸Œæœ›æé«˜ç³»ç»Ÿæ•´ä½“æ€§èƒ½
    
    Returns:
        ä¼˜åŒ–è¿‡ç¨‹çš„ä¿¡æ¯
    """
    log("MCP Server: æ­£åœ¨ä¼˜åŒ–å‘é‡æ•°æ®åº“...")
    
    try:
        result = optimize_vector_store()
        
        if result["status"] == "success":
            response = f"âœ… å‘é‡æ•°æ®åº“ä¼˜åŒ–æˆåŠŸ\n\n"
            response += f"ğŸ“Š ä¼˜åŒ–å‰ç»Ÿè®¡ä¿¡æ¯:\n"
            stats_before = result.get("stats_before", {})
            response += f"   â€¢ æ€»æ–‡æ¡£æ•°: {stats_before.get('total_documents', 'N/A')}\n"
            
            response += f"\nğŸ“Š ä¼˜åŒ–åç»Ÿè®¡ä¿¡æ¯:\n"
            stats_after = result.get("stats_after", {})
            response += f"   â€¢ æ€»æ–‡æ¡£æ•°: {stats_after.get('total_documents', 'N/A')}\n"
            
            response += f"\nğŸš€ ä¼˜åŒ–æ•ˆæœ:\n"
            response += f"   â€¢ æœç´¢é€Ÿåº¦æ›´å¿«\n"
            response += f"   â€¢ ç»“æœç²¾åº¦æ›´é«˜\n"
            response += f"   â€¢ ç´¢å¼•å·²ä¼˜åŒ–\n"
            
        else:
            response = f"âŒ ä¼˜åŒ–æ•°æ®åº“æ—¶å‡ºé”™: {result.get('message', 'æœªçŸ¥é”™è¯¯')}"
            
        return response
        
    except Exception as e:
        log(f"MCP Server Error: ä¼˜åŒ–æ—¶å‡ºé”™: {e}")
        return f"âŒ ä¼˜åŒ–å‘é‡æ•°æ®åº“æ—¶å‡ºé”™: {str(e)}"

def get_vector_database_stats() -> str:
    """
    è·å–å‘é‡æ•°æ®åº“çš„è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯ã€‚
    åŒ…æ‹¬æ–‡æ¡£ã€æ–‡ä»¶ç±»å‹å’Œé…ç½®ä¿¡æ¯ã€‚
    
    ä½¿ç”¨æ­¤å·¥å…·ï¼š
    - éªŒè¯æ•°æ®åº“çŠ¶æ€
    - åˆ†ææ–‡æ¡£åˆ†å¸ƒ
    - è¯Šæ–­æ€§èƒ½é—®é¢˜
    - è§„åˆ’ä¼˜åŒ–
    
    Returns:
        å‘é‡æ•°æ®åº“çš„è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯
    """
    log("MCP Server: æ­£åœ¨è·å–å‘é‡æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯...")
    
    try:
        stats = get_vector_store_stats()
        
        if "error" in stats:
            return f"âŒ è·å–ç»Ÿè®¡ä¿¡æ¯æ—¶å‡ºé”™: {stats['error']}"
        
        response = f"ğŸ“Š å‘é‡æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯\n\n"
        
        response += f"ğŸ“š åŸºæœ¬ä¿¡æ¯:\n"
        response += f"   â€¢ æ–‡æ¡£æ€»æ•°: {stats.get('total_documents', 0)}\n"
        response += f"   â€¢ é›†åˆåç§°: {stats.get('collection_name', 'N/A')}\n"
        response += f"   â€¢ åµŒå…¥ç»´åº¦: {stats.get('embedding_dimension', 'N/A')}\n"
        
        # Tipos de archivo
        file_types = stats.get('file_types', {})
        if file_types:
            response += f"\nğŸ“„ æŒ‰æ–‡ä»¶ç±»å‹åˆ†å¸ƒ:\n"
            for file_type, count in file_types.items():
                response += f"   â€¢ {file_type}: {count} ä¸ªæ–‡æ¡£\n"
        
        # å¤„ç†æ–¹æ³•
        processing_methods = stats.get('processing_methods', {})
        if processing_methods:
            response += f"\nğŸ”§ å¤„ç†æ–¹æ³•:\n"
            for method, count in processing_methods.items():
                response += f"   â€¢ {method}: {count} ä¸ªæ–‡æ¡£\n"
        
        # æ€§èƒ½ä¿¡æ¯
        performance = stats.get('performance', {})
        if performance:
            response += f"\nâš¡ æ€§èƒ½ä¿¡æ¯:\n"
            response += f"   â€¢ ç´¢å¼•æ—¶é—´: {performance.get('indexing_time', 'N/A')}\n"
            response += f"   â€¢ ç´¢å¼•å¤§å°: {performance.get('index_size', 'N/A')}\n"
        
        log(f"MCP Server: æˆåŠŸè·å–å‘é‡æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯")
        return response
        
    except Exception as e:
        log(f"MCP Server: è·å–å‘é‡æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯æ—¶å‡ºé”™: {e}")
        return f"âŒ è·å–å‘é‡æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯æ—¶å‡ºé”™: {str(e)}"

def reindex_vector_database(profile: str = 'auto') -> str:
    """
    ä½¿ç”¨ä¼˜åŒ–é…ç½®é‡æ–°ç´¢å¼•å‘é‡æ•°æ®åº“ã€‚
    æ­¤å·¥å…·ä½¿ç”¨é’ˆå¯¹å½“å‰å¤§å°ä¼˜åŒ–çš„å‚æ•°é‡æ–°åˆ›å»ºç´¢å¼•ã€‚
    
    Args:
        profile: é…ç½®æ¡£æ¡ˆ ('small', 'medium', 'large', 'auto')
                 'auto' è‡ªåŠ¨æ£€æµ‹æœ€ä½³é…ç½®æ¡£æ¡ˆ
    
    ä½¿ç”¨æ­¤å·¥å…·çš„æƒ…å†µï¼š
    - æ›´æ”¹é…ç½®æ¡£æ¡ˆæ—¶
    - æœç´¢éå¸¸ç¼“æ…¢æ—¶
    - å¸Œæœ›é’ˆå¯¹ç‰¹å®šæ•°æ®åº“å¤§å°è¿›è¡Œä¼˜åŒ–æ—¶
    - å­˜åœ¨æŒç»­æ€§èƒ½é—®é¢˜æ—¶
    
    âš ï¸ æ³¨æ„: æ­¤è¿‡ç¨‹å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼Œå…·ä½“å–å†³äºæ•°æ®åº“å¤§å°ã€‚
    
    Returns:
        é‡æ–°ç´¢å¼•è¿‡ç¨‹çš„ä¿¡æ¯
    """
    log(f"MCP Server: æ­£åœ¨ä½¿ç”¨é…ç½®æ¡£æ¡ˆ '{profile}' é‡æ–°ç´¢å¼•å‘é‡æ•°æ®åº“...")
    
    try:
        result = reindex_vector_store(profile=profile)
        
        if result["status"] == "success":
            response = f"âœ… å‘é‡æ•°æ®åº“é‡æ–°ç´¢å¼•æˆåŠŸ\n\n"
            response += f"ğŸ“Š åº”ç”¨çš„é…ç½®æ¡£æ¡ˆ: {result.get('profile', 'N/A')}\n"
            response += f"ğŸ“Š å¤„ç†çš„æ–‡æ¡£æ•°: {result.get('documents_processed', 'N/A')}\n"
            response += f"â±ï¸ é‡æ–°ç´¢å¼•æ—¶é—´: {result.get('reindexing_time', 'N/A')}\n"
            
            response += f"\nğŸš€ é‡æ–°ç´¢å¼•çš„ä¼˜åŠ¿:\n"
            response += f"   â€¢ é’ˆå¯¹å½“å‰å¤§å°ä¼˜åŒ–çš„ç´¢å¼•\n"
            response += f"   â€¢ æ›´å¿«æ›´ç²¾ç¡®çš„æœç´¢\n"
            response += f"   â€¢ æ›´å¥½çš„æ•°æ®åˆ†å¸ƒ\n"
            
        else:
            response = f"âŒ é‡æ–°ç´¢å¼•æ•°æ®åº“æ—¶å‡ºé”™: {result.get('message', 'æœªçŸ¥é”™è¯¯')}"
            
        return response
        
    except Exception as e:
        log(f"MCP Server: é‡æ–°ç´¢å¼•å‘é‡æ•°æ®åº“æ—¶å‡ºé”™: {e}")
        return f"âŒ é‡æ–°ç´¢å¼•å‘é‡æ•°æ®åº“æ—¶å‡ºé”™: {str(e)}" 