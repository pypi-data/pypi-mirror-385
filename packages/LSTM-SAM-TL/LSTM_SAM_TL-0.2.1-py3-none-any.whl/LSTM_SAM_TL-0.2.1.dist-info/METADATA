Metadata-Version: 2.1
Name: LSTM-SAM-TL
Version: 0.2.1
Summary: Bi-LSTM with custom Attention Mechanism and Tuning: Training/Test and TimeSeries CV models
Author: Samuel Daramola
License: MIT
Project-URL: Homepage, https://pypi.org/project/LSTM-SAM-TL/
Classifier: Programming Language :: Python :: 3
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Requires-Python: >=3.9
Description-Content-Type: text/markdown
Requires-Dist: numpy
Requires-Dist: pandas
Requires-Dist: matplotlib
Requires-Dist: scipy
Requires-Dist: scikit-learn
Requires-Dist: tensorflow (>=2.9)
Requires-Dist: keras-tuner (>=1.4.0)

# LSTMâ€‘SAMâ€‘TL: Extreme Water Level Prediction with BiLSTM + Custom Attention and Transfer Learning

This repository provides a compact, userâ€‘friendly pipeline for training, validating, and evaluating sequence models for hourly water level prediction. It focuses on:

- **Two training pipelines**: **Train/Test split** and **Timeâ€‘Series Crossâ€‘Validation (TSCV)** with hyperparameter tuning.
- **A custom attention layer** that emphasizes the most informative time steps in each sequence.
- **Zeroâ€‘shot transferâ€‘learning runners** to test or predict with **trained `.h5` models** on new station datasets.

---

## ğŸ” Project Structure

```
.
â”œâ”€â”€ __init__.py                  # Public entry points for import convenience
â”œâ”€â”€ layers.py                    # CustomAttentionLayer (selfâ€‘attention over time)
â”œâ”€â”€ tt_model.py                  # Train/Test pipeline + KerasTuner search + plots
â”œâ”€â”€ tscv_model.py                # Timeâ€‘Series CV (expanding window) + plots
â”œâ”€â”€ transfer_learning_test.py    # Evaluate preâ€‘trained .h5 models (metrics + plots + CSV)
â”œâ”€â”€ transfer_learning_predict.py # Predictâ€‘only runner for preâ€‘trained models (plots + CSV)
â”œâ”€â”€ utils.py                     # Scaling + windowing helpers
â””â”€â”€ Training.ipynb               # (Optional) interactive training notebook
```

---

## âœ¨ Whatâ€™s implemented

- **BiLSTM Ã—3 + BatchNorm + Dropout** blocks
- **Customâ€‘Attention over time** (upâ€‘weights the most informative time steps)
- **Dense head** for regression
- **MinMax scaling** for inputs/targets and **inverseâ€‘transform** for outputs
- **KerasTuner Bayesian Optimization** of units, L2, dropout, learning rate, and attention emphasis
- **GPUâ€‘aware runners** with quiet logging when desired
- **Consistent file naming** embeds lookback and batch size (e.g., `â€¦128b24hTT.h5`, `â€¦128b24hCV.h5`)
- **Reproducibility hooks** (fixed seeds) and early stopping
- **Hydrology metrics builtâ€‘in**: KGE, NSE, Willmottâ€™s d, mean absolute bias (mBias)
- **Autoâ€‘plotting**: combined timeâ€‘series + 1:1 scatter per training run

---

## ğŸ§° Installation

> Requires Python â‰¥3.9 and TensorFlow â‰¥2.10 (tested with Keras/TensorFlow 2.x).

```bash
pip install tensorflow keras-tuner scikit-learn numpy pandas matplotlib
```

If you plan to use a GPU, ensure your CUDA/cuDNN stack matches your TensorFlow version.

---

## ğŸ“¦ Data format

The training and TL runners expect a CSV with **features + target** and optional datetime columns.

- **Preferred layout** for training:
  - First two columns: `Date`, `Time (GMT)` (if you have them)
  - Last column: `WaterLevel` (target)
  - All other columns in between are features.
- **Flexible layout**: If `Date`/`Time (GMT)` are missing, the code still works as long as the **last column is the target** and the others are features.

Example header (subset):

```
Date,Time (GMT),feat_1,feat_2,...,feat_N,WaterLevel
```

> **Tip:** keep all features and the target in their original physical units; scaling and inverseâ€‘transform are handled internally. Plots/metrics are reported in the original units. Note, the default target metrics is ploted in meters (m).

---

## ğŸ‹ï¸â€â™€ï¸ Training pipelines

### 1) Train/Test split

```python
from LSTM_SAM_TL import TT_model

res = TT_model(
    csv_path="your_station.csv",
    lookback=24,                  # sliding window length (hours)
    epochs=300,
    batch_size=128,
    validation_split=0.3,         # random split *inside* the training windows
    early_stopping=5,             # epochs to wait after no val improvement
    max_trials=10,                # KerasTuner trials
    loss="mae",                  # or "mse"
    seed=42,                      # reproducibility
    model_name="ATL",            # autoâ€‘saves as ATL{batch}b{lookback}hTT.h5 if thresholds met
    verbose=1,
)
print(res["metrics"], res.get("plot_path"))
```

**What happens**
- Loads CSV, drops `Date`/`Time (GMT)` if present, and **MinMaxâ€‘scales** X and y
- Splits 80/20 chronologically â†’ creates sliding windows of length `lookback`
- Runs **Bayesian hyperparameter search** on the training windows
- Trains the best model, evaluates on the test windows, **inverseâ€‘transforms** predictions to original units
- **Saves the model *only if* thresholds are met** (details below)
- Saves a combined **timeâ€‘series + 1:1 scatter** plot to `Visualization/TT_{batch}b{lookback}h_combined.png`

**Outputs**
- `best_hyperparams` (units, dropout, L2, learning rate, attention emphasis)
- `metrics`: `mse`, `rmse`, `mae`, `r2`, **`kge`**, **`nse`**, **`willmott_d`**, **`mbias`**
- `history`: training curves
- `y_test_pred`: flattened predictions on the evaluation window
- `plot_path`: path of the combined plot

---

### 2) Timeâ€‘Series Crossâ€‘Validation (robust)

```python
from LSTM_SAM_TL import TSCV_model

res = TSCV_model(
    csv_path="your_station.csv",
    lookback=24,
    n_splits=10,                  # expandingâ€‘window CV across the training chunk
    epochs=300,
    batch_size=128,
    validation_split=0.2,         # used only for the final model validation
    early_stopping=5,
    max_trials=10,
    loss="mae",
    seed=42,
    model_name="ATL",            # autoâ€‘saves as ATL{batch}b{lookback}hCV.h5 if thresholds met
    verbose=1,
)
print(res["holdout_metrics"], res.get("plot_path"))
```

**What happens**
- Splits data chronologically: **80%** for model selection + **20% holdout** for final testing
- Inside the 80% train chunk: performs a **chronological tuner split** (no leakage)
- Runs **expandingâ€‘window TimeSeriesSplit** (n_splits folds) for stable CV metrics
- Trains a final model on the training windows, evaluates on the **holdout** (inverseâ€‘transformed)
- Saves a combined plot to `Visualization/TSCV_{batch}b{lookback}h_combined.png`

**Outputs**
- `cv_folds`: list of fold metrics (`val_loss`, `val_metric`)
- `best_hyperparams`: winning configuration
- `holdout_metrics`: `mse`, `rmse`, `mae`, `r2`, **`kge`**, **`nse`**, **`willmott_d`**, **`mbias`**
- `y_test_pred`: flattened predictions on the holdout window
- `plot_path`: path of the combined plot

---

## âœ… Thresholdâ€‘gated model saving (IMPORTANT)

Both training pipelines will **save a model only when a candidate save path is provided** and the **thresholds are met** on the evaluation split:

- Provide either `save_model_path` **or** `model_name` to enable saving.
- The model is saved **only if** **KGE â‰¥ 0.70** **and** **NSE â‰¥ 0.70** on the test/holdout evaluation.
- Otherwise, the run completes **without saving** (plots/metrics are still produced).

This policy helps keep the model registry clean by promoting only runs that meet a minimum hydrologic performance bar.

---

## ğŸ§  Model architecture (summary)

- **Backbone:** 3 Ã— BiLSTM blocks (`tanh`), each followed by **BatchNorm + Dropout**
- **Attention:** `CustomAttentionLayer` performs a softmax over time, applies a tunable `emphasis_factor`, and returns a weighted summary over time
- **Head:** `Flatten` â†’ `Dense(1)` for regression

> The attentionâ€™s `emphasis_factor` and each blockâ€™s `units`, `l2`, and `dropout` are part of the KerasTuner search space.

---

## ğŸ” Transferâ€‘learning (zeroâ€‘shot; no retraining)

Use these to apply one or more **trained .h5** models to a **new station dataset**.

### A) Evaluate with test waterâ€‘level data and comparison plots

```python
from LSTM_SAM_TL import transfer_learning_test

out = transfer_learning_test(
    data_path="sandy_1992.csv",
    verbose=1,
    start_date="1992-12-08",    # optional plot/metric window
    end_date="1992-12-14",
)
print(out["metrics"])            # dict per model file
print(out["output_csv"])        # LSTM-SAM predictions/DAT_EWL_sandy_1992.csv
```

**Behavior**
- Autoâ€‘discovers all `*.h5` in the **current working directory**
- **Infers lookback** from filename pattern `â€¦b{lookback}hâ€¦`
- Builds RNN windows, loads each model (with custom attention), predicts, **inverseâ€‘transforms**, computes metrics, and creates:
  - **time series + metrics** plot and **1:1 scatter** under `Visualization/`
  - a merged CSV under `LSTM-SAM predictions/DAT_EWL_<stem>.csv` containing **actuals** and **one prediction column per model**

### B) Prediction (no test waterâ€‘level data)

```python
from LSTM_SAM_TL import transfer_learning_predict

out = transfer_learning_predict(
    data_path="sandy_1992.csv",
    verbose=1,
    start_date="1992-12-08",
    end_date="1992-12-14",
)
print(out["output_csv"])        # LSTM-SAM predictions/PRE_EWL_sandy_1992.csv
print(out["models"])            # list of processed model files
```

**Behavior**
- Same discovery and lookback inference as the test runner
- Produces **predictedâ€‘only** time series plots and a merged CSV under `LSTM-SAM predictions/PRE_EWL_<stem>.csv`

---

## ğŸ“ˆ Plots & metric annotations

Each training run saves a 2â€‘panel figure:

- **Left:** timeâ€‘series of Actual (red dashed) vs Predicted
- **Right:** 1:1 scatter with a dashed identity line. A metrics box is drawn **inside the scatter (upperâ€‘left)** listing **RMSE, KGE, NSE, Willmottâ€™s d, mBias**. The point legend is placed **bottomâ€‘right**.

File names:
- Train/Test: `Visualization/TT_{batch}b{lookback}h_combined.png`
- TSCV:      `Visualization/TSCV_{batch}b{lookback}h_combined.png`

---

## ğŸ”§ Key arguments (common)

- `lookback` â€” sliding window length in time steps (hours)
- `epochs`, `batch_size` â€” training controls
- `max_trials`, `early_stopping` â€” tuner budget and earlyâ€‘stopping patience
- `loss` â€” `"mae"` or `"mse"` (affects loss and reported metric in training)
- `seed` â€” deterministic runs (NumPy + TF)
- `verbose` â€” 0 (silent), 1 (progress), 2 (perâ€‘epoch)

### Notes
- In **TSCV**, `validation_split` applies **only** to the final fit (a chronological tail of the training chunk).
- Provide `save_model_path` or `model_name` to enable **thresholdâ€‘gated saving**.

---

## ğŸ§ª Reproducibility

- Seeds are applied to NumPy and TensorFlow.
- All tuner/train/validation/test splits are **chronological** to avoid leakage.

---

## ğŸ› ï¸ Tips & troubleshooting

- **GPU memory**: the TL runners enable memory growth on the first visible GPU. If you hit OOM, try smaller `batch_size` or reduce `units_*` limits in the tuner.
- **Filename parsing**: ensure your saved models include `â€¦b{lookback}hâ€¦` so the TL runners infer the correct window size.
- **Data gaps**: large gaps can reduce effective window count; consider light imputation upâ€‘front.
- **Scaling**: do not preâ€‘scale your CSV; pipelines handle MinMax scaling and inverseâ€‘transform predictions for metrics.

---

## ğŸ“š Minimal API reference

- `TT_model(...) â†’ Dict` â€” Train/Test pipeline + tuner + metrics + plots
- `TSCV_model(...) â†’ Dict` â€” Timeâ€‘Series CV + tuner + holdout metrics + plots
- `transfer_learning_test(...) â†’ Dict` â€” evaluate trained models, save plots + `DAT_EWL_*.csv`
- `transfer_learning_predict(...) â†’ Dict` â€” predictâ€‘only on new data, save plots + `PRE_EWL_*.csv`
- `CustomAttentionLayer(emphasis_factor=1.5)` â€” attention over time with tunable emphasis

---

## References

Daramola, S., et al. (2025). *Predicting the Evolution of Extreme Water Levels With Long Shortâ€‘Term Memory Stationâ€‘Based Approximated Models and Transfer Learning Techniques.* https://doi.org/10.1029/2024WR039054

