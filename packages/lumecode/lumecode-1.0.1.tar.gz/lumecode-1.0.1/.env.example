# LLM Provider Configuration (ALL FREE TIERS!)
LUMECODE_PROVIDER=groq    # Options: openrouter, groq, ollama, mock
LUMECODE_MODEL=auto       # auto-select or specify model name

# ===== FREE API Providers (Phase 1 - College Project) =====

# OpenRouter (FREE tier - https://openrouter.ai/keys)
# Sign up with email (no credit card needed)
OPENROUTER_API_KEY=sk-or-v1-...
# Free models: meta-llama/llama-3.1-8b-instruct, mistralai/mistral-7b-instruct

# Groq (FREE tier - https://console.groq.com/keys)
# Sign up with email (no credit card needed)
GROQ_API_KEY=gsk_...
# Free models: llama-3.1-70b-versatile, mixtral-8x7b-32768, gemma-7b-it

# ===== Local Models (Phase 2 - Optional) =====

# Ollama (LOCAL - https://ollama.ai)
# Run models locally on your machine (complete offline capability)
OLLAMA_HOST=http://localhost:11434
OLLAMA_MODEL=llama2  # or codellama, mistral, deepseek-coder
# Benefits: Complete offline, no rate limits, privacy

# ===== Caching (Important for free tiers!) =====
LUMECODE_CACHE_ENABLED=true
LUMECODE_CACHE_TTL=3600  # Cache responses for 1 hour

# ===== Advanced Settings =====
LUMECODE_MAX_TOKENS=2048
LUMECODE_TEMPERATURE=0.7

# Usage Tracking (stay within free tier limits)
LUMECODE_TRACK_USAGE=true
LUMECODE_WARN_LIMIT=true

# Debug
LUMECODE_DEBUG=false
