Metadata-Version: 2.1
Name: forgebase
Version: 0.2.11
Summary: Forgebase core libraries (utils, framework e cliente LLM)
Home-page: https://github.com/palhanobrazil/forgebase
License: LGPL
Keywords: forgebase,framework,llm,openai
Author: palhano
Author-email: palhanobrazil@gmail.com
Requires-Python: >=3.11,<4.0
Classifier: License :: Other/Proprietary License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Topic :: Software Development :: Libraries
Classifier: Topic :: Software Development :: Libraries :: Application Frameworks
Requires-Dist: appdirs (>=1.4.4,<2.0.0)
Requires-Dist: backoff (>=2.2.1)
Requires-Dist: httpx (>=0.24.0)
Requires-Dist: pydantic (>=2.5)
Requires-Dist: python-dotenv (>=1.0.0)
Project-URL: Repository, https://github.com/palhanobrazil/forgebase
Description-Content-Type: text/markdown

# Forgebase

Forgebase re√∫ne tr√™s blocos principais usados neste monorepo:

- **forge_utils** ‚Äì logging estruturado e utilidades de paths/configura√ß√£o.
- **forgebase** ‚Äì framework MVC-C enxuto baseado em Pydantic v2 (modelos, commands,
  controllers, views e persist√™ncia).
- **llm_client** ‚Äì cliente agn√≥stico para a OpenAI Responses API com suporte a streaming,
  tool calling e replays offline.

A biblioteca est√° dispon√≠vel em [PyPI](https://pypi.org/project/forgebase/) e tamb√©m no
TestPyPI para valida√ß√£o pr√©via.

## Instala√ß√£o r√°pida

```bash
pip install forgebase
```

Crie um ambiente virtual limpo antes de instalar (`python -m venv .venv && source .venv/bin/activate`).

---

## ‚≠ê Novo na v0.2.2: Streaming + Tools (Responses API)

Esta vers√£o foca em robustez do streaming com tool calling sobre a OpenAI Responses API.

- Headers de streaming: adicionamos `Accept: text/event-stream` e melhoramos timeouts (SSE sem read timeout) no cliente HTTP.
- Normaliza√ß√£o de tools: agora aceitamos tanto o formato top‚Äëlevel da Responses API
  (`{\"type\":\"function\",\"name\":...}`) quanto o formato ‚Äúaninhado‚Äù comum no Chat Completions
  (`{\"type\":\"function\",\"function\": {\"name\":..., \"parameters\":...}}`). O cliente normaliza para o contrato oficial da Responses API antes de enviar.
- Sem fallback para Chat Completions: n√£o utilizamos Chat Completions; todo o fluxo permanece na Responses API.
- Header opcional de beta: √© poss√≠vel habilitar `OpenAI-Beta: responses=v1` via `OPENAI_BETA_RESPONSES=1` caso seu ambiente exija este gate.

Documenta√ß√£o relacionada: `docs/api/openai_responses.md`, `docs/api/openai_responses_tool_calling.md` e `docs/api/streaming_tools_guide.md`.

Impacto: corrige erros intermitentes 400 observados durante streaming + tools e melhora compatibilidade com formatos de tools enviados por clientes legados.

---

## ‚≠ê Novo na v0.2.3: Robustez de Streaming + Follow‚Äëup de Tools

Esta vers√£o refina o fluxo de streaming e o follow‚Äëup de ferramentas:

- Tratamento de erros em SSE:
  - Em 4xx, o cliente drena o corpo e lan√ßa `APIResponseError` com a mensagem do provider (elimina o erro "without read()" e evita conex√µes em mau estado).
- Follow‚Äëups de tool calling (Responses API):
  - O follow‚Äëup inclui o descritor bruto `tool_use` (al√©m de `function_call`) antes do respectivo `*_output`, alinhando com variantes da Responses API em cen√°rios multi‚Äëturn.
- Testes e documenta√ß√£o:
  - Novos testes de streaming e follow‚Äëup.
  - Guia: `docs/api/streaming_tools_guide.md`.

Compatibilidade: sem mudan√ßas de interface ‚Äî sua integra√ß√£o atual continua funcionando.

---

## üß™ Desenvolvimento (v0.2.6+)

Esta se√ß√£o resume como depurar e validar cen√°rios de streaming + tools no ForgeBase.

- Defaults a partir da v0.2.5/0.2.6
  - Follow‚Äëup em streaming: `outputs-only` habilitado (n√£o reenvia descriptors brutos)
  - `tool_choice` no follow‚Äëup: `none`
  - `name` inclu√≠do nos outputs

- Flags de compatibilidade (override)
  - `FORGEBASE_FOLLOWUP_OUTPUTS_ONLY=0|1`
  - `FORGEBASE_FOLLOWUP_TOOL_CHOICE=none|auto|required`
  - `FORGEBASE_OUTPUT_NAME=0|1`
  - `FORGEBASE_OUTPUT_KIND=function_result|tool_result`

- Diagn√≥stico (sem vazar conte√∫do)
  - `FORGEBASE_DEBUG_FOLLOWUP=1` emite em n√≠vel INFO a linha:
    `[follow-up] prev=... outputs=N types=[...] call_ids=[...] unique=N`
  - Recomenda√ß√µes:
    - Habilitar `DEBUG` no logger durante testes locais
    - Usar hooks do provider para imprimir o payload do follow‚Äëup quando necess√°rio

- ReconciliacÃßaÃÉo de call_id (v0.2.8+)
  - IDs `call_*` coletados dos eventos de streaming e, se preciso, via `input_items`
  - Substituem IDs alternativos ao montar os outputs no follow‚Äëup
  - Buffer de outputs particionado por `previous_response_id`

- Testes
  - Rodar toda a su√≠te: `pytest -q`
  - Casos relevantes:
    - `tests/test_streaming_error_handling.py`
    - `tests/test_streaming_tool_use_followup.py`
    - `tests/test_followup_flags_overrides.py`
  - O guia `docs/api/streaming_tools_guide.md` refor√ßa cen√°rios, flags e troubleshooting


## ‚≠ê Novo na v0.2.1: LLMClientFactory

A partir da v0.2.1, use `LLMClientFactory` para criar providers LLM de forma desacoplada:

```python
from forgebase import LLMClientFactory, Tool

# Op√ß√£o 1: Cria√ß√£o manual
provider = LLMClientFactory.create("openai", api_key="sk-...", timeout=60)

# Op√ß√£o 2: Auto-configura√ß√£o via vari√°vel de ambiente (OPENAI_API_KEY)
provider = LLMClientFactory.create_from_env("openai")

# Usar o provider
response = provider.send_message("Hello, world!")
print(response)

# Trocar provider transparentemente (quando dispon√≠vel)
# provider = LLMClientFactory.create("llama")  # Mesma interface!
```

**Por que usar a factory?**
- ‚úÖ **Desacoplamento**: C√≥digo n√£o depende de implementa√ß√£o espec√≠fica (OpenAI, Llama, etc.)
- ‚úÖ **Extensibilidade**: Trocar provider mudando apenas 1 string
- ‚úÖ **Testabilidade**: Mock de interface `ILLMClient` em vez de classe concreta
- ‚úÖ **Preparado para o futuro**: Suporte a m√∫ltiplos providers sem breaking changes

> **‚ö†Ô∏è Depreca√ß√£o:** `OpenAIProvider` ainda funciona na v0.2.1 (backward compatible), mas recomendamos migrar para `LLMClientFactory`. Veja exemplos abaixo.

---

## Pacote por pacote

### forge_utils

- `forge_utils.log_service.LogService` configura logging com console/arquivo rotativo e filtros.
- `forge_utils.log_service.logger` √© a inst√¢ncia global pronta para uso.
- `forge_utils.paths` oferece helpers (`build_app_paths`, `ensure_dirs`) para organizar arquivos
  de configura√ß√£o, hist√≥rico e cache.

### forgebase

Reexporta o framework MVC-C b√°sico. Os pontos de entrada mais usados s√£o:

- `CustomBaseModel` / `BaseModelData`: modelos Pydantic com suporte a *dirty tracking* e observers.
- `CustomCommandBase` + `guard_errors`: encapsulam regras de neg√≥cio, padronizando
  o tratamento de exce√ß√µes (`CommandException`).
- `CustomBaseController` / `CustomBaseView`: composi√ß√£o MVC-C m√≠nima.
- `PersistenceFactory` + `JsonPersistence`: persist√™ncia compat√≠vel com Pydantic v2.

Todos estes nomes est√£o dispon√≠veis diretamente com `from forgebase import ...`.

### llm_client

O cliente LLM tamb√©m √© reexportado por `forgebase` para facilitar o consumo:

**API P√∫blica (v0.2.1+):**
- `LLMClientFactory`: factory para criar providers sem conhecer implementa√ß√£o (‚úÖ **use este!**)
- `ILLMClient`: interface Protocol para type hints e extensibilidade
- `Tool`: modelo Pydantic que representa o schema JSON das ferramentas
- `APIResponseError` / `ConfigurationError`: exce√ß√µes espec√≠ficas do cliente
- `ContentPart`, `OutputMessage`, `ResponseResult`, `TextFormat`, `TextOutputConfig`: modelos
  retornados pelo Responses API

**Legacy (deprecated - mantido para compatibilidade):**
- `OpenAIProvider`: ‚ö†Ô∏è usar `LLMClientFactory.create("openai")` em vez disso
- `LLMOpenAIClient`: ‚ö†Ô∏è interno - n√£o usar diretamente

**Multi-provider ready:** A arquitetura est√° preparada para m√∫ltiplos providers (Llama, Anthropic, OpenRouter)
sem breaking changes no c√≥digo cliente. Use `LLMClientFactory` para garantir compatibilidade futura.

## Guia r√°pido de uso

### Core MVC-C

```python
from forgebase import CustomBaseModel, CustomCommandBase, JsonPersistence, guard_errors

class User(CustomBaseModel):
    id: int
    name: str

class CreateUserCommand(CustomCommandBase):
    @guard_errors
    def execute(self, payload: dict) -> User:
        model = User(**payload)
        # ... l√≥gica de neg√≥cio ...
        return model

storage = JsonPersistence("users.json")
```

### Cliente LLM ‚Äì configura√ß√£o e chamadas

Crie um arquivo `.env` na raiz do projeto (ou exporte no shell) com:

```
OPENAI_API_KEY=sk-...
```

Todos os exemplos abaixo carregam essa chave automaticamente via `python-dotenv`.

#### Chamada s√≠ncrona

```python
import os
from dotenv import load_dotenv
from forgebase import APIResponseError, ConfigurationError, LLMOpenAIClient

load_dotenv()
client = LLMOpenAIClient(api_key=os.environ["OPENAI_API_KEY"], model="gpt-4o-mini")

try:
    # instructions: mensagem de sistema (n√£o confundir com o prompt)
    response = client.send_prompt(
        "Por que o c√©u √© azul?",
        instructions="Voc√™ √© um assistente t√©cnico. Responda de forma objetiva."
    )
    answer = "\n".join(
        part.text.strip()
        for item in response.output
        for part in getattr(item, "content", [item])
        if getattr(part, "text", None)
    )
    print(answer)
except (APIResponseError, ConfigurationError) as exc:
    print(f"Falha na chamada: {exc}")
```

#### Usando system instructions (Responses API)

"Instructions" s√£o a mensagem de sistema da Responses API. Elas definem o comportamento do modelo, e n√£o devem ser confundidas com o prompt do usu√°rio. No Forgebase, voc√™ pode pass√°-las explicitamente nas chamadas do cliente e do provider. Se omitidas, a API do provider usar√° as instru√ß√µes padr√£o do servidor.

Exemplos:

```python
from forgebase import LLMOpenAIClient, OpenAIProvider, LLMClientFactory

# 1) Cliente direto
client = LLMOpenAIClient(api_key=os.environ["OPENAI_API_KEY"], model="gpt-4o")
resp = client.send_prompt(
    "Liste 3 boas pr√°ticas para logs.",
    instructions=(
        "Voc√™ √© um revisor s√™nior. Use linguagem clara, sem jarg√µes desnecess√°rios."
    ),
)

# 2) Provider (recomendado via Factory)
provider = LLMClientFactory.create_from_env("openai")
out = provider.send_message(
    "Explique o conceito de idempot√™ncia.",
    instructions="Padr√£o: respostas curtas, com um exemplo pr√°tico."
)

# 3) Streaming com instructions
for chunk in provider.send_stream(
    "Resuma o papel do backoff exponencial.",
    instructions="Resuma em at√© 2 frases."
):
    print(chunk, end="")
```

Observa√ß√µes:
- Instructions s√£o propagadas em m√∫ltiplas rodadas durante tool calling (follow‚Äëups).
- Instructions n√£o substituem o prompt; s√£o complementares e tratadas como ‚Äúsystem message‚Äù.
- O campo √© enviado como `ResponseRequest.instructions` para a Responses API.

##### Migrando do uso direto da OpenAI API

Antes utiliz√°vamos o SDK oficial diretamente. Agora a camada `LLMOpenAIClient` traz timeout,
hooks e tool calling prontos, al√©m de facilitar a troca de provider.

```python
# Legado: uso direto do SDK da OpenAI
from openai import OpenAI

client = OpenAI(api_key=os.environ["OPENAI_API_KEY"])
resp = client.responses.create(model="gpt-4o-mini", input="Por que o c√©u √© azul?")
print(resp.output[0].content[0].text)
```

```python
# Atual: via LLMOpenAIClient encapsulado
from forgebase import LLMOpenAIClient

client = LLMOpenAIClient(api_key=os.environ["OPENAI_API_KEY"], model="gpt-4o-mini")
resp = client.send_prompt("Por que o c√©u √© azul?")
print(resp.output[0].content[0].text)
```

#### Configurando timeout (‚≠ê novo na v0.2.0)

Por padr√£o, as chamadas HTTP usam timeout de 120 segundos. Voc√™ pode customizar:

```python
# Timeout de 45 segundos
client = LLMOpenAIClient(
    api_key=os.environ["OPENAI_API_KEY"],
    timeout=45  # segundos
)

# Ou via OpenAIProvider
from forgebase import OpenAIProvider

provider = OpenAIProvider(timeout=45)
provider.set_api_key(os.environ["OPENAI_API_KEY"])
result = provider.send_message("Hello")
```

**Importante:** O timeout se aplica ao tempo total incluindo retries. Com `timeout=45` e `max_tries=4`, o sistema n√£o ultrapassar√° 45 segundos mesmo com m√∫ltiplas tentativas.

#### Chamada com streaming

```python
import os
from dotenv import load_dotenv
from forgebase import LLMOpenAIClient

load_dotenv()
client = LLMOpenAIClient(api_key=os.environ["OPENAI_API_KEY"], model="gpt-4o-mini")

stream = client.send_prompt("Conte uma hist√≥ria curta sobre um rob√¥ e uma crian√ßa.", streamed=True)
for delta in stream:
    print(delta, end="", flush=True)
print()
```

#### Chamada multimodal (imagem + √°udio)

```python
import os
from dotenv import load_dotenv
from forgebase import LLMOpenAIClient

load_dotenv()
client = LLMOpenAIClient(api_key=os.environ["OPENAI_API_KEY"], model="gpt-4o")

response = client.send_prompt(
    "Descreva a imagem e comente o √°udio anexado.",
    images=["https://upload.wikimedia.org/wikipedia/commons/9/99/Colorful_sunset.jpg"],
    audio={"base64": "ZGF0YQ==", "mime_type": "audio/wav"},
)
print(response)
```

#### Listar modelos dispon√≠veis

```python
import os
from dotenv import load_dotenv
from forgebase import LLMOpenAIClient

load_dotenv()
client = LLMOpenAIClient(api_key=os.environ["OPENAI_API_KEY"])
models = client.list_models()
print(models["data"][0])
```

#### Tool calling s√≠ncrono (v0.2.1+)

```python
import os
from dotenv import load_dotenv
from forgebase import LLMClientFactory, Tool

load_dotenv()

# ‚úÖ Novo: usar factory
provider = LLMClientFactory.create_from_env("openai")

# 1) Descreva a ferramenta com JSON Schema compat√≠vel com a Responses API
tool = Tool(
    type="function",
    name="say_hello",
    parameters={"type": "object", "properties": {"who": {"type": "string"}}, "required": ["who"]},
)
# 2) Conte ao provider quais ferramentas est√£o dispon√≠veis
provider.configure_tools([tool], tool_choice="required")
# 3) Registre o handler Python que ser√° chamado quando o modelo disparar a ferramenta
provider.register_tool("say_hello", lambda args: f"Ol√°, {args['who']}!")

# 4) Use send_message normalmente: o provider executa as tools e devolve o texto final
print(provider.send_message("Cumprimente Forgebase."))
```

<details>
<summary>üìú C√≥digo antigo (deprecated - clique para expandir)</summary>

```python
# ‚ö†Ô∏è Deprecated: usando OpenAIProvider diretamente
from forgebase import OpenAIProvider, Tool

provider = OpenAIProvider()
provider.set_api_key(os.environ["OPENAI_API_KEY"])
# ... resto do c√≥digo igual
```
</details>

##### Tool calling com orquestra√ß√£o autom√°tica

Providers criados pela factory encapsulam todos os ciclos de *tool calling* da Responses API.
Basta definir o schema das ferramentas, registrar os handlers e chamar `send_message`:

```python
from forgebase import LLMClientFactory, Tool

provider = LLMClientFactory.create("openai", api_key=os.environ["OPENAI_API_KEY"], timeout=60)

weather_tool = Tool(
    type="function",
    name="get_weather",
    description="Retorna a temperatura atual para uma cidade brasileira.",
    parameters={
        "type": "object",
        "properties": {"city": {"type": "string"}},
        "required": ["city"],
    },
)

provider.configure_tools([weather_tool], tool_choice="required")

def get_weather(args: dict[str, str]) -> str:
    city = args["city"]
    return f"{city}, 30 graus."

provider.register_tool("get_weather", get_weather)

print(provider.send_message("Qual a temperatura de hoje em S√£o Paulo?"))
```

Internamente o provider:

- Executa m√∫ltiplas rodadas de tool calling at√© chegar no texto final ‚Äî o mesmo fluxo usado nos testes `test_demo_tool_call_api_forced` e `test_openai_provider_hooks_with_tools`.
- Gerencia `tool_choice`, IDs de chamadas e payloads `function_call_output` / `custom_tool_call_output`, montando o `input_override` correto para cada rodada, como coberto em `tests/test_openai_tool_calling.py`.
- Exp√µe hooks (`before_tool_call`, `after_tool_call`, `tool_error`, etc.) que voc√™ pode monitorar para m√©tricas e logs.
- Funciona tanto com chamadas s√≠ncronas (`send_message`) quanto streaming (`send_stream`) sem alterar o c√≥digo dos handlers.

**Passo a passo resumido**

- Configure o provider com `set_api_key`.
- Descreva as ferramentas com `forgebase.Tool` em `configure_tools`.
- Registre cada handler com `register_tool`.
- Dispare `send_message` (ou `send_stream`) ‚Äî o provider executa as ferramentas e devolve apenas o texto final.
- Opcional: adicione hooks (`register_hook`) para inspecionar `before_tool_call`, `after_tool_call` ou erros.

Para validar fluxos sem acessar a API real, use `LLMOpenAIClient` com uma resposta fake
(`response=<objeto compat√≠vel>`) ou rode `python -m llm_client.demo_tool_call_api` que inclui
um modo offline utilizando as mesmas rotinas de orquestra√ß√£o (vide `tests/test_demo_tool_call_api.py`).

#### Tool calling com streaming

```python
import os
from dotenv import load_dotenv
from forgebase import OpenAIProvider, Tool

load_dotenv()
provider = OpenAIProvider()
provider.set_api_key(os.environ["OPENAI_API_KEY"])

tool = Tool(
    type="function",
    name="summarize_numbers",
    parameters={"type": "object", "properties": {"nums": {"type": "array", "items": {"type": "number"}}}},
)
provider.configure_tools([tool], tool_choice="auto")
provider.register_tool("summarize_numbers", lambda args: sum(args.get("nums", [])))

for chunk in provider.send_stream("Considere os n√∫meros 2, 4, 6 e mostre a soma."):
    print(chunk, end="", flush=True)
print()

# Verifique tests/test_openai_tool_calling.py::test_streaming_tool_call_flow
# para um cen√°rio completo de m√∫ltiplas rodadas no modo streaming.
```

#### Hooks de eventos

Tanto o `LLMOpenAIClient` quanto o `OpenAIProvider` exp√µem um sistema simples de
hooks para instrumentar o fluxo:

```python
from forgebase import LLMOpenAIClient, OpenAIProvider

client = LLMOpenAIClient(api_key=os.environ["OPENAI_API_KEY"])
client.register_hook("before_request", lambda ctx: print("‚ñ∂", ctx["prompt"]))
client.register_hook("after_response", lambda ctx: print("‚óÄ", ctx.get("response")))

provider = OpenAIProvider(client=client)
provider.register_hook("before_tool_call", lambda ctx: print("tool", ctx["tool"]))
provider.register_hook("after_tool_call", lambda ctx: print("tool result", ctx["result"]))
```

Eventos dispon√≠veis:

- `before_request`, `after_response`, `on_error`, `on_cache_hit` (cliente LLM)
- `before_send`, `after_send`, `on_error`, `before_tool_call`, `after_tool_call`, `tool_error`, `cache_hit` (provider)

### Demo completo

O projeto inclui uma demo mais abrangente que cobre respostas diretas, streaming,
(tool calling) e replays offline:

```bash
PYTHONPATH=shared/src:apps/llm_client/src python -m llm_client.example_full_usage
```

No Windows (PowerShell):

```powershell
$env:PYTHONPATH = "shared/src;apps/llm_client/src"
python -m llm_client.example_full_usage
```

O arquivo `apps/llm_client/src/llm_client/example_full_usage.py` comenta cada etapa
(passos para configurar `OPENAI_API_KEY`, habilitar tool calling real com
`DEMO_TOOL_CALLING=1`, e como funciona o replay offline).

## Configura√ß√£o do ambiente

### Op√ß√£o 1 ‚Äì Poetry (recomendada)

O `pyproject.toml` j√° descreve todos os pacotes via `path`. Basta executar na raiz:

```bash
poetry install
poetry run pytest -q
poetry run python -m llm_client.example_full_usage
```

O Poetry cria e gerencia o ambiente virtual automaticamente; n√£o √© necess√°rio ajustar o
`PYTHONPATH` manualmente.

### Op√ß√£o 2 ‚Äì pip + requirements

Se preferir `pip`, gere um virtualenv e instale as depend√™ncias de desenvolvimento com
o arquivo `requirements-dev.txt` gerado a partir do `pyproject`:

```bash
python -m venv .venv
source .venv/bin/activate
python -m pip install -U pip
python -m pip install -r requirements-dev.txt
```

O arquivo pode ser sincronizado com o `pyproject.toml` executando `python requirements-dev.py`.
Depois disso, exporte `PYTHONPATH=shared/src:framework/src:apps/llm_client/src:cli/src` (ou
use `python -m` para os m√≥dulos) e rode `pytest -q` normalmente.

## Desenvolvimento local

- Testes: `pytest -q` (ou `poetry run pytest -q`).
- Linters: `ruff check .` e `mypy --config-file mypy.ini` (com `poetry run` se estiver usando Poetry).
- Build: `python -m build` gera wheel/sdist para publicar em TestPyPI/PyPI.

## Onde continuar

- `docs/api/openai_responses.md`: detalhes da Responses API e eventos de streaming.
 - `docs/api/openai_responses_tool_calling.md`: guia de tool calling, payloads e replays.
 - `docs/api/streaming_tools_guide.md`: guia extensivo de streaming + tools (SSE, timeouts, follow‚Äëups).
- `docs/architecture/forgebase-architecture.md`: vis√£o completa da arquitetura e fluxos.
- `docs/release-guide.md`: processo recomendado de versionamento e publica√ß√£o.
- `docs/testing-strategy.md`: abordagem de testes e boas pr√°ticas.
- `docs/configuration.md`: vari√°veis de ambiente e diret√≥rios importantes.
- `docs/providers/adding-new-provider.md`: instru√ß√µes para suportar novos LLMs.
- `docs/cli/usage.md`: comandos expostos pela CLI.
- `docs/CONTRIBUTING.md`: conven√ß√µes de contribui√ß√£o.
- `docs/adr/README.md`: decis√µes arquiteturais registradas.
- `docs/tech-debts/TD-001-Robustez-Tool-Calling-Responses.md`: backlog de melhorias planejadas.

Sinta-se √† vontade para abrir issues ou PRs com sugest√µes e corre√ß√µes.

