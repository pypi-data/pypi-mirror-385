# ğŸ§  mem-llm# ğŸ§  mem-llm



**Memory-enabled AI assistant that remembers conversations using local LLMs****Memory-enabled AI assistant that remembers conversations using local LLMs**



[![Python](https://img.shields.io/badge/Python-3.8%2B-blue.svg)](https://www.python.org/downloads/)[![Python](https://img.shields.io/badge/Python-3.8%2B-blue.svg)](https://www.python.org/downloads/)

[![PyPI](https://img.shields.io/pypi/v/mem-llm?label=PyPI)](https://pypi.org/project/mem-llm/)[![PyPI](https://img.shields.io/pypi/v/mem-llm?label=PyPI)](https://pypi.org/project/mem-llm/)

[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)



------



## ğŸ¯ What is mem-llm?## ğŸ“š Ä°Ã§indekiler



`mem-llm` is a lightweight Python library that adds **persistent memory** to your local LLM chatbots. Each user gets their own conversation history that persists across sessions.- [ğŸ¯ mem-llm nedir?](#-mem-llm-nedir)

- [âš¡ HÄ±zlÄ± baÅŸlangÄ±Ã§](#-hÄ±zlÄ±-baÅŸlangÄ±Ã§)

**Use Cases:**- [ğŸ§‘â€ğŸ« Tutorial](#-tutorial)

- ğŸ’¬ Customer service bots- [ğŸ’¡ Ã–zellikler](#-Ã¶zellikler)

- ğŸ¤– Personal assistants- [ğŸ“– KullanÄ±m Ã¶rnekleri](#-kullanÄ±m-Ã¶rnekleri)

- ğŸ“ Context-aware applications- [ğŸ”§ YapÄ±landÄ±rma seÃ§enekleri](#-yapÄ±landÄ±rma-seÃ§enekleri)

- ğŸ¢ Business automation solutions- [ğŸ—‚ Bilgi tabanÄ± ve dokÃ¼manlardan yapÄ±landÄ±rma](#-bilgi-tabanÄ±-ve-dokÃ¼manlardan-yapÄ±landÄ±rma)

- [ğŸ”¥ Desteklenen modeller](#-desteklenen-modeller)

---- [ğŸ“¦ Gereksinimler](#-gereksinimler)

- [ğŸ› SÄ±k karÅŸÄ±laÅŸÄ±lan problemler](#-sÄ±k-karÅŸÄ±laÅŸÄ±lan-problemler)

## âš¡ Quick Start

---

### 1. Install the package

## ğŸ¯ mem-llm nedir?

```bash

pip install mem-llm`mem-llm`, yerel bir LLM ile Ã§alÄ±ÅŸan sohbet botlarÄ±nÄ±za **kalÄ±cÄ± hafÄ±za** kazandÄ±ran hafif bir Python kÃ¼tÃ¼phanesidir. Her kullanÄ±cÄ± iÃ§in ayrÄ± bir konuÅŸma geÃ§miÅŸi tutulur ve yapay zeka bu geÃ§miÅŸi bir sonraki oturumda otomatik olarak kullanÄ±r.

```

**Nerelerde kullanÄ±labilir?**

### 2. Start Ollama and download a model (one-time setup)- ğŸ’¬ MÃ¼ÅŸteri hizmetleri botlarÄ±

- ğŸ¤– KiÅŸisel asistanlar

```bash- ğŸ“ BaÄŸlama duyarlÄ± uygulamalar

# Start Ollama service- ğŸ¢ Ä°ÅŸ sÃ¼reÃ§lerini otomatikleÅŸtiren Ã§Ã¶zÃ¼mler

ollama serve

---

# Download lightweight model (~2.5GB)

ollama pull granite4:tiny-h## âš¡ HÄ±zlÄ± baÅŸlangÄ±Ã§

```

### 0. Gereksinimleri kontrol edin

> ğŸ’¡ Keep `ollama serve` running in one terminal, run your Python code in another.

- Python 3.8 veya Ã¼zeri

### 3. Create your first agent- [Ollama](https://ollama.ai/) kurulu ve Ã§alÄ±ÅŸÄ±r durumda

- En az 4GB RAM ve 5GB disk alanÄ±

```python

from mem_llm import MemAgent### 1. Paketi yÃ¼kleyin



# Create agent in one line```bash

agent = MemAgent()pip install mem-llm==1.0.7

```

# Set user (each user gets separate memory)

agent.set_user("john")### 2. Ollama'yÄ± baÅŸlatÄ±n ve modeli indirin (tek seferlik)



# Chat with memory!```bash

response = agent.chat("My name is John")# Ollama servisini baÅŸlatÄ±n

print(response)ollama serve



response = agent.chat("What's my name?")# YaklaÅŸÄ±k 2.5GB'lÄ±k hafif modeli indirin

print(response)  # Output: "Your name is John"ollama pull granite4:tiny-h

``````



### 4. Verify your setup (optional)> ğŸ’¡ Ollama `serve` komutu terminalde aÃ§Ä±k kalmalÄ±dÄ±r. Yeni bir terminal sekmesinde Python kodunu Ã§alÄ±ÅŸtÄ±rabilirsiniz.



```bash### 3. Ä°lk ajanÄ±nÄ±zÄ± Ã§alÄ±ÅŸtÄ±rÄ±n

# Using CLI

mem-llm check```python

from mem_llm import MemAgent

# Or in Python

agent.check_setup()# Tek satÄ±rda ajan oluÅŸturun

```agent = MemAgent()



---# KullanÄ±cÄ±yÄ± belirleyin (her kullanÄ±cÄ± iÃ§in ayrÄ± hafÄ±za tutulur)

agent.set_user("john")

## ğŸ’¡ Features

# Sohbet edin - hafÄ±za devrede!

| Feature | Description |agent.chat("My name is John")

|---------|-------------|agent.chat("What's my name?")  # â†’ "Your name is John"

| ğŸ§  **Memory** | Remembers each user's conversation history |```

| ğŸ‘¥ **Multi-user** | Separate memory for each user |

| ğŸ”’ **Privacy** | 100% local, no cloud/API needed |### 4. Kurulumunuzu doÄŸrulayÄ±n (isteÄŸe baÄŸlÄ±)

| âš¡ **Fast** | Lightweight SQLite/JSON storage |

| ğŸ¯ **Simple** | 3 lines of code to get started |```python

| ğŸ“š **Knowledge Base** | Load information from documents |agent.check_setup()

| ğŸŒ **Multi-language** | Works with any language (Turkish, English, etc.) |# {'ollama': 'running', 'model': 'granite4:tiny-h', 'memory_backend': 'sql', ...}

| ğŸ› ï¸ **CLI Tool** | Built-in command-line interface |```



---<<<<<<< HEAD

| Feature | Description |

## ğŸ“– Usage Examples|---------|-------------|

| ğŸ§  **Memory** | Remembers each user's conversation history |

### Example 1: Basic Conversation with Memory| ğŸ‘¥ **Multi-user** | Separate memory for each user |

| ğŸ”’ **Privacy** | 100% local, no cloud/API needed |

```python| âš¡ **Fast** | Lightweight SQLite/JSON storage |

from mem_llm import MemAgent| ğŸ¯ **Simple** | 3 lines of code to get started |

| ğŸ“š **Knowledge Base** | Config-free document integration |

# Create agent| ğŸŒ **Multi-language** | Works with any language |

print("ğŸ¤– Creating AI agent...")| ğŸ› ï¸ **CLI Tool** | Built-in command-line interface |

agent = MemAgent()

---

# Set user

print("ğŸ‘¤ Setting user: alice\n")## ğŸ”„ Memory Backend Comparison

agent.set_user("alice")

Choose the right backend for your needs:

# First conversation

print("ğŸ’¬ User: I love pizza")| Feature | JSON Mode | SQL Mode |

response1 = agent.chat("I love pizza")|---------|-----------|----------|

print(f"ğŸ¤– Bot: {response1}\n")| **Setup** | âœ… Zero config | âš™ï¸ Minimal config |

| **Conversation Memory** | âœ… Yes | âœ… Yes |

# Memory test - bot remembers!| **User Profiles** | âœ… Yes | âœ… Yes |

print("ğŸ’¬ User: What's my favorite food?")| **Knowledge Base** | âŒ No | âœ… Yes |

response2 = agent.chat("What's my favorite food?")| **Advanced Search** | âŒ No | âœ… Yes |

print(f"ğŸ¤– Bot: {response2}")| **Multi-user Performance** | â­â­ Good | â­â­â­ Excellent |

```| **Data Queries** | âŒ Limited | âœ… Full SQL |

| **Best For** | ğŸ  Personal use | ğŸ¢ Business use |

**Output:**

```**Recommendation:**

ğŸ¤– Creating AI agent...- **JSON Mode**: Perfect for personal assistants and quick prototypes

ğŸ‘¤ Setting user: alice- **SQL Mode**: Ideal for customer service, multi-user apps, and production

=======

ğŸ’¬ User: I love pizzaKurulum sÄ±rasÄ±nda sorun yaÅŸarsanÄ±z [ğŸ› SÄ±k karÅŸÄ±laÅŸÄ±lan problemler](#-sÄ±k-karÅŸÄ±laÅŸÄ±lan-problemler) bÃ¶lÃ¼mÃ¼ne gÃ¶z atÄ±n.

ğŸ¤– Bot: That's great! Pizza is a popular choice...>>>>>>> f002396c8c531e4cde33d19ac6a755494b1b30cd



ğŸ’¬ User: What's my favorite food?---

ğŸ¤– Bot: Based on our conversation, your favorite food is pizza!

```## ğŸ’¡ Ã–zellikler



---<<<<<<< HEAD

### Command Line Interface (CLI)

### Example 2: Multi-User Support

The easiest way to get started:

```python

from mem_llm import MemAgent```bash

# Install with CLI support

agent = MemAgent()pip install mem-llm



# Customer 1# Start interactive chat

print("=" * 60)mem-llm chat --user john

print("ğŸ‘¤ Customer 1: John")

print("=" * 60)# Check system status

agent.set_user("customer_john")mem-llm check



print("ğŸ’¬ John: My order #12345 is delayed")# View statistics

response = agent.chat("My order #12345 is delayed")mem-llm stats

print(f"ğŸ¤– Bot: {response}\n")

# Export user data

# Customer 2 - SEPARATE MEMORY!mem-llm export john --format json --output data.json

print("=" * 60)

print("ğŸ‘¤ Customer 2: Sarah")# Get help

print("=" * 60)mem-llm --help

agent.set_user("customer_sarah")```



print("ğŸ’¬ Sarah: I want to return item #67890")**Available CLI Commands:**

response = agent.chat("I want to return item #67890")

print(f"ğŸ¤– Bot: {response}\n")| Command | Description | Example |

|---------|-------------|---------|

# Back to Customer 1 - remembers previous conversation!| `chat` | Interactive chat session | `mem-llm chat --user alice` |

print("=" * 60)| `check` | Verify system setup | `mem-llm check` |

print("ğŸ‘¤ Back to Customer 1: John")| `stats` | Show statistics | `mem-llm stats --user john` |

print("=" * 60)| `export` | Export user data | `mem-llm export john` |

agent.set_user("customer_john")| `clear` | Delete user data | `mem-llm clear john` |



print("ğŸ’¬ John: What was my order number?")### Basic Chat

response = agent.chat("What was my order number?")=======

print(f"ğŸ¤– Bot: {response}")| Ã–zellik | AÃ§Ä±klama |

```|---------|----------|

| ğŸ§  **KalÄ±cÄ± hafÄ±za** | Her kullanÄ±cÄ±nÄ±n sohbet geÃ§miÅŸi saklanÄ±r |

**Output:**| ğŸ‘¥ **Ã‡oklu kullanÄ±cÄ±** | Her kullanÄ±cÄ± iÃ§in ayrÄ± hafÄ±za yÃ¶netimi |

```| ğŸ”’ **Gizlilik** | Tamamen yerel Ã§alÄ±ÅŸÄ±r, buluta veri gÃ¶ndermez |

============================================================| âš¡ **HÄ±zlÄ±** | Hafif SQLite veya JSON depolama seÃ§enekleri |

ğŸ‘¤ Customer 1: John| ğŸ¯ **Kolay kullanÄ±m** | ÃœÃ§ satÄ±rda Ã§alÄ±ÅŸan Ã¶rnek |

============================================================| ğŸ“š **Bilgi tabanÄ±** | Ek yapÄ±landÄ±rma olmadan dÃ¶kÃ¼manlardan bilgi yÃ¼kleme |

ğŸ’¬ John: My order #12345 is delayed| ğŸŒ **TÃ¼rkÃ§e desteÄŸi** | TÃ¼rkÃ§e diyaloglarda doÄŸal sonuÃ§lar |

ğŸ¤– Bot: I'll help you check your order status...| ğŸ› ï¸ **AraÃ§ entegrasyonu** | GeliÅŸmiÅŸ araÃ§ sistemi ile geniÅŸletilebilir |



============================================================---

ğŸ‘¤ Customer 2: Sarah

============================================================## ğŸ§‘â€ğŸ« Tutorial

ğŸ’¬ Sarah: I want to return item #67890

ğŸ¤– Bot: I can help you with the return process...TamamlanmÄ±ÅŸ Ã¶rnekleri adÄ±m adÄ±m incelemek iÃ§in [examples](examples) klasÃ¶rÃ¼ndeki rehberleri izleyebilirsiniz. Bu dizinde hem temel kullanÄ±m senaryolarÄ± hem de ileri seviye entegrasyonlar yer alÄ±r. Ã–ne Ã§Ä±kan iÃ§erikler:



============================================================- [Basic usage walkthrough](examples/basic_usage.py) â€“ ilk hafÄ±zalÄ± ajanÄ±n nasÄ±l oluÅŸturulacaÄŸÄ±nÄ± gÃ¶sterir.

ğŸ‘¤ Back to Customer 1: John- [Customer support workflow](examples/customer_support.py) â€“ Ã§ok kullanÄ±cÄ±lÄ± mÃ¼ÅŸteri destek senaryosu.

============================================================- [Knowledge base ingestion](examples/knowledge_base.py) â€“ dokÃ¼manlardan bilgi yÃ¼kleme.

ğŸ’¬ John: What was my order number?

ğŸ¤– Bot: Your order number is #12345, which you mentioned was delayed.Her dosyada kodun yanÄ±nda aÃ§Ä±klamalar bulunur; komutlarÄ± kopyalayÄ±p Ã§alÄ±ÅŸtÄ±rarak sonuÃ§larÄ± deneyimleyebilirsiniz.

```

## ğŸ“– KullanÄ±m Ã¶rnekleri

---

### Basic conversation

### Example 3: Turkish Language Support>>>>>>> f002396c8c531e4cde33d19ac6a755494b1b30cd



```python```python

from mem_llm import MemAgentfrom mem_llm import MemAgent



agent = MemAgent()agent = MemAgent()

agent.set_user("alice")

print("ğŸ‡¹ğŸ‡· TÃ¼rkÃ§e KonuÅŸma Ã–rneÄŸi")

print("=" * 60)# Ä°lk konuÅŸma

agent.chat("I love pizza")

agent.set_user("ahmet")

# Later on...

print("ğŸ’¬ KullanÄ±cÄ±: Benim adÄ±m Ahmet ve Ä°stanbul'da yaÅŸÄ±yorum")agent.chat("What's my favorite food?")

response = agent.chat("Benim adÄ±m Ahmet ve Ä°stanbul'da yaÅŸÄ±yorum")# â†’ "Your favorite food is pizza"

print(f"ğŸ¤– Bot: {response}\n")```



print("ğŸ’¬ KullanÄ±cÄ±: Nerede yaÅŸÄ±yorum?")<<<<<<< HEAD

response = agent.chat("Nerede yaÅŸÄ±yorum?")### Multi-language Support

print(f"ğŸ¤– Bot: {response}\n")

```python

print("ğŸ’¬ KullanÄ±cÄ±: AdÄ±mÄ± hatÄ±rlÄ±yor musun?")# Works with any language

response = agent.chat("AdÄ±mÄ± hatÄ±rlÄ±yor musun?")=======

print(f"ğŸ¤– Bot: {response}")### Turkish language support

```

```python

**Output:**# Handles Turkish dialogue naturally

```>>>>>>> f002396c8c531e4cde33d19ac6a755494b1b30cd

ğŸ‡¹ğŸ‡· TÃ¼rkÃ§e KonuÅŸma Ã–rneÄŸiagent.set_user("ahmet")

============================================================agent.chat("Benim adÄ±m Ahmet ve pizza seviyorum")

ğŸ’¬ KullanÄ±cÄ±: Benim adÄ±m Ahmet ve Ä°stanbul'da yaÅŸÄ±yorumagent.chat("AdÄ±mÄ± hatÄ±rlÄ±yor musun?")

ğŸ¤– Bot: Memnun oldum Ahmet! Ä°stanbul gÃ¼zel bir ÅŸehir...# â†’ "Evet, adÄ±nÄ±z Ahmet!"

```

ğŸ’¬ KullanÄ±cÄ±: Nerede yaÅŸÄ±yorum?

ğŸ¤– Bot: Ä°stanbul'da yaÅŸÄ±yorsunuz.### Customer service scenario



ğŸ’¬ KullanÄ±cÄ±: AdÄ±mÄ± hatÄ±rlÄ±yor musun?```python

ğŸ¤– Bot: Evet, adÄ±nÄ±z Ahmet!agent = MemAgent()

```

# MÃ¼ÅŸteri 1

---agent.set_user("customer_001")

agent.chat("My order #12345 is delayed")

### Example 4: User Profile Extraction

# Customer 2 (separate memory!)

```pythonagent.set_user("customer_002")

from mem_llm import MemAgentagent.chat("I want to return item #67890")

```

agent = MemAgent()

agent.set_user("alice")### Inspecting the user profile



print("ğŸ“ Building user profile...")```python

print("=" * 60)# Retrieve automatically extracted user information

profile = agent.get_user_profile()

# Have natural conversations# {'name': 'Alice', 'favorite_food': 'pizza', 'location': 'NYC'}

conversations = [```

    "My name is Alice and I'm 28 years old",

    "I live in New York City",---

    "I work as a software engineer",

    "My favorite food is pizza"## ğŸ”§ YapÄ±landÄ±rma seÃ§enekleri

]

### JSON hafÄ±za (varsayÄ±lan ve basit)

for msg in conversations:

    print(f"ğŸ’¬ User: {msg}")```python

    response = agent.chat(msg)agent = MemAgent(

    print(f"ğŸ¤– Bot: {response}\n")    model="granite4:tiny-h",

    use_sql=False,  # JSON dosyalarÄ± ile hafÄ±za

# Extract profile automatically    memory_dir="memories"

print("=" * 60))

print("ğŸ“Š Extracted User Profile:")```

print("=" * 60)

profile = agent.get_user_profile()### SQL hafÄ±za (geliÅŸmiÅŸ ve hÄ±zlÄ±)



for key, value in profile.items():```python

    print(f"   {key}: {value}")agent = MemAgent(

```    model="granite4:tiny-h",

    use_sql=True,  # SQLite tabanlÄ± hafÄ±za

**Output:**    memory_dir="memories.db"

```)

ğŸ“ Building user profile...```

============================================================

ğŸ’¬ User: My name is Alice and I'm 28 years old### DiÄŸer Ã¶zelleÅŸtirmeler

ğŸ¤– Bot: Nice to meet you, Alice!...

```python

ğŸ’¬ User: I live in New York Cityagent = MemAgent(

ğŸ¤– Bot: New York City is a vibrant place...    model="llama2",  # Herhangi bir Ollama modeli

    ollama_url="http://localhost:11434"

ğŸ’¬ User: I work as a software engineer)

ğŸ¤– Bot: That's an interesting career...```



ğŸ’¬ User: My favorite food is pizza---

ğŸ¤– Bot: Pizza is delicious!...

## ğŸ“š API referansÄ±

============================================================

ğŸ“Š Extracted User Profile:### `MemAgent`

============================================================

   name: Alice```python

   age: 28# Initialize

   location: New York Cityagent = MemAgent(model="granite4:tiny-h", use_sql=False)

   occupation: Software Engineer

   favorite_food: Pizza# Set active user

```agent.set_user(user_id: str, name: Optional[str] = None)



---# Chat

response = agent.chat(message: str, metadata: Optional[Dict] = None) -> str

### Example 5: Complete Customer Service Workflow

# Get profile

```pythonprofile = agent.get_user_profile(user_id: Optional[str] = None) -> Dict

from mem_llm import MemAgent

# System check

# Initialize customer service agentstatus = agent.check_setup() -> Dict

print("ğŸ¢ Customer Service Bot Initializing...")```

agent = MemAgent(use_sql=True)  # SQL for better performance

---

# Simulate customer support session

def handle_customer(customer_id, customer_name):## ğŸ—‚ Bilgi tabanÄ± ve dokÃ¼manlardan yapÄ±landÄ±rma

    print("\n" + "=" * 70)

    print(f"ğŸ“ New Customer Session: {customer_name} (ID: {customer_id})")Kurumsal dokÃ¼manlarÄ±nÄ±zdan otomatik `config.yaml` Ã¼retin:

    print("=" * 70)

    ```python

    agent.set_user(customer_id, name=customer_name)from mem_llm import create_config_from_document

    

    # Customer introduces issue# PDF'den config.yaml Ã¼retin

    print(f"\nğŸ’¬ {customer_name}: Hi, my order hasn't arrived yet")create_config_from_document(

    response = agent.chat("Hi, my order hasn't arrived yet")    doc_path="company_info.pdf",

    print(f"ğŸ¤– Support: {response}")    output_path="config.yaml",

        company_name="Acme Corp"

    # Ask for details)

    print(f"\nğŸ’¬ {customer_name}: My order number is #45678")

    response = agent.chat("My order number is #45678")# OluÅŸan yapÄ±landÄ±rmayÄ± kullanÄ±n

    print(f"ğŸ¤– Support: {response}")agent = MemAgent(config_file="config.yaml")

    ```

    # Follow up later in conversation

    print(f"\nğŸ’¬ {customer_name}: Can you remind me what we were discussing?")---

    response = agent.chat("Can you remind me what we were discussing?")

    print(f"ğŸ¤– Support: {response}")## ğŸ”¥ Desteklenen modeller



# Handle multiple customers[Ollama](https://ollama.ai/) Ã¼zerindeki tÃ¼m modellerle Ã§alÄ±ÅŸÄ±r. Tavsiye edilen modeller:

handle_customer("cust_001", "Emma")

handle_customer("cust_002", "Michael")| Model | Size | Speed | Quality |

|-------|------|-------|---------|

# Return to first customer - memory persists!| `granite4:tiny-h` | 2.5GB | âš¡âš¡âš¡ | â­â­ |

print("\n" + "=" * 70)| `llama2` | 4GB | âš¡âš¡ | â­â­â­ |

print("ğŸ“ Returning Customer: Emma (ID: cust_001)")| `mistral` | 4GB | âš¡âš¡ | â­â­â­â­ |

print("=" * 70)| `llama3` | 5GB | âš¡ | â­â­â­â­â­ |

agent.set_user("cust_001")

```bash

print("\nğŸ’¬ Emma: What was my order number again?")ollama pull <model-name>

response = agent.chat("What was my order number again?")```

print(f"ğŸ¤– Support: {response}")

# Output: "Your order number is #45678"---

```

## ğŸ“¦ Gereksinimler

**Output:**

```- Python 3.8+

ğŸ¢ Customer Service Bot Initializing...- Ollama (LLM iÃ§in)

- Minimum 4GB RAM

======================================================================- 5GB disk alanÄ±

ğŸ“ New Customer Session: Emma (ID: cust_001)

======================================================================**Kurulum ile gelen baÄŸÄ±mlÄ±lÄ±klar:**

- `requests >= 2.31.0`

ğŸ’¬ Emma: Hi, my order hasn't arrived yet- `pyyaml >= 6.0.1`

ğŸ¤– Support: I'm sorry to hear that. I'll help you track your order...- `sqlite3` (Python ile birlikte gelir)



ğŸ’¬ Emma: My order number is #45678---

ğŸ¤– Support: Thank you for providing order #45678. Let me check...

## ğŸ› SÄ±k karÅŸÄ±laÅŸÄ±lan problemler

ğŸ’¬ Emma: Can you remind me what we were discussing?

ğŸ¤– Support: We're discussing your order #45678 that hasn't arrived yet...### Ollama Ã§alÄ±ÅŸmÄ±yor mu?



======================================================================```bash

ğŸ“ New Customer Session: Michael (ID: cust_002)ollama serve

======================================================================```



ğŸ’¬ Michael: Hi, my order hasn't arrived yet### Model bulunamadÄ± hatasÄ± mÄ± alÄ±yorsunuz?

ğŸ¤– Support: I'm sorry to hear that. I'll help you track your order...

```bash

ğŸ’¬ Michael: My order number is #78901ollama pull granite4:tiny-h

ğŸ¤– Support: Thank you for providing order #78901...```



======================================================================### ImportError veya baÄŸlantÄ± hatasÄ± mÄ± var?

ğŸ“ Returning Customer: Emma (ID: cust_001)

======================================================================```bash

pip install --upgrade mem-llm

ğŸ’¬ Emma: What was my order number again?```

ğŸ¤– Support: Your order number is #45678.

```> HÃ¢lÃ¢ sorun yaÅŸÄ±yorsanÄ±z `agent.check_setup()` Ã§Ä±ktÄ±sÄ±nÄ± ve hata mesajÄ±nÄ± issue aÃ§arken paylaÅŸÄ±n.



------



## ğŸ”§ Configuration Options## ğŸ“„ Lisans



### JSON Memory (Simple, Default)MIT LisansÄ± â€” kiÅŸisel veya ticari projelerinizde Ã¶zgÃ¼rce kullanabilirsiniz.



```python---

agent = MemAgent(

    model="granite4:tiny-h",## ğŸ”— FaydalÄ± baÄŸlantÄ±lar

    use_sql=False,  # JSON file-based memory

    memory_dir="memories"- **PyPI:** https://pypi.org/project/mem-llm/

)- **GitHub:** https://github.com/emredeveloper/Mem-LLM

```- **Ollama:** https://ollama.ai/



### SQL Memory (Advanced, Recommended for Production)---



```python## ğŸŒŸ Bize destek olun

agent = MemAgent(

    model="granite4:tiny-h",Proje iÅŸinize yaradÄ±ysa [GitHub](https://github.com/emredeveloper/Mem-LLM) Ã¼zerinden â­ vermeyi unutmayÄ±n!

    use_sql=True,  # SQLite-based memory

    memory_dir="memories.db"---

)

```<div align="center">

Sevgiyle geliÅŸtirildi â€” <a href="https://github.com/emredeveloper">C. Emre KarataÅŸ</a>

### Custom Configuration</div>


```python
agent = MemAgent(
    model="llama2",  # Any Ollama model
    ollama_url="http://localhost:11434",
    check_connection=True  # Verify setup on startup
)
```

---

## ğŸ› ï¸ Command Line Interface

```bash
# Start interactive chat
mem-llm chat --user john

# Check system status
mem-llm check

# View statistics
mem-llm stats

# Export user data
mem-llm export john --format json

# Clear user data
mem-llm clear john

# Get help
mem-llm --help
```

---

## ğŸ”„ Memory Backend Comparison

| Feature | JSON Mode | SQL Mode |
|---------|-----------|----------|
| **Setup** | âœ… Zero config | âš™ï¸ Minimal config |
| **Conversation Memory** | âœ… Yes | âœ… Yes |
| **User Profiles** | âœ… Yes | âœ… Yes |
| **Knowledge Base** | âŒ No | âœ… Yes |
| **Advanced Search** | âŒ No | âœ… Yes |
| **Multi-user Performance** | â­â­ Good | â­â­â­ Excellent |
| **Best For** | ğŸ  Personal use | ğŸ¢ Business use |

**Recommendation:**
- **JSON Mode**: Perfect for personal assistants and quick prototypes
- **SQL Mode**: Ideal for customer service, multi-user apps, and production

---

## ğŸ“š API Reference

### MemAgent Class

```python
# Initialize
agent = MemAgent(
    model="granite4:tiny-h",
    use_sql=True,
    memory_dir=None,
    ollama_url="http://localhost:11434",
    check_connection=False
)

# Set active user
agent.set_user(user_id: str, name: Optional[str] = None)

# Chat (returns response string)
response = agent.chat(message: str, metadata: Optional[Dict] = None) -> str

# Get user profile (auto-extracted from conversations)
profile = agent.get_user_profile(user_id: Optional[str] = None) -> Dict

# System check
status = agent.check_setup() -> Dict
```

---

## ğŸ”¥ Supported Models

Works with any [Ollama](https://ollama.ai/) model. Recommended models:

| Model | Size | Speed | Quality | Best For |
|-------|------|-------|---------|----------|
| `granite4:tiny-h` | 2.5GB | âš¡âš¡âš¡ | â­â­ | Quick testing |
| `llama2` | 4GB | âš¡âš¡ | â­â­â­ | General use |
| `mistral` | 4GB | âš¡âš¡ | â­â­â­â­ | Balanced |
| `llama3` | 5GB | âš¡ | â­â­â­â­â­ | Best quality |

```bash
# Download a model
ollama pull <model-name>

# List installed models
ollama list
```

---

## ğŸ“¦ Requirements

- Python 3.8+
- [Ollama](https://ollama.ai/) (for LLM)
- Minimum 4GB RAM
- 5GB disk space

**Python Dependencies (auto-installed):**
- `requests >= 2.31.0`
- `pyyaml >= 6.0.1`
- `click >= 8.1.0`

---

## ğŸ› Troubleshooting

### Ollama not running?

```bash
ollama serve
```

### Model not found error?

```bash
# Download the model
ollama pull granite4:tiny-h

# Check installed models
ollama list
```

### Connection error?

```bash
# Check if Ollama is running
curl http://localhost:11434

# Restart Ollama
ollama serve
```

### Import error?

```bash
# Upgrade to latest version
pip install --upgrade mem-llm
```

> If issues persist, run `mem-llm check` or `agent.check_setup()` and share the output when opening an issue.

---

## ğŸ“„ License

MIT License - Free to use in personal and commercial projects.

---

## ğŸ”— Links

- **PyPI:** https://pypi.org/project/mem-llm/
- **GitHub:** https://github.com/emredeveloper/Mem-LLM
- **Ollama:** https://ollama.ai/
- **Documentation:** [GitHub Wiki](https://github.com/emredeveloper/Mem-LLM/wiki)

---

## ğŸŒŸ Support Us

If you find this project useful, please â­ [star it on GitHub](https://github.com/emredeveloper/Mem-LLM)!

---

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

---

<div align="center">
Made with â¤ï¸ by <a href="https://github.com/emredeveloper">C. Emre KarataÅŸ</a>
</div>
