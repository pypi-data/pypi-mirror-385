#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
HarborAI ä¸»å®¢æˆ·ç«¯

æä¾›ä¸ OpenAI SDK ä¸€è‡´çš„è°ƒç”¨æ¥å£ï¼Œæ”¯æŒæ¨ç†æ¨¡å‹ã€ç»“æ„åŒ–è¾“å‡ºç­‰åŠŸèƒ½ã€‚
"""

import asyncio
import time
import uuid
from typing import Dict, List, Optional, Union, Any, AsyncGenerator, Iterator
from collections.abc import AsyncIterator

from ..core.client_manager import ClientManager
from ..core.base_plugin import ChatCompletion, ChatCompletionChunk
from ..utils.exceptions import HarborAIError, ValidationError
from ..utils.logger import get_logger, APICallLogger
from ..utils.tracer import TraceContext, get_or_create_trace_id
from ..utils.retry import async_retry_with_backoff, retry_with_backoff
from ..config.settings import get_settings
from ..config.performance import get_performance_config
from ..storage.lifecycle import auto_initialize
from ..core.unified_decorators import smart_decorator, fast_trace, full_trace
from ..core.performance_manager import get_performance_manager, initialize_performance_manager, cleanup_performance_manager

# å¯¼å…¥ä¼˜åŒ–ç»„ä»¶
from ..core.agently_client_pool import get_agently_client_pool, create_agently_client_config
from ..core.parameter_cache import get_parameter_cache_manager
from ..core.fast_structured_output import FastStructuredOutputProcessor, get_fast_structured_output_processor


class ChatCompletions:
    """èŠå¤©å®Œæˆæ¥å£"""
    
    def __init__(self, client_manager: ClientManager):
        self.client_manager = client_manager
        self.logger = get_logger("harborai.chat_completions")
        self.api_logger = APICallLogger(self.logger)
        self.settings = get_settings()
        
        # åˆå§‹åŒ–å¿«é€Ÿç»“æ„åŒ–è¾“å‡ºå¤„ç†å™¨
        self._fast_processor = None
    
    def _get_fast_processor(self) -> FastStructuredOutputProcessor:
        """è·å–å¿«é€Ÿç»“æ„åŒ–è¾“å‡ºå¤„ç†å™¨å®ä¾‹ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰"""
        if self._fast_processor is None:
            from ..core.fast_structured_output import create_fast_structured_output_processor
            self._fast_processor = create_fast_structured_output_processor(client_manager=self.client_manager)
        return self._fast_processor
    
    def create(
        self,
        messages: List[Dict[str, Any]],
        model: str,
        *,
        frequency_penalty: Optional[float] = None,
        function_call: Optional[Union[str, Dict[str, Any]]] = None,
        functions: Optional[List[Dict[str, Any]]] = None,
        logit_bias: Optional[Dict[str, float]] = None,
        logprobs: Optional[bool] = None,
        top_logprobs: Optional[int] = None,
        max_tokens: Optional[int] = None,
        n: Optional[int] = None,
        presence_penalty: Optional[float] = None,
        response_format: Optional[Dict[str, Any]] = None,
        seed: Optional[int] = None,
        stop: Optional[Union[str, List[str]]] = None,
        stream: Optional[bool] = None,
        structured_provider: Optional[str] = None,
        temperature: Optional[float] = None,
        tool_choice: Optional[Union[str, Dict[str, Any]]] = None,
        tools: Optional[List[Dict[str, Any]]] = None,
        top_p: Optional[float] = None,
        user: Optional[str] = None,
        extra_body: Optional[Dict[str, Any]] = None,
        timeout: Optional[float] = None,
        fallback: Optional[List[str]] = None,
        fallback_models: Optional[List[str]] = None,
        retry_policy: Optional[Dict[str, Any]] = None,
        **kwargs
    ) -> Union[ChatCompletion, Iterator[ChatCompletionChunk]]:
        """åˆ›å»ºèŠå¤©å®Œæˆï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼‰"""
        import sys
        sys.stderr.write("=" * 50 + "\n")
        sys.stderr.write("DEBUG: ChatCompletions.create method called\n")
        sys.stderr.write(f"DEBUG: model={model}, messages={messages}\n")
        sys.stderr.write("=" * 50 + "\n")
        sys.stderr.flush()
        
        # è·å–æ€§èƒ½é…ç½®
        perf_config = get_performance_config()
        
        # æ£€æŸ¥æ˜¯å¦å¯ç”¨FASTæ¨¡å¼ä¸”æœ‰ç»“æ„åŒ–è¾“å‡º
        print(f"ğŸ” æ£€æŸ¥å¿«é€Ÿç»“æ„åŒ–è¾“å‡ºæ¡ä»¶:")
        print(f"   - perf_config.mode.value: {perf_config.mode.value}")
        print(f"   - response_format: {response_format}")
        print(f"   - structured_provider: {structured_provider}")
        print(f"   - stream: {stream}")
        
        should_use_fast_structured = (perf_config.mode.value == "fast" and 
            response_format and 
            response_format.get("type") == "json_schema" and
            structured_provider == "agently" and
            not stream)
        
        print(f"   - åº”è¯¥ä½¿ç”¨å¿«é€Ÿç»“æ„åŒ–è¾“å‡º: {should_use_fast_structured}")
        
        if should_use_fast_structured:  # æµå¼è¾“å‡ºæš‚ä¸æ”¯æŒå¿«é€Ÿè·¯å¾„
            print("âœ… ä½¿ç”¨å¿«é€Ÿç»“æ„åŒ–è¾“å‡ºè·¯å¾„")
            return self._create_fast_structured_path(
                messages, model, response_format, structured_provider,
                frequency_penalty=frequency_penalty,
                function_call=function_call,
                functions=functions,
                logit_bias=logit_bias,
                logprobs=logprobs,
                top_logprobs=top_logprobs,
                max_tokens=max_tokens,
                n=n,
                presence_penalty=presence_penalty,
                seed=seed,
                stop=stop,
                stream=stream,
                temperature=temperature,
                tool_choice=tool_choice,
                tools=tools,
                top_p=top_p,
                user=user,
                extra_body=extra_body,
                timeout=timeout,
                fallback=fallback,
                fallback_models=fallback_models,
                retry_policy=retry_policy,
                **kwargs
            )
        
        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨å¿«é€Ÿè·¯å¾„
        if perf_config.should_use_fast_path(model, max_tokens):
            return self._create_fast_path(
                messages=messages, model=model, frequency_penalty=frequency_penalty,
                function_call=function_call, functions=functions, logit_bias=logit_bias,
                logprobs=logprobs, top_logprobs=top_logprobs, max_tokens=max_tokens,
                n=n, presence_penalty=presence_penalty, response_format=response_format,
                seed=seed, stop=stop, stream=stream, structured_provider=structured_provider,
                temperature=temperature, tool_choice=tool_choice, tools=tools,
                top_p=top_p, user=user, extra_body=extra_body, timeout=timeout,
                fallback=fallback, fallback_models=fallback_models,
                retry_policy=retry_policy, **kwargs
            )
        else:
            return self._create_full_path(
                messages=messages, model=model, frequency_penalty=frequency_penalty,
                function_call=function_call, functions=functions, logit_bias=logit_bias,
                logprobs=logprobs, top_logprobs=top_logprobs, max_tokens=max_tokens,
                n=n, presence_penalty=presence_penalty, response_format=response_format,
                seed=seed, stop=stop, stream=stream, structured_provider=structured_provider,
                temperature=temperature, tool_choice=tool_choice, tools=tools,
                top_p=top_p, user=user, extra_body=extra_body, timeout=timeout,
                fallback=fallback, fallback_models=fallback_models,
                retry_policy=retry_policy, **kwargs
            )
    
    @fast_trace
    def _create_fast_structured_path(
        self,
        messages: List[Dict[str, Any]],
        model: str,
        response_format: Dict[str, Any],
        structured_provider: str,
        **kwargs
    ) -> ChatCompletion:
        """å¿«é€Ÿç»“æ„åŒ–è¾“å‡ºè·¯å¾„ - ä½¿ç”¨ä¼˜åŒ–ç»„ä»¶"""
        print("ğŸš€ è¿›å…¥å¿«é€Ÿç»“æ„åŒ–è¾“å‡ºè·¯å¾„")
        
        # è·å–trace_id
        from ..utils.tracer import get_or_create_trace_id
        trace_id = get_or_create_trace_id()
        
        # æå–ç”¨æˆ·è¾“å…¥
        user_input = None
        for msg in reversed(messages):
            if msg["role"] == "user":
                user_input = msg["content"]
                break
        
        print(f"ğŸ“ æå–çš„ç”¨æˆ·è¾“å…¥: {user_input}")
        
        if not user_input:
            # å¦‚æœæ²¡æœ‰ç”¨æˆ·è¾“å…¥ï¼Œå›é€€åˆ°å¸¸è§„è·¯å¾„
            print("âŒ æ²¡æœ‰ç”¨æˆ·è¾“å…¥ï¼Œå›é€€åˆ°å¸¸è§„è·¯å¾„")
            return self._create_core(messages, model, response_format=response_format, 
                                   structured_provider=structured_provider, **kwargs)
        
        try:
            # è®°å½•è¯·æ±‚æ—¥å¿—
            from ..utils.logger import LogContext
            log_context = LogContext(trace_id=trace_id)
            request_params = {
                "model": model,
                "messages": messages,
                "response_format": response_format,
                "structured_provider": structured_provider,
                **kwargs
            }
            self.api_logger.log_request(
                context=log_context,
                request_data=request_params
            )
            
            # ä½¿ç”¨å¿«é€Ÿç»“æ„åŒ–è¾“å‡ºå¤„ç†å™¨
            print("ğŸ”§ è·å–å¿«é€Ÿå¤„ç†å™¨")
            fast_processor = self._get_fast_processor()
            
            # æå–schema
            json_schema = response_format.get('json_schema', {})
            schema = json_schema.get('schema', {})
            print(f"ğŸ“‹ æå–çš„Schema: {schema}")
            
            # è°ƒç”¨å¿«é€Ÿå¤„ç†å™¨
            print("âš¡ è°ƒç”¨å¿«é€Ÿå¤„ç†å™¨")
            start_time = time.time()
            parsed_result = fast_processor.process_structured_output(
                user_query=user_input,
                schema=schema,
                api_key=self.client_manager.client_config.get('api_key'),
                base_url=self.client_manager.client_config.get('base_url'),
                model=model,
                temperature=kwargs.get('temperature', 0.1),
                max_tokens=kwargs.get('max_tokens')
            )
            duration = time.time() - start_time
            print(f"âœ… å¿«é€Ÿå¤„ç†å™¨è¿”å›ç»“æœ: {parsed_result}")
            
            # æ„é€ å…¼å®¹çš„å“åº”å¯¹è±¡
            from ..core.base_plugin import ChatMessage, ChatChoice
            
            response = ChatCompletion(
                id=f"fast-structured-{int(time.time())}",
                object="chat.completion",
                created=int(time.time()),
                model=model,
                choices=[
                    ChatChoice(
                        index=0,
                        message=ChatMessage(
                            role="assistant",
                            content=str(parsed_result)  # å°†ç»“æ„åŒ–ç»“æœè½¬ä¸ºå­—ç¬¦ä¸²
                        ),
                        finish_reason="stop"
                    )
                ]
            )
            
            # è®¾ç½®parsedå±æ€§
            response.choices[0].message.parsed = parsed_result
            
            # è®°å½•å“åº”æ—¥å¿—
            response_data = {
                'status_code': 200,
                'response': response,
                'duration': duration
            }
            self.api_logger.log_response(
                context=log_context,
                response_data=response_data
            )
            
            return response
            
        except Exception as e:
            print(
                f"âš ï¸ å¿«é€Ÿç»“æ„åŒ–è¾“å‡ºå¤„ç†å¤±è´¥ï¼Œå›é€€åˆ°å¸¸è§„è·¯å¾„: {str(e)}"
            )
            # è®°å½•é”™è¯¯æ—¥å¿—
            error_data = {
                'error_type': type(e).__name__,
                'error_message': str(e)
            }
            self.api_logger.log_error(
                context=log_context,
                error_data=error_data
            )
            # å›é€€åˆ°å¸¸è§„è·¯å¾„
            return self._create_core(messages, model, response_format=response_format, 
                                   structured_provider=structured_provider, **kwargs)
    
    @fast_trace
    def _create_fast_path(
        self,
        messages: List[Dict[str, Any]],
        model: str,
        **kwargs
    ) -> Union[ChatCompletion, Iterator[ChatCompletionChunk]]:
        """å¿«é€Ÿè·¯å¾„åˆ›å»ºèŠå¤©å®Œæˆ"""
        return self._create_core(messages, model, **kwargs)
    
    @full_trace
    def _create_full_path(
        self,
        messages: List[Dict[str, Any]],
        model: str,
        **kwargs
    ) -> Union[ChatCompletion, Iterator[ChatCompletionChunk]]:
        """å®Œæ•´è·¯å¾„åˆ›å»ºèŠå¤©å®Œæˆ"""
        return self._create_core(messages, model, **kwargs)
    
    def _create_core(
        self,
        messages: List[Dict[str, Any]],
        model: str,
        frequency_penalty: Optional[float] = None,
        function_call: Optional[Union[str, Dict[str, Any]]] = None,
        functions: Optional[List[Dict[str, Any]]] = None,
        logit_bias: Optional[Dict[str, float]] = None,
        logprobs: Optional[bool] = None,
        top_logprobs: Optional[int] = None,
        max_tokens: Optional[int] = None,
        n: Optional[int] = None,
        presence_penalty: Optional[float] = None,
        response_format: Optional[Dict[str, Any]] = None,
        seed: Optional[int] = None,
        stop: Optional[Union[str, List[str]]] = None,
        stream: Optional[bool] = None,
        structured_provider: Optional[str] = None,
        temperature: Optional[float] = None,
        tool_choice: Optional[Union[str, Dict[str, Any]]] = None,
        tools: Optional[List[Dict[str, Any]]] = None,
        top_p: Optional[float] = None,
        user: Optional[str] = None,
        extra_body: Optional[Dict[str, Any]] = None,
        timeout: Optional[float] = None,
        fallback: Optional[List[str]] = None,
        fallback_models: Optional[List[str]] = None,
        retry_policy: Optional[Dict[str, Any]] = None,
        **kwargs
    ) -> Union[ChatCompletion, Iterator[ChatCompletionChunk]]:
        """æ ¸å¿ƒåˆ›å»ºé€»è¾‘"""
        import sys
        sys.stderr.write("DEBUG: _create_core method called\n")
        sys.stderr.flush()
        
        # éªŒè¯æ¶ˆæ¯
        self._validate_messages(messages)
        
        # éªŒè¯temperatureå‚æ•°
        if temperature is not None:
            if not isinstance(temperature, (int, float)):
                raise ValueError("temperature must be a number")
            if temperature < 0 or temperature > 2.0:
                raise ValueError("temperature must be between 0 and 2")
        
        # éªŒè¯max_tokenså‚æ•°
        if max_tokens is not None:
            from ..core.models import get_model_capabilities
            from ..core.exceptions import ValidationError as CoreValidationError
            
            if not isinstance(max_tokens, int):
                raise ValueError("max_tokens must be an integer")
            if max_tokens <= 0:
                raise ValueError("max_tokens must be positive")
            
            capabilities = get_model_capabilities(model)
            if capabilities and capabilities.max_tokens_limit:
                if max_tokens > capabilities.max_tokens_limit:
                    raise CoreValidationError(
                        f"max_tokens ({max_tokens}) exceeds limit for model {model}: {capabilities.max_tokens_limit}"
                    )
        
        # å¤„ç†fallbackå‚æ•°å…¼å®¹æ€§ï¼šfallback_modelsä¼˜å…ˆçº§é«˜äºfallback
        if fallback_models is not None:
            fallback = fallback_models
        elif fallback is None:
            # ä½¿ç”¨settingsä¸­é…ç½®çš„é»˜è®¤é™çº§æ¨¡å‹
            from ..config.settings import get_settings
            settings = get_settings()
            if settings.enable_fallback and settings.fallback_models:
                fallback = settings.fallback_models
            else:
                fallback = []
        
        # å¤„ç†structured_provideré»˜è®¤å€¼ï¼šå¦‚æœç”¨æˆ·æœªæŒ‡å®šï¼Œä½¿ç”¨é…ç½®ä¸­çš„é»˜è®¤å€¼
        if structured_provider is None:
            from ..config.settings import get_settings
            settings = get_settings()
            structured_provider = settings.default_structured_provider
        
        trace_id = get_or_create_trace_id()
        
        with TraceContext(trace_id):
            # éªŒè¯structured_providerå‚æ•°
            if structured_provider and structured_provider not in ["agently", "native"]:
                raise ValidationError(
                    f"Invalid structured_provider '{structured_provider}'. "
                    "Must be 'agently' or 'native'"
                )
            
            # æ„å»ºè¯·æ±‚å‚æ•°
            request_params = {
                "messages": messages,
                "model": model,
                "frequency_penalty": frequency_penalty,
                "function_call": function_call,
                "functions": functions,
                "logit_bias": logit_bias,
                "logprobs": logprobs,
                "top_logprobs": top_logprobs,
                "max_tokens": max_tokens,
                "n": n,
                "presence_penalty": presence_penalty,
                "response_format": response_format,
                "seed": seed,
                "stop": stop,
                "stream": stream,
                "structured_provider": structured_provider,
                "temperature": temperature,
                "tool_choice": tool_choice,
                "tools": tools,
                "top_p": top_p,
                "user": user,
                "extra_body": extra_body,
                "timeout": timeout,
                "fallback": fallback,
                "retry_policy": retry_policy,
                **kwargs
            }
            
            # ç§»é™¤ None å€¼
            request_params = {k: v for k, v in request_params.items() if v is not None}
            
            # å¯¹æ¨ç†æ¨¡å‹è¿›è¡Œå‚æ•°è¿‡æ»¤å’Œæ¶ˆæ¯å¤„ç†
            from ..core.models import filter_parameters_for_model, is_reasoning_model
            if is_reasoning_model(model):
                # è¿‡æ»¤ä¸æ”¯æŒçš„å‚æ•°
                request_params = filter_parameters_for_model(model, request_params)
                
                # å¤„ç†æ¨ç†æ¨¡å‹çš„systemæ¶ˆæ¯
                messages = self._process_messages_for_reasoning_model(messages)
                request_params["messages"] = messages
            
            try:
                # è®°å½•è¯·æ±‚æ—¥å¿—
                import sys
                sys.stderr.write(f"[DEBUG] å‡†å¤‡è°ƒç”¨ api_logger.log_request\n")
                sys.stderr.flush()
                
                from ..utils.logger import LogContext
                log_context = LogContext(trace_id=trace_id)
                
                sys.stderr.write(f"[DEBUG] è°ƒç”¨ api_logger.log_request: {self.api_logger}\n")
                sys.stderr.flush()
                
                self.api_logger.log_request(
                    context=log_context,
                    request_data=request_params
                )
                
                sys.stderr.write(f"[DEBUG] api_logger.log_request è°ƒç”¨å®Œæˆ\n")
                sys.stderr.flush()
                
                # ä½¿ç”¨é‡è¯•è£…é¥°å™¨
                # è½¬æ¢å­—å…¸æ¶ˆæ¯ä¸ºChatMessageå¯¹è±¡
                from ..core.base_plugin import ChatMessage
                chat_messages = [
                    ChatMessage(
                        role=msg["role"],
                        content=msg.get("content"),
                        name=msg.get("name"),
                        function_call=msg.get("function_call"),
                        tool_calls=msg.get("tool_calls"),
                        tool_call_id=msg.get("tool_call_id")
                    )
                    for msg in messages
                ]
                
                # é…ç½®é‡è¯•ç­–ç•¥
                from ..utils.retry import RetryConfig
                retry_config = None
                if retry_policy:
                    retry_config = RetryConfig(
                        max_attempts=retry_policy.get('max_attempts', 3),
                        base_delay=retry_policy.get('base_delay', 1.0),
                        max_delay=retry_policy.get('max_delay', 60.0),
                        exponential_base=retry_policy.get('exponential_base', 2.0),
                        jitter=retry_policy.get('jitter', True)
                    )
                
                @retry_with_backoff(config=retry_config)
                def _create_with_retry():
                    return self.client_manager.chat_completion_sync_with_fallback(
                        model=model,
                        messages=chat_messages,
                        fallback=fallback,
                        **{k: v for k, v in request_params.items() if k not in ['model', 'messages', 'fallback', 'retry_policy']}
                    )
                
                response = _create_with_retry()
                
                # è®°å½•å“åº”æ—¥å¿—
                response_data = {
                    'status_code': 200,
                    'response': response
                }
                self.api_logger.log_response(
                    context=log_context,
                    response_data=response_data
                )
                
                return response
                
            except Exception as e:
                # è®°å½•é”™è¯¯æ—¥å¿—
                error_context = LogContext(trace_id=trace_id)
                error_data = {
                    'error_type': type(e).__name__,
                    'error_message': str(e)
                }
                self.api_logger.log_error(
                    context=error_context,
                    error_data=error_data
                )
                raise e
    
    async def acreate(
        self,
        messages: List[Dict[str, Any]],
        model: str,
        *,
        frequency_penalty: Optional[float] = None,
        function_call: Optional[Union[str, Dict[str, Any]]] = None,
        functions: Optional[List[Dict[str, Any]]] = None,
        logit_bias: Optional[Dict[str, float]] = None,
        logprobs: Optional[bool] = None,
        top_logprobs: Optional[int] = None,
        max_tokens: Optional[int] = None,
        n: Optional[int] = None,
        presence_penalty: Optional[float] = None,
        response_format: Optional[Dict[str, Any]] = None,
        seed: Optional[int] = None,
        stop: Optional[Union[str, List[str]]] = None,
        stream: Optional[bool] = None,
        structured_provider: Optional[str] = None,
        temperature: Optional[float] = None,
        tool_choice: Optional[Union[str, Dict[str, Any]]] = None,
        tools: Optional[List[Dict[str, Any]]] = None,
        top_p: Optional[float] = None,
        user: Optional[str] = None,
        extra_body: Optional[Dict[str, Any]] = None,
        timeout: Optional[float] = None,
        fallback: Optional[List[str]] = None,
        fallback_models: Optional[List[str]] = None,
        retry_policy: Optional[Dict[str, Any]] = None,
        **kwargs
    ) -> Union[ChatCompletion, AsyncIterator[ChatCompletionChunk]]:
        """åˆ›å»ºèŠå¤©å®Œæˆï¼ˆå¼‚æ­¥ç‰ˆæœ¬ï¼‰"""
        # è·å–æ€§èƒ½é…ç½®
        perf_config = get_performance_config()
        
        # æ£€æŸ¥æ˜¯å¦å¯ç”¨FASTæ¨¡å¼ä¸”æœ‰ç»“æ„åŒ–è¾“å‡º
        if (perf_config.mode.value == "fast" and 
            response_format and 
            response_format.get("type") == "json_schema" and
            structured_provider == "agently" and
            not stream):  # æµå¼è¾“å‡ºæš‚ä¸æ”¯æŒå¿«é€Ÿè·¯å¾„
            
            return await self._acreate_fast_structured_path(
                messages, model, response_format, structured_provider,
                frequency_penalty=frequency_penalty,
                function_call=function_call,
                functions=functions,
                logit_bias=logit_bias,
                logprobs=logprobs,
                top_logprobs=top_logprobs,
                max_tokens=max_tokens,
                n=n,
                presence_penalty=presence_penalty,
                seed=seed,
                stop=stop,
                stream=stream,
                temperature=temperature,
                tool_choice=tool_choice,
                tools=tools,
                top_p=top_p,
                user=user,
                extra_body=extra_body,
                timeout=timeout,
                fallback=fallback,
                fallback_models=fallback_models,
                retry_policy=retry_policy,
                **kwargs
            )
        
        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨å¿«é€Ÿè·¯å¾„
        if perf_config.should_use_fast_path(model, max_tokens):
            return await self._acreate_fast_path(
                messages=messages, model=model, frequency_penalty=frequency_penalty,
                function_call=function_call, functions=functions, logit_bias=logit_bias,
                logprobs=logprobs, top_logprobs=top_logprobs, max_tokens=max_tokens,
                n=n, presence_penalty=presence_penalty, response_format=response_format,
                seed=seed, stop=stop, stream=stream, structured_provider=structured_provider,
                temperature=temperature, tool_choice=tool_choice, tools=tools,
                top_p=top_p, user=user, extra_body=extra_body, timeout=timeout,
                fallback=fallback, fallback_models=fallback_models,
                retry_policy=retry_policy, **kwargs
            )
        else:
            return await self._acreate_full_path(
                messages=messages, model=model, frequency_penalty=frequency_penalty,
                function_call=function_call, functions=functions, logit_bias=logit_bias,
                logprobs=logprobs, top_logprobs=top_logprobs, max_tokens=max_tokens,
                n=n, presence_penalty=presence_penalty, response_format=response_format,
                seed=seed, stop=stop, stream=stream, structured_provider=structured_provider,
                temperature=temperature, tool_choice=tool_choice, tools=tools,
                top_p=top_p, user=user, extra_body=extra_body, timeout=timeout,
                fallback=fallback, fallback_models=fallback_models,
                retry_policy=retry_policy, **kwargs
            )
    
    @fast_trace
    async def _acreate_fast_structured_path(
        self,
        messages: List[Dict[str, Any]],
        model: str,
        response_format: Dict[str, Any],
        structured_provider: str,
        **kwargs
    ) -> ChatCompletion:
        """å¿«é€Ÿç»“æ„åŒ–è¾“å‡ºè·¯å¾„ï¼ˆå¼‚æ­¥ç‰ˆæœ¬ï¼‰"""
        try:
            # æå–ç”¨æˆ·è¾“å…¥
            user_input = None
            for msg in reversed(messages):
                if msg.get("role") == "user" and msg.get("content"):
                    user_input = msg["content"]
                    break
            
            if not user_input:
                # æ²¡æœ‰ç”¨æˆ·è¾“å…¥ï¼Œå›é€€åˆ°å¸¸è§„è·¯å¾„
                return await self._acreate_core(messages, model, **kwargs)
            
            # è·å–å¿«é€Ÿå¤„ç†å™¨
            fast_processor = self._get_fast_processor()
            
            # æå–schema
            json_schema = response_format.get('json_schema', {})
            schema = json_schema.get('schema', {})
            
            # ä½¿ç”¨å¿«é€Ÿå¤„ç†å™¨å¤„ç†ç»“æ„åŒ–è¾“å‡ºï¼ˆåŒæ­¥æ–¹æ³•ï¼Œåœ¨å¼‚æ­¥ä¸Šä¸‹æ–‡ä¸­è¿è¡Œï¼‰
            result = fast_processor.process_structured_output(
                user_query=user_input,
                schema=schema,
                api_key=self.client_manager.client_config.get('api_key'),
                base_url=self.client_manager.client_config.get('base_url'),
                model=model,
                temperature=kwargs.get('temperature', 0.1),
                max_tokens=kwargs.get('max_tokens')
            )
            
            # æ„é€ å…¼å®¹çš„å“åº”å¯¹è±¡
            from ..core.base_plugin import ChatMessage, ChatChoice
            
            response = ChatCompletion(
                id=f"fast-structured-async-{int(time.time())}",
                object="chat.completion",
                created=int(time.time()),
                model=model,
                choices=[
                    ChatChoice(
                        index=0,
                        message=ChatMessage(
                            role="assistant",
                            content=str(result)  # å°†ç»“æ„åŒ–ç»“æœè½¬ä¸ºå­—ç¬¦ä¸²
                        ),
                        finish_reason="stop"
                    )
                ]
            )
            
            # è®¾ç½®parsedå±æ€§
            response.choices[0].message.parsed = result
            
            return response
            
        except Exception as e:
            # å¿«é€Ÿè·¯å¾„å¤±è´¥ï¼Œå›é€€åˆ°å¸¸è§„è·¯å¾„
            return await self._acreate_core(messages, model, **kwargs)

    @fast_trace
    async def _acreate_fast_path(
        self,
        messages: List[Dict[str, Any]],
        model: str,
        **kwargs
    ) -> Union[ChatCompletion, AsyncIterator[ChatCompletionChunk]]:
        """å¿«é€Ÿè·¯å¾„å¼‚æ­¥åˆ›å»ºèŠå¤©å®Œæˆ"""
        return await self._acreate_core(messages, model, **kwargs)
    
    @full_trace
    async def _acreate_full_path(
        self,
        messages: List[Dict[str, Any]],
        model: str,
        **kwargs
    ) -> Union[ChatCompletion, AsyncIterator[ChatCompletionChunk]]:
        """å®Œæ•´è·¯å¾„å¼‚æ­¥åˆ›å»ºèŠå¤©å®Œæˆ"""
        return await self._acreate_core(messages, model, **kwargs)
    
    async def _acreate_core(
        self,
        messages: List[Dict[str, Any]],
        model: str,
        frequency_penalty: Optional[float] = None,
        function_call: Optional[Union[str, Dict[str, Any]]] = None,
        functions: Optional[List[Dict[str, Any]]] = None,
        logit_bias: Optional[Dict[str, float]] = None,
        logprobs: Optional[bool] = None,
        top_logprobs: Optional[int] = None,
        max_tokens: Optional[int] = None,
        n: Optional[int] = None,
        presence_penalty: Optional[float] = None,
        response_format: Optional[Dict[str, Any]] = None,
        seed: Optional[int] = None,
        stop: Optional[Union[str, List[str]]] = None,
        stream: Optional[bool] = None,
        structured_provider: Optional[str] = None,
        temperature: Optional[float] = None,
        tool_choice: Optional[Union[str, Dict[str, Any]]] = None,
        tools: Optional[List[Dict[str, Any]]] = None,
        top_p: Optional[float] = None,
        user: Optional[str] = None,
        extra_body: Optional[Dict[str, Any]] = None,
        timeout: Optional[float] = None,
        fallback: Optional[List[str]] = None,
        fallback_models: Optional[List[str]] = None,
        retry_policy: Optional[Dict[str, Any]] = None,
        **kwargs
    ) -> Union[ChatCompletion, AsyncIterator[ChatCompletionChunk]]:
        """å¼‚æ­¥æ ¸å¿ƒåˆ›å»ºé€»è¾‘"""
        # éªŒè¯æ¶ˆæ¯
        self._validate_messages(messages)
        
        # éªŒè¯temperatureå‚æ•°
        if temperature is not None:
            if not isinstance(temperature, (int, float)):
                raise ValueError("temperature must be a number")
            if temperature < 0 or temperature > 2.0:
                raise ValueError("temperature must be between 0 and 2")
        
        # éªŒè¯max_tokenså‚æ•°
        if max_tokens is not None:
            from ..core.models import get_model_capabilities
            from ..core.exceptions import ValidationError as CoreValidationError
            
            if not isinstance(max_tokens, int):
                raise ValueError("max_tokens must be an integer")
            if max_tokens <= 0:
                raise ValueError("max_tokens must be positive")
            
            capabilities = get_model_capabilities(model)
            if capabilities and capabilities.max_tokens_limit:
                if max_tokens > capabilities.max_tokens_limit:
                    raise CoreValidationError(
                        f"max_tokens ({max_tokens}) exceeds limit for model {model}: {capabilities.max_tokens_limit}"
                    )
        
        # å¤„ç†fallbackå‚æ•°å…¼å®¹æ€§ï¼šfallback_modelsä¼˜å…ˆçº§é«˜äºfallback
        if fallback_models is not None:
            fallback = fallback_models
        elif fallback is None:
            # ä½¿ç”¨settingsä¸­é…ç½®çš„é»˜è®¤é™çº§æ¨¡å‹
            from ..config.settings import get_settings
            settings = get_settings()
            if settings.enable_fallback and settings.fallback_models:
                fallback = settings.fallback_models
            else:
                fallback = []
        
        trace_id = get_or_create_trace_id()
        
        async with TraceContext(trace_id):
            # éªŒè¯structured_providerå‚æ•°
            if structured_provider and structured_provider not in ["agently", "native"]:
                raise ValidationError(
                    f"Invalid structured_provider '{structured_provider}'. "
                    "Must be 'agently' or 'native'"
                )
            
            # æ„å»ºè¯·æ±‚å‚æ•°
            request_params = {
                "messages": messages,
                "model": model,
                "frequency_penalty": frequency_penalty,
                "function_call": function_call,
                "functions": functions,
                "logit_bias": logit_bias,
                "logprobs": logprobs,
                "top_logprobs": top_logprobs,
                "max_tokens": max_tokens,
                "n": n,
                "presence_penalty": presence_penalty,
                "response_format": response_format,
                "seed": seed,
                "stop": stop,
                "stream": stream,
                "structured_provider": structured_provider or "agently",
                "temperature": temperature,
                "tool_choice": tool_choice,
                "tools": tools,
                "top_p": top_p,
                "user": user,
                "extra_body": extra_body,
                "timeout": timeout,
                "fallback": fallback,
                "retry_policy": retry_policy,
                **kwargs
            }
            
            # ç§»é™¤ None å€¼
            request_params = {k: v for k, v in request_params.items() if v is not None}
            
            # å¯¹æ¨ç†æ¨¡å‹è¿›è¡Œå‚æ•°è¿‡æ»¤å’Œæ¶ˆæ¯å¤„ç†
            from ..core.models import filter_parameters_for_model, is_reasoning_model
            if is_reasoning_model(model):
                # è¿‡æ»¤ä¸æ”¯æŒçš„å‚æ•°
                request_params = filter_parameters_for_model(model, request_params)
                
                # å¤„ç†æ¨ç†æ¨¡å‹çš„systemæ¶ˆæ¯
                messages = self._process_messages_for_reasoning_model(messages)
                request_params["messages"] = messages
            
            try:
                # è®°å½•è¯·æ±‚æ—¥å¿—
                from ..utils.logger import LogContext
                log_context = LogContext(trace_id=trace_id)
                request_data = {
                    "method": "POST",
                    "url": "/chat/completions",
                    "model": model,
                    "messages": messages,
                    **request_params
                }
                await self.api_logger.alog_request(log_context, request_data)
                
                # è½¬æ¢å­—å…¸æ¶ˆæ¯ä¸ºChatMessageå¯¹è±¡
                from ..core.base_plugin import ChatMessage
                chat_messages = [
                    ChatMessage(
                        role=msg["role"],
                        content=msg.get("content"),
                        name=msg.get("name"),
                        function_call=msg.get("function_call"),
                        tool_calls=msg.get("tool_calls"),
                        tool_call_id=msg.get("tool_call_id")
                    )
                    for msg in messages
                ]
                
                # é…ç½®é‡è¯•ç­–ç•¥
                from ..utils.retry import RetryConfig
                retry_config = None
                if retry_policy:
                    retry_config = RetryConfig(
                        max_attempts=retry_policy.get('max_attempts', 3),
                        base_delay=retry_policy.get('base_delay', 1.0),
                        max_delay=retry_policy.get('max_delay', 60.0),
                        exponential_base=retry_policy.get('exponential_base', 2.0),
                        jitter=retry_policy.get('jitter', True)
                    )
                
                # ä½¿ç”¨é‡è¯•è£…é¥°å™¨
                @async_retry_with_backoff(config=retry_config)
                async def _acreate_with_retry():
                    return await self.client_manager.chat_completion_with_fallback(
                        model=model,
                        messages=chat_messages,
                        fallback=fallback,
                        **{k: v for k, v in request_params.items() if k not in ['model', 'messages', 'fallback', 'retry_policy']}
                    )
                
                response = await _acreate_with_retry()
                
                # è®°å½•å“åº”æ—¥å¿—
                response_data = {
                    "response": response,
                    "model": model
                }
                await self.api_logger.alog_response(log_context, response_data)
                
                return response
                
            except Exception as e:
                # è®°å½•é”™è¯¯æ—¥å¿—
                error_data = {
                    "error": str(e),
                    "model": model,
                    "plugin_name": "unknown",
                    "latency_ms": 0
                }
                await self.api_logger.alog_error(log_context, error_data)
                raise e
    
    def _validate_messages(self, messages: List[Dict[str, Any]]) -> None:
        """éªŒè¯æ¶ˆæ¯æ ¼å¼"""
        if not messages:
            raise ValidationError("Messages cannot be empty")
        
        valid_roles = {"system", "user", "assistant", "tool", "function"}
        
        for i, message in enumerate(messages):
            if not isinstance(message, dict):
                raise ValidationError(f"Message at index {i} must be a dictionary")
            
            if "role" not in message:
                raise ValidationError(f"Message at index {i} must have a 'role' field")
            
            if message["role"] not in valid_roles:
                raise ValidationError(
                    f"Message at index {i} has invalid role '{message['role']}'. "
                    f"Valid roles are: {', '.join(valid_roles)}"
                )
            
            if "content" not in message and "tool_calls" not in message:
                raise ValidationError(
                    f"Message at index {i} must have either 'content' or 'tool_calls' field"
                )
    
    def _process_messages_for_reasoning_model(self, messages: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """å¤„ç†æ¨ç†æ¨¡å‹çš„æ¶ˆæ¯ï¼Œå°†systemæ¶ˆæ¯åˆå¹¶åˆ°useræ¶ˆæ¯ä¸­"""
        processed_messages = []
        system_content = ""
        
        # æ”¶é›†æ‰€æœ‰systemæ¶ˆæ¯çš„å†…å®¹
        for message in messages:
            if message.get("role") == "system":
                if message.get("content"):
                    system_content += message["content"] + "\n"
            else:
                processed_messages.append(message)
        
        # å¦‚æœæœ‰systemå†…å®¹ï¼Œå°†å…¶æ·»åŠ åˆ°ç¬¬ä¸€ä¸ªuseræ¶ˆæ¯ä¸­
        if system_content and processed_messages:
            for i, message in enumerate(processed_messages):
                if message.get("role") == "user":
                    original_content = message.get("content", "")
                    processed_messages[i] = {
                        **message,
                        "content": f"{system_content.strip()}\n\n{original_content}"
                    }
                    break
        
        return processed_messages


class Chat:
    """èŠå¤©æ¥å£"""
    
    def __init__(self, client_manager: ClientManager):
        self.completions = ChatCompletions(client_manager)


class HarborAI:
    """HarborAI ä¸»å®¢æˆ·ç«¯"""
    
    def __init__(
        self,
        api_key: Optional[str] = None,
        organization: Optional[str] = None,
        project: Optional[str] = None,
        base_url: Optional[str] = None,
        timeout: Optional[float] = None,
        max_retries: Optional[int] = None,
        default_headers: Optional[Dict[str, str]] = None,
        default_query: Optional[Dict[str, str]] = None,
        http_client: Optional[Any] = None,
        **kwargs
    ):
        """åˆå§‹åŒ– HarborAI å®¢æˆ·ç«¯"""
        # è‡ªåŠ¨åˆå§‹åŒ–ç”Ÿå‘½å‘¨æœŸç®¡ç†ï¼ˆåŒ…æ‹¬PostgreSQLæ—¥å¿—è®°å½•å™¨ï¼‰
        auto_initialize()
        
        self.logger = get_logger("harborai.client")
        self.settings = get_settings()
        
        # åˆå§‹åŒ–æ€§èƒ½ç®¡ç†å™¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        self._performance_manager = None
        if self.settings.enable_performance_manager:
            self._performance_manager = get_performance_manager()
            # å¼‚æ­¥åˆå§‹åŒ–å°†åœ¨ç¬¬ä¸€æ¬¡ä½¿ç”¨æ—¶è¿›è¡Œ
        
        # å­˜å‚¨å®¢æˆ·ç«¯é…ç½®
        self.config = {
            "api_key": api_key,
            "organization": organization,
            "project": project,
            "base_url": base_url,
            "timeout": timeout or self.settings.default_timeout,
            "max_retries": max_retries or self.settings.max_retries,
            "default_headers": default_headers or {},
            "default_query": default_query or {},
            "http_client": http_client,
            **kwargs
        }
        
        # åˆå§‹åŒ–å®¢æˆ·ç«¯ç®¡ç†å™¨ï¼Œä¼ é€’å®¢æˆ·ç«¯é…ç½®
        self.client_manager = ClientManager(client_config=self.config)
        
        # åˆå§‹åŒ–æ¥å£
        self.chat = Chat(self.client_manager)
        
        # åˆå§‹åŒ–æˆæœ¬è·Ÿè¸ªå™¨ï¼ˆå¯é€‰ï¼‰
        self.cost_tracker = None
        
        trace_id = get_or_create_trace_id()
        config_info = {
            k: v for k, v in self.config.items() 
            if k not in ['api_key', 'http_client']
        }
        self.logger.info(
            f"HarborAI client initialized [trace_id={trace_id}] - "
            f"config: {config_info}, "
            f"available_plugins: {list(self.client_manager.plugins.keys())}, "
            f"available_models: {len(self.client_manager.model_to_plugin)}"
        )
    
    def get_available_models(self) -> List[str]:
        """è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
        model_infos = self.client_manager.get_available_models()
        # å¦‚æœè¿”å›çš„æ˜¯ModelInfoå¯¹è±¡åˆ—è¡¨ï¼Œæå–idï¼›å¦‚æœæ˜¯å­—ç¬¦ä¸²åˆ—è¡¨ï¼Œç›´æ¥è¿”å›
        if model_infos and hasattr(model_infos[0], 'id'):
            return [model_info.id for model_info in model_infos]
        else:
            return model_infos
    
    def get_plugin_info(self) -> Dict[str, Any]:
        """è·å–æ’ä»¶ä¿¡æ¯"""
        return self.client_manager.get_plugin_info()
    
    def register_plugin(self, plugin) -> None:
        """æ³¨å†Œæ’ä»¶"""
        self.client_manager.register_plugin(plugin)
    
    def unregister_plugin(self, plugin_name: str) -> None:
        """æ³¨é”€æ’ä»¶"""
        self.client_manager.unregister_plugin(plugin_name)
    
    @property
    def client(self):
        """ä¸ºäº†å…¼å®¹æ€§æä¾›çš„åˆ«åå±æ€§"""
        return self
    
    def get_total_cost(self) -> float:
        """è·å–æ€»æˆæœ¬
        
        Returns:
            æ€»æˆæœ¬
        """
        if self.cost_tracker is not None:
            return self.cost_tracker.get_total_cost()
        return 0.0
    
    def reset_cost(self) -> None:
        """é‡ç½®æˆæœ¬è®¡æ•°å™¨"""
        if self.cost_tracker is not None:
            self.cost_tracker.reset()
    
    async def aclose(self) -> None:
        """å¼‚æ­¥å…³é—­å®¢æˆ·ç«¯"""
        # æ¸…ç†æ€§èƒ½ç®¡ç†å™¨
        if self._performance_manager and self._performance_manager.is_initialized():
            try:
                await cleanup_performance_manager()
            except Exception as e:
                trace_id = get_or_create_trace_id()
                self.logger.warning(
                    f"Error cleaning up performance manager [trace_id={trace_id}] - "
                    f"error: {str(e)}"
                )
        
        # æ¸…ç†èµ„æº
        for plugin in self.client_manager.plugins.values():
            if hasattr(plugin, 'aclose'):
                try:
                    await plugin.aclose()
                except Exception as e:
                    trace_id = get_or_create_trace_id()
                    self.logger.warning(
                        f"Error closing plugin [trace_id={trace_id}] - "
                        f"plugin: {plugin.name}, error: {str(e)}"
                    )
        
        trace_id = get_or_create_trace_id()
        self.logger.info(f"HarborAI client closed [trace_id={trace_id}]")
    
    def close(self) -> None:
        """åŒæ­¥å…³é—­å®¢æˆ·ç«¯"""
        # åŒæ­¥æ¸…ç†æ€§èƒ½ç®¡ç†å™¨ï¼ˆé€šè¿‡å¼‚æ­¥è¿è¡Œï¼‰
        if self._performance_manager and self._performance_manager.is_initialized():
            try:
                import asyncio
                loop = asyncio.get_event_loop()
                if loop.is_running():
                    # å¦‚æœäº‹ä»¶å¾ªç¯æ­£åœ¨è¿è¡Œï¼Œåˆ›å»ºä»»åŠ¡
                    asyncio.create_task(cleanup_performance_manager())
                else:
                    # å¦‚æœäº‹ä»¶å¾ªç¯æœªè¿è¡Œï¼Œç›´æ¥è¿è¡Œ
                    loop.run_until_complete(cleanup_performance_manager())
            except Exception as e:
                trace_id = get_or_create_trace_id()
                self.logger.warning(
                    f"Error cleaning up performance manager [trace_id={trace_id}] - "
                    f"error: {str(e)}"
                )
        
        # æ¸…ç†èµ„æº
        for plugin in self.client_manager.plugins.values():
            if hasattr(plugin, 'close'):
                try:
                    plugin.close()
                except Exception as e:
                    trace_id = get_or_create_trace_id()
                    self.logger.warning(
                        f"Error closing plugin [trace_id={trace_id}] - "
                        f"plugin: {plugin.name}, error: {str(e)}"
                    )
        
        trace_id = get_or_create_trace_id()
        self.logger.info(f"HarborAI client closed [trace_id={trace_id}]")
    
    def __enter__(self):
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        self.close()
    
    async def __aenter__(self):
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        await self.aclose()


# ä¸ºäº†å…¼å®¹æ€§ï¼Œæä¾›åˆ«å
Client = HarborAI