# Testing Requirements Contract

**Feature**: [spec.md](../spec.md)
**Data Model**: [data-model.md](../data-model.md)

## Purpose

ã“ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¯ã€`llm-discovery`ã®ãƒ†ã‚¹ãƒˆè¦ä»¶ã¨ã‚«ãƒãƒ¬ãƒƒã‚¸åŸºæº–ã‚’å®šç¾©ã—ã¾ã™ã€‚å¥‘ç´„ãƒ†ã‚¹ãƒˆã€çµ±åˆãƒ†ã‚¹ãƒˆã€ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆã€ã‚«ãƒãƒ¬ãƒƒã‚¸ç›®æ¨™ã‚’è¦å®šã—ã€ã™ã¹ã¦ã®æ©Ÿèƒ½è¦ä»¶ã¨User StoriesãŒæ¤œè¨¼å¯èƒ½ã§ã‚ã‚‹ã“ã¨ã‚’ä¿è¨¼ã—ã¾ã™ï¼ˆFR-020ï¼‰ã€‚

## Specification

### Coverage Requirements

**å¿…é ˆã‚«ãƒãƒ¬ãƒƒã‚¸**: 90%ä»¥ä¸Šï¼ˆFR-020ï¼‰

**æ¸¬å®šå¯¾è±¡**:
- ãƒ©ã‚¤ãƒ³ ã‚«ãƒãƒ¬ãƒƒã‚¸ï¼ˆLine Coverageï¼‰
- ãƒ–ãƒ©ãƒ³ãƒã‚«ãƒãƒ¬ãƒƒã‚¸ï¼ˆBranch Coverageï¼‰

**ãƒ„ãƒ¼ãƒ«**:
- pytest-cov
- coverage.py

**å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰**:
```bash
pytest --cov=llm_discovery --cov-report=html --cov-report=term
```

**ã‚«ãƒãƒ¬ãƒƒã‚¸ç›®æ¨™ï¼ˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åˆ¥ï¼‰**:

| Module                        | Target Coverage | Priority |
|-------------------------------|-----------------|----------|
| `llm_discovery/models/`       | 95-100%         | P0       |
| `llm_discovery/cli/`          | 90-95%          | P1       |
| `llm_discovery/api/`          | 90-95%          | P1       |
| `llm_discovery/providers/`    | 90-95%          | P1       |
| `llm_discovery/export/`       | 90-95%          | P2       |
| `llm_discovery/cache/`        | 90-95%          | P1       |
| `llm_discovery/exceptions.py` | 100%            | P0       |

### Test Categories

#### Category 1: Contract Tests

**ç›®çš„**: CLIå¥‘ç´„ã€Python APIå¥‘ç´„ã€ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¥‘ç´„ã‚’æ¤œè¨¼

**é…ç½®**: `tests/contract/`

**ãƒ†ã‚¹ãƒˆå¯¾è±¡**:

1. **CLI Interface Contract**:
   - ã‚³ãƒãƒ³ãƒ‰æ§‹é€ ã®æ­£ç¢ºæ€§
   - ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®å‹•ä½œ
   - çµ‚äº†ã‚³ãƒ¼ãƒ‰ã®ä¸€è²«æ€§
   - ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®å†…å®¹

2. **Python API Contract**:
   - ã‚¯ãƒ©ã‚¹ãƒ»ãƒ¡ã‚½ãƒƒãƒ‰ã‚·ã‚°ãƒãƒãƒ£ã®æ­£ç¢ºæ€§
   - å‹ãƒ’ãƒ³ãƒˆã®æ•´åˆæ€§
   - ä¾‹å¤–éšå±¤ã®æ•´åˆæ€§
   - éåŒæœŸAPIã®å‹•ä½œ

3. **Data Format Contract**:
   - JSONã‚¹ã‚­ãƒ¼ãƒæº–æ‹ 
   - CSVã€YAMLã€TOMLã€Markdownå½¢å¼ã®æ­£ç¢ºæ€§
   - ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå½¢å¼ã®ä¸€è²«æ€§

**ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«**:

- `tests/contract/test_cli_interface.py`:
  - User Story 1ã®Acceptance Scenariosï¼ˆå…¨6ã‚·ãƒŠãƒªã‚ªï¼‰
  - User Story 2ã®Acceptance Scenariosï¼ˆå…¨5ã‚·ãƒŠãƒªã‚ªï¼‰
  - User Story 3ã®Acceptance Scenariosï¼ˆå…¨4ã‚·ãƒŠãƒªã‚ªï¼‰

  ```python
  def test_user_story_1_scenario_1_initial_fetch():
      """
      Given åˆå›å®Ÿè¡Œæ™‚
      When `uvx llm-discovery list` ã‚’å®Ÿè¡Œ
      Then OpenAIã€Googleã€Anthropicã®ãƒ¢ãƒ‡ãƒ«ãŒAPI/æ‰‹å‹•ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å–å¾—ã•ã‚Œã€
           TOMLå½¢å¼ã§ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜ã•ã‚Œã‚‹
      """
      result = subprocess.run(
          ["llm-discovery", "list"],
          capture_output=True,
          text=True
      )
      assert result.returncode == 0
      assert "openai" in result.stdout
      assert "google" in result.stdout
      assert "anthropic" in result.stdout

      # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ãŒä½œæˆã•ã‚ŒãŸã“ã¨ã‚’ç¢ºèª
      cache_path = Path.home() / ".cache/llm-discovery/models_cache.toml"
      assert cache_path.exists()
  ```

- `tests/contract/test_python_api.py`:
  - User Story 4ã®Acceptance Scenariosï¼ˆå…¨4ã‚·ãƒŠãƒªã‚ªï¼‰

  ```python
  @pytest.mark.asyncio
  async def test_user_story_4_scenario_2_python_api():
      """
      Given Pythonã‚¹ã‚¯ãƒªãƒ—ãƒˆ
      When `from llm_discovery import DiscoveryClient` ã§ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
      Then CLIã¨åŒã˜æ©Ÿèƒ½ã‚’Python APIã¨ã—ã¦åˆ©ç”¨ã§ãã‚‹
      """
      from llm_discovery import DiscoveryClient

      client = DiscoveryClient()
      models = await client.fetch_models()

      assert isinstance(models, list)
      assert len(models) > 0
      assert all(isinstance(m, Model) for m in models)
  ```

- `tests/contract/test_data_formats.py`:
  - User Story 2ã®Acceptance Scenariosã‚’å„å½¢å¼ã”ã¨ã«æ¤œè¨¼

  ```python
  def test_user_story_2_scenario_1_json_export():
      """
      Given ãƒ¢ãƒ‡ãƒ«ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã™ã‚‹çŠ¶æ…‹
      When `uvx llm-discovery export --format json` ã‚’å®Ÿè¡Œ
      Then CI/CDçµ±åˆã«æœ€é©åŒ–ã•ã‚ŒãŸJSONå½¢å¼ã§ãƒ‡ãƒ¼ã‚¿ãŒå‡ºåŠ›ã•ã‚Œã‚‹
      """
      result = subprocess.run(
          ["llm-discovery", "export", "--format", "json"],
          capture_output=True,
          text=True
      )
      assert result.returncode == 0

      # JSONã‚¹ã‚­ãƒ¼ãƒæ¤œè¨¼
      data = json.loads(result.stdout)
      assert "metadata" in data
      assert "models" in data
      assert data["metadata"]["version"] == "1.0"
  ```

#### Category 2: Integration Tests

**ç›®çš„**: è¤‡æ•°ã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®çµ±åˆå‹•ä½œã‚’æ¤œè¨¼

**é…ç½®**: `tests/integration/`

**ãƒ†ã‚¹ãƒˆå¯¾è±¡**:
- APIå–å¾—â†’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜â†’èª­ã¿è¾¼ã¿ã®ä¸€é€£ã®ãƒ•ãƒ­ãƒ¼
- ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆâ†’ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆâ†’å†…å®¹æ¤œè¨¼
- å·®åˆ†æ¤œå‡ºâ†’changes.jsonç”Ÿæˆâ†’CHANGELOG.mdæ›´æ–°

**ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«**:

- `tests/integration/cli/test_cli_workflow.py`:
  - åˆå›å®Ÿè¡Œâ†’ã‚­ãƒ£ãƒƒã‚·ãƒ¥â†’å·®åˆ†æ¤œå‡ºã®ä¸€é€£ã®ãƒ•ãƒ­ãƒ¼

  ```python
  def test_full_cli_workflow(tmp_path, monkeypatch):
      """å®Œå…¨ãªCLIãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’æ¤œè¨¼"""
      # ç’°å¢ƒå¤‰æ•°è¨­å®š
      monkeypatch.setenv("LLM_DISCOVERY_CACHE_DIR", str(tmp_path))

      # åˆå›å®Ÿè¡Œ
      result1 = subprocess.run(
          ["llm-discovery", "list"],
          capture_output=True,
          text=True
      )
      assert result1.returncode == 0

      # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç¢ºèª
      cache_file = tmp_path / "models_cache.toml"
      assert cache_file.exists()

      # å·®åˆ†æ¤œå‡º
      result2 = subprocess.run(
          ["llm-discovery", "list", "--detect-changes"],
          capture_output=True,
          text=True
      )
      assert result2.returncode == 0
  ```

- `tests/integration/api/test_api_workflow.py`:
  - Python APIçµŒç”±ã®å–å¾—â†’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆâ†’ä¿å­˜

  ```python
  @pytest.mark.asyncio
  async def test_api_export_workflow(tmp_path):
      """Python APIã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’æ¤œè¨¼"""
      from llm_discovery import DiscoveryClient, export_json

      client = DiscoveryClient()
      models = await client.fetch_models()

      # JSONå½¢å¼ã§ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
      json_str = export_json(models)

      # ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
      output_path = tmp_path / "models.json"
      output_path.write_text(json_str)

      # å†…å®¹æ¤œè¨¼
      data = json.loads(json_str)
      assert len(data["models"]) == len(models)
  ```

#### Category 3: Unit Tests

**ç›®çš„**: å€‹åˆ¥ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãƒ»é–¢æ•°ãƒ»ã‚¯ãƒ©ã‚¹ã®å‹•ä½œã‚’æ¤œè¨¼

**é…ç½®**: `tests/unit/`

**ãƒ†ã‚¹ãƒˆå¯¾è±¡**:
- Pydanticãƒ¢ãƒ‡ãƒ«ã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
- ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆé–¢æ•°ã®å‡ºåŠ›å½¢å¼
- ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ­ã‚¸ãƒƒã‚¯
- ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®èª­ã¿æ›¸ã

**ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«**:

- `tests/unit/models/test_model.py`:
  - `Model`ã‚¯ãƒ©ã‚¹ã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³

  ```python
  def test_model_validation_success():
      """æ­£å¸¸ãªãƒ¢ãƒ‡ãƒ«ä½œæˆ"""
      model = Model(
          model_id="gpt-4",
          model_name="GPT-4",
          provider_name="openai",
          source=ModelSource.API,
          fetched_at=datetime.now(),
          metadata={"context_window": 8192}
      )
      assert model.model_id == "gpt-4"

  def test_model_validation_empty_id():
      """ç©ºã®model_idã¯ã‚¨ãƒ©ãƒ¼"""
      with pytest.raises(ValueError, match="model_id cannot be empty"):
          Model(
              model_id="",
              model_name="GPT-4",
              provider_name="openai",
              source=ModelSource.API
          )
  ```

- `tests/unit/export/test_json_export.py`:
  - JSONå½¢å¼ã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ

  ```python
  def test_json_export_format():
      """JSONå½¢å¼ã®æ­£ç¢ºæ€§"""
      models = [
          Model(
              model_id="gpt-4",
              model_name="GPT-4",
              provider_name="openai",
              source=ModelSource.API,
              fetched_at=datetime.now()
          )
      ]

      json_str = export_json(models)
      data = json.loads(json_str)

      assert "metadata" in data
      assert "models" in data
      assert len(data["models"]) == 1
  ```

- `tests/unit/errors/test_error_messages.py`:
  - ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®å†…å®¹

  ```python
  def test_provider_fetch_error_message():
      """ProviderFetchErrorã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å½¢å¼"""
      error = ProviderFetchError(
          provider_name="openai",
          cause="Connection timeout"
      )

      assert "openai" in str(error)
      assert "Connection timeout" in str(error)
  ```

#### Category 4: Error Handling Tests

**ç›®çš„**: ã‚¨ãƒ©ãƒ¼å‡¦ç†ã®æ­£ç¢ºæ€§ã‚’æ¤œè¨¼

**é…ç½®**: `tests/error/`

**ãƒ†ã‚¹ãƒˆå¯¾è±¡**:
- å„ã‚¨ãƒ©ãƒ¼ã‚¯ãƒ©ã‚¹ã®ç”Ÿæˆã¨å±æ€§
- ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®å†…å®¹
- ä¾‹å¤–ãƒã‚§ãƒ¼ãƒ³ï¼ˆ`raise ... from e`ï¼‰
- ãƒªã‚«ãƒãƒªæˆ¦ç•¥

**ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«**:

- `tests/error/test_cli_errors.py`:
  - CLIå®Ÿè¡Œæ™‚ã®ã‚¨ãƒ©ãƒ¼

  ```python
  def test_api_failure_error_message():
      """APIéšœå®³æ™‚ã®ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸"""
      # OpenAI APIã‚’éšœå®³çŠ¶æ…‹ã«ãƒ¢ãƒƒã‚¯
      with patch("httpx.AsyncClient.get", side_effect=httpx.TimeoutException):
          result = subprocess.run(
              ["llm-discovery", "list"],
              capture_output=True,
              text=True
          )

          assert result.returncode == 1
          assert "Failed to fetch models from OpenAI API" in result.stderr
          assert "Suggested actions" in result.stderr
  ```

- `tests/error/test_api_exceptions.py`:
  - Python APIã®ä¾‹å¤–

  ```python
  @pytest.mark.asyncio
  async def test_partial_fetch_error_attributes():
      """PartialFetchErrorã®å±æ€§"""
      error = PartialFetchError(
          successful_providers=["openai", "anthropic"],
          failed_providers=["google"]
      )

      assert error.successful_providers == ["openai", "anthropic"]
      assert error.failed_providers == ["google"]
      assert "openai" in str(error)
  ```

#### Category 5: Edge Case Tests

**ç›®çš„**: ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹ã®å‡¦ç†ã‚’æ¤œè¨¼

**é…ç½®**: `tests/edge/`

**ãƒ†ã‚¹ãƒˆå¯¾è±¡**:
- ç©ºã®ãƒ¢ãƒ‡ãƒ«ãƒªã‚¹ãƒˆ
- ç‰¹æ®Šæ–‡å­—ã‚’å«ã‚€ãƒ¢ãƒ‡ãƒ«å
- å¤§é‡ãƒ‡ãƒ¼ã‚¿ï¼ˆ1000ãƒ¢ãƒ‡ãƒ«ä»¥ä¸Šï¼‰
- Unicodeæ–‡å­—åˆ—ï¼ˆæ—¥æœ¬èªã€çµµæ–‡å­—ç­‰ï¼‰
- ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãªã—ã®ãƒ¢ãƒ‡ãƒ«

**ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«**:

- `tests/edge/test_edge_cases.py`:

  ```python
  def test_empty_model_list():
      """ç©ºã®ãƒ¢ãƒ‡ãƒ«ãƒªã‚¹ãƒˆã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
      models = []

      with pytest.raises(ValueError, match="models cannot be empty"):
          export_json(models)

  def test_special_characters_in_model_name():
      """ç‰¹æ®Šæ–‡å­—ã‚’å«ã‚€ãƒ¢ãƒ‡ãƒ«å"""
      model = Model(
          model_id="model-with-special-chars-ğŸš€",
          model_name="Model with \"quotes\" and, commas",
          provider_name="openai",
          source=ModelSource.API
      )

      json_str = export_json([model])
      data = json.loads(json_str)

      assert data["models"][0]["model_id"] == "model-with-special-chars-ğŸš€"
  ```

### Test Execution Strategy

#### Local Development

```bash
# ã™ã¹ã¦ã®ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ
pytest

# ã‚«ãƒãƒ¬ãƒƒã‚¸ãƒ¬ãƒãƒ¼ãƒˆä»˜ã
pytest --cov=llm_discovery --cov-report=html --cov-report=term

# ç‰¹å®šã®ã‚«ãƒ†ã‚´ãƒªã®ã¿å®Ÿè¡Œ
pytest tests/contract/
pytest tests/unit/
pytest tests/integration/

# ç‰¹å®šã®ãƒãƒ¼ã‚«ãƒ¼ã®ã¿å®Ÿè¡Œ
pytest -m "not slow"
```

#### CI/CD Pipeline

```yaml
# .github/workflows/test.yml
name: Tests

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.13'

      - name: Install dependencies
        run: |
          pip install uv
          uv pip install -e ".[dev]"

      - name: Run tests
        run: |
          pytest --cov=llm_discovery --cov-report=xml --cov-report=term

      - name: Upload coverage
        uses: codecov/codecov-action@v3
        with:
          files: ./coverage.xml
```

### Test Fixtures

**å…±é€šãƒ•ã‚£ã‚¯ã‚¹ãƒãƒ£**: `tests/conftest.py`

```python
import pytest
from pathlib import Path
from llm_discovery.models import Model, ModelSource
from datetime import datetime

@pytest.fixture
def sample_models():
    """ã‚µãƒ³ãƒ—ãƒ«ãƒ¢ãƒ‡ãƒ«ã®ãƒªã‚¹ãƒˆ"""
    return [
        Model(
            model_id="gpt-4",
            model_name="GPT-4",
            provider_name="openai",
            source=ModelSource.API,
            fetched_at=datetime.now()
        ),
        Model(
            model_id="gemini-1.5-pro",
            model_name="Gemini 1.5 Pro",
            provider_name="google",
            source=ModelSource.API,
            fetched_at=datetime.now()
        ),
        Model(
            model_id="claude-3-opus",
            model_name="Claude 3 Opus",
            provider_name="anthropic",
            source=ModelSource.MANUAL,
            fetched_at=datetime.now()
        )
    ]

@pytest.fixture
def temp_cache_dir(tmp_path, monkeypatch):
    """ä¸€æ™‚ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª"""
    cache_dir = tmp_path / "llm-discovery"
    cache_dir.mkdir()
    monkeypatch.setenv("LLM_DISCOVERY_CACHE_DIR", str(cache_dir))
    return cache_dir
```

### Test Markers

**ã‚«ã‚¹ã‚¿ãƒ ãƒãƒ¼ã‚«ãƒ¼**: `pytest.ini`

```ini
[pytest]
markers =
    slow: marks tests as slow (deselect with '-m "not slow"')
    integration: integration tests
    contract: contract tests
    unit: unit tests
    edge: edge case tests
```

**ä½¿ç”¨ä¾‹**:

```python
@pytest.mark.slow
@pytest.mark.integration
async def test_large_dataset_export():
    """å¤§é‡ãƒ‡ãƒ¼ã‚¿ã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆï¼ˆæ™‚é–“ãŒã‹ã‹ã‚‹ï¼‰"""
    models = [create_sample_model() for _ in range(10000)]
    json_str = export_json(models)
    assert len(json.loads(json_str)["models"]) == 10000
```

### Coverage Enforcement

**pytest-covè¨­å®š**: `pyproject.toml`

```toml
[tool.coverage.run]
source = ["llm_discovery"]
omit = [
    "*/tests/*",
    "*/conftest.py",
    "*/__main__.py"
]

[tool.coverage.report]
fail_under = 90
precision = 2
show_missing = true
skip_covered = false

[tool.coverage.html]
directory = "htmlcov"
```

**CI/CDã§ã®å¼·åˆ¶**:

```yaml
- name: Check coverage threshold
  run: |
    pytest --cov=llm_discovery --cov-report=term --cov-fail-under=90
```

### Test Documentation

**å¿…é ˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ**: å„ãƒ†ã‚¹ãƒˆã«docstring

```python
def test_json_export_schema_compliance():
    """
    JSONå½¢å¼ã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆãŒJSONSchemaã«æº–æ‹ ã—ã¦ã„ã‚‹ã“ã¨ã‚’æ¤œè¨¼

    Given: æœ‰åŠ¹ãªãƒ¢ãƒ‡ãƒ«ã®ãƒªã‚¹ãƒˆ
    When: export_json()ã‚’å®Ÿè¡Œ
    Then: å‡ºåŠ›ãŒJSONSchemaã«æº–æ‹ ã—ã€å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒå«ã¾ã‚Œã‚‹

    Reference: User Story 2 Scenario 1
    """
    models = [create_sample_model()]
    json_str = export_json(models)

    # JSONSchemaã§æ¤œè¨¼
    schema = load_json_schema()
    validate(json.loads(json_str), schema)
```

## Test Requirements Summary

| Test Category       | Location              | Priority | Coverage Target | Count (Est.) |
|---------------------|-----------------------|----------|-----------------|--------------|
| Contract Tests      | `tests/contract/`     | P0       | 100%            | 20+          |
| Integration Tests   | `tests/integration/`  | P1       | 95%             | 15+          |
| Unit Tests          | `tests/unit/`         | P1       | 95%             | 100+         |
| Error Handling      | `tests/error/`        | P1       | 100%            | 20+          |
| Edge Cases          | `tests/edge/`         | P2       | 90%             | 10+          |
| **Total**           | **All**               | -        | **90%+**        | **165+**     |

## References

- **FR-020**: ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸90%ä»¥ä¸Š
- **User Story 1**: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ¢ãƒ‡ãƒ«ä¸€è¦§å–å¾—ï¼ˆ6ã‚·ãƒŠãƒªã‚ªï¼‰
- **User Story 2**: ãƒãƒ«ãƒãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆï¼ˆ5ã‚·ãƒŠãƒªã‚ªï¼‰
- **User Story 3**: æ–°ãƒ¢ãƒ‡ãƒ«æ¤œçŸ¥ã¨å·®åˆ†ãƒ¬ãƒãƒ¼ãƒˆï¼ˆ4ã‚·ãƒŠãƒªã‚ªï¼‰
- **User Story 4**: CI/CDçµ±åˆã¨Python APIåˆ©ç”¨ï¼ˆ4ã‚·ãƒŠãƒªã‚ªï¼‰
- **pytest Documentation**: https://docs.pytest.org/
- **pytest-cov Documentation**: https://pytest-cov.readthedocs.io/
