"""
æµ‹è¯•å¥—ä»¶è¿è¡Œå™¨ - ç»Ÿä¸€ç®¡ç†å’Œè¿è¡Œæ‰€æœ‰æµ‹è¯•
"""

import pytest
import sys
import os
import time
import json
from datetime import datetime
from typing import Dict, List, Any, Optional
from pathlib import Path
import subprocess
import coverage


class TestSuiteRunner:
    """æµ‹è¯•å¥—ä»¶è¿è¡Œå™¨"""
    
    def __init__(self, project_root: str = None):
        self.project_root = Path(project_root) if project_root else Path.cwd()
        self.test_dir = self.project_root / "tests"
        self.results = {}
        self.coverage_data = {}
        
    def run_unit_tests(self, verbose: bool = True) -> Dict[str, Any]:
        """è¿è¡Œå•å…ƒæµ‹è¯•"""
        print("Running Unit Tests...")
        start_time = time.time()
        
        # å•å…ƒæµ‹è¯•æ–‡ä»¶æ¨¡å¼
        unit_test_patterns = [
            "test_config.py",
            "test_models.py", 
            "test_cache.py",
            "test_errors.py",
            "test_formatter.py",
            "test_validators.py",
            "test_data_manager.py",
            "test_providers.py",
            "test_baostock_provider.py",
            "test_eastmoney_provider.py",
            "test_tonghuashun_provider.py"
        ]
        
        args = ["-v"] if verbose else []
        args.extend([str(self.test_dir / pattern) for pattern in unit_test_patterns])
        
        result = pytest.main(args)
        
        execution_time = time.time() - start_time
        
        unit_results = {
            'status': 'passed' if result == 0 else 'failed',
            'execution_time': execution_time,
            'exit_code': result,
            'test_count': len(unit_test_patterns)
        }
        
        self.results['unit_tests'] = unit_results
        return unit_results
    
    def run_integration_tests(self, verbose: bool = True) -> Dict[str, Any]:
        """è¿è¡Œé›†æˆæµ‹è¯•"""
        print("Running Integration Tests...")
        start_time = time.time()
        
        integration_test_patterns = [
            "test_integration.py",
            "test_client_integration.py",
            "test_data_source_coordination.py",
            "test_data_source_manager.py"
        ]
        
        args = ["-v"] if verbose else []
        args.extend([str(self.test_dir / pattern) for pattern in integration_test_patterns])
        
        result = pytest.main(args)
        
        execution_time = time.time() - start_time
        
        integration_results = {
            'status': 'passed' if result == 0 else 'failed',
            'execution_time': execution_time,
            'exit_code': result,
            'test_count': len(integration_test_patterns)
        }
        
        self.results['integration_tests'] = integration_results
        return integration_results
    
    def run_end_to_end_tests(self, verbose: bool = True) -> Dict[str, Any]:
        """è¿è¡Œç«¯åˆ°ç«¯æµ‹è¯•"""
        print("Running End-to-End Tests...")
        start_time = time.time()
        
        e2e_test_patterns = [
            "test_end_to_end.py",
            "test_client.py"
        ]
        
        args = ["-v"] if verbose else []
        args.extend([str(self.test_dir / pattern) for pattern in e2e_test_patterns])
        
        result = pytest.main(args)
        
        execution_time = time.time() - start_time
        
        e2e_results = {
            'status': 'passed' if result == 0 else 'failed',
            'execution_time': execution_time,
            'exit_code': result,
            'test_count': len(e2e_test_patterns)
        }
        
        self.results['end_to_end_tests'] = e2e_results
        return e2e_results
    
    def run_performance_tests(self, verbose: bool = True) -> Dict[str, Any]:
        """è¿è¡Œæ€§èƒ½æµ‹è¯•"""
        print("Running Performance Tests...")
        start_time = time.time()
        
        performance_test_patterns = [
            "test_performance_benchmarks.py",
            "test_memory_optimization.py",
            "test_concurrency_features.py"
        ]
        
        args = ["-v", "-s"]  # -s æ˜¾ç¤ºprintè¾“å‡º
        if verbose:
            args.append("--tb=short")
        
        args.extend([str(self.test_dir / pattern) for pattern in performance_test_patterns])
        
        result = pytest.main(args)
        
        execution_time = time.time() - start_time
        
        performance_results = {
            'status': 'passed' if result == 0 else 'failed',
            'execution_time': execution_time,
            'exit_code': result,
            'test_count': len(performance_test_patterns)
        }
        
        self.results['performance_tests'] = performance_results
        return performance_results
    
    def run_coverage_analysis(self) -> Dict[str, Any]:
        """è¿è¡Œä»£ç è¦†ç›–ç‡åˆ†æ"""
        print("Running Coverage Analysis...")
        
        try:
            # åˆå§‹åŒ–coverage
            cov = coverage.Coverage(
                source=['quickstock'],
                omit=[
                    '*/tests/*',
                    '*/test_*',
                    '*/__pycache__/*',
                    '*/venv/*',
                    '*/env/*'
                ]
            )
            
            cov.start()
            
            # è¿è¡Œæ‰€æœ‰æµ‹è¯•ï¼ˆé™¤äº†æ€§èƒ½æµ‹è¯•ï¼‰
            test_patterns = [
                "test_config.py",
                "test_models.py",
                "test_cache.py",
                "test_errors.py",
                "test_formatter.py",
                "test_validators.py",
                "test_integration.py",
                "test_client.py"
            ]
            
            args = ["-q"]  # å®‰é™æ¨¡å¼
            args.extend([str(self.test_dir / pattern) for pattern in test_patterns])
            
            pytest.main(args)
            
            cov.stop()
            cov.save()
            
            # ç”Ÿæˆè¦†ç›–ç‡æŠ¥å‘Š
            coverage_report = {}
            
            # è·å–è¦†ç›–ç‡æ•°æ®
            total_lines = 0
            covered_lines = 0
            
            for filename in cov.get_data().measured_files():
                if 'quickstock' in filename and 'test' not in filename:
                    analysis = cov.analysis2(filename)
                    file_total = len(analysis[1]) + len(analysis[2])
                    file_covered = len(analysis[1])
                    
                    total_lines += file_total
                    covered_lines += file_covered
                    
                    coverage_report[filename] = {
                        'total_lines': file_total,
                        'covered_lines': file_covered,
                        'coverage_percent': (file_covered / file_total * 100) if file_total > 0 else 0
                    }
            
            overall_coverage = (covered_lines / total_lines * 100) if total_lines > 0 else 0
            
            coverage_results = {
                'overall_coverage': overall_coverage,
                'total_lines': total_lines,
                'covered_lines': covered_lines,
                'file_coverage': coverage_report,
                'status': 'passed' if overall_coverage >= 80 else 'warning'
            }
            
            self.coverage_data = coverage_results
            return coverage_results
            
        except Exception as e:
            print(f"Coverage analysis failed: {e}")
            return {
                'status': 'failed',
                'error': str(e),
                'overall_coverage': 0
            }
    
    def run_all_tests(self, include_performance: bool = False, 
                     include_coverage: bool = True) -> Dict[str, Any]:
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        print("="*60)
        print("QUICKSTOCK SDK - COMPREHENSIVE TEST SUITE")
        print("="*60)
        
        start_time = time.time()
        
        # è¿è¡Œå„ç±»æµ‹è¯•
        self.run_unit_tests()
        self.run_integration_tests()
        self.run_end_to_end_tests()
        
        if include_performance:
            self.run_performance_tests()
        
        if include_coverage:
            self.run_coverage_analysis()
        
        total_time = time.time() - start_time
        
        # æ±‡æ€»ç»“æœ
        summary = self._generate_summary(total_time)
        self.results['summary'] = summary
        
        # æ‰“å°ç»“æœ
        self._print_results()
        
        # ä¿å­˜ç»“æœ
        self._save_results()
        
        return self.results
    
    def _generate_summary(self, total_time: float) -> Dict[str, Any]:
        """ç”Ÿæˆæµ‹è¯•æ‘˜è¦"""
        total_tests = 0
        passed_tests = 0
        failed_tests = 0
        
        for test_type, result in self.results.items():
            if test_type != 'summary' and 'test_count' in result:
                total_tests += result['test_count']
                if result['status'] == 'passed':
                    passed_tests += result['test_count']
                else:
                    failed_tests += result['test_count']
        
        return {
            'total_execution_time': total_time,
            'total_tests': total_tests,
            'passed_tests': passed_tests,
            'failed_tests': failed_tests,
            'success_rate': (passed_tests / total_tests * 100) if total_tests > 0 else 0,
            'overall_status': 'passed' if failed_tests == 0 else 'failed',
            'timestamp': datetime.now().isoformat()
        }
    
    def _print_results(self):
        """æ‰“å°æµ‹è¯•ç»“æœ"""
        print("\n" + "="*60)
        print("TEST RESULTS SUMMARY")
        print("="*60)
        
        for test_type, result in self.results.items():
            if test_type == 'summary':
                continue
                
            status_symbol = "âœ“" if result['status'] == 'passed' else "âœ—"
            print(f"{status_symbol} {test_type.replace('_', ' ').title()}: {result['status'].upper()}")
            print(f"   Execution Time: {result['execution_time']:.2f}s")
            
            if 'test_count' in result:
                print(f"   Test Count: {result['test_count']}")
        
        # è¦†ç›–ç‡ä¿¡æ¯
        if self.coverage_data:
            print(f"\nğŸ“Š Code Coverage: {self.coverage_data['overall_coverage']:.1f}%")
            print(f"   Lines Covered: {self.coverage_data['covered_lines']}/{self.coverage_data['total_lines']}")
        
        # æ€»ä½“æ‘˜è¦
        if 'summary' in self.results:
            summary = self.results['summary']
            print(f"\nğŸ¯ Overall Results:")
            print(f"   Total Tests: {summary['total_tests']}")
            print(f"   Passed: {summary['passed_tests']}")
            print(f"   Failed: {summary['failed_tests']}")
            print(f"   Success Rate: {summary['success_rate']:.1f}%")
            print(f"   Total Time: {summary['total_execution_time']:.2f}s")
            
            if summary['overall_status'] == 'passed':
                print("\nğŸ‰ ALL TESTS PASSED!")
            else:
                print("\nâŒ SOME TESTS FAILED!")
        
        print("="*60)
    
    def _save_results(self):
        """ä¿å­˜æµ‹è¯•ç»“æœåˆ°æ–‡ä»¶"""
        results_file = self.project_root / "test_results.json"
        
        try:
            with open(results_file, 'w', encoding='utf-8') as f:
                json.dump(self.results, f, indent=2, ensure_ascii=False)
            print(f"\nğŸ“„ Test results saved to: {results_file}")
        except Exception as e:
            print(f"Failed to save results: {e}")
    
    def run_specific_test(self, test_pattern: str, verbose: bool = True) -> int:
        """è¿è¡Œç‰¹å®šæµ‹è¯•"""
        print(f"Running specific test: {test_pattern}")
        
        args = ["-v"] if verbose else []
        args.append(str(self.test_dir / test_pattern))
        
        return pytest.main(args)
    
    def run_tests_by_marker(self, marker: str, verbose: bool = True) -> int:
        """æ ¹æ®æ ‡è®°è¿è¡Œæµ‹è¯•"""
        print(f"Running tests with marker: {marker}")
        
        args = ["-v"] if verbose else []
        args.extend(["-m", marker])
        args.append(str(self.test_dir))
        
        return pytest.main(args)


class ContinuousIntegrationRunner:
    """æŒç»­é›†æˆè¿è¡Œå™¨"""
    
    def __init__(self, project_root: str = None):
        self.project_root = Path(project_root) if project_root else Path.cwd()
        self.runner = TestSuiteRunner(project_root)
    
    def run_ci_pipeline(self) -> bool:
        """è¿è¡ŒCIæµæ°´çº¿"""
        print("Starting CI Pipeline...")
        
        # 1. ä»£ç è´¨é‡æ£€æŸ¥
        if not self._run_code_quality_checks():
            print("âŒ Code quality checks failed")
            return False
        
        # 2. è¿è¡Œæµ‹è¯•å¥—ä»¶
        results = self.runner.run_all_tests(
            include_performance=False,  # CIä¸­è·³è¿‡æ€§èƒ½æµ‹è¯•
            include_coverage=True
        )
        
        # 3. æ£€æŸ¥ç»“æœ
        if results['summary']['overall_status'] != 'passed':
            print("âŒ Test suite failed")
            return False
        
        # 4. æ£€æŸ¥è¦†ç›–ç‡
        if 'overall_coverage' in self.runner.coverage_data:
            coverage = self.runner.coverage_data['overall_coverage']
            if coverage < 80:
                print(f"âŒ Coverage too low: {coverage:.1f}% (minimum: 80%)")
                return False
        
        print("âœ… CI Pipeline passed!")
        return True
    
    def _run_code_quality_checks(self) -> bool:
        """è¿è¡Œä»£ç è´¨é‡æ£€æŸ¥"""
        print("Running code quality checks...")
        
        # æ£€æŸ¥Pythonè¯­æ³•
        try:
            result = subprocess.run([
                sys.executable, '-m', 'py_compile', 
                str(self.project_root / 'quickstock')
            ], capture_output=True, text=True)
            
            if result.returncode != 0:
                print(f"Syntax check failed: {result.stderr}")
                return False
                
        except Exception as e:
            print(f"Failed to run syntax check: {e}")
            return False
        
        print("âœ… Code quality checks passed")
        return True
    
    def generate_ci_config(self):
        """ç”ŸæˆCIé…ç½®æ–‡ä»¶"""
        # GitHub Actionsé…ç½®
        github_actions_config = """
name: QuickStock SDK Tests

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

jobs:
  test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.8, 3.9, '3.10', '3.11']

    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v3
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-cov pytest-asyncio
    
    - name: Run tests
      run: |
        python -m pytest tests/ -v --cov=quickstock --cov-report=xml
    
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella
"""
        
        github_dir = self.project_root / ".github" / "workflows"
        github_dir.mkdir(parents=True, exist_ok=True)
        
        with open(github_dir / "tests.yml", 'w') as f:
            f.write(github_actions_config)
        
        print(f"âœ… GitHub Actions config generated: {github_dir / 'tests.yml'}")


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='QuickStock SDK Test Suite Runner')
    parser.add_argument('--type', choices=['unit', 'integration', 'e2e', 'performance', 'all'], 
                       default='all', help='Test type to run')
    parser.add_argument('--coverage', action='store_true', help='Include coverage analysis')
    parser.add_argument('--performance', action='store_true', help='Include performance tests')
    parser.add_argument('--ci', action='store_true', help='Run in CI mode')
    parser.add_argument('--generate-ci', action='store_true', help='Generate CI configuration')
    parser.add_argument('--verbose', '-v', action='store_true', help='Verbose output')
    parser.add_argument('--marker', '-m', help='Run tests with specific marker')
    parser.add_argument('--pattern', '-p', help='Run specific test pattern')
    
    args = parser.parse_args()
    
    if args.generate_ci:
        ci_runner = ContinuousIntegrationRunner()
        ci_runner.generate_ci_config()
        return
    
    runner = TestSuiteRunner()
    
    if args.ci:
        ci_runner = ContinuousIntegrationRunner()
        success = ci_runner.run_ci_pipeline()
        sys.exit(0 if success else 1)
    
    if args.pattern:
        result = runner.run_specific_test(args.pattern, args.verbose)
        sys.exit(result)
    
    if args.marker:
        result = runner.run_tests_by_marker(args.marker, args.verbose)
        sys.exit(result)
    
    if args.type == 'unit':
        runner.run_unit_tests(args.verbose)
    elif args.type == 'integration':
        runner.run_integration_tests(args.verbose)
    elif args.type == 'e2e':
        runner.run_end_to_end_tests(args.verbose)
    elif args.type == 'performance':
        runner.run_performance_tests(args.verbose)
    else:  # all
        runner.run_all_tests(
            include_performance=args.performance,
            include_coverage=args.coverage
        )


if __name__ == "__main__":
    main()