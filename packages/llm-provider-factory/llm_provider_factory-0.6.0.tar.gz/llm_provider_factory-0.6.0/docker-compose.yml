version: '3.8'

services:
  # LLM Provider Factory Service
  llm-provider-factory:
    build: .
    container_name: llm_provider_factory
    environment:
      # Cloud LLM API Keys (set your own)
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY:-}
      - GOOGLE_API_KEY=${GOOGLE_API_KEY:-}
      - GOOGLE_CLOUD_PROJECT=${GOOGLE_CLOUD_PROJECT:-}
      
      # Local LLM Configuration
      - OLLAMA_BASE_URL=http://ollama:11434
      
    depends_on:
      - ollama
    networks:
      - llm_network
    volumes:
      - ./examples:/app/examples:ro
    command: >
      sh -c "
        echo 'ðŸš€ Starting LLM Provider Factory...';
        python -c 'from llm_provider import LLMProviderFactory; 
                   factory = LLMProviderFactory(); 
                   print(f\"âœ… Available providers: {list(factory._providers.keys())}\");
                   print(\"ðŸŽ‰ LLM Provider Factory is ready!\")';
        tail -f /dev/null
      "

  # Ollama Local LLM Service
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    networks:
      - llm_network
    environment:
      - OLLAMA_ORIGINS=*
    command: >
      sh -c "
        ollama serve &
        sleep 10;
        echo 'ðŸ“¥ Pulling llama3.1 model...';
        ollama pull llama3.1:latest;
        echo 'ðŸ“¥ Pulling llama2 model...';
        ollama pull llama2;
        echo 'âœ… Ollama models ready!';
        wait
      "

  # Optional: Web UI for Ollama
  ollama-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: ollama_webui
    ports:
      - "3000:8080"
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
    depends_on:
      - ollama
    networks:
      - llm_network
    volumes:
      - ollama_webui_data:/app/backend/data

volumes:
  ollama_data:
  ollama_webui_data:

networks:
  llm_network:
    driver: bridge