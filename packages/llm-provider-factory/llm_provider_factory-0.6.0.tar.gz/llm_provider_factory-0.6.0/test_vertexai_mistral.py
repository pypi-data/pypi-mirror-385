#!/usr/bin/env python3
"""
Test script for VertexAI with Mistral model
Bu script sizin orijinal yaklaÅŸÄ±mÄ±nÄ±zÄ± test eder
"""

import asyncio
import os
import sys
from pathlib import Path

# Add the src directory to the path
src_path = Path(__file__).parent / "src"
sys.path.insert(0, str(src_path))

from llm_provider import LLMProviderFactory
from llm_provider.utils.config import VertexAIConfig

async def test_mistral_with_vertexai():
    """Test Mistral model with VertexAI"""
    
    print("ğŸ§ª VertexAI Mistral Test BaÅŸlÄ±yor...")
    
    # Configuration
    config = VertexAIConfig(
        project_id="your-project-id",  # GerÃ§ek project ID'nizi buraya yazÄ±n
        location="us-central1",
        model="mistral-large-2411",
        credentials_path="/path/to/your/credentials.json",  # GerÃ§ek credentials path'ini yazÄ±n
        temperature=0.1,
        max_tokens=1000
    )
    
    try:
        # Create factory
        factory = LLMProviderFactory()
        
        # Create provider
        provider = factory.create_provider("vertexai", config)
        
        print(f"âœ… Provider oluÅŸturuldu: {provider.get_provider_info().display_name}")
        
        # Test basic response
        test_prompt = "Merhaba! Sen kimsin ve nasÄ±l yardÄ±m edebilirsin?"
        
        print(f"ğŸ“ Test prompt: {test_prompt}")
        
        # Check availability
        if not provider.is_available():
            print("âŒ Provider kullanÄ±lamÄ±yor")
            return
        
        # Generate response
        from llm_provider.settings import GenerationRequest, Message, MessageRole
        
        request = GenerationRequest(
            messages=[
                Message(role=MessageRole.SYSTEM, content="Sen yardÄ±mcÄ± bir AI asistanÄ±sÄ±n. TÃ¼rkÃ§e cevap ver."),
                Message(role=MessageRole.USER, content=test_prompt)
            ],
            max_tokens=500,
            temperature=0.1
        )
        
        print("ğŸš€ Cevap oluÅŸturuluyor...")
        response = await provider.generate(request)
        
        print(f"âœ… Cevap alÄ±ndÄ±!")
        print(f"Model: {response.model}")
        print(f"Ä°Ã§erik: {response.content}")
        print(f"KullanÄ±m: {response.usage}")
        
        # Test conversation history
        print("\nğŸ”„ Conversation history testi...")
        
        history_request = GenerationRequest(
            messages=[
                Message(role=MessageRole.SYSTEM, content="Sen yardÄ±mcÄ± bir AI asistanÄ±sÄ±n."),
                Message(role=MessageRole.USER, content="Python hakkÄ±nda bir ÅŸey sor"),
                Message(role=MessageRole.ASSISTANT, content="Python hangi alanda kullanmak istiyorsun?"),
                Message(role=MessageRole.USER, content="Web geliÅŸtirme iÃ§in")
            ],
            max_tokens=300,
            temperature=0.1
        )
        
        history_response = await provider.generate(history_request)
        print(f"âœ… History cevabÄ±: {history_response.content}")
        
    except Exception as e:
        print(f"âŒ Hata oluÅŸtu: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    print("ğŸ”§ VertexAI + Mistral Test")
    print("âš ï¸  Ã–nce credentials ve project ID'yi gÃ¼ncelleyin!")
    
    # Check if configuration is updated
    if "your-project-id" in open(__file__).read():
        print("âŒ LÃ¼tfen test scriptindeki 'your-project-id' ve credentials path'ini gÃ¼ncelleyin")
        sys.exit(1)
    
    asyncio.run(test_mistral_with_vertexai())