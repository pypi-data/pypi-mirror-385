#!/usr/bin/env python3
"""
ğŸ¦™ Ollama Provider KullanÄ±m Ã–rnekleri
Local LLM'leri nasÄ±l kullanacaÄŸÄ±nÄ±zÄ± gÃ¶sterir
"""

import asyncio
from llm_provider import LLMProviderFactory
from llm_provider.utils.config import OllamaConfig
from llm_provider.settings import GenerationRequest, Message, MessageRole


async def basic_ollama_example():
    """ğŸ§ª Temel Ollama KullanÄ±mÄ±"""
    print("ğŸ¦™ Temel Ollama Ã–rneÄŸi")
    print("=" * 40)
    
    # 1. Config oluÅŸtur
    config = OllamaConfig(
        base_url="http://localhost:11434",  # Ollama server
        model="llama3.1:latest",           # KullanÄ±lacak model
        max_tokens=150,
        temperature=0.7
    )
    
    # 2. Provider oluÅŸtur
    factory = LLMProviderFactory()
    provider = factory.create_provider("ollama", config)
    
    # 3. Basit soru sor
    request = GenerationRequest(
        prompt="Python nedir? KÄ±sa aÃ§Ä±kla.",
        max_tokens=100,
        temperature=0.7
    )
    
    response = await provider.generate(request)
    print(f"ğŸ¤– Cevap: {response.content}")
    print(f"â±ï¸ SÃ¼re: {response.usage.total_time:.2f}s" if response.usage else "")


async def conversation_example():
    """ğŸ’¬ Konversasyon Ã–rneÄŸi"""
    print("\nğŸ’¬ Konversasyon Ã–rneÄŸi")
    print("=" * 40)
    
    config = OllamaConfig(
        base_url="http://localhost:11434",
        model="llama3.1:latest",
        max_tokens=200,
        temperature=0.8
    )
    
    factory = LLMProviderFactory()
    provider = factory.create_provider("ollama", config)
    
    # Konversasyon geÃ§miÅŸi
    messages = [
        Message(role=MessageRole.USER, content="Merhaba! Ben Python Ã¶ÄŸreniyorum."),
        Message(role=MessageRole.ASSISTANT, content="Merhaba! Python Ã¶ÄŸrenmen harika. Hangi konularda yardÄ±m edebilirim?"),
        Message(role=MessageRole.USER, content="Liste comprehension nasÄ±l kullanÄ±lÄ±r?")
    ]
    
    request = GenerationRequest(
        messages=messages,
        max_tokens=200,
        temperature=0.8
    )
    
    response = await provider.generate(request)
    print(f"ğŸ¤– AI YanÄ±tÄ±: {response.content}")


async def streaming_example():
    """ğŸ“¡ Streaming Ã–rneÄŸi"""
    print("\nğŸ“¡ Streaming Ã–rneÄŸi")
    print("=" * 40)
    
    config = OllamaConfig(
        base_url="http://localhost:11434",
        model="llama3.1:latest",
        max_tokens=300,
        temperature=0.9
    )
    
    factory = LLMProviderFactory()
    provider = factory.create_provider("ollama", config)
    
    request = GenerationRequest(
        prompt="Yapay zeka hakkÄ±nda kÄ±sa bir hikaye yaz.",
        max_tokens=300,
        temperature=0.9
    )
    
    print("ğŸ¤– AI Hikayesi (CanlÄ±): ", end="", flush=True)
    
    full_response = ""
    chunk_count = 0
    
    async for chunk in provider.stream_generate(request):
        print(chunk.content, end="", flush=True)
        full_response += chunk.content
        chunk_count += 1
    
    print(f"\n\nğŸ“Š {chunk_count} chunk alÄ±ndÄ±")
    print(f"ğŸ“ Toplam {len(full_response)} karakter")


async def multiple_models_example():
    """ğŸ”„ Ã‡oklu Model Ã–rneÄŸi"""
    print("\nğŸ”„ Ã‡oklu Model KarÅŸÄ±laÅŸtÄ±rmasÄ±")
    print("=" * 40)
    
    models = ["llama3.1:latest", "llama2", "codellama"]
    prompt = "Hello world programÄ±nÄ± Python ile nasÄ±l yazarÄ±m?"
    
    factory = LLMProviderFactory()
    
    for model in models:
        try:
            print(f"\nğŸ§  Model: {model}")
            print("-" * 20)
            
            config = OllamaConfig(
                base_url="http://localhost:11434",
                model=model,
                max_tokens=100,
                temperature=0.5
            )
            
            provider = factory.create_provider("ollama", config)
            
            request = GenerationRequest(
                prompt=prompt,
                max_tokens=100,
                temperature=0.5
            )
            
            response = await provider.generate(request)
            print(f"âœ… Cevap: {response.content[:200]}...")
            
        except Exception as e:
            print(f"âŒ {model} modeli kullanÄ±lamÄ±yor: {e}")


async def code_generation_example():
    """ğŸ’» Kod Ãœretimi Ã–rneÄŸi"""
    print("\nğŸ’» Kod Ãœretimi Ã–rneÄŸi")
    print("=" * 40)
    
    config = OllamaConfig(
        base_url="http://localhost:11434",
        model="codellama",  # Kod iÃ§in Ã¶zel model
        max_tokens=300,
        temperature=0.3  # Kod iÃ§in dÃ¼ÅŸÃ¼k temperature
    )
    
    factory = LLMProviderFactory()
    
    try:
        provider = factory.create_provider("ollama", config)
        
        request = GenerationRequest(
            prompt="""
Bir Python fonksiyonu yaz:
- Fibonacci sayÄ±sÄ± hesaplar
- Recursive ve iterative versiyonlarÄ± olsun
- Docstring ekle
- Type hints kullan
""",
            max_tokens=300,
            temperature=0.3
        )
        
        response = await provider.generate(request)
        print("ğŸ¤– Ãœretilen Kod:")
        print("```python")
        print(response.content)
        print("```")
        
    except Exception as e:
        print(f"âŒ CodeLlama modeli yok, llama3.1 ile deneyelim: {e}")
        
        # Fallback to llama3.1
        config.model = "llama3.1:latest"
        provider = factory.create_provider("ollama", config)
        response = await provider.generate(request)
        print("ğŸ¤– Ãœretilen Kod (llama3.1):")
        print("```python")
        print(response.content)
        print("```")


async def advanced_configuration_example():
    """âš™ï¸ GeliÅŸmiÅŸ KonfigÃ¼rasyon"""
    print("\nâš™ï¸ GeliÅŸmiÅŸ KonfigÃ¼rasyon")
    print("=" * 40)
    
    # Ã–zel parametrelerle config
    config = OllamaConfig(
        base_url="http://localhost:11434",
        model="llama3.1:latest",
        max_tokens=500,
        temperature=0.8,
        top_p=0.9,
        top_k=40,
        repeat_penalty=1.1
    )
    
    factory = LLMProviderFactory()
    provider = factory.create_provider("ollama", config)
    
    # Ã–zel system prompt ile
    messages = [
        Message(
            role=MessageRole.SYSTEM, 
            content="Sen uzman bir Python geliÅŸtiricisisin. AÃ§Ä±k ve detaylÄ± Ã¶rnekler verirsin."
        ),
        Message(
            role=MessageRole.USER, 
            content="Django ile REST API nasÄ±l oluÅŸturulur?"
        )
    ]
    
    request = GenerationRequest(
        messages=messages,
        max_tokens=500,
        temperature=0.8
    )
    
    response = await provider.generate(request)
    print(f"ğŸ¤– Uzman CevabÄ±: {response.content}")


async def error_handling_example():
    """ğŸš¨ Hata YÃ¶netimi"""
    print("\nğŸš¨ Hata YÃ¶netimi Ã–rneÄŸi")
    print("=" * 40)
    
    # Olmayan model ile test
    config = OllamaConfig(
        base_url="http://localhost:11434",
        model="nonexistent-model",
        max_tokens=100
    )
    
    factory = LLMProviderFactory()
    
    try:
        provider = factory.create_provider("ollama", config)
        
        request = GenerationRequest(
            prompt="Test prompt",
            max_tokens=100
        )
        
        response = await provider.generate(request)
        print(f"âœ… Beklenmedik baÅŸarÄ±: {response.content}")
        
    except Exception as e:
        print(f"âŒ Beklenen hata yakalandÄ±: {type(e).__name__}: {e}")
    
    # YanlÄ±ÅŸ server adresi
    config = OllamaConfig(
        base_url="http://localhost:99999",  # YanlÄ±ÅŸ port
        model="llama3.1:latest"
    )
    
    try:
        provider = factory.create_provider("ollama", config)
        request = GenerationRequest(prompt="Test", max_tokens=50)
        response = await provider.generate(request)
        
    except Exception as e:
        print(f"âŒ BaÄŸlantÄ± hatasÄ± yakalandÄ±: {type(e).__name__}: {e}")


async def main():
    """ğŸ¯ Ana fonksiyon - TÃ¼m Ã¶rnekleri Ã§alÄ±ÅŸtÄ±r"""
    
    print("ğŸ¦™ OLLAMA PROVIDER KULLANIM Ã–RNEKLERÄ°")
    print("=" * 50)
    print("Ã–nkoÅŸul: Ollama server Ã§alÄ±ÅŸÄ±yor olmalÄ± (localhost:11434)")
    print("Model indirme: ollama pull llama3.1:latest")
    print("=" * 50)
    
    try:
        await basic_ollama_example()
        await conversation_example()
        await streaming_example()
        await multiple_models_example()
        await code_generation_example()
        await advanced_configuration_example()
        await error_handling_example()
        
        print("\nğŸ‰ TÃ¼m Ã¶rnekler tamamlandÄ±!")
        print("ğŸ’¡ Ä°pucu: Bu kodu geliÅŸtirerek kendi uygulamanÄ±zÄ± yapabilirsiniz!")
        
    except Exception as e:
        print(f"\nâŒ Genel hata: {e}")
        print("ğŸ’¡ Ollama server Ã§alÄ±ÅŸÄ±yor mu kontrol edin:")
        print("   curl http://localhost:11434/api/tags")


if __name__ == "__main__":
    asyncio.run(main())