# ðŸ¦™ Ollama Setup Rehberi

## ðŸ“¥ 1. Ollama Kurulumu

### macOS:
```bash
# Homebrew ile
brew install ollama

# Veya direkt indirin
curl -fsSL https://ollama.ai/install.sh | sh
```

### Linux:
```bash
curl -fsSL https://ollama.ai/install.sh | sh
```

### Windows:
- [ollama.ai](https://ollama.ai) adresinden indirin

## ðŸš€ 2. Ollama Server BaÅŸlatma

```bash
# Server'Ä± baÅŸlat (background'da Ã§alÄ±ÅŸÄ±r)
ollama serve

# Veya macOS'ta service olarak
brew services start ollama
```

## ðŸ“¦ 3. Model Ä°ndirme

```bash
# Ã–nerilen modeller
ollama pull llama3.1:latest     # En yeni, gÃ¼Ã§lÃ¼
ollama pull llama2              # HÄ±zlÄ±, orta boyut
ollama pull codellama           # Kod iÃ§in Ã¶zel
ollama pull mistral             # Hafif ve hÄ±zlÄ±

# Modelleri listele
ollama list
```

## ðŸ§ª 4. Test Etme

```bash
# Terminal'de test
ollama run llama3.1:latest "Python nedir?"

# API test
curl http://localhost:11434/api/tags
```

## ðŸ”§ 5. LLM Provider Factory ile KullanÄ±m

```python
from llm_provider import LLMProviderFactory
from llm_provider.utils.config import OllamaConfig

# Config
config = OllamaConfig(
    base_url="http://localhost:11434",
    model="llama3.1:latest"
)

# Provider
factory = LLMProviderFactory()
provider = factory.create_provider("ollama", config)
```

## ðŸ“Š 6. Desteklenen Modeller

| Model | Boyut | KullanÄ±m | Ã–nerilen |
|-------|-------|----------|----------|
| llama3.1:latest | ~4.7GB | Genel amaÃ§lÄ± | âœ… En iyi |
| llama2 | ~3.8GB | HÄ±zlÄ± cevaplar | âœ… BaÅŸlangÄ±Ã§ |
| codellama | ~3.8GB | Kod Ã¼retimi | ðŸ’» Kod iÃ§in |
| mistral | ~4.1GB | Matematik/Logic | ðŸ§® Analitik |
| neural-chat | ~4.1GB | Konversasyon | ðŸ’¬ Chat iÃ§in |

## âš¡ 7. Performance Ä°puÃ§larÄ±

### HÄ±z iÃ§in:
```python
config = OllamaConfig(
    model="llama2",           # Daha kÃ¼Ã§Ã¼k model
    max_tokens=100,           # KÄ±sa cevaplar
    temperature=0.3           # DÃ¼ÅŸÃ¼k creativity
)
```

### Kalite iÃ§in:
```python
config = OllamaConfig(
    model="llama3.1:latest",  # En iyi model
    max_tokens=500,           # Uzun cevaplar
    temperature=0.8,          # YÃ¼ksek creativity
    top_p=0.9
)
```

## ðŸš¨ 8. Sorun Giderme

### Server Ã§alÄ±ÅŸmÄ±yor:
```bash
# Process kontrol
ps aux | grep ollama

# Port kontrol  
lsof -i :11434

# Yeniden baÅŸlat
killall ollama
ollama serve
```

### Model yok hatasÄ±:
```bash
# Mevcut modeller
ollama list

# Model indir
ollama pull llama3.1:latest
```

### BaÄŸlantÄ± hatasÄ±:
```python
# URL kontrol
config = OllamaConfig(
    base_url="http://localhost:11434",  # DoÄŸru port
    model="llama3.1:latest"
)
```

## ðŸŽ¯ 9. Production Ä°puÃ§larÄ±

### Docker ile:
```dockerfile
FROM ollama/ollama
RUN ollama pull llama3.1:latest
EXPOSE 11434
```

### Monitoring:
```bash
# Resource kullanÄ±m
htop

# Model boyutlarÄ±
du -sh ~/.ollama/models/*
```

### API Limits:
```python
config = OllamaConfig(
    max_tokens=1000,      # Token limiti
    timeout=30,           # Timeout (saniye)
    temperature=0.7       # Deterministik iÃ§in dÃ¼ÅŸÃ¼k
)
```

## ðŸ“š 10. Ã–rnekler

DetaylÄ± Ã¶rnekler iÃ§in:
- `ollama_quickstart.py` - HÄ±zlÄ± baÅŸlangÄ±Ã§
- `ollama_examples.py` - KapsamlÄ± Ã¶rnekler
- `test_ollama_only.py` - Test dosyasÄ±

## ðŸ†˜ YardÄ±m

- [Ollama Documentation](https://github.com/ollama/ollama)
- [Model Library](https://ollama.ai/library)
- [GitHub Issues](https://github.com/sadikhanecioglu/llmfactory.py/issues)