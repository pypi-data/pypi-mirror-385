#!/usr/bin/env python3
"""
ğŸ¦™ Ollama Provider Test - Local LLM Testing
Sadece Ollama provider'Ä±nÄ± test eder
"""

import asyncio
import os
import time
from llm_provider import LLMProviderFactory
from llm_provider.utils.config import OllamaConfig
from llm_provider.settings import GenerationRequest


async def test_ollama():
    """Ollama provider'Ä±nÄ± test et"""
    
    print("ğŸ¦™ Ollama Provider Test")
    print("=" * 50)
    
    # Ollama server kontrolÃ¼
    ollama_url = "http://localhost:11434"
    print(f"ğŸ”— Ollama URL: {ollama_url}")
    
    try:
        # Config oluÅŸtur
        config = OllamaConfig(
            base_url=ollama_url,
            model="llama3.1:latest",
            max_tokens=100,
            temperature=0.7
        )
        
        print(f"ğŸ“¦ Model: {config.model}")
        print(f"ğŸ›ï¸ Max tokens: {config.max_tokens}")
        print(f"ğŸŒ¡ï¸ Temperature: {config.temperature}")
        
        # Provider oluÅŸtur
        factory = LLMProviderFactory()
        provider = factory.create_provider("ollama", config)
        
        print("\nğŸ§ª Test 1: Basic Generation")
        print("-" * 30)
        
        # Test isteÄŸi
        request = GenerationRequest(
            prompt="Merhaba! Sen kimsin? Ã‡ok kÄ±sa cevap ver.",
            max_tokens=100,
            temperature=0.7
        )
        
        start_time = time.time()
        response = await provider.generate(request)
        duration = time.time() - start_time
        
        print(f"âœ… BaÅŸarÄ±lÄ±! ({duration:.2f}s)")
        print(f"ğŸ’¬ Cevap: {response.content}")
        
        print("\nğŸ§ª Test 2: Conversation")
        print("-" * 30)
        
        # Konversasyon testi
        conv_request = GenerationRequest(
            prompt="Python hakkÄ±nda 2 cÃ¼mle sÃ¶yle.",
            max_tokens=80,
            temperature=0.5
        )
        
        start_time = time.time()
        conv_response = await provider.generate(conv_request)
        duration = time.time() - start_time
        
        print(f"âœ… BaÅŸarÄ±lÄ±! ({duration:.2f}s)")
        print(f"ğŸ’¬ Cevap: {conv_response.content}")
        
        print("\nğŸ§ª Test 3: Streaming")
        print("-" * 30)
        
        # Streaming test
        stream_request = GenerationRequest(
            prompt="AI hakkÄ±nda bir ÅŸiir yaz. KÄ±sa olsun.",
            max_tokens=120,
            temperature=0.8
        )
        
        start_time = time.time()
        
        print("ğŸ“¡ Streaming baÅŸlÄ±yor...")
        chunks = []
        async for chunk in provider.stream_generate(stream_request):
            chunks.append(chunk.content)
            print(f"ğŸ“¦ Chunk: {chunk.content}", end="", flush=True)
        
        duration = time.time() - start_time
        full_response = "".join(chunks)
        
        print(f"\nâœ… Streaming tamamlandÄ±! ({duration:.2f}s)")
        print(f"ğŸ“Š {len(chunks)} chunk alÄ±ndÄ±")
        print(f"ğŸ’¬ Tam cevap: {full_response}")
        
        print("\nğŸ‰ TÃ¼m Ollama testleri baÅŸarÄ±lÄ±!")
        
    except Exception as e:
        print(f"âŒ Hata: {e}")
        print(f"ğŸ“ Hata tÃ¼rÃ¼: {type(e).__name__}")
        
        # Ollama server kontrol Ã¶nerisi
        print("\nğŸ’¡ Ã‡Ã¶zÃ¼m Ã¶nerileri:")
        print("1. Ollama server Ã§alÄ±ÅŸÄ±yor mu kontrol edin:")
        print("   curl http://localhost:11434/api/tags")
        print("2. llama3.1:latest modeli yÃ¼klÃ¼ mÃ¼ kontrol edin:")
        print("   ollama list")
        print("3. Model yoksa indirin:")
        print("   ollama pull llama3.1:latest")


if __name__ == "__main__":
    asyncio.run(test_ollama())