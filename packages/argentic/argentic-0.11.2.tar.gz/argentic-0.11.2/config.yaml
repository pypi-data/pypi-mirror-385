llm:
  # provider: ollama # Renamed from backend
  # Ollama specific settings
  ollama_model_name: gemma3:12b-it-qat # Example: if provider is ollama
  ollama_use_chat_model: true # To distinguish between OllamaLLM and ChatOllama
  ollama_base_url: http://localhost:11434

  # Ollama Advanced Parameters
  ollama_parameters:
    # Core sampling parameters
    temperature: 0.7
    top_p: 0.9
    top_k: 40
    num_predict: 128
    repeat_penalty: 1.1
    repeat_last_n: 64

    # Advanced sampling
    tfs_z: 1.0
    typical_p: 1.0
    presence_penalty: 0.0
    frequency_penalty: 0.0

    # Context and performance
    num_ctx: 2048
    num_batch: 512
    num_gpu: 0
    main_gpu: 0
    num_thread: -1 # -1 for auto

    # Control parameters
    seed: -1 # -1 for random
    stop: [] # List of stop sequences

    # Performance optimizations
    numa: false
    use_mmap: true
    use_mlock: false

  # Llama.cpp Server specific settings
  llama_cpp_server_binary: ~/llama.cpp/build/bin/llama-server
  llama_cpp_server_args:
    - --host
    - 127.0.0.1
    - --port
    - 5000
    - -m
    - ~/llama.cpp/models/gemma-3-12b-it-q4_0.gguf
  llama_cpp_server_host: 127.0.0.1
  llama_cpp_server_port: 5000
  llama_cpp_server_auto_start: false

  # Llama.cpp Server Advanced Parameters
  llama_cpp_server_parameters:
    # Core sampling parameters
    temperature: 0.8
    top_k: 40
    top_p: 0.95
    min_p: 0.05
    n_predict: 128
    repeat_penalty: 1.1
    repeat_last_n: 64

    # Advanced sampling
    tfs_z: 1.0
    typical_p: 1.0
    presence_penalty: 0.0
    frequency_penalty: 0.0

    # Mirostat sampling
    mirostat: 0 # 0=disabled, 1=Mirostat, 2=Mirostat 2.0
    mirostat_tau: 5.0
    mirostat_eta: 0.1

    # Context management
    n_ctx: 2048
    n_keep: 0
    n_batch: 512
    cache_prompt: false

    # Control parameters
    seed: -1
    stop: []
    ignore_eos: false
    penalize_nl: true

    # Performance
    n_threads: -1 # -1 for auto
    n_gpu_layers: 0

  # Llama.cpp CLI specific settings
  llama_cpp_cli_binary: ~/llama.cpp/build/bin/llama-gemma3-cli
  llama_cpp_cli_model_path: ~/llama.cpp/models/gemma-3-12b-it-q4_0.gguf # Added for clarity
  llama_cpp_cli_args:
    - --temp
    - 0.7
    - --n-predict # llama.cpp uses --n-predict, not --n
    - 128

  # Llama.cpp CLI Advanced Parameters (converted to CLI args automatically)
  llama_cpp_cli_parameters:
    # Core sampling
    temperature: 0.8
    top_k: 40
    top_p: 0.95
    repeat_penalty: 1.1

    # Context and performance
    ctx_size: 2048
    batch_size: 512
    threads: -1
    n_gpu_layers: 0

    # Control
    seed: -1
    n_predict: 128

    # Performance optimizations
    mlock: false
    no_mmap: false


  # Google Gemini specific settings
  provider: google_gemini
  google_gemini_api_key: YOUR_GEMINI_API_KEY # Store securely, e.g., via environment variable
  google_gemini_model_name: gemini-2.0-flash # Updated to match official example

  # Google Gemini Advanced Parameters
  google_gemini_parameters:
    # Core sampling
    temperature: 0.7
    top_p: 0.95
    top_k: 40
    max_output_tokens: 2048
    # candidate_count: 1  # Uncomment only if you need multiple response candidates (1-8)

    # Control
    stop_sequences: []

    # Safety settings (optional)
    safety_settings: []
    # Example safety settings:
    # safety_settings:
    #   - category: HARM_CATEGORY_HARASSMENT
    #     threshold: BLOCK_MEDIUM_AND_ABOVE
    #   - category: HARM_CATEGORY_HATE_SPEECH
    #     threshold: BLOCK_MEDIUM_AND_ABOVE

    # Structured output (optional)
    response_mime_type: null # e.g., "application/json"
    response_schema: null # JSON schema for structured output

messaging:
  # Connection settings
  protocol: mqtt
  broker_address: localhost
  port: 1883
  keepalive: 60

  # Client IDs
  client_id: ai_agent_client
  tool_client_id: ai_tool_service
  cli_client_id: cli_client

# Agent configuration
agent:
  # System prompt for the AI agent - defines behavior and response format
  # Comment out or remove to use the built-in default system prompt
  system_prompt: |
    You are a highly capable AI assistant that MUST follow these strict response format rules:

    RESPONSE FORMATS:
    1. Tool Call Format (use when you need to use a tool):
    ```json
    {
        "type": "tool_call",
        "tool_calls": [
            {
                "tool_id": "<exact_tool_id_from_list>",
                "arguments": {
                    "<param1>": "<value1>",
                    "<param2>": "<value2>"
                }
            }
        ]
    }
    ```

    2. Direct Answer Format (use when you can answer directly without tools):
    ```json
    {
        "type": "direct",
        "content": "<your_answer_here>"
    }
    ```

    3. Tool Result Format (use ONLY after receiving results from a tool call to provide the final answer):
    ```json
    {
        "type": "tool_result",
        "tool_id": "<tool_id_of_the_executed_tool>",
        "result": "<final_answer_incorporating_tool_results_if_relevant>"
    }
    ```

    WHEN TO USE EACH FORMAT:
    1. Use "tool_call" when:
       - You need external information or actions via a tool to answer the question.
    2. Use "direct" when:
       - You can answer the question directly using your general knowledge without needing tools.
       - You need to explain a tool execution error.
    3. Use "tool_result" ONLY when:
       - You have just received results from a tool call (role: tool messages in history).
       - You are providing the final answer to the original question.
       - Incorporate the tool results into your answer *if they are relevant and helpful*. If the tool results are not helpful or empty, state that briefly and answer using your general knowledge.

    STRICT RULES:
    1. ALWAYS wrap your response in a markdown code block (```json ... ```).
    2. ALWAYS use one of the three formats above.
    3. NEVER use any other "type" value.
    4. NEVER include text outside the JSON structure.
    5. NEVER use markdown formatting inside the content/result fields.
    6. ALWAYS use the exact tool_id from the available tools list for "tool_call".
    7. ALWAYS provide complete, well-formatted JSON.
    8. ALWAYS keep responses concise but complete.

    HANDLING TOOL RESULTS:
    - If a tool call fails (you receive an error message in the tool role), respond with a "direct" answer explaining the error.
    - If you receive successful tool results (role: tool):
        - Analyze the results.
        - If the results help answer the original question, incorporate them into your final answer and use the "tool_result" format.
        - If the results are empty or not relevant to the original question, briefly state that the tool didn't provide useful information, then answer the original question using your general knowledge, still using the "tool_result" format but explaining the situation in the 'result' field.
    - If you're unsure after getting tool results, use the "tool_result" format and explain your reasoning in the 'result' field.
    - Never make another tool call immediately after receiving tool results unless absolutely necessary and clearly justified.

    {format_instructions}

# Topic structure
topics:
  # Agent command topics
  commands:
    ask_question: agent/command/ask_question
    status_request: agent/status/request

  # Agent response topics
  responses:
    answer: agent/response/answer
    status: agent/status/info

  # Logging
  log: agent/log

  # Tool communication
  tools:
    register: agent/tools/register
    call: agent/tools/call
    response_base: agent/tools/response
    status: agent/tools/status

  # Subscription mapping
  subscriptions:
    agent/command/ask_question: handle_ask_question
    agent/status/request: handle_status_request
    agent/response/answer: handle_agent_answer # Add this for CLI client
