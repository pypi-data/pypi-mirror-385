# `Model` ‚Äî Unified Interface for AI Models

The [`Model` ](../api-reference/models/model.md) class in `msgflux` serves as a **high-level factory and registry** for loading AI models across various providers and modalities. It abstracts away boilerplate code and offers a **simple, consistent API** to instantiate models like chatbots, text embedders, speakers, image and video generators, and more.

```base
pip install msgflux[openai]
```


```python
import msgflux as mf

mf.set_envs(OPENAI_API_KEY="sk-...")
model = mf.Model.chat_completion("openai/gpt-4.1-nano", temperature=0.7)
```

---

## ‚ú¶‚Çä‚Å∫ Overview

### 1. **Unified API**
No need to memorize individual client APIs or custom wrappers. Just specify the model type, path and parameters:

·Øì‚òÖ ‚û° ñ°é ‚ö°Ô∏é ‚öõ üïÆ ‚ï∞‚îà‚û§ ‚úî


```python
mf.Model.text_to_speech("openai/tts-1")
mf.Model.text_embedder("openai/text-embedding-ada-003")
```

### 2. **Supported Types**
Supports a wide range of AI capabilities:

| Type                | Description                   
|---------------------|-------------------------------
| [`chat_completion`](../models/chat_completion.md)  | Understanding and multimodal generation 
| `image_embedder`    | Generates a vector representation of an images |
| `image_text_to_image` | Image edit |
| `moderation` | Checks if the content is safe |
| `speech_to_text`  | Voice transcription |       
| `text_classifier`     | Classify text |        
| `text_embedder`     | Generates a vector representation of a text |
| `text_reranker`     | Rerank text options given a query |
| `text_to_image`    | Image Generation |
| `text_to_speech`    | Generates voice from text |

You can check all supported model types using:
```python
print(mf.Model.supported_model_types)
```

and also what are the providers availables for each type:
```python
print(mf.Model.providers_by_model_type)
```

### 3. **Resilience**

API-based models are protected by a decorator (`@model_retry`) that applies retry in case of failures.

Can manage multiple keys, this is useful in case a key becomes invalid. Separate with commas.

```python
mf.set_envs(OPENAI_API_KEY="sk-1.., sk-2..")
```

### 4. **Responses**

Each model returns a model response instance. Making the model response type explicit helps manage that response because you already know what's in that object.

The model response which can be one of:

#### 4.1 **ModelResponse**

Ideal for non-streaming tasks like embeddings, classification, speech-to-text, etc.

```python
from msgflux.models.response import ModelResponse
response = ModelResponse()
response.set_response_type("text_embedding")
response.add([1.2, 3.4]) # Can be any datatype
print(response.response_type)
print(response.consume())
```

#### 4.2 **ModelStreamResponse**

Designed for tasks where data is generated in real time ‚Äî such as text and speech generation, tool-calling, etc.

```python
from msgflux.models.response import ModelResponse
stream_response = ModelStreamResponse()
stream_response.set_response_type("text_generation")

stream_response.add("Hello ")
stream_response.first_chunk_event.set() # Informs that consumption can now begin
stream_response.add("world!")
stream_response.add(None)  # Signals the end

async for chunk in stream_response.consume():
    print(chunk, flush=True)  # ‚Üí "Hello ", after "world!"
```

### 5. **Model Info**

Some information present when serializing a model is also accessible using

```python
print(model.model_type)
```

```python
'chat_completion'
```

```python
print(model.instance_type())
```

```python
{'model_type': 'chat_completion'}
```

```python
print(model.get_model_info())
```

```python
{'model_id': 'gpt-4.1-nano', 'provider': 'openai'}
```

### 6. **Serialization**

You can export the internal state of an object from a Model

```python
model = mf.Model.chat_completion("openai/gpt-4.1-nano")
model_state = model.serialize()
print(model_state)
```

```python
{
    'msgflux_type': 'model',
    'provider': 'openai',
    'model_type': 'chat_completion',
    'state': {
        'model_id': 'gpt-4.1-nano',
        'sampling_params': {'organization': None, 'project': None},
        'sampling_run_params': {
            'max_tokens': 512,
            'temperature': None,
            'top_p': None,
            'modalities': ['text'],
            'audio': None
        }
    }
}
```

So re-create a model from a serialized object

```python
model = mf.Model.from_serialized(**model_state)
```

Internally the model is created using `__new__` and then `_initialize` method is called which initializes the internal state.