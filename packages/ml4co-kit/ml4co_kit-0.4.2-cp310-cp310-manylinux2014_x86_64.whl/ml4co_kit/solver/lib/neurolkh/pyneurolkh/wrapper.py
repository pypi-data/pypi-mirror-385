import os
import pathlib
import tempfile
import subprocess
import numpy as np
from ml4co_kit.extension import tsplib95
from ml4co_kit.solver.lib.neurolkh.pyneurolkh.alkh import neurolkh_alkh_tool_v1, neurolkh_alkh_tool_v2


def neurolkh_wrapper(
    points: np.ndarray, 
    penalty: np.ndarray,
    heatmap: np.ndarray,
    full_edge_index: np.ndarray,
    lkh_scale: int,
    lkh_max_trials: int,
    lkh_path: pathlib.Path,
    lkh_runs: int,
    lkh_seed: int,
    lkh_special: bool,
    sparse_factor: int,
    lkh_tree_cands_num: int,
    lkh_search_cands_num: int, 
    lkh_initial_period: int,
) -> np.ndarray:  
    # nodes_num and sparse factor
    nodes_num = points.shape[0]
    if nodes_num <= sparse_factor:
        raise ValueError(
            "the ``sparse_factor`` can not be equal to or larger than the number of nodes"
        )
    
    # initial candidates accroding to heatmap
    heatmap = heatmap.reshape(nodes_num, sparse_factor)
    row_index = np.argsort(-heatmap, axis=1)
    f_edge_index: np.ndarray = full_edge_index[1].reshape(nodes_num, sparse_factor)
    line_index = np.arange(nodes_num).reshape(-1, 1).repeat(sparse_factor, 1)
    candidates = f_edge_index[line_index, row_index]
    in_candidates = candidates[:, :lkh_tree_cands_num]
    in_candidates = in_candidates.astype(np.int32).reshape(-1)

    # call alkh to improve penalty
    penalty, _, _ = neurolkh_alkh_tool_v2(
        points=points,
        penalty=penalty, 
        candidates=in_candidates,
        in_candidates_num=lkh_tree_cands_num,
        out_candidates_num=lkh_search_cands_num,
        scale=1e5, 
        lr=0.02,
        initial_period=lkh_initial_period
    )
    candidates = candidates[:, :lkh_search_cands_num]
    
    # files to generate
    tmp_tsp_file = tempfile.NamedTemporaryFile(mode="w", delete=False, suffix=".tsp")
    tmp_pi_file = tempfile.NamedTemporaryFile(mode="w", delete=False, suffix=".pi")
    tmp_par_file = tempfile.NamedTemporaryFile(mode="w", delete=False, suffix=".par")
    tmp_tour_file = tempfile.NamedTemporaryFile(mode="w", delete=False, suffix=".tour")
    tmp_cand_file = tempfile.NamedTemporaryFile(mode="w", delete=False, suffix=".cand")
    tmp_log_file = tempfile.NamedTemporaryFile(mode="w", delete=False, suffix=".log")
    
    tmp_tsp_path = tmp_tsp_file.name
    tmp_pi_path = tmp_pi_file.name
    tmp_par_path = tmp_par_file.name
    tmp_tour_path = tmp_tour_file.name
    tmp_cand_path = tmp_cand_file.name
    tmp_log_path = tmp_log_file.name
    
    # generate cand file
    with open(tmp_cand_path, "w") as f:
        f.write(f"{nodes_num}\n")
        for i in range(nodes_num):
            f.write(f"{i+1} 0 {lkh_search_cands_num} ")
            for alpha_base, nodes_id in enumerate(candidates[i]):
                alpha = int(alpha_base * 100)
                f.write(f"{nodes_id+1} {alpha} ")
            f.write("\n")
        f.write("-1\nEOF\n")
    
    # generate pi file
    with open(tmp_pi_path, "w") as f:
        f.write(f"{nodes_num}\n")
        for i in range(nodes_num):
            pi = int(penalty[i] * lkh_scale)
            f.write(f"{i+1} {pi}\n")
        f.write("-1\nEOF\n")
        
    # generate par file
    with open(tmp_par_path, "w") as f:
        f.write(f"PROBLEM_FILE = {tmp_tsp_path}\n")
        f.write(f"RUNS = {lkh_runs}\n")
        f.write(f"MAX_TRIALS = {lkh_max_trials}\n")
        f.write(f"TOUR_FILE = {tmp_tour_path}\n")
        f.write(f"MAX_CANDIDATES = {lkh_search_cands_num}\n")
        f.write(f"SEED = {lkh_seed}\n")
        f.write(f"PI_FILE = {tmp_pi_path}\n")
        f.write(f"CANDIDATE_FILE = {tmp_cand_path}\n")
        if lkh_special:
            f.write("SPECIAL\n")

    # Generate TSP File
    with open(tmp_tsp_path, "w") as f:
        f.write(f"NAME : Generated by ML4CO-Kit\n")
        f.write(f"COMMENT : Generated by ML4CO-Kit\n")
        f.write("TYPE : TSP\n")
        f.write(f"DIMENSION : {nodes_num}\n")
        f.write(f"EDGE_WEIGHT_TYPE : EUC_2D\n")
        f.write("NODE_COORD_SECTION\n")
        for i in range(nodes_num):
            x, y = points[i] * lkh_scale
            x, y = int(x), int(y)
            f.write(f"{i+1} {x} {y}\n")
        f.write("EOF\n")

    # Use LKH Solver
    with open(tmp_log_path, "w") as f:
        subprocess.check_call([lkh_path, tmp_par_path], stdout=f)
        
    # read solution from file
    tour = tsplib95.load(tmp_tour_path).tours[0]
    tour = np.array(tour) - 1
    tour = np.append(tour, 0)
    
    # delete tmp files
    tmp_file_list = [
        tmp_tsp_path, tmp_pi_path, tmp_par_path, 
        tmp_log_path, tmp_tour_path, tmp_cand_path
    ]
    for file_path in tmp_file_list:
        if os.path.exists(file_path):
            os.remove(file_path)
            
    return tour


def alkh_wrapper(
    points: np.ndarray, 
    lkh_tree_cands_num: int, 
    lkh_search_cands_num: int, 
    lkh_scale: int,
    lkh_max_trials: int,
    lkh_path: pathlib.Path,
    lkh_runs: int,
    lkh_seed: int,
    lkh_special: bool,
    lkh_initial_period: int
) -> np.ndarray:  
    # nodes_num and sparse factor
    nodes_num = points.shape[0]

    # call alkh to improve penalty
    penalty = np.zeros(shape=(nodes_num,))
    penalty, _, _ = neurolkh_alkh_tool_v1(
        points=points,
        penalty=penalty, 
        in_candidates_num=lkh_tree_cands_num,
        out_candidates_num=lkh_search_cands_num,
        scale=1e5, 
        lr=0.02, 
        initial_period=lkh_initial_period
    )

    # files to generate
    tmp_tsp_file = tempfile.NamedTemporaryFile(mode="w", delete=False, suffix=".tsp")
    tmp_pi_file = tempfile.NamedTemporaryFile(mode="w", delete=False, suffix=".pi")
    tmp_par_file = tempfile.NamedTemporaryFile(mode="w", delete=False, suffix=".par")
    tmp_tour_file = tempfile.NamedTemporaryFile(mode="w", delete=False, suffix=".tour")
    tmp_log_file = tempfile.NamedTemporaryFile(mode="w", delete=False, suffix=".log")
    
    tmp_tsp_path = tmp_tsp_file.name
    tmp_pi_path = tmp_pi_file.name
    tmp_par_path = tmp_par_file.name
    tmp_tour_path = tmp_tour_file.name
    tmp_log_path = tmp_log_file.name

    # generate pi file
    with open(tmp_pi_path, "w") as f:
        f.write(f"{nodes_num}\n")
        for i in range(nodes_num):
            pi = int(penalty[i] * lkh_scale)
            f.write(f"{i+1} {pi}\n")
        f.write("-1\nEOF\n")
        
    # generate par file
    with open(tmp_par_path, "w") as f:
        f.write(f"PROBLEM_FILE = {tmp_tsp_path}\n")
        f.write(f"RUNS = {lkh_runs}\n")
        f.write(f"MAX_TRIALS = {lkh_max_trials}\n")
        f.write(f"TOUR_FILE = {tmp_tour_path}\n")
        f.write(f"MAX_CANDIDATES = {lkh_search_cands_num}\n")
        f.write(f"SEED = {lkh_seed}\n")
        f.write(f"PI_FILE = {tmp_pi_path}\n")
        if lkh_special:
            f.write("SPECIAL\n")

    # Generate TSP File
    with open(tmp_tsp_path, "w") as f:
        f.write(f"NAME : Generated by ML4CO-Kit\n")
        f.write(f"COMMENT : Generated by ML4CO-Kit\n")
        f.write("TYPE : TSP\n")
        f.write(f"DIMENSION : {nodes_num}\n")
        f.write(f"EDGE_WEIGHT_TYPE : EUC_2D\n")
        f.write("NODE_COORD_SECTION\n")
        for i in range(nodes_num):
            x, y = points[i] * lkh_scale
            x, y = int(x), int(y)
            f.write(f"{i+1} {x} {y}\n")
        f.write("EOF\n")

    # Use LKH Solver
    with open(tmp_log_path, "w") as f:
        subprocess.check_call([lkh_path, tmp_par_path], stdout=f)
        
    # read solution from file
    tour = tsplib95.load(tmp_tour_path).tours[0]
    tour = np.array(tour) - 1
    tour = np.append(tour, 0)

    # delete tmp files
    tmp_file_list = [
        tmp_tsp_path, tmp_pi_path, tmp_par_path, tmp_log_path, tmp_tour_path
    ]
    for file_path in tmp_file_list:
        if os.path.exists(file_path):
            os.remove(file_path)
            
    return tour