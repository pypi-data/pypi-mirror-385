"""
Utility functions for S3.
"""

import logging
import subprocess
import tempfile

import pandas as pd
import requests
import s3fs
import trimesh

logger = logging.getLogger(__name__)

s3 = s3fs.S3FileSystem(anon=True)  # All on public bucket

S3_PUBLIC_URL_BASE = "https://aind-scratch-data.s3.us-west-2.amazonaws.com/aind-patchseq-data"
S3_PATH_BASE = "aind-scratch-data/aind-patchseq-data"


def sync_directory(local_dir, destination, if_copy=False):
    """
    Sync the local directory with the given S3 destination using aws s3 sync.
    Returns a status string based on the command output.
    """
    try:
        if not destination.startswith("s3://"):
            destination = "s3://" + destination

        if if_copy:
            # Run aws s3 cp command and capture the output
            result = subprocess.run(
                ["aws", "s3", "cp", local_dir, destination], capture_output=True, text=True
            )
        else:
            # Run aws s3 sync command and capture the output
            result = subprocess.run(
                ["aws", "s3", "sync", local_dir, destination], capture_output=True, text=True
            )
        output = result.stdout + result.stderr

        # Check output: if "upload:" appears, files were sent;
        # otherwise, assume that nothing needed uploading.
        if "upload:" in output:
            logger.info(f"Uploaded {local_dir} to {destination}!")
            return "successfully uploaded"
        else:
            logger.info(output)
            logger.info(f"Already exists, skip {local_dir}.")
            return "already exists, skip"
    except Exception as e:
        return f"error during sync: {e}"


def check_s3_public_url_exists(s3_url: str) -> bool:
    """Check if a given s3 url exists."""
    response = requests.get(s3_url)
    return response.status_code == 200


def get_public_url_sweep(ephys_roi_id: str, sweep_number: int) -> str:
    """Get the public URL for a sweep."""

    s3_sweep = (
        f"{S3_PUBLIC_URL_BASE}/efel/plots/{ephys_roi_id}/{ephys_roi_id}_sweep_{sweep_number}.png"
    )
    s3_spikes = (
        f"{S3_PUBLIC_URL_BASE}/efel/plots/{ephys_roi_id}/"
        f"{ephys_roi_id}_sweep_{sweep_number}_spikes.png"
    )

    # Check if the file exists on s3 public
    urls = {}
    if check_s3_public_url_exists(s3_sweep):
        urls["sweep"] = s3_sweep
    if check_s3_public_url_exists(s3_spikes):
        urls["spikes"] = s3_spikes
    return urls


def get_public_efel_cell_level_stats():
    """Get the cell level stats that are generated by eFEL (and merged with the metadata)."""
    csv_url = f"{S3_PUBLIC_URL_BASE}/efel/cell_stats/cell_level_stats.csv"
    if check_s3_public_url_exists(csv_url):
        return pd.read_csv(csv_url)
    else:
        raise FileNotFoundError(f"Cell level stats CSV file not found at {csv_url}")


def get_public_url_cell_summary(ephys_roi_id: str, if_check_exists: bool = True) -> str:
    """Get the public URL for a cell summary plot."""
    s3_url = f"{S3_PUBLIC_URL_BASE}/efel/cell_stats/{ephys_roi_id}_cell_summary.png"
    if if_check_exists:
        if check_s3_public_url_exists(s3_url):
            return s3_url
        else:
            return None
    else:
        return s3_url


def get_public_representative_spikes() -> pd.DataFrame:
    """Get the representative spikes for a cell."""
    s3_url = f"{S3_PUBLIC_URL_BASE}/efel/cell_stats/cell_level_spike_waveforms.pkl"
    if check_s3_public_url_exists(s3_url):
        return pd.read_pickle(s3_url)
    else:
        raise FileNotFoundError(f"Pickle file not found at {s3_url}")


def get_public_seq_preselected() -> pd.DataFrame:
    """Get the preselected sequencing data from S3.

    Returns:
        pd.DataFrame: DataFrame containing the preselected sequencing data.

    Raises:
        FileNotFoundError: If the CSV file is not found at the expected S3 URL.
    """
    s3_url = f"{S3_PUBLIC_URL_BASE}/seq/seq_preselected.csv"
    if check_s3_public_url_exists(s3_url):
        logger.info(f"Loading sequencing data from {s3_url}")
        return pd.read_csv(s3_url)
    else:
        raise FileNotFoundError(f"Sequencing data CSV file not found at {s3_url}")


def get_public_mapmycells(filename="mapmycells_20250618.csv"):
    """
    Load Yoh's MapMyCells result from public S3
    """
    s3_path = f"{S3_PATH_BASE}/seq/{filename}"
    try:
        with s3.open(s3_path, "rb") as f:
            # Skip the first four rows
            df = pd.read_csv(f, skiprows=4)

            # Add a new column "subclass_category" based on if "subclass_name" == "251 NTS Dbh Glut"
            df["subclass_category"] = df["subclass_name"].apply(
                lambda x: "251 NTS Dbh Glut" if x == "251 NTS Dbh Glut" else "Non-Dbh cells"
            )
            return df
    except Exception as e:
        raise FileNotFoundError(f"CSV file not found at {s3_path}: {str(e)}")


def get_public_morphology(filename="LC_patchseq_RawFeatureWide.csv"):
    """
    Load morphology features from public S3
    """
    s3_path = f"{S3_PATH_BASE}/morphology/{filename}"
    try:
        with s3.open(s3_path, "rb") as f:
            # Skip the first four rows
            df = pd.read_csv(f)
            return df
    except Exception as e:
        raise FileNotFoundError(f"CSV file not found at {s3_path}: {str(e)}")


def load_dict_from_hdf5(filename: str):
    """
    Load a dictionary of DataFrames from an HDF5 file using pandas.HDFStore.

    Args:
        filename: path to .h5 file

    Returns:
        dict: Dictionary of DataFrames
    """
    with pd.HDFStore(filename, mode="r") as store:
        dict_key = [key.replace("/", "") for key in store.keys()]
        return {key: store[key] for key in dict_key}


def load_efel_features_from_roi(roi_id: str, if_from_s3=False):
    """
    Load eFEL features from ROI ID.

    Args:
        roi_id: The ROI ID to load features for
        if_from_s3: If True, load from S3 instead of local file

    Returns:
        Dictionary of DataFrames containing eFEL features
    """
    if if_from_s3:
        s3_path = f"{S3_PATH_BASE}/efel/features/{roi_id}_efel.h5"
        with s3.open(s3_path, "rb") as f:
            with tempfile.NamedTemporaryFile(suffix=".h5") as tmp_file:
                tmp_file.write(f.read())
                tmp_file.flush()
                logger.info(f"Loaded eFEL features from {s3_path} to {tmp_file.name}")
                return load_dict_from_hdf5(tmp_file.name)
    else:
        from LCNE_patchseq_analysis import RESULTS_DIRECTORY

        filename = f"{RESULTS_DIRECTORY}/features/{roi_id}_efel.h5"
        return load_dict_from_hdf5(filename)


def load_mesh_from_s3(mesh_filename="250513_LC_core_67_mesh_shrunk.obj"):
    """
    Load a mesh file from S3 using trimesh.

    Args:
        mesh_filename: The filename of the mesh (e.g., "250513_LC_core_67_mesh_shrunk.obj")

    Returns:
        trimesh.Trimesh: The loaded mesh object

    Raises:
        FileNotFoundError: If the mesh file is not found at the expected S3 path
    """
    s3_path = f"{S3_PATH_BASE}/mesh/{mesh_filename}"
    try:
        with s3.open(s3_path, "rb") as f:
            with tempfile.NamedTemporaryFile(suffix=".obj", delete=False) as tmp_file:
                tmp_file.write(f.read())
                tmp_file.flush()
                logger.info(f"Loaded mesh from {s3_path} to {tmp_file.name}")
                mesh = trimesh.load_mesh(tmp_file.name)
                return mesh
    except Exception as e:
        raise FileNotFoundError(f"Mesh file not found at {s3_path}: {str(e)}")


if __name__ == "__main__":
    # print(get_public_url_sweep("1212546732", 46))
    print(get_public_efel_cell_level_stats())

    df_test_mapmycells = get_public_mapmycells()
    print(df_test_mapmycells.head())

    # Example of loading a mesh from S3
    mesh = load_mesh_from_s3()
    print(f"Loaded mesh with {len(mesh.vertices)} vertices and {len(mesh.faces)} faces")
