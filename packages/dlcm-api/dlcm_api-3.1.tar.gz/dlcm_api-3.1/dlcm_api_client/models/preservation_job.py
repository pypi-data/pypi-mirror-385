# coding: utf-8

"""
    DLCM Solution API

    Repository for Research Datasets

    The version of the OpenAPI document: 3.1.1-SNAPSHOT
    Contact: admin@dlcm.ch
    Generated by OpenAPI Generator (https://openapi-generator.tech)

    Do not edit the class manually.
"""  # noqa: E501


from __future__ import annotations
import pprint
import re  # noqa: F401
import json

from pydantic import BaseModel, ConfigDict, Field, StrictBool, StrictInt, StrictStr, field_validator
from typing import Any, ClassVar, Dict, List, Optional
from typing_extensions import Annotated
from dlcm_api_client.models.change_info import ChangeInfo
from dlcm_api_client.models.job_scheduling import JobScheduling
from dlcm_api_client.models.link import Link
from typing import Optional, Set
from typing_extensions import Self

class PreservationJob(BaseModel):
    """
    The preservation job allows to run manage mass processing of archives. There are different types of job: - ARCHIVE_CHECK => Run the archive check on all archives - ARCHIVE_PRELOAD_BIG => Preload the archives bigger than 4GB to prepare DIP - ARCHIVE_PRELOAD_SMALL => Preload the archives smaller than 4GB to prepare DIP - CHECK_COMPLIANCE_LEVEL => coming soon - CLEAN_SUBMISSION => Purge deposits & SIP when archives are completed and replicated based on submission policy - SIMPLE_CLEAN_SUBMISSION => Purge deposits & SIP when archives are completed without checking replication based on submission policy - DISPOSAL => Trigger the disposal process - FIXITY => Run the fixity check for all archives - METADATA_MIGRATION => Metadata Version Upgrade - PURGE_ORDER => Purge order data when order completed - PURGE_SUBMISSION_TEMP_FILES => Purge temporary files of submission process - REINDEX => Re-index all archives on main storage - REINDEX_ALL => Re-index all archives on all storages - RELOAD => Reload all archives from the storage - RELOAD_INGEST => Reload all SIP & deposits from the archives - REPLICATION => Run the replication process on different storage nodes - REPLICATION_CHECK => Run the replication check on all archives 
    """ # noqa: E501
    creation: Optional[ChangeInfo] = None
    last_update: Optional[ChangeInfo] = Field(default=None, alias="lastUpdate")
    res_id: Optional[Annotated[str, Field(min_length=0, strict=True, max_length=50)]] = Field(default=None, description="The identifier of the resource. The default format is a Universally Unique IDentifier (UUID).", alias="resId")
    enable: Optional[StrictBool] = Field(default=None, description="If the preservation job is enable.")
    job_recurrence: Optional[StrictStr] = Field(default=None, description="The recurrence of the preservation job.", alias="jobRecurrence")
    job_type: Optional[StrictStr] = Field(default=None, description="Preservation job type", alias="jobType")
    name: Optional[Annotated[str, Field(min_length=1, strict=True, max_length=255)]] = Field(default=None, description="The name of the preservation job.")
    scheduling: Optional[JobScheduling] = None
    max_items: Optional[StrictInt] = Field(default=None, description="The maximum items to process of the preservation job. The 0 value means no limit => all items. ", alias="maxItems")
    parameters: Optional[StrictStr] = Field(default=None, description="The parameters of the preservation job. The format is 'param1=value1&param2=value2&...&paramN=valueN. Parameters (not exhaustive list): - filter on metadata version: info.metadataVersion=<value> - filter on org. unit: info.organizationalUnitId=<value> - filter on archive size: archiveSize=<value> - filter on archive unit: archivalUnit=true - sort in last modification: sort=lastUpdate.when,asc ")
    last_execution_status: Optional[StrictStr] = Field(default=None, description="The status of the last job execution.", alias="lastExecutionStatus")
    execution_number: Optional[StrictInt] = Field(default=None, description="The number of the preservation job executions.", alias="executionNumber")
    links: Optional[Dict[str, Link]] = Field(default=None, alias="_links")
    __properties: ClassVar[List[str]] = ["creation", "lastUpdate", "resId", "enable", "jobRecurrence", "jobType", "name", "scheduling", "maxItems", "parameters", "lastExecutionStatus", "executionNumber", "_links"]

    @field_validator('job_recurrence')
    def job_recurrence_validate_enum(cls, value):
        """Validates the enum"""
        if value is None:
            return value

        if value not in set(['DAILY', 'MONTHLY', 'ONCE', 'WEEKLY', 'YEARLY']):
            raise ValueError("must be one of enum values ('DAILY', 'MONTHLY', 'ONCE', 'WEEKLY', 'YEARLY')")
        return value

    @field_validator('job_type')
    def job_type_validate_enum(cls, value):
        """Validates the enum"""
        if value is None:
            return value

        if value not in set(['ARCHIVE_CHECK', 'ARCHIVE_PRELOAD_BIG', 'ARCHIVE_PRELOAD_SMALL', 'CHECK_COMPLIANCE_LEVEL', 'CLEAN_SUBMISSION', 'COMPLIANCE_LEVEL_UPDATE', 'DISPOSAL', 'FIXITY', 'METADATA_MIGRATION', 'PURGE_ORDER', 'PURGE_SUBMISSION_TEMP_FILES', 'REINDEX_ALL', 'REINDEX', 'RELOAD', 'RELOAD_INGEST', 'REPLICATION_CHECK', 'REPLICATION', 'SIMPLE_CLEAN_SUBMISSION']):
            raise ValueError("must be one of enum values ('ARCHIVE_CHECK', 'ARCHIVE_PRELOAD_BIG', 'ARCHIVE_PRELOAD_SMALL', 'CHECK_COMPLIANCE_LEVEL', 'CLEAN_SUBMISSION', 'COMPLIANCE_LEVEL_UPDATE', 'DISPOSAL', 'FIXITY', 'METADATA_MIGRATION', 'PURGE_ORDER', 'PURGE_SUBMISSION_TEMP_FILES', 'REINDEX_ALL', 'REINDEX', 'RELOAD', 'RELOAD_INGEST', 'REPLICATION_CHECK', 'REPLICATION', 'SIMPLE_CLEAN_SUBMISSION')")
        return value

    @field_validator('last_execution_status')
    def last_execution_status_validate_enum(cls, value):
        """Validates the enum"""
        if value is None:
            return value

        if value not in set(['COMPLETED', 'CREATED', 'IN_ERROR', 'IN_PROGRESS', 'READY']):
            raise ValueError("must be one of enum values ('COMPLETED', 'CREATED', 'IN_ERROR', 'IN_PROGRESS', 'READY')")
        return value

    model_config = ConfigDict(
        populate_by_name=True,
        validate_assignment=True,
        protected_namespaces=(),
    )


    def to_str(self) -> str:
        """Returns the string representation of the model using alias"""
        return pprint.pformat(self.model_dump(by_alias=True))

    def to_json(self) -> str:
        """Returns the JSON representation of the model using alias"""
        # TODO: pydantic v2: use .model_dump_json(by_alias=True, exclude_unset=True) instead
        return json.dumps(self.to_dict())

    @classmethod
    def from_json(cls, json_str: str) -> Optional[Self]:
        """Create an instance of PreservationJob from a JSON string"""
        return cls.from_dict(json.loads(json_str))

    def to_dict(self) -> Dict[str, Any]:
        """Return the dictionary representation of the model using alias.

        This has the following differences from calling pydantic's
        `self.model_dump(by_alias=True)`:

        * `None` is only added to the output dict for nullable fields that
          were set at model initialization. Other fields with value `None`
          are ignored.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        """
        excluded_fields: Set[str] = set([
            "last_execution_status",
            "execution_number",
        ])

        _dict = self.model_dump(
            by_alias=True,
            exclude=excluded_fields,
            exclude_none=True,
        )
        # override the default output from pydantic by calling `to_dict()` of creation
        if self.creation:
            _dict['creation'] = self.creation.to_dict()
        # override the default output from pydantic by calling `to_dict()` of last_update
        if self.last_update:
            _dict['lastUpdate'] = self.last_update.to_dict()
        # override the default output from pydantic by calling `to_dict()` of scheduling
        if self.scheduling:
            _dict['scheduling'] = self.scheduling.to_dict()
        # override the default output from pydantic by calling `to_dict()` of each value in links (dict)
        _field_dict = {}
        if self.links:
            for _key_links in self.links:
                if self.links[_key_links]:
                    _field_dict[_key_links] = self.links[_key_links].to_dict()
            _dict['_links'] = _field_dict
        return _dict

    @classmethod
    def from_dict(cls, obj: Optional[Dict[str, Any]]) -> Optional[Self]:
        """Create an instance of PreservationJob from a dict"""
        if obj is None:
            return None

        if not isinstance(obj, dict):
            return cls.model_validate(obj)

        _obj = cls.model_validate({
            "creation": ChangeInfo.from_dict(obj["creation"]) if obj.get("creation") is not None else None,
            "lastUpdate": ChangeInfo.from_dict(obj["lastUpdate"]) if obj.get("lastUpdate") is not None else None,
            "resId": obj.get("resId"),
            "enable": obj.get("enable"),
            "jobRecurrence": obj.get("jobRecurrence"),
            "jobType": obj.get("jobType"),
            "name": obj.get("name"),
            "scheduling": JobScheduling.from_dict(obj["scheduling"]) if obj.get("scheduling") is not None else None,
            "maxItems": obj.get("maxItems"),
            "parameters": obj.get("parameters"),
            "lastExecutionStatus": obj.get("lastExecutionStatus"),
            "executionNumber": obj.get("executionNumber"),
            "_links": dict(
                (_k, Link.from_dict(_v))
                for _k, _v in obj["_links"].items()
            )
            if obj.get("_links") is not None
            else None
        })
        return _obj


