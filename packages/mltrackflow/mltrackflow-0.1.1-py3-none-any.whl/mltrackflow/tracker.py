"""
Deney takibi ve yÃ¶netimi iÃ§in ana modÃ¼l.

Bu modÃ¼l, ML deneylerini kaydetme, izleme ve karÅŸÄ±laÅŸtÄ±rma iÅŸlevlerini saÄŸlar.
"""

import os
import json
import hashlib
import time
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Union
import warnings

import numpy as np
import pandas as pd
import joblib
from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    mean_squared_error,
    mean_absolute_error,
    r2_score,
    classification_report,
)


class ExperimentTracker:
    """
    ML deneylerini takip etmek iÃ§in ana sÄ±nÄ±f.
    
    Bu sÄ±nÄ±f, model eÄŸitimi sÄ±rasÄ±nda parametreleri, metrikleri ve modelleri
    otomatik olarak kaydeder ve organize eder.
    
    Attributes:
        experiment_name: Deney adÄ±
        storage_path: KayÄ±t klasÃ¶rÃ¼ yolu
        auto_log: Otomatik loglama aktif mi?
    
    Example:
        >>> tracker = ExperimentTracker(experiment_name="my_experiment")
        >>> with tracker.start_run(run_name="test_1"):
        ...     tracker.log_params({"lr": 0.01})
        ...     tracker.log_metric("accuracy", 0.95)
    """
    
    def __init__(
        self,
        experiment_name: str = "default_experiment",
        storage_path: str = "./experiments",
        auto_log: bool = True,
        log_data_hash: bool = True,
        verbose: bool = True,
    ):
        """
        Args:
            experiment_name: Deney adÄ±
            storage_path: Deneylerin kaydedileceÄŸi klasÃ¶r
            auto_log: Otomatik loglama aktif mi?
            log_data_hash: Veri hash'ini kaydet mi?
            verbose: DetaylÄ± Ã§Ä±ktÄ± gÃ¶ster mi?
        """
        self.experiment_name = experiment_name
        self.storage_path = Path(storage_path)
        self.auto_log = auto_log
        self.log_data_hash = log_data_hash
        self.verbose = verbose
        
        # Deney klasÃ¶rÃ¼nÃ¼ oluÅŸtur
        self.experiment_path = self.storage_path / experiment_name
        self.experiment_path.mkdir(parents=True, exist_ok=True)
        
        # Metadata dosyasÄ±
        self.metadata_file = self.experiment_path / "experiment_metadata.json"
        self._load_or_create_metadata()
        
        # Aktif run bilgileri
        self.current_run = None
        self.current_run_data = None
        
        if self.verbose:
            print(f"âœ¨ ExperimentTracker baÅŸlatÄ±ldÄ±: {experiment_name}")
            print(f"ðŸ“ KayÄ±t yolu: {self.experiment_path}")
    
    def _load_or_create_metadata(self):
        """Deney metadata'sÄ±nÄ± yÃ¼kle veya oluÅŸtur."""
        if self.metadata_file.exists():
            with open(self.metadata_file, 'r', encoding='utf-8') as f:
                self.metadata = json.load(f)
        else:
            self.metadata = {
                "experiment_name": self.experiment_name,
                "created_at": datetime.now().isoformat(),
                "runs": {},
                "total_runs": 0,
            }
            self._save_metadata()
    
    def _save_metadata(self):
        """Metadata'yÄ± kaydet."""
        with open(self.metadata_file, 'w', encoding='utf-8') as f:
            json.dump(self.metadata, f, indent=2, ensure_ascii=False)
    
    def start_run(self, run_name: Optional[str] = None) -> 'RunContext':
        """
        Yeni bir run baÅŸlat.
        
        Args:
            run_name: Run adÄ± (opsiyonel, otomatik oluÅŸturulur)
        
        Returns:
            RunContext: Context manager
        
        Example:
            >>> with tracker.start_run("test_run"):
            ...     # EÄŸitim kodu
            ...     pass
        """
        if run_name is None:
            run_name = f"run_{self.metadata['total_runs'] + 1}_{int(time.time())}"
        
        return RunContext(self, run_name)
    
    def _initialize_run(self, run_name: str):
        """Run'Ä± baÅŸlat (internal)."""
        self.current_run = run_name
        self.current_run_data = {
            "run_name": run_name,
            "start_time": datetime.now().isoformat(),
            "end_time": None,
            "duration_seconds": None,
            "params": {},
            "metrics": {},
            "tags": {},
            "artifacts": [],
            "status": "running",
        }
        
        # Run klasÃ¶rÃ¼nÃ¼ oluÅŸtur
        run_path = self.experiment_path / run_name
        run_path.mkdir(exist_ok=True)
        
        if self.verbose:
            print(f"\nðŸš€ Run baÅŸlatÄ±ldÄ±: {run_name}")
    
    def _finalize_run(self):
        """Run'Ä± sonlandÄ±r (internal)."""
        if self.current_run_data is None:
            return
        
        end_time = datetime.now()
        self.current_run_data["end_time"] = end_time.isoformat()
        
        # SÃ¼reyi hesapla
        start = datetime.fromisoformat(self.current_run_data["start_time"])
        duration = (end_time - start).total_seconds()
        self.current_run_data["duration_seconds"] = duration
        self.current_run_data["status"] = "completed"
        
        # Metadata'ya ekle
        self.metadata["runs"][self.current_run] = self.current_run_data
        self.metadata["total_runs"] += 1
        self._save_metadata()
        
        # Run verilerini kaydet
        run_file = self.experiment_path / self.current_run / "run_data.json"
        with open(run_file, 'w', encoding='utf-8') as f:
            json.dump(self.current_run_data, f, indent=2, ensure_ascii=False)
        
        if self.verbose:
            print(f"âœ… Run tamamlandÄ±: {self.current_run}")
            print(f"â±ï¸  SÃ¼re: {duration:.2f} saniye")
        
        self.current_run = None
        self.current_run_data = None
    
    def log_params(self, params: Dict[str, Any]):
        """
        Parametreleri kaydet.
        
        Args:
            params: Parametre sÃ¶zlÃ¼ÄŸÃ¼
        
        Example:
            >>> tracker.log_params({"learning_rate": 0.01, "epochs": 100})
        """
        if self.current_run_data is None:
            warnings.warn("Aktif run yok. Ã–nce start_run() Ã§aÄŸÄ±rÄ±n.")
            return
        
        self.current_run_data["params"].update(params)
        
        if self.verbose:
            print(f"ðŸ“ Parametreler kaydedildi: {list(params.keys())}")
    
    def log_metric(self, key: str, value: Union[int, float]):
        """
        Tek bir metrik kaydet.
        
        Args:
            key: Metrik adÄ±
            value: Metrik deÄŸeri
        
        Example:
            >>> tracker.log_metric("accuracy", 0.95)
        """
        if self.current_run_data is None:
            warnings.warn("Aktif run yok. Ã–nce start_run() Ã§aÄŸÄ±rÄ±n.")
            return
        
        self.current_run_data["metrics"][key] = value
        
        if self.verbose:
            print(f"ðŸ“Š Metrik kaydedildi: {key} = {value:.4f}")
    
    def log_metrics(self, metrics: Dict[str, Union[int, float]]):
        """
        Birden fazla metrik kaydet.
        
        Args:
            metrics: Metrik sÃ¶zlÃ¼ÄŸÃ¼
        
        Example:
            >>> tracker.log_metrics({"accuracy": 0.95, "f1_score": 0.93})
        """
        for key, value in metrics.items():
            self.log_metric(key, value)
    
    def log_model_metrics(
        self,
        model,
        X_test,
        y_test,
        task_type: str = "auto",
    ):
        """
        Model metriklerini otomatik hesapla ve kaydet.
        
        Args:
            model: EÄŸitilmiÅŸ model
            X_test: Test Ã¶zellikleri
            y_test: Test hedef deÄŸerleri
            task_type: 'classification', 'regression' veya 'auto'
        
        Example:
            >>> tracker.log_model_metrics(model, X_test, y_test)
        """
        y_pred = model.predict(X_test)
        
        # Task tipini otomatik belirle
        if task_type == "auto":
            if hasattr(model, "predict_proba"):
                task_type = "classification"
            else:
                # Hedef deÄŸerlere bak
                if len(np.unique(y_test)) < 20:
                    task_type = "classification"
                else:
                    task_type = "regression"
        
        metrics = {}
        
        if task_type == "classification":
            metrics["accuracy"] = accuracy_score(y_test, y_pred)
            metrics["precision"] = precision_score(y_test, y_pred, average='weighted', zero_division=0)
            metrics["recall"] = recall_score(y_test, y_pred, average='weighted', zero_division=0)
            metrics["f1_score"] = f1_score(y_test, y_pred, average='weighted', zero_division=0)
            
            # Classification report'u kaydet
            report = classification_report(y_test, y_pred)
            report_file = self.experiment_path / self.current_run / "classification_report.txt"
            with open(report_file, 'w') as f:
                f.write(report)
            
        elif task_type == "regression":
            metrics["mse"] = mean_squared_error(y_test, y_pred)
            metrics["rmse"] = np.sqrt(metrics["mse"])
            metrics["mae"] = mean_absolute_error(y_test, y_pred)
            metrics["r2_score"] = r2_score(y_test, y_pred)
        
        self.log_metrics(metrics)
        
        if self.verbose:
            print(f"ðŸ“ˆ {task_type.capitalize()} metrikleri otomatik kaydedildi")
    
    def save_model(self, model, model_name: str):
        """
        Modeli kaydet.
        
        Args:
            model: Kaydedilecek model
            model_name: Model dosya adÄ±
        
        Example:
            >>> tracker.save_model(model, "my_model")
        """
        if self.current_run_data is None:
            warnings.warn("Aktif run yok. Ã–nce start_run() Ã§aÄŸÄ±rÄ±n.")
            return
        
        model_path = self.experiment_path / self.current_run / f"{model_name}.joblib"
        joblib.dump(model, model_path)
        
        self.current_run_data["artifacts"].append({
            "type": "model",
            "name": model_name,
            "path": str(model_path),
        })
        
        if self.verbose:
            print(f"ðŸ’¾ Model kaydedildi: {model_path}")
    
    def log_dataset_hash(self, X, y=None):
        """
        Veri setinin hash'ini hesapla ve kaydet.
        
        Args:
            X: Ã–zellikler
            y: Hedef deÄŸerler (opsiyonel)
        """
        if not self.log_data_hash:
            return
        
        # Hash hesapla
        X_bytes = pd.DataFrame(X).to_json().encode()
        data_hash = hashlib.md5(X_bytes).hexdigest()
        
        self.log_params({"data_hash_X": data_hash})
        
        if y is not None:
            y_bytes = pd.Series(y).to_json().encode()
            y_hash = hashlib.md5(y_bytes).hexdigest()
            self.log_params({"data_hash_y": y_hash})
        
        if self.verbose:
            print(f"ðŸ”’ Veri hash'i kaydedildi: {data_hash[:8]}...")
    
    def list_runs(self) -> pd.DataFrame:
        """
        TÃ¼m run'larÄ± listele.
        
        Returns:
            DataFrame: Run bilgileri
        
        Example:
            >>> df = tracker.list_runs()
            >>> print(df)
        """
        if not self.metadata["runs"]:
            print("HenÃ¼z run kaydÄ± yok.")
            return pd.DataFrame()
        
        runs_data = []
        for run_name, run_info in self.metadata["runs"].items():
            row = {
                "run_name": run_name,
                "start_time": run_info["start_time"],
                "duration_seconds": run_info.get("duration_seconds", None),
                "status": run_info["status"],
            }
            row.update(run_info["metrics"])
            runs_data.append(row)
        
        return pd.DataFrame(runs_data)
    
    def get_best_run(self, metric: str, maximize: bool = True) -> Optional[str]:
        """
        En iyi run'Ä± bul.
        
        Args:
            metric: KarÅŸÄ±laÅŸtÄ±rma metriÄŸi
            maximize: True ise maksimize, False ise minimize et
        
        Returns:
            str: En iyi run adÄ±
        
        Example:
            >>> best = tracker.get_best_run("accuracy", maximize=True)
        """
        df = self.list_runs()
        if df.empty or metric not in df.columns:
            return None
        
        if maximize:
            best_idx = df[metric].idxmax()
        else:
            best_idx = df[metric].idxmin()
        
        return df.loc[best_idx, "run_name"]
    
    def load_model(self, run_name: str, model_name: str):
        """
        KaydedilmiÅŸ modeli yÃ¼kle.
        
        Args:
            run_name: Run adÄ±
            model_name: Model adÄ±
        
        Returns:
            Model nesnesi
        
        Example:
            >>> model = tracker.load_model("run_1", "my_model")
        """
        model_path = self.experiment_path / run_name / f"{model_name}.joblib"
        if not model_path.exists():
            raise FileNotFoundError(f"Model bulunamadÄ±: {model_path}")
        
        return joblib.load(model_path)
    
    def generate_report(self, output_path: Optional[str] = None) -> str:
        """
        HTML rapor oluÅŸtur.
        
        Args:
            output_path: Ã‡Ä±ktÄ± dosya yolu (opsiyonel)
        
        Returns:
            str: Rapor dosya yolu
        """
        from .report_generator import ReportGenerator
        
        generator = ReportGenerator(self)
        report_path = generator.generate(output_path)
        
        if self.verbose:
            print(f"ðŸ“„ Rapor oluÅŸturuldu: {report_path}")
        
        return report_path
    
    @classmethod
    def from_config(cls, config_path: str) -> 'ExperimentTracker':
        """
        YAML config dosyasÄ±ndan tracker oluÅŸtur.
        
        Args:
            config_path: Config dosya yolu
        
        Returns:
            ExperimentTracker: Yeni tracker instance
        """
        from .utils import load_config
        
        config = load_config(config_path)
        exp_config = config.get("experiment", {})
        tracking_config = config.get("tracking", {})
        
        return cls(
            experiment_name=exp_config.get("name", "default_experiment"),
            storage_path=exp_config.get("storage_path", "./experiments"),
            auto_log=tracking_config.get("auto_log", True),
            log_data_hash=tracking_config.get("log_data_hash", True),
        )


class RunContext:
    """Context manager for experiment runs."""
    
    def __init__(self, tracker: ExperimentTracker, run_name: str):
        self.tracker = tracker
        self.run_name = run_name
    
    def __enter__(self):
        self.tracker._initialize_run(self.run_name)
        return self.tracker
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        if exc_type is not None:
            self.tracker.current_run_data["status"] = "failed"
            self.tracker.current_run_data["error"] = str(exc_val)
        self.tracker._finalize_run()
        return False


