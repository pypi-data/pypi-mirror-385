
[build-system]
requires = ["setuptools>=61.0", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "sparkforge"
version = "1.3.0"
description = "A simplified, production-ready data pipeline builder for Apache Spark and Delta Lake"
readme = "README.md"
license = {text = "MIT"}
authors = [
    {name = "Odos Matthews", email = "odosmatthews@gmail.com"}
]
maintainers = [
    {name = "Odos Matthews", email = "odosmatthews@gmail.com"}
]
keywords = [
    "spark", "databricks", "pipeline", "etl", "data-engineering",
    "data-lakehouse", "bronze-silver-gold", "delta-lake", "big-data"
]
classifiers = [
    "Development Status :: 3 - Alpha",
    "Intended Audience :: Developers",
    "Intended Audience :: Science/Research",
    "License :: OSI Approved :: MIT License",
    "Operating System :: OS Independent",
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3.8",
    "Programming Language :: Python :: 3.9",
    "Programming Language :: Python :: 3.10",
    "Programming Language :: Python :: 3.11",
    "Topic :: Scientific/Engineering :: Information Analysis",
    "Topic :: Software Development :: Libraries :: Python Modules",
    "Topic :: Database",
    "Topic :: Software Development :: Build Tools",
]
requires-python = ">=3.8"
dependencies = [
    "pandas>=1.3.0",
    "psutil>=5.8.0",
]

[project.optional-dependencies]
pyspark = [
    "pyspark==3.2.4",
    "delta-spark>=1.2.0,<2.0.0",
]
mock = [
    "mock-spark==2.4.0",
]
dev = [
    "black>=23.0.0",
    "flake8>=6.0.0",
    "mypy>=1.3.0",
    "isort>=5.12.0",
    "pylint>=2.17.0",
    "bandit>=1.7.0",
    "pre-commit>=3.0.0",
    "ruff>=0.0.270",
    "mypy-extensions>=1.0.0",
    "types-requests>=2.28.0",
    "pytest>=7.0.0",
    "pytest-cov>=4.0.0",
    "pytest-xdist>=3.0.0",
]
docs = [
    "sphinx>=4.0.0",
    "sphinx-rtd-theme>=1.0.0",
    "myst-parser>=0.17.0",
]
test = [
    "mock-spark==2.4.0",
    "pytest>=7.0.0",
    "pytest-cov>=4.0.0",
    "pytest-xdist>=3.0.0",
    "hypothesis>=6.0.0",
    "pyspark==3.2.4",
    "delta-spark>=1.2.0,<2.0.0",
]
compat-test = [
    "pyspark==3.2.4",
    "delta-spark>=1.2.0,<2.0.0",
]

[project.urls]
Homepage = "https://github.com/eddiethedean/sparkforge"
Documentation = "https://sparkforge.readthedocs.io/"
Repository = "https://github.com/eddiethedean/sparkforge"
"Bug Tracker" = "https://github.com/eddiethedean/sparkforge/issues"

[tool.setuptools.packages.find]
where = ["."]
include = ["sparkforge*"]

[tool.black]
line-length = 88
target-version = ['py38']
include = '\.pyi?$'
extend-exclude = '''
/(
  # directories
  \.eggs
  | \.git
  | \.hg
  | \.mypy_cache
  | \.tox
  | \.venv
  | build
  | dist
)/
'''

[tool.isort]
profile = "black"
multi_line_output = 3
line_length = 88
known_first_party = ["sparkforge"]

[tool.mypy]
python_version = "3.8"
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = true
disallow_incomplete_defs = true
check_untyped_defs = true
disallow_untyped_decorators = true
no_implicit_optional = true
warn_redundant_casts = true
warn_unused_ignores = true
warn_no_return = true
warn_unreachable = true
strict_equality = true
show_error_codes = true
show_column_numbers = true
show_error_context = true
pretty = true
color_output = true
error_summary = true

[[tool.mypy.overrides]]
module = [
    "pyspark.*",
    "delta.*",
    "mock_spark.*",
]
ignore_missing_imports = true

[tool.pylint]
max-line-length = 88
disable = [
    "C0330",  # wrong-import-position
    "C0326",  # bad-whitespace
    "R0903",  # too-few-public-methods
    "R0913",  # too-many-arguments
    "W0613",  # unused-argument
    "C0103",  # invalid-name
]
enable = [
    "C0114",  # missing-module-docstring
    "C0116",  # missing-function-docstring
    "C0115",  # missing-class-docstring
]
ignore-paths = [
    "tests/",
    "build/",
    "dist/",
    ".venv/",
    "venv/",
]

[tool.bandit]
exclude_dirs = ["tests", "build", "dist", ".venv", "venv"]
skips = ["B101", "B601"]  # Skip assert_used and shell=True
severity = "medium"
confidence = "medium"
recursive = true
format = "json"
output = "bandit-report.json"

[tool.safety]
output = "json"
file = "requirements.txt"

[tool.ruff]
target-version = "py38"
line-length = 88
exclude = [
    ".bzr",
    ".direnv",
    ".eggs",
    ".git",
    ".git-rewrite",
    ".hg",
    ".mypy_cache",
    ".nox",
    ".pants.d",
    ".pytype",
    ".ruff_cache",
    ".svn",
    ".tox",
    ".venv",
    "__pypackages__",
    "_build",
    "buck-out",
    "build",
    "dist",
    "node_modules",
    "venv",
]

[tool.ruff.lint]
select = [
    "E",  # pycodestyle errors
    "W",  # pycodestyle warnings
    "F",  # pyflakes
    "I",  # isort
    "B",  # flake8-bugbear
    "C4", # flake8-comprehensions
    "UP", # pyupgrade
]
ignore = [
    "E501",  # line too long, handled by black
    "B008",  # do not perform function calls in argument defaults
    "C901",  # too complex
    "UP006",  # Use dict instead of Dict - disabled for Python 3.8 compatibility
    "UP007",  # Use X | Y instead of Union[X, Y] - disabled for Python 3.8 compatibility
]

[tool.ruff.lint.per-file-ignores]
"tests/*" = ["S101", "S106", "S108"]  # Allow assert, hardcoded passwords, temp files in tests

[tool.pytest.ini_options]
testpaths = ["tests"]
python_files = ["test_*.py"]
python_classes = ["Test*"]
python_functions = ["test_*"]
addopts = [
    "-v",
    "--tb=short",
    "--strict-markers",
    "--disable-warnings",
    "--cov=sparkforge",
    "--cov-report=term-missing",
    "--cov-report=html:htmlcov"
]
markers = [
    "slow: marks tests as slow (deselect with '-m \"not slow\"')",
    "integration: marks tests as integration tests",
    "unit: marks tests as unit tests",
    "spark: marks tests that require Spark session",
    "delta: marks tests that require Delta Lake",
    "pyspark_compat: marks tests that require PySpark (skipped if not installed)"
]
