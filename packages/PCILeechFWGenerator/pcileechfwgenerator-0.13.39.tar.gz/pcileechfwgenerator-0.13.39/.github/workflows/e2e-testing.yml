name: End-to-End Testing

on:
  push:
  pull_request:
    branches: [ main ]
  workflow_dispatch:
    inputs:
      test_type:
        description: 'Type of E2E test to run'
        required: false
        default: 'all'
        type: choice
        options:
        - all
        - dependencies
        - discovery
        - pipeline
        - templates
        - cli
        - vivado
        - container
        - performance
        - fallbacks
      verbose:
        description: 'Enable verbose output'
        required: false
        default: false
        type: boolean
      no_cleanup:
        description: 'Preserve test artifacts'
        required: false
        default: false
        type: boolean

permissions:
  contents: read
  security-events: write
  actions: read
  pull-requests: write

concurrency:
  group: e2e-${{ github.ref }}
  cancel-in-progress: false

env:
  # Environment variables for testing
  PCILEECH_ALLOW_MOCK_DATA: "true"
  PCILEECH_PRODUCTION_MODE: "false"
  PCILEECH_AUTO_INSTALL: "1"
  PYTHONPATH: "${{ github.workspace }}/src"
  NO_INTERACTIVE: "1"

jobs:
  e2e-test:
    name: End-to-End Testing
    runs-on: ubuntu-latest
    timeout-minutes: 60
    
    strategy:
      fail-fast: false
      matrix:
        python-version: [ '3.11', '3.12']
        container-engine: ['podman']
        include:
          # Test with full dependencies
          - python-version: '3.11'
            container-engine: 'podman'
            test-mode: 'full'

    steps:
      - name: Checkout repository
        uses: actions/checkout@v5
        with:
          fetch-depth: 0
          submodules: recursive

      - name: Python setup base
        uses: ./.github/actions/python-setup
        with:
          python-version: ${{ matrix.python-version }}
          editable: 'false'

      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y --no-install-recommends \
            build-essential \
            linux-headers-generic \
            pciutils \
            kmod \
            git \
            curl \
            jq

      - name: Install container engine
        if: matrix.container-engine == 'podman'
        run: |
          sudo apt-get install -y podman
          podman --version

      - name: Install minimal dependencies
        if: matrix.test-mode == 'minimal'
        uses: ./.github/actions/python-setup
        with:
          python-version: ${{ matrix.python-version }}
          requirements-files: 'requirements.txt'
          editable: 'true'

      - name: Install full dependencies
        if: matrix.test-mode == 'full' || matrix.test-mode == ''
        uses: ./.github/actions/python-setup
        with:
          python-version: ${{ matrix.python-version }}
          requirements-files: 'requirements.txt,requirements-test.txt,requirements-tui.txt'
          editable: 'true'
          skip-missing: 'true'
          extras: ''

      - name: Build VFIO constants
        run: |
          chmod +x build_vfio_constants.sh
          ./build_vfio_constants.sh || echo "VFIO constants build failed (expected in CI)"

      - name: Set up mock environment
        run: |
          # Create mock kernel modules directory
          sudo mkdir -p /lib/modules/$(uname -r)/kernel/vfio
          
          # Create mock VFIO devices
          sudo mkdir -p /dev/vfio
          sudo touch /dev/vfio/vfio
          sudo touch /dev/vfio/42
          
          # Set permissions
          sudo chmod 666 /dev/vfio/vfio /dev/vfio/42
          
          # Create mock module info
          echo "Mock VFIO modules loaded" | sudo tee /proc/modules > /dev/null || true

      - name: Run E2E tests (all)
        if: github.event.inputs.test_type == '' || github.event.inputs.test_type == 'all'
        run: |
          python scripts/e2e_test_github_actions.py \
            ${{ github.event.inputs.no_cleanup == 'true' && '--no-cleanup' || '' }}

      - name: Run E2E tests (specific)
        if: github.event.inputs.test_type != '' && github.event.inputs.test_type != 'all'
        run: |
          python scripts/e2e_test_github_actions.py \
            --test ${{ github.event.inputs.test_type }} \
            ${{ github.event.inputs.no_cleanup == 'true' && '--no-cleanup' || '' }}

      - name: Collect system information
        if: always()
        run: |
          echo "=== System Information ===" > system_info.txt
          echo "OS: $(lsb_release -a 2>/dev/null || cat /etc/os-release)" >> system_info.txt
          echo "Kernel: $(uname -a)" >> system_info.txt
          echo "Python: $(python --version)" >> system_info.txt
          echo "Available Memory: $(free -h)" >> system_info.txt
          echo "Available Disk: $(df -h /)" >> system_info.txt
          echo "Container Engine: ${{ matrix.container-engine }}" >> system_info.txt
          echo "Test Mode: ${{ matrix.test-mode }}" >> system_info.txt

          if command -v podman >/dev/null 2>&1; then
            echo "Podman Version: $(podman --version)" >> system_info.txt
          fi
          
          echo "=== Python Packages ===" >> system_info.txt
          pip list >> system_info.txt || echo "Failed to list packages" >> system_info.txt

      - name: Upload test artifacts
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: e2e-test-artifacts-py${{ matrix.python-version }}-${{ matrix.container-engine }}
          path: |
            tests/e2e_temp/
            system_info.txt
            *.log
          if-no-files-found: warn
          retention-days: 7

      - name: Upload test reports
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: e2e-test-reports-py${{ matrix.python-version }}-${{ matrix.container-engine }}
          path: |
            tests/e2e_temp/**/e2e_test_*.json
          if-no-files-found: warn
          retention-days: 30

  security-scan:
    name: Security Scan
    runs-on: ubuntu-latest
    needs: e2e-test
    if: github.event_name == 'schedule' || github.event_name == 'workflow_dispatch'
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v5

      - name: Python setup & deps
        uses: ./.github/actions/python-setup
        with:
          python-version: '3.11'
          extra-packages: 'bandit safety'
          editable: 'false'

      - name: Run Bandit security scan
        run: |
          bandit -r src/ -f json -o bandit-report.json || true
          bandit -r src/ -f txt

      - name: Run Safety dependency scan
        run: |
          safety check --json --output safety-report.json || true
          safety check

      - name: Upload security reports
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: security-scan-reports
          path: |
            bandit-report.json
            safety-report.json
          retention-days: 30

  performance-analysis:
    name: Performance Analysis
    runs-on: ubuntu-latest
    needs: e2e-test
    if: github.event_name == 'schedule' || github.event_name == 'workflow_dispatch'
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v5

      - name: Python setup & deps
        uses: ./.github/actions/python-setup
        with:
          python-version: '3.11'
          requirements-files: 'requirements.txt'
          extra-packages: 'psutil memory-profiler pytest-benchmark'
          editable: 'false'

      - name: Run performance tests
        run: |
          PCILEECH_ALLOW_MOCK_DATA=true python scripts/e2e_test_github_actions.py --test performance --verbose

      - name: Generate performance report
        run: |
          echo "=== Performance Analysis ===" > performance_report.txt
          echo "Timestamp: $(date -u)" >> performance_report.txt
          echo "Commit: ${{ github.sha }}" >> performance_report.txt
          echo "Branch: ${{ github.ref_name }}" >> performance_report.txt
          
          # Add performance test results
          if [ -f tests/e2e_temp/e2e_test_performance_report.json ]; then
            echo "=== Performance Test Results ===" >> performance_report.txt
            jq '.tests[] | select(.test_name == "Performance Benchmarks")' tests/e2e_temp/e2e_test_performance_report.json >> performance_report.txt
          fi

      - name: Upload performance reports
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: performance-analysis-reports
          path: |
            performance_report.txt
            tests/e2e_temp/**/e2e_test_performance_report.json
          retention-days: 30


  integration-summary:
    name: Integration Test Summary
    runs-on: ubuntu-latest
    needs: [e2e-test, security-scan, performance-analysis]
    if: always()
    
    steps:
      - name: Download all artifacts
        uses: actions/download-artifact@v5
        with:
          path: artifacts/

      - name: Generate summary report
        run: |
          echo "# E2E Integration Test Summary" > SUMMARY.md
          echo "" >> SUMMARY.md
          echo "**Workflow Run:** ${{ github.run_number }}" >> SUMMARY.md
          echo "**Commit:** ${{ github.sha }}" >> SUMMARY.md
          echo "**Branch:** ${{ github.ref_name }}" >> SUMMARY.md
          echo "**Triggered by:** ${{ github.event_name }}" >> SUMMARY.md
          echo "**Timestamp:** $(date -u)" >> SUMMARY.md
          echo "" >> SUMMARY.md
          
          echo "## Test Results" >> SUMMARY.md
          echo "" >> SUMMARY.md
          
          # Analyze E2E test results
          echo "### End-to-End Tests" >> SUMMARY.md
          for report in artifacts/e2e-test-reports-*/tests/e2e_temp/**/e2e_test_*.json; do
            if [ -f "$report" ]; then
              echo "Processing: $report" >> SUMMARY.md
              # Extract summary information (would need jq in real implementation)
            fi
          done
          
          # Check if any tests failed
          if [ "${{ needs.e2e-test.result }}" != "success" ]; then
            echo "❌ **E2E Tests:** FAILED" >> SUMMARY.md
          else
            echo "✅ **E2E Tests:** PASSED" >> SUMMARY.md
          fi
          
          if [ "${{ needs.security-scan.result }}" != "success" ] && [ "${{ needs.security-scan.result }}" != "skipped" ]; then
            echo "❌ **Security Scan:** FAILED" >> SUMMARY.md
          elif [ "${{ needs.security-scan.result }}" == "success" ]; then
            echo "✅ **Security Scan:** PASSED" >> SUMMARY.md
          else
            echo "⏸️ **Security Scan:** SKIPPED" >> SUMMARY.md
          fi
          
          if [ "${{ needs.performance-analysis.result }}" != "success" ] && [ "${{ needs.performance-analysis.result }}" != "skipped" ]; then
            echo "❌ **Performance Analysis:** FAILED" >> SUMMARY.md
          elif [ "${{ needs.performance-analysis.result }}" == "success" ]; then
            echo "✅ **Performance Analysis:** PASSED" >> SUMMARY.md
          else
            echo "⏸️ **Performance Analysis:** SKIPPED" >> SUMMARY.md
          fi
          
          # Container tests merged into packaging workflow; no standalone job here.
          
          echo "" >> SUMMARY.md
          echo "## Artifacts" >> SUMMARY.md
          echo "" >> SUMMARY.md
          echo "- Test reports and logs available in workflow artifacts" >> SUMMARY.md
          echo "- Artifacts retained for 7-30 days depending on type" >> SUMMARY.md
          
          cat SUMMARY.md

      - name: Upload integration summary
        uses: actions/upload-artifact@v4
        with:
          name: integration-test-summary
          path: SUMMARY.md
          retention-days: 90

      - name: Comment on PR (if applicable)
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v8
        with:
          script: |
            const fs = require('fs');
            const summary = fs.readFileSync('SUMMARY.md', 'utf8');
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: summary
            });
