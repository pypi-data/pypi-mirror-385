project_id: ""
project_name: "Your Project Name"
project_description: "A new RLHF project for training language models with custom reward functions"
dataset_id: ""
dataset_type: "rl"  # "sft" for supervised fine-tuning, "rl" for reinforcement learning
dataset_name: "Your Dataset Name"
dataset_description: "A new dataset for reinforcement learning from human feedback training"
organization_id: ""
params:
  model: "meta-llama/Llama-3.2-1B"  # Must be one of the allowed models
  batch_size: 8
  num_epochs: 3
  learning_rate: 0.0001  # 1e-4
  max_steps: null  # Optional: Set to override num_epochs

  # LoRA parameters
  qlora_rank: 32
  qlora_alpha: null  # Defaults to 2 * qlora_rank if not set

  # Validation frequency (choose one)
  val_steps: 100  # Validate every N steps
  val_epochs: null  # Or validate every N epochs

  # Save frequency (choose one)
  save_steps: null  # Save checkpoint every N steps
  save_epochs: 1  # Or save checkpoint every N epochs

  # RL-specific parameters (only for dataset_type="rl")
  loss_fn: "ppo"  # Loss function: "ppo" or "importance_sampling"
  adv_estimator: "grpo"  # Advantage estimator: "grpo", "gae", or "reinforce"
  kl_penalty_coef: 0.01  # KL divergence penalty coefficient
  training_mode: "sync"  # Training mode: "sync", "async", or "stream_minibatch"

  # Additional training parameters
  gradient_checkpointing: true  # Use gradient checkpointing to save memory
  fp16: true  # Use mixed precision training
  num_workers: 4  # Number of data loading workers
  checkpoint_dir: "/workspace/checkpoints"  # Directory for saving checkpoints