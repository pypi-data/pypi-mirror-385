# Core configuration
storage_config_path: "agentmap_config_storage.yaml"

# Directory paths
paths:
  custom_agents: "agentmap_data/custom_agents"
  functions: "agentmap_data/custom_functions"
  csv_repository: "agentmap_data/workflows"  # Directory for storing workflow CSV files

# Memory configuration
memory:
  enabled: false
  default_type: "buffer"  # Options: buffer, buffer_window, summary, token_buffer
  buffer_window_size: 5
  max_token_limit: 2000
  memory_key: "conversation_memory"

# Execution tracking configuration
execution:
  # What to record during execution
  tracking:
    enabled: true
    track_outputs: true
    track_inputs: true

  # How to determine success
  success_policy:
    type: "all_nodes"
    critical_nodes: []
    custom_function: ""

# Separate LangChain tracing for debugging
tracing:
  enabled: false
  mode: "local"
  local_exporter: "file"
  local_directory: "agentmap_data/traces"
  project: "your_project_name"  # Replace with your project name`
  langsmith_api_key: ""
  trace_all: true
  trace_graphs: []

# Logging configuration
logging:
  version: 1
  disable_existing_loggers: False
  file_path: null  # Set to a path to enable file logging
  formatters:
    default:
      format: "[%(asctime)s] [%(levelname)s] %(name)s: %(message)s"
  handlers:
    console:
      class: logging.StreamHandler
      formatter: default
      level: DEBUG # TRACE, DEBUG, INFO, WARNING, ERROR
  root:
    level: INFO
    handlers: [console]

# Storage-specific configurations
# storage: look in separate file storage_config.yaml

# LLM Provider Configuration
# Configure API keys and default models for each provider
# Note: Model validation is delegated to provider APIs - any model name is accepted
llm:
  openai:
    api_key: ""  # Or use env: OPENAI_API_KEY
    model: "gpt-4o-mini-2024-07-18"  # Default model (can be any valid OpenAI model)
    temperature: 0.7
    max_tokens: null  # Optional: limit response length
    top_p: 1.0  # Optional: nucleus sampling parameter
    frequency_penalty: 0.0  # Optional: reduce repetition
    presence_penalty: 0.0  # Optional: encourage topic diversity

  anthropic:
    api_key: ""  # Or use env: ANTHROPIC_API_KEY
    model: "claude-3-5-sonnet-20241022"  # Default model (can be any valid Anthropic model)
    temperature: 0.7
    max_tokens: 4096  # Optional: Claude default is 4096
    top_p: 1.0  # Optional: nucleus sampling
    top_k: null  # Optional: top-k sampling (Claude-specific)

  google:
    api_key: ""  # Or use env: GOOGLE_API_KEY
    model: "gemini-1.5-flash"  # Default model (can be any valid Google model)
    temperature: 0.7
    max_tokens: null  # Optional: limit response length
    top_p: 1.0  # Optional: nucleus sampling
    top_k: null  # Optional: top-k sampling

routing:
  enabled: true

  # Provider × Complexity Matrix
  # Maps (provider, complexity) → model for intelligent routing and fallback
  # Used by tiered fallback system when primary model fails
  routing_matrix:
    anthropic:
      low: "claude-3-5-haiku-20241022"        # Fast, cost-effective (128k context)
      medium: "claude-3-5-sonnet-20241022"    # Balanced performance (200k context)
      high: "claude-sonnet-4-20250514"        # High capability (200k context)
      critical: "claude-opus-4-20250514"      # Maximum capability (200k context)

    openai:
      low: "gpt-4o-mini-2024-07-18"           # Fast, cost-effective (128k context)
      medium: "gpt-4o-2024-11-20"             # Balanced performance (128k context)
      high: "gpt-4o-2024-11-20"               # High capability (128k context)
      critical: "o3-mini-2025-01-31"          # Reasoning-focused (128k context)

    google:
      low: "gemini-1.5-flash"                 # Fast, cost-effective (1M context)
      medium: "gemini-1.5-pro"                # Balanced performance (2M context)
      high: "gemini-1.5-pro"                  # High capability (2M context)
      critical: "gemini-1.5-pro"              # Maximum capability (2M context)

  # Complexity Analysis Configuration
  complexity_analysis:
    # Prompt length thresholds (characters)
    prompt_length_thresholds:
      low: 100      # < 100 chars = low complexity
      medium: 300   # 100-300 chars = medium complexity
      high: 800     # 300-800 chars = high complexity
      # > 800 chars = critical complexity

    # Enable different analysis methods
    methods:
      prompt_length: true
      keyword_analysis: true
      context_analysis: true
      memory_analysis: true

    # Keyword weighting (how much each type of keyword affects complexity)
    keyword_weights:
      complexity_keywords: 0.4
      task_specific_keywords: 0.3
      prompt_structure: 0.3

    # Context analysis settings
    context_analysis:
      memory_size_threshold: 10  # Messages in memory that indicate complexity
      input_field_count_threshold: 5  # Number of input fields

  # Cost Optimization Settings
  cost_optimization:
    enabled: true
    # Prefer cheaper models when complexity allows
    prefer_cost_effective: true
    # Maximum cost tier (low, medium, high, critical)
    max_cost_tier: "medium"

  # Tiered Fallback Configuration
  # Automatic fallback when LLM calls fail (new models work without code changes)
  fallback:
    # Tier 1: Same provider, low complexity model from routing_matrix
    # Tier 2: Switch to this fallback provider
    default_provider: "anthropic"
    # Tier 3: Emergency fallback to first available provider
    # Tier 4: Raise error with full context

    # Default model when routing fails (overrides routing_matrix)
    default_model: "claude-3-5-haiku-20241022"

    # Retry with lower complexity if high-tier model unavailable
    retry_with_lower_complexity: true

    # Log fallback attempts for debugging
    log_fallback_attempts: true

  # Activity-Based Routing Configuration
  # Define specific activities with explicit provider/model pairs per complexity tier
  # Activities are evaluated FIRST, before task_types and routing_matrix
  activities:
    code_generation:
      low:
        primary:
          provider: "anthropic"
          model: "claude-3-5-haiku-20241022"
        fallbacks:
          - provider: "openai"
            model: "gpt-4o-mini-2024-07-18"
      medium:
        primary:
          provider: "anthropic"
          model: "claude-3-5-sonnet-20241022"
        fallbacks:
          - provider: "openai"
            model: "gpt-4o-2024-11-20"
      high:
        primary:
          provider: "anthropic"
          model: "claude-sonnet-4-20250514"
        fallbacks:
          - provider: "openai"
            model: "gpt-4o-2024-11-20"
      critical:
        primary:
          provider: "openai"
          model: "o3-mini-2025-01-31"
        fallbacks:
          - provider: "anthropic"
            model: "claude-opus-4-20250514"

    data_analysis:
      low:
        primary:
          provider: "openai"
          model: "gpt-4o-mini-2024-07-18"
        fallbacks:
          - provider: "google"
            model: "gemini-1.5-flash"
      medium:
        primary:
          provider: "openai"
          model: "gpt-4o-2024-11-20"
        fallbacks:
          - provider: "anthropic"
            model: "claude-3-5-sonnet-20241022"
      high:
        primary:
          provider: "openai"
          model: "gpt-4o-2024-11-20"
        fallbacks:
          - provider: "anthropic"
            model: "claude-sonnet-4-20250514"

    research:
      # Use 'any' for complexity-agnostic routing
      any:
        primary:
          provider: "google"
          model: "gemini-1.5-pro"  # 2M context window
        fallbacks:
          - provider: "anthropic"
            model: "claude-3-5-sonnet-20241022"
          - provider: "openai"
            model: "gpt-4o-2024-11-20"

    creative_writing:
      low:
        primary:
          provider: "anthropic"
          model: "claude-3-5-haiku-20241022"
      medium:
        primary:
          provider: "anthropic"
          model: "claude-3-5-sonnet-20241022"
        fallbacks:
          - provider: "openai"
            model: "gpt-4o-2024-11-20"
      high:
        primary:
          provider: "anthropic"
          model: "claude-sonnet-4-20250514"

    customer_support:
      any:
        primary:
          provider: "anthropic"
          model: "claude-3-5-haiku-20241022"  # Fast and cheap
        fallbacks:
          - provider: "openai"
            model: "gpt-4o-mini-2024-07-18"

  # Task Type Definitions (for complexity analysis and provider preferences)
  # Used when activity is NOT specified in routing_context
  task_types:
    general:
      description: "General purpose tasks and queries"
      provider_preference: ["anthropic", "openai", "google"]
      default_complexity: "medium"
      complexity_keywords:
        low: ["simple", "basic", "quick", "summarize"]
        medium: ["analyze", "process", "standard", "explain"]
        high: ["complex", "detailed", "comprehensive", "advanced"]
        critical: ["urgent", "critical", "important", "emergency"]

    code_generation:
      description: "Writing, refactoring, or debugging code"
      provider_preference: ["anthropic", "openai"]  # Claude/GPT excel at code
      default_complexity: "high"
      complexity_keywords:
        low: ["comment", "format", "simple function"]
        medium: ["implement", "refactor", "debug"]
        high: ["architecture", "system design", "complex algorithm"]
        critical: ["security critical", "performance critical", "production"]

    data_analysis:
      description: "Analyzing data, creating reports, insights"
      provider_preference: ["openai", "anthropic", "google"]
      default_complexity: "medium"
      complexity_keywords:
        low: ["count", "sum", "average"]
        medium: ["trend", "pattern", "correlation"]
        high: ["prediction", "forecasting", "statistical"]
        critical: ["real-time", "mission critical", "financial"]

    creative_writing:
      description: "Creative content generation, storytelling"
      provider_preference: ["anthropic", "openai"]  # Claude excels at creative
      default_complexity: "medium"
      complexity_keywords:
        low: ["tweet", "headline", "caption"]
        medium: ["article", "blog post", "story"]
        high: ["novel chapter", "screenplay", "comprehensive"]
        critical: ["publication ready", "professional editing"]

    research:
      description: "Research, information synthesis, academic"
      provider_preference: ["google", "anthropic", "openai"]  # Gemini's long context
      default_complexity: "high"
      complexity_keywords:
        low: ["definition", "summary", "overview"]
        medium: ["comparison", "analysis", "review"]
        high: ["comprehensive research", "literature review", "synthesis"]
        critical: ["publication", "peer review", "academic"]

    customer_support:
      description: "Customer service, support tickets, FAQs"
      provider_preference: ["anthropic", "openai"]
      default_complexity: "low"
      complexity_keywords:
        low: ["faq", "simple question", "basic support"]
        medium: ["troubleshooting", "detailed question"]
        high: ["complex issue", "escalation", "technical"]
        critical: ["urgent", "vip customer", "critical issue"]

    reasoning:
      description: "Logic puzzles, problem solving, step-by-step reasoning"
      provider_preference: ["openai", "anthropic"]  # o3-mini for reasoning
      default_complexity: "high"
      complexity_keywords:
        low: ["basic logic", "simple calculation"]
        medium: ["multi-step problem", "reasoning chain"]
        high: ["complex reasoning", "proof", "theorem"]
        critical: ["advanced mathematics", "research level"]

  # Performance Settings
  performance:
    # Cache routing decisions for identical inputs
    enable_routing_cache: true
    # Cache TTL in seconds (5 minutes)
    cache_ttl: 300
    # Maximum cache size (number of entries)
    max_cache_size: 1000
    # Timeout for LLM API calls (seconds)
    request_timeout: 60
    # Max retries before giving up
    max_retries: 3
    # Exponential backoff for retries
    retry_backoff_factor: 2

# Environment Variable Overrides
# These allow runtime configuration without changing the file
environment_overrides:
  routing_enabled: ${AGENTMAP_ROUTING_ENABLED:true}
  default_task_type: ${AGENTMAP_DEFAULT_TASK_TYPE:general}
  cost_optimization: ${AGENTMAP_COST_OPTIMIZATION:true}
  routing_cache_enabled: ${AGENTMAP_ROUTING_CACHE:true}
  llm_openai_api_key: ${OPENAI_API_KEY:}
  llm_anthropic_api_key: ${ANTHROPIC_API_KEY:}
  llm_google_api_key: ${GOOGLE_API_KEY:}

# Messaging Configuration
# Configure message publishing for workflow events, graph triggers, and auto-resume
messaging:
  # Default messaging provider (local, aws, gcp, azure)
  default_provider: "local"

  # Provider configurations
  providers:
    local:
      enabled: true
      storage_path: "agentmap_data/messages"  # Where to store local messages

    # AWS SNS/SQS configuration (uncomment to enable)
    # aws:
    #   enabled: false
    #   region: "us-east-1"
    #   sns_topic_arn: ""  # ARN of SNS topic
    #   sqs_queue_url: ""  # URL of SQS queue
    #   access_key_id: ""  # Or use AWS_ACCESS_KEY_ID env var
    #   secret_access_key: ""  # Or use AWS_SECRET_ACCESS_KEY env var

    # GCP Pub/Sub configuration (uncomment to enable)
    # gcp:
    #   enabled: false
    #   project_id: ""
    #   topic_id: ""
    #   credentials_path: ""  # Path to service account JSON

    # Azure Service Bus configuration (uncomment to enable)
    # azure:
    #   enabled: false
    #   connection_string: ""  # Or use AZURE_SERVICEBUS_CONNECTION_STRING env var
    #   topic_name: ""

  # Retry policy for message publishing
  retry_policy:
    max_retries: 3
    backoff_seconds: [1, 2, 4]

  # Message Templates
  # Templates use Python string.Template syntax with $variable_name
  # Available variables depend on message type (see documentation)
  message_templates:
    # Graph Trigger Template - Used to trigger another AgentMap graph
    # Use with: send_graph_message: true, graph_message_template: "default_graph_trigger"
    default_graph_trigger:
      event_type: "$event_type"
      graph: "$graph"  # Which graph to execute
      state: "$inputs"  # Inputs for serverless handler (renamed from inputs for compatibility)
      thread_id: "$thread_id"  # Parent thread ID for correlation
      node_name: "$node_name"
      workflow: "$workflow"
      timestamp: "$timestamp"
      context: "$context"

    # Node Suspend Template - Used to trigger external processing
    # Use with: send_node_message: true, node_message_template: "default_node_suspend"
    default_node_suspend:
      event_type: "$event_type"
      thread_id: "$thread_id"  # Thread to resume after external processing
      node_name: "$node_name"
      workflow: "$workflow"
      graph: "$graph"
      timestamp: "$timestamp"
      inputs: "$inputs"  # NOT "state" - for external processing reference
      context: "$context"

    # Auto Resume Template - Used for serverless auto-resume triggers
    # Use with: send_resume_message: true, resume_message_template: "default_auto_resume"
    default_auto_resume:
      event_type: "$event_type"
      action: "resume"  # Tells serverless handler to resume
      thread_id: "$thread_id"  # Which thread to resume
      resume_value: "$resume_value"  # Value to pass to resumed workflow
      node_name: "$node_name"
      workflow: "$workflow"
      graph: "$graph"
      timestamp: "$timestamp"
      suspension_duration_seconds: "$suspension_duration_seconds"
      context: "$context"

    # Custom Templates - Add your own templates here
    # Example: Lambda trigger with custom fields
    # lambda_approval_trigger:
    #   trigger_type: "workflow_suspended"
    #   function_name: "approval-processor"
    #   thread_id: "$thread_id"
    #   workflow: "$workflow"
    #   approval_data: "$inputs"
    #   correlation_id: "$context.correlation_id"

