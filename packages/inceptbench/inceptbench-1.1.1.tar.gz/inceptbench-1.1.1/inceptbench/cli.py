"""CLI for Incept Eval"""
import click
import json
import sys
import os
from pathlib import Path
import requests
from .client import InceptClient

def get_api_key(api_key=None):
    """Get API key - now optional since we run locally"""
    if api_key:
        return api_key
    if os.getenv('INCEPT_API_KEY'):
        return os.getenv('INCEPT_API_KEY')
    config_file = Path.home() / '.incept' / 'config'
    if config_file.exists():
        try:
            with open(config_file) as f:
                return json.load(f).get('api_key')
        except:
            pass
    # API key is now optional - we run locally
    return None

@click.group()
@click.version_option(version='1.0.7')
def cli():
    """Incept Eval - Evaluate educational questions via Incept API

    \b
    CLI tool for evaluating educational questions with comprehensive
    assessment including V3 scaffolding, answer verification, and
    EduBench task evaluation.

    \b
    Commands:
      evaluate    Evaluate questions from a JSON file
      benchmark   Process many questions in parallel (high throughput)
      example     Generate sample input JSON file
      help        Show detailed help and usage examples

    \b
    Quick Start:
      1. Configure your API key:
         $ inceptbench configure YOUR_API_KEY

      2. Generate a sample file:
         $ inceptbench example

      3. Evaluate questions:
         $ inceptbench evaluate qs.json --verbose

    \b
    Examples:
      # Basic evaluation (simplified scores)
      $ inceptbench evaluate questions.json

      # Full detailed evaluation results
      $ inceptbench evaluate questions.json --full

      # Save full results to file
      $ inceptbench evaluate questions.json --full -o results.json

      # Append multiple evaluations to one file
      $ inceptbench evaluate test1.json -a all_results.json
      $ inceptbench evaluate test2.json -a all_results.json

      # Use local API server
      $ inceptbench evaluate test.json --api-url http://localhost:8000

    \b
    For detailed help, run: inceptbench help
    """
    pass

@cli.command()
@click.argument('input_file', type=click.Path(exists=True))
@click.option('--output', '-o', type=click.Path(), help='Save results to file (overwrites)')
@click.option('--append', '-a', type=click.Path(), help='Append results to file (creates if not exists)')
@click.option('--api-key', '-k', envvar='INCEPT_API_KEY', help='API key for authentication')
@click.option('--api-url', default='https://uae-poc.inceptapi.com', help='API endpoint URL')
@click.option('--timeout', '-t', type=int, default=600, help='Request timeout in seconds (default: 600)')
@click.option('--pretty', is_flag=True, default=True, help='Show only scores (default: enabled)')
@click.option('--verbose', '-v', is_flag=True, help='Show progress messages')
@click.option('--full', '-f', is_flag=True, help='Return full detailed evaluation results (default: simplified scores only)')
def evaluate(input_file, output, append, api_key, api_url, timeout, pretty, verbose, full):
    """Evaluate questions from JSON file via Incept API

    Sends questions to the Incept API for comprehensive evaluation including:
    - V3 scaffolding and DI compliance scoring
    - Answer correctness verification
    - EduBench task evaluation (QA, EC, IP)

    By default, shows only scores in pretty format. Use --no-pretty for full results.
    """
    try:
        api_key = get_api_key(api_key)
        if verbose:
            click.echo(f"ğŸ“‚ Loading: {input_file}")

        with open(input_file) as f:
            data = json.load(f)

        # Add verbose flag to the data
        data['verbose'] = full

        client = InceptClient(api_key, api_url, timeout=timeout)
        result = client.evaluate_dict(data)

        # Always output full results - pretty only controls formatting
        json_output = json.dumps(result, indent=2 if pretty else None, ensure_ascii=False)

        # Handle output options
        if output:
            # Overwrite mode
            with open(output, 'w', encoding='utf-8') as f:
                f.write(json_output)
            if verbose:
                click.echo(f"âœ… Saved to: {output}")
        elif append:
            # Append mode - load existing evaluations or create new list
            existing_data = []
            if Path(append).exists():
                try:
                    with open(append, 'r', encoding='utf-8') as f:
                        existing_data = json.load(f)
                        if not isinstance(existing_data, list):
                            # If file exists but isn't a list, wrap it
                            existing_data = [existing_data]
                except json.JSONDecodeError:
                    if verbose:
                        click.echo(f"âš ï¸  File exists but is invalid JSON, creating new file")
                    existing_data = []

            # Append new result
            existing_data.append(result)

            # Write back to file
            with open(append, 'w', encoding='utf-8') as f:
                json.dump(existing_data, f, indent=2, ensure_ascii=False)

            if verbose:
                click.echo(f"âœ… Appended to: {append} (total: {len(existing_data)} evaluations)")
        else:
            # Print to stdout
            click.echo(json_output)

    except requests.HTTPError as e:
        click.echo(f"âŒ API Error: {e.response.status_code}", err=True)
        sys.exit(1)
    except Exception as e:
        click.echo(f"âŒ Error: {e}", err=True)
        sys.exit(1)

@cli.command()
@click.argument('input_file', type=click.Path(exists=True))
@click.option('--output', '-o', type=click.Path(), help='Save results to file')
@click.option('--workers', '-w', type=int, default=100, help='Number of parallel workers (default: 100)')
@click.option('--verbose', '-v', is_flag=True, help='Show progress messages')
def benchmark(input_file, output, workers, verbose):
    """Benchmark mode: Process many questions in parallel

    Evaluates all questions using parallel workers for maximum throughput.
    Returns one score per question plus failed IDs.

    Example:
        inceptbench benchmark questions.json --workers 100 -o results.json
    """
    try:
        if verbose:
            click.echo(f"ğŸ“‚ Loading: {input_file}")

        with open(input_file) as f:
            data = json.load(f)

        # Import benchmark function
        from .client import InceptClient
        client = InceptClient()

        # Use benchmark mode
        if verbose:
            click.echo(f"ğŸš€ Benchmark mode: {len(data.get('generated_questions', []))} questions with {workers} workers")

        result = client.benchmark(data, max_workers=workers)

        # Format output
        json_output = json.dumps(result, indent=2, ensure_ascii=False)

        # Handle output
        if output:
            with open(output, 'w', encoding='utf-8') as f:
                f.write(json_output)
            if verbose:
                click.echo(f"âœ… Saved to: {output}")
                click.echo(f"ğŸ“Š Results: {result['successful']}/{result['total_questions']} successful")
                click.echo(f"â±ï¸  Time: {result['evaluation_time_seconds']:.2f}s")
                click.echo(f"ğŸ“ˆ Avg Score: {result['avg_score']:.3f}")
                if result['failed_ids']:
                    click.echo(f"âŒ Failed IDs: {', '.join(result['failed_ids'])}")
        else:
            click.echo(json_output)

    except Exception as e:
        click.echo(f"âŒ Error: {e}", err=True)
        sys.exit(1)

@cli.command()
@click.argument('api_key')
def configure(api_key):
    """Save API key to config file"""
    try:
        config_dir = Path.home() / '.incept'
        config_dir.mkdir(exist_ok=True)
        config_file = config_dir / 'config'

        with open(config_file, 'w') as f:
            json.dump({'api_key': api_key}, f)

        config_file.chmod(0o600)
        click.echo(f"âœ… API key saved to {config_file}")
    except Exception as e:
        click.echo(f"âŒ Error: {e}", err=True)
        sys.exit(1)

@cli.command()
def help():
    """Show detailed help and usage examples"""
    help_text = """
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    INCEPT-EVAL CLI HELP                           â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

OVERVIEW:
  Incept Eval is a CLI tool for evaluating educational questions using
  the Incept API. It supports comprehensive evaluation including:
  - V3 scaffolding/DI compliance scoring
  - Answer correctness verification
  - EduBench task evaluation (QA, EC, IP, AG)

INSTALLATION:
  pip install inceptbench

COMMANDS:

  1. configure - Save your API key
     Usage: inceptbench configure YOUR_API_KEY

     This saves your API key to ~/.incept/config for future use.

  2. example - Generate sample input file
     Usage: inceptbench example [OPTIONS]

     Options:
       -o, --output PATH    Save to file (default: qs.json)

     Examples:
       inceptbench example                    # Creates qs.json
       inceptbench example -o sample.json     # Creates sample.json

  3. evaluate - Evaluate questions from JSON file
     Usage: inceptbench evaluate INPUT_FILE [OPTIONS]

     Options:
       -o, --output PATH      Save results to file (overwrites)
       -a, --append PATH      Append results to file (creates if not exists)
       -k, --api-key KEY      API key (or use INCEPT_API_KEY env var)
       --api-url URL          API endpoint (default: https://uae-poc.inceptapi.com)
       --pretty               Show only scores (default: enabled)
       --no-pretty            Show full results including EduBench details
       -v, --verbose          Show progress messages
       -f, --full             Return full detailed evaluation (default: simplified scores)

     Examples:
       # Basic evaluation (simplified scores only)
       inceptbench evaluate test.json

       # Full detailed evaluation results
       inceptbench evaluate test.json --full

       # Save to file (overwrite)
       inceptbench evaluate test.json -o results.json

       # Append to file (creates if not exists)
       inceptbench evaluate test.json -a evaluations.json --verbose

       # Full detailed results with progress messages
       inceptbench evaluate test.json --full --verbose

       # Local API testing
       inceptbench evaluate test.json --api-url http://localhost:8000

API KEY CONFIGURATION (3 methods):

  1. Config file (recommended):
     inceptbench configure YOUR_API_KEY

  2. Environment variable:
     export INCEPT_API_KEY=your_api_key
     inceptbench evaluate test.json

  3. Command line flag:
     inceptbench evaluate test.json --api-key your_api_key

INPUT FILE FORMAT:

  The input JSON file must contain:
  - submodules_to_run: List of evaluation modules to enable
    ["math_qa_evaluator", "answer_verification", "math_content_evaluator", "reading_question_qc"]
  - generated_questions: Array of questions to evaluate with:
    - id: Unique question identifier
    - type: "mcq" or "fill-in"
    - question: Question text
    - answer: Correct answer
    - answer_explanation: Step-by-step explanation
    - answer_options: MCQ options (for MCQ type)
    - skill: Optional skill metadata

  Use 'inceptbench example' to see a complete example.

OUTPUT FORMAT:

  The response includes:
  - request_id: Unique evaluation identifier
  - evaluations: Per-question evaluation results:
    - math_qa_evaluator: Comprehensive quality scores (0-1 scale)
    - answer_verification: Answer correctness verification
    - external_edubench: EduBench task scores (0-10 scale)
    - final_score: Combined score from all modules (0-1 scale)
    - math_content_evaluator: Math content quality scores
    - reading_question_qc: Reading question quality scores
  - evaluation_time_seconds: Total evaluation time

QUICK START:

  # 1. Configure API key
  inceptbench configure YOUR_API_KEY

  # 2. Generate sample file
  inceptbench example

  # 3. Evaluate questions
  inceptbench evaluate qs.json --verbose

  # 4. Save results (overwrite)
  inceptbench evaluate test.json -o results.json

  # 5. Append multiple evaluations to one file
  inceptbench evaluate test1.json -a all_results.json
  inceptbench evaluate test2.json -a all_results.json
  inceptbench evaluate test3.json -a all_results.json

LOCAL TESTING:

  To test against a local API server:
  inceptbench evaluate test.json --api-url http://localhost:8000

For more information, visit: https://github.com/incept-ai/inceptbench
"""
    click.echo(help_text)

@cli.command()
@click.option('--output', '-o', type=click.Path(), default='qs.json', help='Save to file (default: qs.json)')
def example(output):
    """Generate sample test_questions.json file

    Creates a complete example with Arabic math question for the new
    universal unified benchmark evaluation format.

    By default, saves to qs.json in the current directory.
    """
    example_data = {
        "submodules_to_run": [
            "math_qa_evaluator",
            "answer_verification",
            "reading_question_qc",
            "math_content_evaluator"
        ],
        "generated_questions": [
            {
                "id": "q1",
                "type": "mcq",
                "question": "Ø¥Ø°Ø§ ÙƒØ§Ù† Ø«Ù…Ù† 2 Ù‚Ù„Ù… Ù‡Ùˆ 14 Ø±ÙŠØ§Ù„Ù‹Ø§ØŒ ÙÙ…Ø§ Ø«Ù…Ù† 5 Ø£Ù‚Ù„Ø§Ù… Ø¨Ù†ÙØ³ Ø§Ù„Ù…Ø¹Ø¯Ù„ØŸ",
                "answer": "35 Ø±ÙŠØ§Ù„Ù‹Ø§",
                "answer_explanation": "Ø§Ù„Ø®Ø·ÙˆØ© 1: ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø³Ø£Ù„Ø© â€” Ù„Ø¯ÙŠÙ†Ø§ Ø«Ù…Ù† 2 Ù‚Ù„Ù… ÙˆÙ‡Ùˆ 14 Ø±ÙŠØ§Ù„Ù‹Ø§. Ù†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ù…Ø¹Ø±ÙØ© Ø«Ù…Ù† 5 Ø£Ù‚Ù„Ø§Ù… Ø¨Ù†ÙØ³ Ø§Ù„Ù…Ø¹Ø¯Ù„. ÙŠØ¬Ø¨ Ø§Ù„ØªÙÙƒÙŠØ± ÙÙŠ Ø§Ù„Ø¹Ù„Ø§Ù‚Ø© Ø¨ÙŠÙ† Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ù‚Ù„Ø§Ù… ÙˆØ§Ù„Ø³Ø¹Ø± ÙˆÙƒÙŠÙÙŠØ© ØªØ­ÙˆÙŠÙ„ Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ù‚Ù„Ø§Ù… Ø¨Ù…Ø¹Ø¯Ù„ Ø«Ø§Ø¨Øª.\nØ§Ù„Ø®Ø·ÙˆØ© 2: ØªØ·ÙˆÙŠØ± Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© â€” ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø£ÙˆÙ„Ù‹Ø§ Ø¥ÙŠØ¬Ø§Ø¯ Ø«Ù…Ù† Ù‚Ù„Ù… ÙˆØ§Ø­Ø¯ Ø¨Ù‚Ø³Ù…Ø© 14 Ã· 2 = 7 Ø±ÙŠØ§Ù„ØŒ Ø«Ù… Ø¶Ø±Ø¨Ù‡ ÙÙŠ 5 Ù„Ø¥ÙŠØ¬Ø§Ø¯ Ø«Ù…Ù† 5 Ø£Ù‚Ù„Ø§Ù…: 7 Ã— 5 = 35 Ø±ÙŠØ§Ù„Ù‹Ø§.\nØ§Ù„Ø®Ø·ÙˆØ© 3: Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ ÙˆØ§Ù„ØªØ­Ù‚Ù‚ â€” Ù†ØªØ­Ù‚Ù‚ Ù…Ù† Ù…Ù†Ø·Ù‚ÙŠØ© Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø³Ø¹Ø± Ø¨Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ù‚Ù„Ø§Ù…. Ø§Ù„Ø³Ø¹Ø± ÙŠØªÙ†Ø§Ø³Ø¨ Ø·Ø±Ø¯ÙŠÙ‹Ø§ Ù…Ø¹ Ø§Ù„Ø¹Ø¯Ø¯ØŒ ÙˆØ¨Ø§Ù„ØªØ§Ù„ÙŠ 35 Ø±ÙŠØ§Ù„Ù‹Ø§ Ù‡ÙŠ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„ØµØ­ÙŠØ­Ø© ÙˆØ§Ù„Ù…Ù†Ø·Ù‚ÙŠØ©.",
                "answer_options": {
                    "A": "28 Ø±ÙŠØ§Ù„Ù‹Ø§",
                    "B": "70 Ø±ÙŠØ§Ù„Ù‹Ø§",
                    "C": "30 Ø±ÙŠØ§Ù„Ù‹Ø§",
                    "D": "35 Ø±ÙŠØ§Ù„Ù‹Ø§"
                },
                "skill": {
                    "title": "Grade 6 Mid-Year Comprehensive Assessment",
                    "grade": "6",
                    "subject": "mathematics",
                    "difficulty": "medium",
                    "description": "Apply proportional reasoning, rational number operations, algebraic thinking, geometric measurement, and statistical analysis to solve multi-step real-world problems",
                    "language": "ar"
                },
                "image_url": None,
                "additional_details": "ğŸ”¹ **Question generation logic:**\nThis question targets proportional reasoning for Grade 6 students, testing their ability to apply ratios and unit rates to real-world problems. It follows a classic proportionality structure â€” starting with a known ratio (2 items for 14 riyals) and scaling it up to 5 items. The stepwise reasoning develops algebraic thinking and promotes estimation checks to confirm logical correctness.\n\nğŸ”¹ **Personalized insight examples:**\n- Choosing 28 Ø±ÙŠØ§Ù„Ù‹Ø§ shows a misunderstanding by doubling instead of proportionally scaling.\n- Choosing 7 Ø±ÙŠØ§Ù„Ù‹Ø§ indicates the learner found the unit rate but didn't scale it up to 5.\n- Choosing 14 Ø±ÙŠØ§Ù„Ù‹Ø§ confuses the given 2-item cost with the required 5-item cost.\n\nğŸ”¹ **Instructional design & DI integration:**\nThe question aligns with *Percent, Ratio, and Probability* learning targets. In DI format 15.7, it models how equivalent fractions and proportional relationships can predict outcomes across different scales. This builds foundational understanding for probability and proportional reasoning. By using a simple, relatable context (price of pens), it connects mathematical ratios to practical real-world applications, supporting concept transfer and cognitive engagement."
            }
        ]
    }

    json_output = json.dumps(example_data, indent=2, ensure_ascii=False)

    with open(output, 'w', encoding='utf-8') as f:
        f.write(json_output)
    click.echo(f"âœ… Sample file saved to: {output}")

if __name__ == '__main__':
    cli()
