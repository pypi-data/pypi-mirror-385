from __future__ import annotations
import re, ast
from math import log2

__all__ = ["scan_ctx"]

ALLOWED_FILE_SUFFIXES = (".py", ".pyi", ".pyw")

PROVIDER_PATTERNS = [
    ("github", re.compile(r"(ghp|gho|ghu|ghs|ghr|gpat)_[A-Za-z0-9]{36,}")),
    ("gitlab", re.compile(r"glpat-[A-Za-z0-9_-]{20,}")),
    ("slack", re.compile(r"xox[abprs]-[A-Za-z0-9-]{10,48}")),
    ("stripe", re.compile(r"sk_(live|test)_[A-Za-z0-9]{16,}")),
    ("aws_access_key_id", re.compile(r"\b(AKIA|ASIA|AGPA|AIDA|AROA|AIPA)[0-9A-Z]{16}\b")),
    ("google_api_key", re.compile(r"\bAIza[0-9A-Za-z\-_]{35}\b")),
    ("sendgrid", re.compile(r"\bSG\.[A-Za-z0-9_-]{16,}\.[A-Za-z0-9_-]{16,}\b")),
    ("twilio", re.compile(r"\bSK[0-9a-fA-F]{32}\b")),
    ("private_key_block", re.compile(r"-----BEGIN (?:RSA|DSA|EC|OPENSSH|PGP) PRIVATE KEY-----")),
]

GENERIC_VALUE = re.compile(r"""(?ix)
    (?:
      (token|api[_-]?key|secret|password|passwd|pwd|bearer|auth[_-]?token|access[_-]?token)
      \s*[:=]\s*(?P<q>['"])(?P<val>[^'"]{16,})(?P=q)
    )
    |
    (?P<bare>
      (?=[A-Za-z0-9_-]{32,}\b)
      (?=.*[A-Z])
      (?=.*[a-z])
      (?=.*\d)
      [A-Za-z0-9_-]+
    )
""")

SAFE_TEST_HINTS = {
    "example", "sample", "fake", "placeholder", "dummy", "test_", "_test", "test_test_", 
    "changeme", "password", "secret", "not_a_real", "do_not_use",
}

_IDENTIFIER = re.compile(r"^[A-Za-z_][A-Za-z0-9_]*$")

IGNORE_DIRECTIVE = "skylos: ignore[SKY-S101]"
DEFAULT_MIN_ENTROPY = 3.9

IS_TEST_PATH = re.compile(r"(^|/)(tests?(/|$)|test_[^/]+\.py$)")

def _entropy(s):
    if len(s) == 0:
        return 0.0
    
    char_counts = {}
    for character in s:
        if character in char_counts:
            char_counts[character] += 1
        else:
            char_counts[character] = 1
    
    total_chars = len(s)
    entropy = 0.0
    
    for count in char_counts.values():
        probability = count / total_chars
        entropy -= probability * log2(probability)
    
    return entropy

def _mask(tok):
    token_length = len(tok)
    
    if token_length <= 8:
        return "*" * token_length
    
    else:
        first_part = tok[:4]
        last_part = tok[-4:]
        return first_part + "â€¦" + last_part

def _looks_like_identifier(s):
    return bool(_IDENTIFIER.fullmatch(s))

def _docstring_lines(tree):
    if tree is None:
        return set()
    
    docstring_line_numbers = set()

    def find_docstring_lines(node):
        if not hasattr(node, "body") or not node.body:
            return
        
        first_statement = node.body[0]
        
        is_expression = isinstance(first_statement, ast.Expr)
        if not is_expression:
            return
            
        value = getattr(first_statement, "value", None)
        if not isinstance(value, ast.Constant):
            return
            
        if not isinstance(value.value, str):
            return
        
        start_line = getattr(first_statement, "lineno", None)
        end_line = getattr(first_statement, "end_lineno", start_line)
        
        if start_line is not None:
            if end_line is None:
                end_line = start_line
            
            for line_num in range(start_line, end_line + 1):
                docstring_line_numbers.add(line_num)

    if isinstance(tree, ast.Module):
        find_docstring_lines(tree)

    for node in ast.walk(tree):
        if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef)):
            find_docstring_lines(node)
    
    return docstring_line_numbers

def scan_ctx(ctx, *, min_entropy= DEFAULT_MIN_ENTROPY, scan_comments= True,
              scan_docstrings= True, allowlist_patterns= None, ignore_path_substrings= None, ignore_tests=True):
    
    rel_path = ctx.get("relpath", "")
    if not rel_path.endswith(ALLOWED_FILE_SUFFIXES):
        return []
    
    if ignore_tests and IS_TEST_PATH.search(rel_path.replace("\\", "/")):
        return []
    
    if ignore_path_substrings:
        for substring in ignore_path_substrings:
            if substring and substring in rel_path:
                return []
    
    file_lines = ctx.get("lines") or []
    syntax_tree = ctx.get("tree")
    
    allowlist_regexes = []
    if allowlist_patterns:
        for pattern in allowlist_patterns:
            compiled_regex = re.compile(pattern)
            allowlist_regexes.append(compiled_regex)
    
    if scan_docstrings:
        docstring_lines = set()
    else:
        docstring_lines = _docstring_lines(syntax_tree)
    
    findings = []
    
    for line_number, raw_line in enumerate(file_lines, start=1):
        line_content = raw_line.rstrip("\n")
        
        if IGNORE_DIRECTIVE in line_content:
            continue
        
        stripped_line = line_content.lstrip()
        if not scan_comments and stripped_line.startswith("#"):
            continue
        
        if not scan_docstrings and line_number in docstring_lines:
            continue
        
        should_skip_line = False
        for regex_pattern in allowlist_regexes:
            if regex_pattern.search(line_content):
                should_skip_line = True
                break
        
        if should_skip_line:
            continue
        
        for provider_name, pattern_regex in PROVIDER_PATTERNS:
            pattern_matches = pattern_regex.finditer(line_content)
            
            for regex_match in pattern_matches:
                potential_secret = regex_match.group(0)
                
                token_lowercase = potential_secret.lower()
                has_safe_hint = False
                
                for safe_hint in SAFE_TEST_HINTS:
                    if safe_hint in token_lowercase:
                        has_safe_hint = True
                        break
                
                if has_safe_hint:
                    continue
                
                col_pos = line_content.find(potential_secret)
                
                finding = {
                    "rule_id": "SKY-S101",
                    "severity": "CRITICAL",
                    "provider": provider_name,
                    "message": f"Potential {provider_name} secret detected",
                    "file": rel_path,
                    "line": line_number,
                    "col": max(0, col_pos),
                    "end_col": max(1, col_pos + len(potential_secret)),
                    "preview": _mask(potential_secret),
                }
                findings.append(finding)
        
        aws_key_indicators = ["AWS_SECRET_ACCESS_KEY", "aws_secret_access_key"]
        line_has_aws_key = False
        
        for indicator in aws_key_indicators:
            if indicator in line_content or indicator in line_content.lower():
                line_has_aws_key = True
                break
        
        if line_has_aws_key:
            aws_secret_pattern = r"['\"]?([A-Za-z0-9/+=]{40})['\"]?"
            aws_match = re.search(aws_secret_pattern, line_content)
            
            if aws_match:
                aws_token = aws_match.group(1)
                tok_entropy = _entropy(aws_token)
                
                if tok_entropy >= min_entropy:
                    col_pos = line_content.find(aws_token)
                    
                    aws_finding = {
                        "rule_id": "SKY-S101",
                        "severity": "CRITICAL",
                        "provider": "aws_secret_access_key",
                        "message": "Potential AWS secret access key detected",
                        "file": rel_path,
                        "line": line_number,
                        "col": max(0, col_pos),
                        "end_col": max(1, col_pos + len(aws_token)),
                        "preview": _mask(aws_token),
                        "entropy": round(tok_entropy, 2),
                    }
                    findings.append(aws_finding)

        in_tests = bool(IS_TEST_PATH.search(rel_path.replace("\\", "/")))

        if in_tests:
            generic_match = None
        else:
            generic_match= GENERIC_VALUE.search(line_content)

        if generic_match:
            val_group = generic_match.group("val")
            bare_group = generic_match.group("bare")
            
            is_bare = False
            if val_group:
                extracted_token = val_group
            elif bare_group:
                extracted_token = bare_group
                is_bare = True
            else:
                extracted_token = ""
            
            clean_token = extracted_token.strip()
            
            if clean_token:
                if is_bare and _looks_like_identifier(clean_token):
                    continue
                
                token_lowercase = clean_token.lower()
                has_safe_hint = False
                
                for safe_hint in SAFE_TEST_HINTS:
                    if safe_hint in token_lowercase:
                        has_safe_hint = True
                        break
                
                if not has_safe_hint:
                    tok_entropy = _entropy(clean_token)
                    
                    if tok_entropy >= min_entropy and len(clean_token) >= 20:
                        col_pos = line_content.find(clean_token)
                        
                        generic_finding = {
                            "rule_id": "SKY-S101",
                            "severity": "CRITICAL",
                            "provider": "generic",
                            "message": f"High-entropy value detected (entropy={tok_entropy:.2f})",
                            "file": rel_path,
                            "line": line_number,
                            "col": max(0, col_pos),
                            "end_col": max(1, col_pos + len(clean_token)),
                            "preview": _mask(clean_token),
                            "entropy": round(tok_entropy, 2),
                        }
                        findings.append(generic_finding)
    
    return findings