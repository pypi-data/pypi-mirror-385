{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2242dd13-f045-48bc-812a-69c923cb76fc",
   "metadata": {},
   "source": [
    "# Check Idea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "726e140f-9c62-489f-91e0-2dea08dbfe2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from denario import Denario\n",
    "\n",
    "project_dir = \"/Users/boris/CMBAgents/AstroPilotApp/project_app\"\n",
    "den = Denario(project_dir=project_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed8e4f02-8c00-4442-bdce-74ccebdae801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# den.set_idea(project_dir+\"/input_files/idea.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c70a6945-c4e7-4642-aea8-234ca699c2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "den.set_idea()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34512cf0-43a4-4316-9348-52388a2797cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "idea = den.research.idea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aae90aaa-85b3-41ec-87eb-f53b3f0a57f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**IDEA:** Dynamic Narrative Adaptation in PlayStation Games Using Player Emotion Recognition and Generative AI\n",
      "\n",
      "**Description:** This paper explores dynamic narrative adaptation in PlayStation games by leveraging player emotion recognition from facial expressions (using the PlayStation Camera) and generative AI to modify the storyline in real-time. The system will analyze player emotions (e.g., frustration, excitement, boredom) and use a generative AI model to adjust narrative elements, such as dialogue, plot twists, and character interactions, to maintain optimal engagement and emotional investment.\n"
     ]
    }
   ],
   "source": [
    "print(idea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "014a2e06-fb1f-4cb1-aa3e-bdafe0b22b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from futurehouse_client import FutureHouseClient, JobNames\n",
    "from futurehouse_client.models import (\n",
    "    TaskRequest,\n",
    ")\n",
    "import os\n",
    "fhkey = os.getenv(\"FUTURE_HOUSE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a32fa0ca-b5d7-4224-ad35-cf01bbb63784",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fh_client = FutureHouseClient(\n",
    "    api_key=fhkey,\n",
    ")\n",
    "\n",
    "check_idea_prompt = rf\"\"\"\n",
    "Has anyone worked on or explored the following idea?\n",
    "\n",
    "{idea}\n",
    " \n",
    "<DESIRED_RESPONSE_FORMAT>\n",
    "Answer: <yes or no>\n",
    "\n",
    "Related previous work: <describe previous literature on the topic>\n",
    "</DESIRED_RESPONSE_FORMAT>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0bad188-5872-402f-bb4b-2177ba332743",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "task_data = TaskRequest(\n",
    "    name=JobNames.from_string(\"owl\"),\n",
    "    query=check_idea_prompt,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1394d006-5c56-442c-bb77-97efd26bd11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_response = fh_client.run_tasks_until_done(task_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a25e3a98-bc06-498d-913a-839fdcf86d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: No\n",
      "\n",
      "Related previous work: The reviewed literature reveals significant investigation into affective computing in games but does not demonstrate a system that combines real-time facial emotion recognition with generative AI for dynamic narrative adaptation in PlayStation games. Sinković (sinkovic2024aibasedaffectivemirroring pages 60-62) clearly explains that, due to limitations in current emotion recognition tools and integration challenges, AI was not employed for dynamic dialogue selection or real-time narrative adaptation, and manual control was instead preferred for narrative management. Similarly, other excerpts by Sinković (sinkovic2024aibasedaffectivemirroring pages 10-13, sinkovic2024aibasedaffectivemirroring pages 13-19, sinkovic2024aibasedaffectivemirroring pages 19-28, sinkovic2024aibasedaffectivemirroring pages 39-43, sinkovic2024aibasedaffectivemirroring pages 84-87, sinkovic2024aibasedaffectivemirroring pages 28-32) emphasize the exploration of facial expression analysis for NPC behavioral adaptation but do not report an implementation that modifies the narrative dynamically in response to player emotion using generative AI on PlayStation platforms.\n",
      "\n",
      "Christou’s work (christou2017anaffectivegaming pages 15-20, christou2017anaffectivegaming pages 20-23) demonstrates affective gaming through devices collecting physiological signals and facial expressions to adjust gameplay and other elements; however, these studies do not specifically address the adaptation of narrative storylines in real time on PlayStation games via generative AI. Kumaran et al. (kumaran2023scenecraftautomatinginteractive pages 1-2) present SCENECRAFT, a framework for generating interactive narrative scenes using natural language processing, yet they rely on high-level narrative inputs rather than integrating real-time facial emotion data from PlayStation hardware.\n",
      "\n",
      "The Emo Tale project by Manogyna et al. (manogyna2025emotaleshaping pages 3-6, manogyna2025emotaleshaping pages 1-3) explores dynamically adapting narrative trajectories based on readers’ affective states using multimodal emotion recognition and generative AI, but it does not extend to a PlayStation gaming context or employ facial emotion recognition from the PlayStation Camera. Hutson (hutson2024adaptiveworldsgenerative pages 1-3, hutson2024adaptiveworldsgenerative pages 3-4) focuses on the use of generative AI for real-time adaptive content and narrative generation across gaming environments, but without leveraging player biometric data such as facial expressions for live narrative modifications on PlayStation. Finally, Li (li2025embodiedaiagent pages 1-3) discusses embodied AI that adapts its interactive responses based on detected emotions, yet this work does not involve modifying game storylines dynamically on commercial consoles.\n",
      "\n",
      "In summary, while these works collectively establish foundational concepts in affective computing, emotion recognition, and generative narrative techniques, none of them specifically combine player emotion recognition from facial expressions with generative AI to drive real-time storyline modifications in PlayStation games.\n"
     ]
    }
   ],
   "source": [
    "print(task_response[0].answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "de4d38e6-4b31-412b-9bd6-48976a70caef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      "Has anyone worked on or explored the following idea?\n",
      "\n",
      "**IDEA:** Dynamic Narrative Adaptation in PlayStation Games Using Player Emotion Recognition and Generative AI\n",
      "\n",
      "**Description:** This paper explores dynamic narrative adaptation in PlayStation games by leveraging player emotion recognition from facial expressions (using the PlayStation Camera) and generative AI to modify the storyline in real-time. The system will analyze player emotions (e.g., frustration, excitement, boredom) and use a generative AI model to adjust narrative elements, such as dialogue, plot twists, and character interactions, to maintain optimal engagement and emotional investment.\n",
      " \n",
      "<DESIRED_RESPONSE_FORMAT>\n",
      "Answer: <yes or no>\n",
      "\n",
      "Related previous work: <describe previous literature on the topic>\n",
      "</DESIRED_RESPONSE_FORMAT>\n",
      "\n",
      "\n",
      "Answer: No\n",
      "\n",
      "Related previous work: The reviewed literature reveals significant investigation into affective computing in games but does not demonstrate a system that combines real-time facial emotion recognition with generative AI for dynamic narrative adaptation in PlayStation games. Sinković (sinkovic2024aibasedaffectivemirroring pages 60-62) clearly explains that, due to limitations in current emotion recognition tools and integration challenges, AI was not employed for dynamic dialogue selection or real-time narrative adaptation, and manual control was instead preferred for narrative management. Similarly, other excerpts by Sinković (sinkovic2024aibasedaffectivemirroring pages 10-13, sinkovic2024aibasedaffectivemirroring pages 13-19, sinkovic2024aibasedaffectivemirroring pages 19-28, sinkovic2024aibasedaffectivemirroring pages 39-43, sinkovic2024aibasedaffectivemirroring pages 84-87, sinkovic2024aibasedaffectivemirroring pages 28-32) emphasize the exploration of facial expression analysis for NPC behavioral adaptation but do not report an implementation that modifies the narrative dynamically in response to player emotion using generative AI on PlayStation platforms.\n",
      "\n",
      "Christou’s work (christou2017anaffectivegaming pages 15-20, christou2017anaffectivegaming pages 20-23) demonstrates affective gaming through devices collecting physiological signals and facial expressions to adjust gameplay and other elements; however, these studies do not specifically address the adaptation of narrative storylines in real time on PlayStation games via generative AI. Kumaran et al. (kumaran2023scenecraftautomatinginteractive pages 1-2) present SCENECRAFT, a framework for generating interactive narrative scenes using natural language processing, yet they rely on high-level narrative inputs rather than integrating real-time facial emotion data from PlayStation hardware.\n",
      "\n",
      "The Emo Tale project by Manogyna et al. (manogyna2025emotaleshaping pages 3-6, manogyna2025emotaleshaping pages 1-3) explores dynamically adapting narrative trajectories based on readers’ affective states using multimodal emotion recognition and generative AI, but it does not extend to a PlayStation gaming context or employ facial emotion recognition from the PlayStation Camera. Hutson (hutson2024adaptiveworldsgenerative pages 1-3, hutson2024adaptiveworldsgenerative pages 3-4) focuses on the use of generative AI for real-time adaptive content and narrative generation across gaming environments, but without leveraging player biometric data such as facial expressions for live narrative modifications on PlayStation. Finally, Li (li2025embodiedaiagent pages 1-3) discusses embodied AI that adapts its interactive responses based on detected emotions, yet this work does not involve modifying game storylines dynamically on commercial consoles.\n",
      "\n",
      "In summary, while these works collectively establish foundational concepts in affective computing, emotion recognition, and generative narrative techniques, none of them specifically combine player emotion recognition from facial expressions with generative AI to drive real-time storyline modifications in PlayStation games.\n",
      "\n",
      "References\n",
      "\n",
      "1. (sinkovic2024aibasedaffectivemirroring pages 60-62): H Šinković. Ai-based affective mirroring in video game npcs. Unknown journal, 2024.\n",
      "\n",
      "2. (christou2017anaffectivegaming pages 15-20): C Christou. An affective gaming scenario using the kinect sensors. Unknown journal, 2017.\n",
      "\n",
      "3. (christou2017anaffectivegaming pages 20-23): C Christou. An affective gaming scenario using the kinect sensors. Unknown journal, 2017.\n",
      "\n",
      "4. (kumaran2023scenecraftautomatinginteractive pages 1-2): Vikram Kumaran, Jonathan Rowe, Bradford W. Mott, and James C. Lester. Scenecraft: automating interactive narrative scene generation in digital games with large language models. Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment, 19:86-96, Oct 2023. URL: https://doi.org/10.1609/aiide.v19i1.27504, doi:10.1609/aiide.v19i1.27504. This article has 61 citations.\n",
      "\n",
      "5. (manogyna2025emotaleshaping pages 3-6): K. Sai Manogyna, M. Vasudevan, B. Susindra Reddy, S. Hussna Begum, D. Madhu Babu, and S. Ibrahim. Emo tale: shaping narrative trajectories with reader's affective states. Challenges in Information, Communication and Computing Technology, pages 678-683, Nov 2025. URL: https://doi.org/10.1201/9781003559092-117, doi:10.1201/9781003559092-117. This article has 0 citations.\n",
      "\n",
      "6. (sinkovic2024aibasedaffectivemirroring pages 10-13): H Šinković. Ai-based affective mirroring in video game npcs. Unknown journal, 2024.\n",
      "\n",
      "7. (sinkovic2024aibasedaffectivemirroring pages 13-19): H Šinković. Ai-based affective mirroring in video game npcs. Unknown journal, 2024.\n",
      "\n",
      "8. (sinkovic2024aibasedaffectivemirroring pages 19-28): H Šinković. Ai-based affective mirroring in video game npcs. Unknown journal, 2024.\n",
      "\n",
      "9. (sinkovic2024aibasedaffectivemirroring pages 39-43): H Šinković. Ai-based affective mirroring in video game npcs. Unknown journal, 2024.\n",
      "\n",
      "10. (sinkovic2024aibasedaffectivemirroring pages 84-87): H Šinković. Ai-based affective mirroring in video game npcs. Unknown journal, 2024.\n",
      "\n",
      "11. (hutson2024adaptiveworldsgenerative pages 1-3): J Hutson. Adaptive worlds: generative ai in game design and future of gaming, and interactive media. Unknown journal, 2024.\n",
      "\n",
      "12. (hutson2024adaptiveworldsgenerative pages 3-4): J Hutson. Adaptive worlds: generative ai in game design and future of gaming, and interactive media. Unknown journal, 2024.\n",
      "\n",
      "13. (manogyna2025emotaleshaping pages 1-3): K. Sai Manogyna, M. Vasudevan, B. Susindra Reddy, S. Hussna Begum, D. Madhu Babu, and S. Ibrahim. Emo tale: shaping narrative trajectories with reader's affective states. Challenges in Information, Communication and Computing Technology, pages 678-683, Nov 2025. URL: https://doi.org/10.1201/9781003559092-117, doi:10.1201/9781003559092-117. This article has 0 citations.\n",
      "\n",
      "14. (sinkovic2024aibasedaffectivemirroring pages 28-32): H Šinković. Ai-based affective mirroring in video game npcs. Unknown journal, 2024.\n",
      "\n",
      "15. (li2025embodiedaiagent pages 1-3): Embodied AI Agent for Co-creation Ecosystem: Elevating Human-AI Co-creation through Emotion Recognition and Dynamic Personality Adaptation\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(task_response[0].formatted_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f5e536-a186-496c-b8f8-8a46a30cbf38",
   "metadata": {},
   "source": [
    "# Check Idea fast (using semantic scholar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79e14321-3bb8-4129-9605-1ce1b747d5f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OxWalk Annotated Step Count Dataset\n",
      "\n",
      "DATA DESCRIPTION:\n",
      "Annotated step data during unscripted, free living in 39 healthy adult volunteers (aged 18 and above) with no lower limb injury within the previous 6 months and who were able to walk without an assistive device. \n",
      "Participants wore four triaxial accelerometers concurrently (AX3, Axivity, Newcastle, UK), two placed side-by-side on the dominant wrist and two clipped to the dominant-side hip at the midsagittal plane. Accelerometers were synchronised using the Open Movement GUI software (v.1.0.0.42), with one recording at 100 Hz and the other at 25 Hz at each body location. Foot-facing video was captured using an action camera (Action Camera CT9500, Crosstour, Shenzhen, China) mounted at the participant’s beltline. \n",
      "\n",
      "DATA ANNOTATION:\n",
      "Foot-facing video was captured for up to one hour using an action camera (Action Camera CT9500, Crosstour, Shenzhen, China) mounted at the participant’s beltline. Annotation of steps was conducted within video annotation software (Elan 6.0, The Language Archive, Nijmegen, Netherlands), where a step was defined as the act of purposeful lifting a foot and placing it in a new location. Steps were not required to be part of a repeating pattern and did not include foot shuffling, changing of foot alignment via pivoting, or shifting of weight from one foot to the other. \n",
      "\n",
      "DATASETS:\n",
      "The data is located in /mnt/ceph/users/fvillaescusa/AstroPilot/Aidan/data. In that folder there are 4 folders (Hip_25Hz, Hip_100Hz, Wrist_25Hz, Wrist_100Hz) and one file (metadata.csv).\n",
      "\n",
      "Datasets are as follows:\n",
      "1) \"Wrist_100Hz\": One Axivity AX3 accelerometer, recording at 100 Hz and +/- 8g on the dominant wrist within a silicone wristband, with axes aligned as prescribed by the manufacturer. <https://axivity.com/userguides/ax3/technical/#axis-alignment>\n",
      "2) \"Wrist_25Hz\": One Axivity AX3 accelerometer, recording at 25 Hz and +/- 8g on the dominant wrist within a silicone wristband, with axes aligned as prescribed by the manufacturer. <https://axivity.com/userguides/ax3/technical/#axis-alignment>\n",
      "3) \"Hip_100Hz\": One Axivity AX3 accelerometer, recording at 100 Hz and +/- 8g cliped at the beltline, laterally above the dominant leg, with the +X axis approximately aligned in the superior direction, and the positive Y axis aligned to face anteriorly.\n",
      "4) \"Hip_25Hz\": One Axivity AX3 accelerometer, recording at 100 Hz and +/- 8g cliped at the beltline, laterally above the dominant leg, with the +X axis approximately aligned in the superior direction, and the positive Y axis aligned to face anteriorly.\n",
      "5) Participant sex and age range are provided in metadata.csv\n",
      "\n",
      "Inside each of these folders, there are 39 files, one for each participant. The files are called as PX_hip25.csv, for participant X (01, 02, 03, ...39) and hip25 is for the files inside Hip_25Hz.\n",
      "\n",
      "The accelerometer data in each file has been resampled and calibrated using the Open Movement GUI software package. Within each CSV file, a step is annotated by a single \"1\" at the approximate time of heel strike. \n",
      "\n",
      "\n",
      "The primary purpose of this data collection is the development of step-counting algorithms, which, given just the raw accelerometer data, should be able to correctly estimate the number of steps. This is particularly useful to compare a variety of algorithms and machine learning models, currently used for this purpose. With the concurrent collection of data from 2 different locations, we could also observe how the performance of these step-counting algorithms compares between the wrist and the hip. Using the metadata file, we can further explore if there are noted differences in different age groups or sexes. \n",
      "\n",
      "Of particular interest to me is observing how step-counting algorithms are affected by the reduction in raw sampling frequency. While sampling at a lower frequency may cause the monitor to miss some high-frequency behaviour, the vast majority of human movement occurs at far lower frequencies than the monitor can observe, sampling at 100Hz. Sampling data at a lower frequency can allow for longer periods of monitoring and requires less computing power, which is ideal, as long as it does not cause a significant drop in model performance. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from denario import Denario\n",
    "\n",
    "# set Denario telling it where the project folder is\n",
    "folder = \"Project1\" #Project folder\n",
    "astro_pilot = Denario(project_dir=folder)\n",
    "\n",
    "# set data description. This file contains the description of the data\n",
    "astro_pilot.set_data_description(f\"{folder}/input.md\")\n",
    "\n",
    "# show the content of the data description file\n",
    "print(astro_pilot.research.data_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196b5832-7345-4cb6-868d-dae8448a8c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Addressing idea novelty: round 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing novelty:  40%|██████████               | 2/5 [00:22<00:33, 11.21s/try]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision made: query\n",
      "Query: deep learning step counting accelerometer sensor location invariance sampling frequency invariance demographic adaptation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling Semantic Scholar:   1%|▏               | 2/200 [00:01<02:44,  1.20try/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total papers 0\n",
      "\n",
      "Addressing idea novelty: round 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing novelty:  20%|█████                    | 1/5 [00:18<01:15, 18.81s/try]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision made: query\n",
      "Query: deep learning step counting sensor location invariance sampling frequency invariance demographic adaptation accelerometer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling Semantic Scholar:   0%|                        | 0/200 [00:00<?, ?try/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total papers 0\n",
      "\n",
      "Addressing idea novelty: round 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing novelty:   0%|                                 | 0/5 [00:09<?, ?try/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision made: query\n",
      "Query: deep learning step counting \"sampling frequency invariance\" \"sensor location invariance\" \"demographic adaptation\" accelerometer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling Semantic Scholar:   0%|                        | 0/200 [00:00<?, ?try/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total papers 0\n",
      "\n",
      "Addressing idea novelty: round 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing novelty:   0%|                                 | 0/5 [00:07<?, ?try/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision made: query\n",
      "Query: deep learning step counting accelerometer sensor location invariance sampling frequency invariance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling Semantic Scholar:   0%|                | 1/200 [00:01<04:17,  1.29s/try]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 potentially relevant papers\n",
      "Total papers 2\n",
      "\n",
      "Addressing idea novelty: round 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing novelty:  20%|█████                    | 1/5 [00:28<01:54, 28.63s/try]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision made: query\n",
      "Query: deep learning step counting frequency invariant sensor location invariant hip wrist demographic age sex\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling Semantic Scholar:   1%|▏               | 2/200 [00:01<03:03,  1.08try/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total papers 2\n",
      "\n",
      "Addressing idea novelty: round 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing novelty:   0%|                                 | 0/5 [00:11<?, ?try/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision made: query\n",
      "Query: \"deep learning gait representations step counting accelerometer\" AND (\"frequency invariance\" OR \"sampling rate invariance\" OR \"multi-frequency\") AND (\"location invariance\" OR \"sensor placement invariance\" OR \"multi-location\" OR \"hip wrist\") AND (\"demographic adaptation\" OR \"age-specific\" OR \"sex-specific\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling Semantic Scholar:   1%|▏               | 2/200 [00:01<03:00,  1.10try/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total papers 2\n",
      "\n",
      "Addressing idea novelty: round 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing novelty:  20%|█████                    | 1/5 [00:26<01:46, 26.53s/try]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision made: novel\n",
      "The proposed idea, \"Universal Gait Fingerprints: Learning Demographically-Adaptive, Frequency- and Location-Invariant Representations for Robust Step Counting in Free-Living,\" can be considered **novel** based on the provided literature search iterations.\n",
      "\n",
      "The core novelty lies in the ambitious integration of several challenging aspects:\n",
      "1.  **Specific Task:** Focusing on *step counting* rather than broader human activity recognition (HAR).\n",
      "2.  **Multi-Modal Invariance:** Aiming to learn \"gait fingerprints\" that are *inherently invariant* to *both* sampling frequency (100Hz vs. 25Hz) and sensor location (hip vs. wrist) *simultaneously*.\n",
      "3.  **Advanced Representation Learning:** Utilizing novel deep learning architectures like contrastive learning, meta-learning, or multi-task learning specifically for extracting these universal, low-dimensional representations.\n",
      "4.  **Demographic Adaptation:** Explicitly investigating the influence of participant age and sex to develop demographically-adaptive models or identify common/unique gait signatures.\n",
      "5.  **Dataset Utilization:** Leveraging the unique characteristics of the OxWalk dataset, which provides synchronized multi-location and multi-frequency accelerometer data with precise step annotations and demographic metadata, to directly address these challenges.\n",
      "\n",
      "The literature search, spanning multiple iterations, yielded very limited relevant results, indicating a gap in existing research that combines these specific objectives and methodologies.\n",
      "\n",
      "**Most Similar and Relevant Found Paper:**\n",
      "\n",
      "The most similar paper found was:\n",
      "*   **A Hybrid Approach for Human Activity Recognition with Support Vector Machine and 1D Convolutional Neural Network**\n",
      "    *   Link: https://www.semanticscholar.org/paper/c03205cf41047786b6e9e716490f9cfc72479ae67\n",
      "\n",
      "**Similarities:**\n",
      "*   **Deep Learning for Wearables:** Both the proposed idea and this paper utilize deep learning (specifically 1D Convolutional Neural Networks) for processing accelerometer data from wearable sensors.\n",
      "*   **Accelerometer Data:** Both approaches rely on accelerometer data as input.\n",
      "*   **Generalization/Invariance Concept:** The found paper mentions \"scale invariance\" in the context of Human Activity Recognition (HAR), suggesting an attempt to generalize across some variations in movement. It uses waist-mounted accelerometers, which aligns with one of the sensor locations in the OxWalk dataset.\n",
      "\n",
      "**Differences:**\n",
      "*   **Task Specificity:** The found paper focuses on *Human Activity Recognition* (HAR), a broader task, whereas the proposed idea specifically targets *step counting*, which requires fine-grained detection of discrete events rather than classification of continuous activities.\n",
      "*   **Type of Invariance:** While the found paper mentions \"scale invariance,\" it does not explicitly address or demonstrate invariance to *varying sampling frequencies* (e.g., 100Hz vs. 25Hz) or *distinct sensor locations* (hip *and* wrist simultaneously) for the purpose of step counting. The proposed idea's core is to achieve invariance across these specific sensor configurations.\n",
      "*   **Methodology for Invariance:** The proposed idea explicitly plans to use advanced deep learning techniques such as contrastive learning, meta-learning, or multi-task learning to *learn* these invariant representations (\"gait fingerprints\"). The found paper's \"scale invariance\" is a more general concept within its CNN architecture for HAR, not a dedicated approach for cross-sensor/frequency representation learning.\n",
      "*   **Demographic Adaptation:** A critical component of the proposed idea is the investigation and adaptation to demographic factors like age and sex. The found paper does not mention or address demographic influences on its HAR models.\n",
      "*   **Scope of Data:** The proposed idea leverages a dataset (OxWalk) specifically designed with synchronized multi-frequency and multi-location data, allowing for a direct investigation of the proposed invariances, which is not evident in the found paper.\n",
      "\n",
      "The second paper, \"Adaptive Modular Architectures for Rich Motor Skills ICT-248311 D 5 . 2 March 2012 ( 24 months ) Technical report on Hierarchical Reservoir Computing architectures,\" could not be assessed due to the lack of an abstract.\n",
      "\n",
      "In conclusion, while individual aspects like deep learning for accelerometer data or general activity recognition exist, the specific combination of learning frequency- and location-invariant representations for *robust step counting* using advanced deep learning techniques (contrastive, meta-learning) and explicitly incorporating *demographic adaptation* is not clearly addressed in the identified literature. This integrated approach, especially tailored to the unique characteristics of the OxWalk dataset, strongly supports the novelty of the proposed idea.\n",
      "done 24684 23337\n",
      "Literature checked in 2 min 23 sec.\n"
     ]
    }
   ],
   "source": [
    "# beyond the Project1/input.md file, the module requires a file called Project1/idea.md, that contains the actual idea to check\n",
    "\n",
    "# run the agentic module (it will go through a maximum of 7 iterations to find relevant papers)\n",
    "astro_pilot.check_idea(llm='gemini-2.5-flash', max_iterations=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf58035-228c-4389-b13a-3187152ccbf3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
