{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-end demo with MTRAG benchmark data\n",
    "\n",
    "This notebook shows several examples of end-to-end RAG use cases that use the retrieval\n",
    "IO processor in conjunction with the IO processors for other Granite-based LoRA \n",
    "adapters. More information about the models used here can be found in our [technical\n",
    "report](https://arxiv.org/html/2504.11704v1).\n",
    "\n",
    "This notebook can run its own vLLM server to perform inference, or you can host the \n",
    "models on your own server. To use your own server, set the `run_server` variable below\n",
    "to `False` and set appropriate values for the constants in the cell marked\n",
    "`# Constants go here`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "from granite_io.io.granite_3_3.input_processors.granite_3_3_input_processor import (\n",
    "    Granite3Point3Inputs,\n",
    ")\n",
    "from granite_io import make_io_processor, make_backend\n",
    "from granite_io.io.base import RewriteRequestProcessor\n",
    "from granite_io.io.voting import MBRDMajorityVotingProcessor\n",
    "from granite_io.io.retrieval.util import download_mtrag_embeddings\n",
    "from granite_io.io.retrieval import InMemoryRetriever, RetrievalRequestProcessor\n",
    "from granite_io.io.answerability import (\n",
    "    AnswerabilityIOProcessor,\n",
    "    AnswerabilityCompositeIOProcessor,\n",
    ")\n",
    "from granite_io.io.query_rewrite import QueryRewriteIOProcessor\n",
    "from granite_io.io.citations import CitationsCompositeIOProcessor\n",
    "from granite_io.io.hallucinations import HallucinationsCompositeIOProcessor\n",
    "from granite_io.backend.vllm_server import LocalVLLMServer\n",
    "from granite_io.io.certainty import CertaintyIOProcessor\n",
    "from IPython.display import display, Markdown\n",
    "from granite_io.io.rag_agent_lib import obtain_loras\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants go here\n",
    "temp_data_dir = \"../data/test_retrieval_temp\"\n",
    "corpus_name = \"govt\"\n",
    "embeddings_data_file = pathlib.Path(temp_data_dir) / f\"{corpus_name}_embeds.parquet\"\n",
    "embedding_model_name = \"multi-qa-mpnet-base-dot-v1\"\n",
    "model_name = \"ibm-granite/granite-3.3-8b-instruct\"\n",
    "\n",
    "query_rewrite_lora_name = \"query_rewrite\"\n",
    "citations_lora_name = \"citation_generation\"\n",
    "answerability_lora_name = \"answerability_prediction\"\n",
    "hallucination_lora_name = \"hallucination_detection\"\n",
    "certainty_lora_name = \"certainty\"\n",
    "all_lora_names = [\n",
    "    query_rewrite_lora_name,\n",
    "    citations_lora_name,\n",
    "    answerability_lora_name,\n",
    "    hallucination_lora_name,\n",
    "    certainty_lora_name,\n",
    "]\n",
    "\n",
    "run_server = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_server:\n",
    "    # Start by firing up a local vLLM server and connecting a backend instance to it.\n",
    "    # Download and cache LoRA adapters.\n",
    "    lora_model_paths = obtain_loras(all_lora_names)\n",
    "    server = LocalVLLMServer(\n",
    "        model_name,\n",
    "        lora_adapters=lora_model_paths,\n",
    "    )\n",
    "    server.wait_for_startup(200)\n",
    "    query_rewrite_lora_backend = server.make_lora_backend(query_rewrite_lora_name)\n",
    "    citations_lora_backend = server.make_lora_backend(citations_lora_name)\n",
    "    answerability_lora_backend = server.make_lora_backend(answerability_lora_name)\n",
    "    hallucination_lora_backend = server.make_lora_backend(hallucination_lora_name)\n",
    "    certainty_lora_backend = server.make_lora_backend(certainty_lora_name)\n",
    "    backend = server.make_backend()\n",
    "else:  # if not run_server\n",
    "    # Use an existing server.\n",
    "    # The constants here are for the server that local_vllm_server.ipynb starts.\n",
    "    # Modify as needed.\n",
    "    openai_base_url = \"http://localhost:55555/v1\"\n",
    "    openai_api_key = \"granite_intrinsics_1234\"\n",
    "    backend = make_backend(\n",
    "        \"openai\",\n",
    "        {\n",
    "            \"model_name\": model_name,\n",
    "            \"openai_base_url\": openai_base_url,\n",
    "            \"openai_api_key\": openai_api_key,\n",
    "        },\n",
    "    )\n",
    "    query_rewrite_lora_backend = make_backend(\n",
    "        \"openai\",\n",
    "        {\n",
    "            \"model_name\": query_rewrite_lora_name,\n",
    "            \"openai_base_url\": openai_base_url,\n",
    "            \"openai_api_key\": openai_api_key,\n",
    "        },\n",
    "    )\n",
    "    citations_lora_backend = make_backend(\n",
    "        \"openai\",\n",
    "        {\n",
    "            \"model_name\": citations_lora_name,\n",
    "            \"openai_base_url\": openai_base_url,\n",
    "            \"openai_api_key\": openai_api_key,\n",
    "        },\n",
    "    )\n",
    "    answerability_lora_backend = make_backend(\n",
    "        \"openai\",\n",
    "        {\n",
    "            \"model_name\": answerability_lora_name,\n",
    "            \"openai_base_url\": openai_base_url,\n",
    "            \"openai_api_key\": openai_api_key,\n",
    "        },\n",
    "    )\n",
    "    hallucination_lora_backend = make_backend(\n",
    "        \"openai\",\n",
    "        {\n",
    "            \"model_name\": hallucination_lora_name,\n",
    "            \"openai_base_url\": openai_base_url,\n",
    "            \"openai_api_key\": openai_api_key,\n",
    "        },\n",
    "    )\n",
    "    certainty_lora_backend = make_backend(\n",
    "        \"openai\",\n",
    "        {\n",
    "            \"model_name\": certainty_lora_name,\n",
    "            \"openai_base_url\": openai_base_url,\n",
    "            \"openai_api_key\": openai_api_key,\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the indexed corpus if it hasn't already been downloaded.\n",
    "# This notebook uses a subset of the government corpus from the MTRAG benchmark.\n",
    "embeddings_location = f\"{temp_data_dir}/{corpus_name}_embeds.parquet\"\n",
    "if not os.path.exists(embeddings_location):\n",
    "    download_mtrag_embeddings(embedding_model_name, corpus_name, embeddings_location)\n",
    "embeddings_location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by creating an example chat completion request. \n",
    "\n",
    "This chat completion request simulates a scenario where the user is chatting with the\n",
    "automated help desk agent of the California State Parks and is asking about internship\n",
    "opportunities. The agent is about to respond to the user's question, \"Cool, how to I\n",
    "sign up?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an example chat completions request.\n",
    "chat_input = Granite3Point3Inputs.model_validate(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"Welcome to the California State Parks help desk.\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"I'm a student. Do you have internships?\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"The California State Parks hires Student Assistants \"\n",
    "                \"to perform a variety of tasks that require limited or no previous \"\n",
    "                \"work experience.\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": \"Cool, how do I sign up?\"},\n",
    "        ],\n",
    "        \"generate_inputs\": {\n",
    "            \"temperature\": 0.0,\n",
    "            \"max_tokens\": 4096,\n",
    "        },\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by passing the chat completion request directly to the language model,\n",
    "without using retrieval-augmented generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spin up an IO processor for the base model\n",
    "io_proc = make_io_processor(model_name, backend=backend)\n",
    "\n",
    "# Use the IO processor to generate a chat completion\n",
    "non_rag_result = io_proc.create_chat_completion(chat_input)\n",
    "display(Markdown(non_rag_result.results[0].next_message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result is a hallucination. The actual correct answer can be found [here](\n",
    "    https://www.parks.ca.gov/?page_id=848\n",
    ").\n",
    "\n",
    "We can use the \n",
    "[Granite 3.3 8b Instruct - Uncertainty LoRA](\n",
    "    https://huggingface.co/ibm-granite/granite-3.3-8b-rag-agent-lib/blob/main/certainty_lora/README.md)\n",
    "LoRA adapter to flag cases such as this one that are not covered by the base model's \n",
    "training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "certainty_io_proc = CertaintyIOProcessor(certainty_lora_backend)\n",
    "certainty_score = (\n",
    "    certainty_io_proc.create_chat_completion(chat_input).results[0].next_message.content\n",
    ")\n",
    "print(f\"Certainty score is {certainty_score} out of 1.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The low certainty score indicates that the model's training data does not align closely\n",
    "with this question.\n",
    "\n",
    "To answer this question properly, we need to provide the model with domain-specific \n",
    "information. In this case, the relevant information can be found in the Government \n",
    "corpus of the [MTRAG multi-turn RAG benchmark](https://github.com/IBM/mt-rag-benchmark).\n",
    "Let's spin up an in-memory vector database, using embeddings that we've precomputed\n",
    "offline from this corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spin up an in-memory vector database\n",
    "retriever = InMemoryRetriever(embeddings_data_file, embedding_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can send string queries against this vector database to retrieve relevant documents.\n",
    "Here we query the database with the user's last turn from our example conversation, \n",
    "\"Cool, how do I sign up?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The vector database fetches document snippets that match a given query.\n",
    "# For example, the user's question in the conversation above:\n",
    "print(f\"Query is: '{chat_input.messages[-1].content}'\")\n",
    "print(\"Matching document snippets:\")\n",
    "pd.set_option(\"display.max_colwidth\", 120)\n",
    "retriever.retrieve(chat_input.messages[-1].content, top_k=3).to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we attach a GraniteIO RetrievalRequestProcessor to our vector database, we can use\n",
    "this RequestProcessor to augment the original chat completion request with the document\n",
    "snippets that the retriever fetches when fed the last user turn as a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_request_proc = RetrievalRequestProcessor(retriever, top_k=3)\n",
    "chat_input_with_docs = retrieval_request_proc.process(chat_input)[0]\n",
    "chat_input_with_docs.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the retriever here operates over the last user turn. In this particular \n",
    "conversation, the last user turn is the phrase, \"Cool, how do I sign up?\", which is \n",
    "missing crucial information for retrieving relevant documents -- specifically, what is\n",
    "the user attempting to sign up for? \n",
    "\n",
    "As a result, the snippets retrieved are not specific to the user's intended question.\n",
    "Instead, they cover the general topic of signing up for things.\n",
    "\n",
    "Let's see what happens if we run our request through the model using these low-quality \n",
    "document snippets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_result = io_proc.create_chat_completion(chat_input_with_docs)\n",
    "display(Markdown(rag_result.results[0].next_message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the model correctly refuses to answer the question.\n",
    "\n",
    "Unfortunately, the training data for most LLMs is biased against \n",
    "producing this type of result, leading to frequent hallucinations in the presence of\n",
    "faulty retrieved documents. For example, if the last user turn in our example \n",
    "conversation is \"How to I sign up?\", instead of \"*Cool,* how do I sign up?\", the model\n",
    "produces an entirely different response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the last user turn from \"Cool, how do I sign up?\" to \"How to I sign up?\"\n",
    "messages_no_cool = chat_input_with_docs.messages.copy()\n",
    "messages_no_cool[-1].content = \"How do I sign up?\"\n",
    "chat_input_no_cool = chat_input_with_docs.model_copy(\n",
    "    update={\"messages\": messages_no_cool}\n",
    ")\n",
    "rag_result_no_cool = io_proc.create_chat_completion(chat_input_no_cool)\n",
    "display(Markdown(rag_result_no_cool.results[0].next_message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [LoRA Adapter for Answerability Classification](\n",
    "    https://huggingface.co/ibm-granite/granite-3.3-8b-rag-agent-lib/blob/main/answerability_prediction_lora/README.md\n",
    ")\n",
    "provides a more robust way to detect this kind of problem. Here's what happens if we \n",
    "run the chat completion request with faulty documents snippets through the \n",
    "answerability model, using the\n",
    "`granite_io` IO processor for the model to handle input and output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answerability_proc = AnswerabilityIOProcessor(answerability_lora_backend)\n",
    "answerability_proc.create_chat_completion(chat_input_with_docs).results[\n",
    "    0\n",
    "].next_message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The answerability model detects that the documents we have retrieved cannot be used to\n",
    "answer the user's question. We use use a composite IO processor to wrap this check in\n",
    "a flow that falls back on canned response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answerability_composite_proc = AnswerabilityCompositeIOProcessor(\n",
    "    io_proc, answerability_proc\n",
    ")\n",
    "composite_result = answerability_composite_proc.create_chat_completion(\n",
    "    chat_input_with_docs\n",
    ").results[0]\n",
    "print(composite_result.next_message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we've improved our model output from a hallucinated response to a refusal\n",
    "to answer the question. This result is an improvement, but we can do better if we can\n",
    "retrieve document snippets that are relevant to the user's intent as expressed in the\n",
    "*entire* conversation, not just the last turn.\n",
    "\n",
    "We can use use the [LoRA Adapter for Query Rewrite](\n",
    "    https://huggingface.co/ibm-granite/granite-3.2-8b-lora-rag-query-rewrite\n",
    ") to rewrite\n",
    "the last user turn into a string that is more useful for retrieving document snippets.\n",
    "Here's what we get if we call this model directly on the original request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewrite_io_proc = QueryRewriteIOProcessor(query_rewrite_lora_backend)\n",
    "rewrite_io_proc.create_chat_completion(chat_input).results[0].next_message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the LoRA Adapter for Query Rewrite is a language model, we can ask it to generate\n",
    "multiple different rewrites. We'll use this capability later on to improve end-to-end\n",
    "result quality further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_rewrites = rewrite_io_proc.create_chat_completion(\n",
    "    chat_input.with_addl_generate_params({\"n\": 10, \"temperature\": 0.8})\n",
    ").results\n",
    "[r.next_message.content for r in multiple_rewrites]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can wrap the IO processor for this model in a request processor that rewrites\n",
    "the last turn of the chat completion request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewrite_request_proc = RewriteRequestProcessor(rewrite_io_proc)\n",
    "rewritten_chat_input = rewrite_request_proc.process(chat_input)[0]\n",
    "print(\"Messages after rewrite:\")\n",
    "[{\"role\": m.role, \"content\": m.content} for m in rewritten_chat_input.messages]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can fetch documents with the rewritten query, then use use the answerability IO processor to check that the fetched documents answer the rewritten query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewritten_chat_input_with_docs = retrieval_request_proc.process(rewritten_chat_input)[0]\n",
    "answerability_proc.create_chat_completion(rewritten_chat_input_with_docs).results[\n",
    "    0\n",
    "].next_message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also verify that the fetched documents answer the *original* query prior to the rewrite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewritten_chat_input_with_docs = retrieval_request_proc.process(rewritten_chat_input)[0]\n",
    "chat_input_with_docs_from_rewrite = rewritten_chat_input_with_docs.model_copy(\n",
    "    update={\"messages\": chat_input.messages}  # Reinstate original messages\n",
    ")\n",
    "answerability_proc.create_chat_completion(chat_input_with_docs_from_rewrite).results[\n",
    "    0\n",
    "].next_message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can chain all of these request processors together with the IO processor for \n",
    "the answerability model to create a single flow that processes requests in multiple\n",
    "steps:\n",
    "1. Rewrite the last user message for retrieval\n",
    "1. Retrieve documents and attach them to the request\n",
    "1. Check for answerability with the retrieved documents\n",
    "1. If the answerability check passes, then send the request to the base model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewrite_request_proc = RewriteRequestProcessor(rewrite_io_proc)\n",
    "\n",
    "request = rewrite_request_proc.process(chat_input)[0]\n",
    "request = retrieval_request_proc.process(request)[0]\n",
    "\n",
    "# Switch back to original version of last turn\n",
    "request = request.model_copy(update={\"messages\": chat_input.messages})\n",
    "\n",
    "# Check for answerability and generate if documents pass the check\n",
    "response = answerability_composite_proc.create_chat_completion(request)\n",
    "rag_rewrite_result = response.results[0]\n",
    "display(Markdown(rag_rewrite_result.next_message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the responses we've seen so far, this response provides information that is both\n",
    "relevant to the user's intended question and grounded in documents retrieved from the \n",
    "corpus.\n",
    "\n",
    "We can use the [LoRA Adapter for Citation Generation](https://huggingface.co/ibm-granite/granite-3.3-8b-rag-agent-lib/blob/main/citation_generation_lora/README.md\n",
    ") to explain exactly how this response is grounded in the documents that the rewritten\n",
    "user query retrieves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for answerability, generate a response, then add citations\n",
    "citations_composite_proc = CitationsCompositeIOProcessor(\n",
    "    answerability_composite_proc, citations_lora_backend\n",
    ")\n",
    "result_with_citations = citations_composite_proc.create_chat_completion(\n",
    "    request\n",
    ").results[0]\n",
    "\n",
    "print(\"Assistant response:\")\n",
    "display(Markdown(result_with_citations.next_message.content))\n",
    "print(\"Citations:\")\n",
    "pd.set_option(\"display.max_colwidth\", 1500)\n",
    "pd.DataFrame.from_records(\n",
    "    [c.model_dump() for c in result_with_citations.next_message.citations]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the [LoRA Adapter for Hallucination Detection in RAG outputs](\n",
    "    https://huggingface.co/ibm-granite/granite-3.3-8b-rag-agent-lib/blob/main/hallucination_detection_lora/README.md\n",
    ") to further verify that each sentence of the assistant response is consistent with the\n",
    "information in the retrieved documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a hallucination check over the preceding response\n",
    "hallucinations_composite_proc = HallucinationsCompositeIOProcessor(\n",
    "    io_proc, hallucination_lora_backend\n",
    ")\n",
    "result_with_hallucinations = hallucinations_composite_proc.create_chat_completion(\n",
    "    request\n",
    ").results[0]\n",
    "print(\"Hallucination Checks:\")\n",
    "display(\n",
    "    pd.DataFrame.from_records(\n",
    "        [h.model_dump() for h in result_with_hallucinations.next_message.hallucinations]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can wrap all of the functionality we've shown so far in a single class that \n",
    "inherits from the `InputOutputProcessor` interface in `granite-io`. Packaging things\n",
    "this way lets applications treat this multi-step flow as if it was a single chat \n",
    "completion request to a base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from granite_io.io.base import InputOutputProcessor, RequestProcessor\n",
    "from granite_io.backend import Backend\n",
    "from granite_io.io.base import ChatCompletionInputs, ChatCompletionResults\n",
    "import asyncio\n",
    "\n",
    "\n",
    "class MyRAGCompositeIOProcessor(InputOutputProcessor):\n",
    "    def __init__(\n",
    "        self,\n",
    "        io_proc: InputOutputProcessor,\n",
    "        rewrite_request_proc: RequestProcessor,\n",
    "        retrieval_request_proc: RequestProcessor,\n",
    "        answerability_proc: InputOutputProcessor,\n",
    "        certainty_io_proc: InputOutputProcessor,\n",
    "        citations_lora_backend: Backend,\n",
    "        hallucination_lora_backend: Backend,\n",
    "    ):\n",
    "        self.io_proc = io_proc\n",
    "        self.certainty_io_proc = certainty_io_proc\n",
    "\n",
    "        self.rewrite_request_proc = rewrite_request_proc\n",
    "        self.retrieval_request_proc = retrieval_request_proc\n",
    "\n",
    "        # Build up a chain of IO processors:\n",
    "        # answerability -> base model -> citations -> hallucinations\n",
    "        chain = AnswerabilityCompositeIOProcessor(io_proc, answerability_proc)\n",
    "        chain = CitationsCompositeIOProcessor(chain, citations_lora_backend)\n",
    "        chain = HallucinationsCompositeIOProcessor(chain, hallucination_lora_backend)\n",
    "        self.io_proc_chain = chain\n",
    "\n",
    "    async def acreate_chat_completion(\n",
    "        self, inputs: ChatCompletionInputs\n",
    "    ) -> ChatCompletionResults:\n",
    "        \"\"\"\n",
    "        Chat completions API inherited from the ``InputOutputProcessor`` base class.\n",
    "\n",
    "        :param inputs: Structured representation of the inputs to a chat completion\n",
    "            request, possibly including additional fields that only this input-output\n",
    "            processor can consume\n",
    "\n",
    "        :returns: The next message that the model produces when fed the specified\n",
    "            inputs, plus additional information about the low-level request.\n",
    "        \"\"\"\n",
    "        original_inputs = inputs\n",
    "\n",
    "        # Start by checking whether retrieval is necessary to answer this query.\n",
    "        certainty_result = await self.certainty_io_proc.acreate_chat_completion(\n",
    "            inputs.with_addl_generate_params({\"n\": 1, \"temperature\": 0.0})\n",
    "        )\n",
    "        certainty_score = float(certainty_result.results[0].next_message.content)\n",
    "        if certainty_score > 0.8:\n",
    "            # If control reaches here, the base model can respond with high certainty\n",
    "            # without using RAG, so skip retrieval and go directly to generation.\n",
    "            return await self.io_proc.acreate_chat_completion(inputs)\n",
    "\n",
    "        # If control reaches here, the base model needs additional context to generate\n",
    "        # a high-quality response. Perform query rewrite, followed by retrieval\n",
    "        # augmented generation.\n",
    "\n",
    "        # Interpret the \"n\" parameter in the request as \"number of times to run\n",
    "        # retrieval and generation\".\n",
    "        rewritten_inputs = await rewrite_request_proc.aprocess(inputs)\n",
    "\n",
    "        # Do the remaining workflow steps once per rewritten query.\n",
    "        coroutines = []\n",
    "        for inputs in rewritten_inputs:\n",
    "            # Retrieve documents for each rewritten version of the query.\n",
    "            inputs = inputs.with_addl_generate_params({\"n\": 1})\n",
    "            inputs = (await retrieval_request_proc.aprocess(inputs))[0]\n",
    "\n",
    "            # Switch back to original version of last turn\n",
    "            inputs = inputs.with_messages(original_inputs.messages)\n",
    "\n",
    "            # Perform answerability check, generate a response, add citations, and check\n",
    "            # for hallucinations. Do these steps in parallel across retrievals.\n",
    "            coroutines.append(self.io_proc_chain.acreate_chat_completion(inputs))\n",
    "\n",
    "        # Merge results from parallel invocations\n",
    "        sub_results = await asyncio.gather(*coroutines)\n",
    "        return ChatCompletionResults(\n",
    "            results=[sub_result.results[0] for sub_result in sub_results]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_io_proc = MyRAGCompositeIOProcessor(\n",
    "    io_proc,\n",
    "    rewrite_request_proc=rewrite_request_proc,\n",
    "    retrieval_request_proc=retrieval_request_proc,\n",
    "    certainty_io_proc=certainty_io_proc,\n",
    "    answerability_proc=answerability_proc,\n",
    "    citations_lora_backend=citations_lora_backend,\n",
    "    hallucination_lora_backend=hallucination_lora_backend,\n",
    ")\n",
    "\n",
    "rag_result = rag_io_proc.create_chat_completion(chat_input).results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Assistant response:\")\n",
    "display(Markdown(rag_result.next_message.content))\n",
    "print(\"Citations:\")\n",
    "pd.set_option(\"display.max_colwidth\", 1500)\n",
    "display(\n",
    "    pd.DataFrame.from_records(\n",
    "        [c.model_dump() for c in rag_result.next_message.citations]\n",
    "    )\n",
    ")\n",
    "print(\"Hallucination Checks:\")\n",
    "display(\n",
    "    pd.DataFrame.from_records(\n",
    "        [h.model_dump() for h in rag_result.next_message.hallucinations]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the end-to-end flow is an `InputOutputProcessor`, we  can generate multiple \n",
    "alternative responses by changing the generation parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "many_rag_results = rag_io_proc.create_chat_completion(\n",
    "    chat_input.with_addl_generate_params({\"n\": 10, \"temperature\": 0.8})\n",
    ").results\n",
    "many_rag_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use inference-time scaling techniques to rank these results and pick the \n",
    "\"best\" one. \n",
    "Here we use Minimum Bayesian Risk (MBR) decoding to choose the result that is closest\n",
    "to the average of all other results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_io_proc = MBRDMajorityVotingProcessor(rag_io_proc)\n",
    "mbrd_result = voting_io_proc.create_chat_completion(\n",
    "    chat_input.with_addl_generate_params({\"n\": 10, \"temperature\": 0.8})\n",
    ").results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Assistant response with Minimum Bayesian Risk decoding:\")\n",
    "display(Markdown(mbrd_result.next_message.content))\n",
    "print(\"Citations:\")\n",
    "pd.set_option(\"display.max_colwidth\", 1500)\n",
    "display(\n",
    "    pd.DataFrame.from_records(\n",
    "        [c.model_dump() for c in mbrd_result.next_message.citations]\n",
    "    )\n",
    ")\n",
    "print(\"Hallucination Checks:\")\n",
    "display(\n",
    "    pd.DataFrame.from_records(\n",
    "        [h.model_dump() for h in mbrd_result.next_message.hallucinations]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free up GPU resources\n",
    "if \"server\" in locals():\n",
    "    server.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
