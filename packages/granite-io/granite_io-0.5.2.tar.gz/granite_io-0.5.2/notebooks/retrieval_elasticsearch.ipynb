{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo of retrieval IO processor with MTRAG benchmark data\n",
    "\n",
    "This notebook shows how to use the retrieval IO processor to implement the retrieval \n",
    "phase of Retrieval-Augmented Generation (RAG) on top of Granite 3.3.\n",
    "\n",
    "This notebook can run its own vLLM server to perform inference, or you can host the \n",
    "model on your own server. \n",
    "\n",
    "To use your own server, set the `run_server` variable below\n",
    "to `False` and set appropriate values for the constants in the cell marked\n",
    "`# Constants go here`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from granite_io.io.granite_3_3.input_processors.granite_3_3_input_processor import (\n",
    "    Granite3Point3Inputs,\n",
    ")\n",
    "from granite_io import make_io_processor, make_backend\n",
    "from granite_io.io.retrieval import ElasticsearchRetriever, RetrievalRequestProcessor\n",
    "from granite_io.backend.vllm_server import LocalVLLMServer\n",
    "from IPython.display import display, Markdown\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants go here\n",
    "host = \"\"\n",
    "corpus_name = \"\"\n",
    "model_name = \"ibm-granite/granite-3.3-8b-instruct\"\n",
    "\n",
    "run_server = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_server:\n",
    "    # Start by firing up a local vLLM server and connecting a backend instance to it.\n",
    "    server = LocalVLLMServer(model_name)\n",
    "    server.wait_for_startup(200)\n",
    "    backend = server.make_backend()\n",
    "else:  # if not run_server\n",
    "    # Use an existing server.\n",
    "    # The constants here are for the server that local_vllm_server.ipynb starts.\n",
    "    # Modify as needed.\n",
    "    openai_base_url = \"http://localhost:36101/v1\"\n",
    "    openai_api_key = \"granite_intrinsics_1234\"\n",
    "    backend = make_backend(\n",
    "        \"openai\",\n",
    "        {\n",
    "            \"model_name\": model_name,\n",
    "            \"openai_base_url\": openai_base_url,\n",
    "            \"openai_api_key\": openai_api_key,\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<granite_io.io.granite_3_3.granite_3_3.Granite3Point3InputOutputProcessor at 0x7f5d49af94b0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spin up an IO processor for the base model\n",
    "io_proc = make_io_processor(model_name, backend=backend)\n",
    "io_proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Granite3Point3Inputs(messages=[AssistantMessage(content='Welcome to the California Appellate Courts help desk.', role='assistant', tool_calls=[], reasoning_content=None, citations=None, documents=None, hallucinations=None, stop_reason=None), UserMessage(content='I need to do some legal research to be prepared for my oral argument. Can I visit the law library?', role='user')], tools=[], generate_inputs=GenerateInputs(prompt=None, model=None, best_of=None, echo=None, frequency_penalty=None, logit_bias=None, logprobs=None, max_tokens=4096, n=None, presence_penalty=None, stop=None, stream=None, stream_options=None, suffix=None, temperature=0.0, top_p=None, user=None, extra_headers=None, extra_body={}), documents=[], controls=None, thinking=False, sanitize=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an example chat completions request\n",
    "chat_input = Granite3Point3Inputs.model_validate(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"Welcome to the California Appellate Courts help desk.\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"I need to do some legal research to be prepared for my \"\n",
    "                \"oral argument. Can I visit the law library?\",\n",
    "            },\n",
    "        ],\n",
    "        \"generate_inputs\": {\n",
    "            \"temperature\": 0.0,\n",
    "            \"max_tokens\": 4096,\n",
    "        },\n",
    "    }\n",
    ")\n",
    "chat_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "As an AI, I don't have real-time access to physical locations or their current operating conditions. However, I can guide you on how to proceed with your legal research. \n",
       "\n",
       "Many appellate courts, including those in California, provide access to their law libraries for attorneys and the public. You would typically need to contact the specific court where your case is being heard to inquire about visiting their law library. \n",
       "\n",
       "Due to the COVID-19 pandemic, many courts have adjusted their operations and might offer remote access or curbside pickup for legal materials. It's advisable to check the court's website or contact them directly for the most current information.\n",
       "\n",
       "Additionally, you can conduct legal research remotely using online legal databases such as Westlaw, LexisNexis, or a free resource like Justia or the California Courts' self-help website, which provides free access to California appellate court opinions and other legal resources.\n",
       "\n",
       "Remember, it's crucial to stay updated with the latest rules and procedures of the court due to potential changes resulting from the pandemic or other factors."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run the chat completion request through the base model without RAG.\n",
    "# The result should be a refusal message that starts with, \"As an AI, I don't have\n",
    "# physical locations or resources.\"\n",
    "non_rag_result = io_proc.create_chat_completion(chat_input)\n",
    "display(Markdown(non_rag_result.results[0].next_message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spin up an in-memory vector database\n",
    "retriever = ElasticsearchRetriever(\n",
    "    corpus_name=corpus_name, host=host, verify_certs=False, ssl_show_warn=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a RetrievalRequestProcessor to augment the chat completion request with documents.\n",
    "rag_processor = RetrievalRequestProcessor(retriever)\n",
    "rag_chat_input = rag_processor.process(chat_input)[0]\n",
    "pd.set_option(\"display.max_colwidth\", 200)\n",
    "print(\"Documents:\")\n",
    "pd.DataFrame.from_records([d.model_dump() for d in rag_chat_input.documents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Yes, you can visit the law library to conduct your legal research. Law libraries contain case-law reports, legal periodicals, and legislation, which are essential resources for determining the current state of the law."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run the same request through the base model with RAG documents\n",
    "rag_result = io_proc.create_chat_completion(rag_chat_input)\n",
    "display(Markdown(rag_result.results[0].next_message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free up GPU resources\n",
    "if \"server\" in locals():\n",
    "    server.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intrinsics-server",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
