{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstration of the Granite answerability intrisic\n",
    "\n",
    "This notebook shows the usage of the IO processor for the Granite answerability intrisic, \n",
    "also known as the [LoRA Adapter for Answerability Classification](\n",
    "    https://huggingface.co/ibm-granite/granite-3.3-8b-rag-agent-lib/blob/main/answerability_prediction_lora/README.md\n",
    ")\n",
    "\n",
    "This notebook can run its own vLLM server to perform inference, or you can host the \n",
    "models on your own server. \n",
    "\n",
    "To use your own server, set the `run_server` variable below\n",
    "to `False` and set appropriate values for the constants \n",
    "`openai_base_url`, `openai_base_model_name` and `openai_lora_model_name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports go here\n",
    "from granite_io.backend.vllm_server import LocalVLLMServer\n",
    "from granite_io import UserMessage, make_backend\n",
    "from granite_io.io.granite_3_3.input_processors.granite_3_3_input_processor import (\n",
    "    Granite3Point3Inputs,\n",
    ")\n",
    "from granite_io.io.answerability import AnswerabilityIOProcessor\n",
    "from granite_io.io.rag_agent_lib import obtain_lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants go here\n",
    "base_model_name = \"ibm-granite/granite-3.3-8b-instruct\"\n",
    "lora_model_name = \"answerability_prediction\"\n",
    "run_server = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_server:\n",
    "    # Start by firing up a local vLLM server and connecting a backend instance to it.\n",
    "    # Download and cache the model's LoRA adapter.\n",
    "    lora_model_path = obtain_lora(lora_model_name)\n",
    "    print(f\"Local path to LoRA adapter: {lora_model_path}\")\n",
    "    server = LocalVLLMServer(\n",
    "        base_model_name, lora_adapters=[(lora_model_name, lora_model_path)]\n",
    "    )\n",
    "    server.wait_for_startup(200)\n",
    "    lora_backend = server.make_lora_backend(lora_model_name)\n",
    "    backend = server.make_backend()\n",
    "else:  # if not run_server\n",
    "    # Use an existing server.\n",
    "    # Modify the constants here as needed.\n",
    "    openai_base_url = \"http://localhost:55555/v1\"\n",
    "    openai_api_key = \"granite_intrinsics_1234\"\n",
    "    openai_base_model_name = base_model_name\n",
    "    openai_lora_model_name = lora_model_name\n",
    "    backend = make_backend(\n",
    "        \"openai\",\n",
    "        {\n",
    "            \"model_name\": openai_base_model_name,\n",
    "            \"openai_base_url\": openai_base_url,\n",
    "            \"openai_api_key\": openai_api_key,\n",
    "        },\n",
    "    )\n",
    "    lora_backend = make_backend(\n",
    "        \"openai\",\n",
    "        {\n",
    "            \"model_name\": openai_lora_model_name,\n",
    "            \"openai_base_url\": openai_base_url,\n",
    "            \"openai_api_key\": openai_api_key,\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an example chat completion with a user question and two documents.\n",
    "chat_input = Granite3Point3Inputs.model_validate(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"assistant\", \"content\": \"Welcome to pet questions!\"},\n",
    "            {\"role\": \"user\", \"content\": \"Does my dog have fleas?\"},\n",
    "        ],\n",
    "        \"documents\": [\n",
    "            {\"doc_id\": 1, \"text\": \"My dog has fleas.\"},\n",
    "            {\"doc_id\": 2, \"text\": \"My cat does not have fleas.\"},\n",
    "        ],\n",
    "        \"generate_inputs\": {\"temperature\": 0.0},\n",
    "    }\n",
    ")\n",
    "\n",
    "chat_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the I/O processor for the answerability LoRA adapter\n",
    "io_proc = AnswerabilityIOProcessor(lora_backend)\n",
    "\n",
    "# Pass our example input thorugh the I/O processor and retrieve the result\n",
    "chat_result = await io_proc.acreate_chat_completion(chat_input)\n",
    "chat_result.results[0].next_message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try some variations on the original question\n",
    "variations = [\n",
    "    \"Does my cat have no fleas?\",  # Answerable\n",
    "    \"Does my cat have green eyes?\",  # Unanswerable\n",
    "    \"Does my elephant have fleas?\",  # Unanswerable\n",
    "    \"Which of my pets have fleas?\",  # Answerable\n",
    "    \"What is the population of Australia?\",  # Unanswerable\n",
    "]\n",
    "\n",
    "for variation in variations:\n",
    "    updated_messages = chat_input.messages.copy()\n",
    "    updated_messages[-1] = UserMessage(content=variation)\n",
    "    chat_result = await io_proc.acreate_chat_completion(\n",
    "        chat_input.model_copy(update={\"messages\": updated_messages})\n",
    "    )\n",
    "    print(f\"'{variation}' => {chat_result.results[0].next_message.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free up GPU resources\n",
    "if \"server\" in locals():\n",
    "    server.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
