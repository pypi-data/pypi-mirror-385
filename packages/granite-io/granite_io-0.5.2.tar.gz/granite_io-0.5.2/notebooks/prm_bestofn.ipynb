{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d5c952a-d97e-4c94-93ec-6c4b89b5c460",
   "metadata": {},
   "source": [
    "# Demonstration of the Granite PRM intrinsic\n",
    "\n",
    "This notebook shows the usage of the IO processor for the Granite Process Reward Model (PRM) intrisic, also known as the Granite 3.3 8B Instruct Math PRM LoRA. Specifically, we show how to use the PRM as a standalone evaluator to score a given response to a math question, and we also demonstrate how it can be used in a best-of-N (BoN) setting to choose the best response from a set of assistant responses to a math problem.\n",
    "\n",
    "This notebook can run its own vLLM server to perform inference, or you can host the models on your own server.\n",
    "\n",
    "To use your own server, set the run_server variable below to False and set appropriate values for the constants openai_base_url, openai_base_model_name and openai_lora_model_name.\n",
    "\n",
    "For more details, please refer to the model card at https://huggingface.co/ibm-granite/granite-3.3-8b-lora-math-prm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41608866-760c-48d3-931a-3567fa1a11dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from granite_io.backend.vllm_server import LocalVLLMServer\n",
    "\n",
    "from granite_io.io.granite_3_3.input_processors.granite_3_3_input_processor import (\n",
    "    Granite3Point3Inputs,\n",
    ")\n",
    "from granite_io import make_io_processor, make_backend\n",
    "from granite_io.io.process_reward_model.best_of_n import (\n",
    "    ProcessRewardModelIOProcessor,\n",
    "    PRMBestOfNCompositeIOProcessor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755e48fa-caad-474c-a06a-9c8b29465bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "base_model_name = \"ibm-granite/granite-3.3-8b-instruct\"\n",
    "lora_model_name = \"ibm-granite/granite-3.3-8b-lora-math-prm\"\n",
    "\n",
    "run_server = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93a4797-a46c-4515-9f6f-3a7b3eedb690",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_server:\n",
    "    # Start by firing up a local vLLM server and connecting a backend instance to it.\n",
    "    server = LocalVLLMServer(\n",
    "        base_model_name, lora_adapters=[(lora_model_name, lora_model_name)]\n",
    "    )\n",
    "    server.wait_for_startup(200)\n",
    "    lora_backend = server.make_lora_backend(lora_model_name)\n",
    "    backend = server.make_backend()\n",
    "else:  # if not run_server\n",
    "    # Use an existing server.\n",
    "    # Modify the constants here as needed.\n",
    "    openai_base_url = \"http://localhost:55555/v1\"\n",
    "    openai_api_key = \"granite_intrinsics_1234\"\n",
    "    openai_base_model_name = base_model_name\n",
    "    openai_lora_model_name = lora_model_name\n",
    "    backend = make_backend(\n",
    "        \"openai\",\n",
    "        {\n",
    "            \"model_name\": openai_base_model_name,\n",
    "            \"openai_base_url\": openai_base_url,\n",
    "            \"openai_api_key\": openai_api_key,\n",
    "        },\n",
    "    )\n",
    "    lora_backend = make_backend(\n",
    "        \"openai\",\n",
    "        {\n",
    "            \"model_name\": openai_lora_model_name,\n",
    "            \"openai_base_url\": openai_base_url,\n",
    "            \"openai_api_key\": openai_api_key,\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94589404-aaea-4f81-9fe0-a7eb48f55833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an example chat completion with a user question\n",
    "chat_input = Granite3Point3Inputs.model_validate(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Weng earns $12 an hour for babysitting. \"\n",
    "                \"Yesterday, she just did 50 minutes of babysitting. \"\n",
    "                \"How much did she earn?\",\n",
    "            },\n",
    "        ],\n",
    "        \"generate_inputs\": {\n",
    "            \"temperature\": 0.0,\n",
    "            \"max_tokens\": 4096,\n",
    "        },\n",
    "    }\n",
    ")\n",
    "chat_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b329f387-82e2-444a-ab99-67797eeccd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass the example input through Granite 3.3 to get an answer\n",
    "granite_io_proc = make_io_processor(\"Granite 3.3\", backend=backend)\n",
    "result = await granite_io_proc.acreate_chat_completion(chat_input)\n",
    "result.results[0].next_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f008186a-477c-4931-b65a-60f996f7269a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append the model's output to the chat\n",
    "next_chat_input = chat_input.with_next_message(result.results[0].next_message)\n",
    "next_chat_input.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbd4f4d-58e2-4934-a0bc-62a2c45bbb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the I/O processor for the PRM intrinsic\n",
    "io_proc = ProcessRewardModelIOProcessor(backend=lora_backend)\n",
    "\n",
    "# Set temperature to 0 because we are not sampling from the intrinsic's output\n",
    "next_chat_input = next_chat_input.with_addl_generate_params({\"temperature\": 0.0})\n",
    "\n",
    "# Pass our example input through the I/O processor and retrieve the result\n",
    "chat_result = await io_proc.acreate_chat_completion(next_chat_input)\n",
    "\n",
    "print(\n",
    "    f\"PRM score for the original response is \"\n",
    "    f\"{chat_result.results[0].next_message.content}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc735b4-2b15-48a1-a3f1-82301ee151de",
   "metadata": {},
   "outputs": [],
   "source": [
    "io_proc.inputs_to_generate_inputs(next_chat_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66eca6b-699e-4ff9-8b59-2d96beb37f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try with an artifical poor-quality assistant response.\n",
    "from granite_io.types import AssistantMessage\n",
    "\n",
    "chat_result_2 = await io_proc.acreate_chat_completion(\n",
    "    chat_input.with_next_message(\n",
    "        AssistantMessage(\n",
    "            content=\"Weng earns 12/60 = 0.5 per minute. \"\n",
    "            \"Working 50 minutes, she earned 0.5 x 50 = 250. \"\n",
    "            \"Thus Weng earns $250 for 50 minutes of babysitting.\"\n",
    "        )\n",
    "    ).with_addl_generate_params({\"temperature\": 0.0})\n",
    ")\n",
    "print(\n",
    "    f\"PRM score for the low-quality response is \"\n",
    "    f\"{chat_result_2.results[0].next_message.content}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a792f3-f0c0-4792-9b22-43248735e501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the composite processor to generate multiple completions and select the completion\n",
    "# with the highest PRM score\n",
    "composite_proc = PRMBestOfNCompositeIOProcessor(\n",
    "    granite_io_proc, lora_backend, include_score=True\n",
    ")\n",
    "composite_results, all_results = await composite_proc.acreate_chat_completion(\n",
    "    chat_input.with_addl_generate_params({\"n\": 5, \"temperature\": 1.0})\n",
    ")\n",
    "composite_results.results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e57d0dd-4b4e-4d96-8e0c-dad6779d201c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free up GPU resources\n",
    "if \"server\" in locals():\n",
    "    server.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
