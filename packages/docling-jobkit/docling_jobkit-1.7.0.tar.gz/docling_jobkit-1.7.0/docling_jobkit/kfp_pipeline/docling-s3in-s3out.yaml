# PIPELINE DEFINITION
# Name: inputs-s3in-s3out
# Inputs:
#    batch_size: int [Default: 20.0]
#    convertion_options: dict [Default: {'force_ocr': False, 'do_code_enrichment': False, 'do_formula_enrichment': False, 'do_picture_classification': False, 'to_formats': ['md', 'json', 'html', 'text', 'doctags'], 'return_as_file': False, 'include_images': True, 'abort_on_error': False, 'ocr_lang': [], 'do_picture_description': False, 'ocr_engine': 'easyocr', 'table_mode': 'accurate', 'images_scale': 2.0, 'image_export_mode': 'placeholder', 'generate_picture_images': False, 'do_ocr': True, 'from_formats': ['docx', 'pptx', 'html', 'image', 'pdf', 'asciidoc', 'md', 'xlsx', 'xml_uspto', 'xml_jats', 'json_docling'], 'pdf_backend': 'dlparse_v2', 'do_table_structure': True}]
#    source: dict [Default: {'endpoint': 's3.eu-de.cloud-object-storage.appdomain.cloud', 'access_key': '123454321', 'verify_ssl': True, 'key_prefix': 'my-docs', 'bucket': 'source-bucket', 'secret_key': 'secretsecret'}]
#    target: dict [Default: {'endpoint': 's3.eu-de.cloud-object-storage.appdomain.cloud', 'access_key': '123454321', 'verify_ssl': True, 'key_prefix': 'my-docs', 'bucket': 'target-bucket', 'secret_key': 'secretsecret'}]
components:
  comp-compute-batches:
    executorLabel: exec-compute-batches
    inputDefinitions:
      parameters:
        batch_size:
          defaultValue: 10.0
          isOptional: true
          parameterType: NUMBER_INTEGER
        source:
          parameterType: STRUCT
        target:
          parameterType: STRUCT
    outputDefinitions:
      artifacts:
        dataset:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
      parameters:
        batch_indices:
          parameterType: LIST
  comp-convert-payload:
    executorLabel: exec-convert-payload
    inputDefinitions:
      artifacts:
        dataset:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
      parameters:
        batch_index:
          parameterType: NUMBER_INTEGER
        options:
          parameterType: STRUCT
        source:
          parameterType: STRUCT
        target:
          parameterType: STRUCT
    outputDefinitions:
      parameters:
        Output:
          parameterType: LIST
  comp-for-loop-1:
    dag:
      tasks:
        convert-payload:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-convert-payload
          inputs:
            artifacts:
              dataset:
                componentInputArtifact: pipelinechannel--compute-batches-dataset
            parameters:
              batch_index:
                componentInputParameter: pipelinechannel--compute-batches-batch_indices-loop-item
              options:
                componentInputParameter: pipelinechannel--convertion_options
              source:
                componentInputParameter: pipelinechannel--source
              target:
                componentInputParameter: pipelinechannel--target
          taskInfo:
            name: convert-payload
    inputDefinitions:
      artifacts:
        pipelinechannel--compute-batches-dataset:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
      parameters:
        pipelinechannel--compute-batches-batch_indices:
          parameterType: LIST
        pipelinechannel--compute-batches-batch_indices-loop-item:
          parameterType: NUMBER_INTEGER
        pipelinechannel--convertion_options:
          parameterType: STRUCT
        pipelinechannel--source:
          parameterType: STRUCT
        pipelinechannel--target:
          parameterType: STRUCT
deploymentSpec:
  executors:
    exec-compute-batches:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - compute_batches
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.13.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'pydantic' 'boto3~=1.35.36'\
          \ 'git+https://github.com/docling-project/docling-jobkit@main' && \"$0\"\
          \ \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef compute_batches(\n    source: dict,\n    target: dict,\n    dataset:\
          \ Output[Dataset],\n    batch_size: int = 10,\n) -> NamedTuple(\"outputs\"\
          , [(\"batch_indices\", List[int])]):  # type: ignore[valid-type]\n    import\
          \ json\n    from typing import NamedTuple\n\n    from docling_jobkit.connectors.s3_helper\
          \ import (\n        check_target_has_source_converted,\n        generate_batch_keys,\n\
          \        get_s3_connection,\n        get_source_files,\n    )\n    from\
          \ docling_jobkit.datamodel.s3_coords import S3Coordinates\n\n    # validate\
          \ inputs\n    s3_coords_source = S3Coordinates.model_validate(source)\n\
          \    s3_target_coords = S3Coordinates.model_validate(target)\n\n    s3_source_client,\
          \ s3_source_resource = get_s3_connection(s3_coords_source)\n    source_objects_list\
          \ = get_source_files(\n        s3_source_client, s3_source_resource, s3_coords_source\n\
          \    )\n    filtered_source_keys = check_target_has_source_converted(\n\
          \        s3_target_coords, source_objects_list, s3_coords_source.key_prefix\n\
          \    )\n    batch_keys = generate_batch_keys(\n        filtered_source_keys,\n\
          \        batch_size=batch_size,\n    )\n\n    # store batches on s3 for\
          \ debugging\n    s3_target_client, s3_target_resource = get_s3_connection(s3_target_coords)\n\
          \    try:\n        s3_target_client.put_object(\n            Body=json.dumps(batch_keys),\n\
          \            Bucket=s3_target_coords.bucket,\n            Key=f\"{s3_target_coords.key_prefix}/buckets.json\"\
          ,\n            ContentType=\"application/json\",\n        )\n    except\
          \ Exception as e:\n        print(\"Uploading batch.json to s3 failed\",\
          \ exc_info=e)\n\n    with open(dataset.path, \"w\") as out_batches:\n  \
          \      json.dump(batch_keys, out_batches)\n\n    batch_indices = list(range(len(batch_keys)))\n\
          \    outputs = NamedTuple(\"outputs\", [(\"batch_indices\", List[int])])\n\
          \    print(f\"Batches to convert: {len(batch_keys)}\")\n    return outputs(batch_indices)\n\
          \n"
        image: ghcr.io/docling-project/docling-serve-cpu:v1.5.1
    exec-convert-payload:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - convert_payload
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.13.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'docling==2.50.0'\
          \ 'boto3~=1.35.36' 'git+https://github.com/docling-project/docling-jobkit@main'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef convert_payload(\n    options: dict,\n    source: dict,\n   \
          \ target: dict,\n    batch_index: int,\n    # source_keys: List[str],\n\
          \    dataset: Input[Dataset],\n) -> list:\n    import json\n    import logging\n\
          \    from pathlib import Path\n\n    from docling_jobkit.connectors.s3_helper\
          \ import (\n        ResultsProcessor,\n        generate_presign_url,\n \
          \       get_s3_connection,\n    )\n    from docling_jobkit.convert.manager\
          \ import (\n        DoclingConverterManager,\n        DoclingConverterManagerConfig,\n\
          \    )\n    from docling_jobkit.datamodel.convert import ConvertDocumentsOptions\n\
          \    from docling_jobkit.datamodel.s3_coords import S3Coordinates\n\n  \
          \  logging.basicConfig(level=logging.INFO)\n\n    # validate coords\n  \
          \  s3_coords_source = S3Coordinates.model_validate(source)\n    target_s3_coords\
          \ = S3Coordinates.model_validate(target)\n\n    s3_source_client, s3_source_resource\
          \ = get_s3_connection(s3_coords_source)\n\n    convert_options = ConvertDocumentsOptions.model_validate(options)\n\
          \n    config = DoclingConverterManagerConfig()\n    # Points to the pvc\
          \ mounted on the pod\n    config.artifacts_path = Path(\"/modelcache\")\n\
          \    converter = DoclingConverterManager(config)\n\n    with open(dataset.path)\
          \ as f:\n        batches = json.load(f)\n    source_keys = batches[batch_index]\n\
          \n    presign_filtered_source_keys = [\n        generate_presign_url(s3_source_client,\
          \ key, s3_coords_source.bucket)\n        for key in source_keys\n    ]\n\
          \n    results = []\n    result_processor = ResultsProcessor(\n        target_s3_coords=target_s3_coords,\n\
          \        to_formats=[v.value for v in convert_options.to_formats],\n   \
          \     generate_page_images=convert_options.include_images,\n        generate_picture_images=convert_options.include_images,\n\
          \    )\n    for item in result_processor.process_documents(\n        converter.convert_documents(\n\
          \            presign_filtered_source_keys, options=convert_options\n   \
          \     )\n    ):\n        results.append(item)\n        logging.info(\"Convertion\
          \ result: {}\".format(item))\n\n    return results\n\n"
        image: ghcr.io/docling-project/docling-serve-cpu:v1.5.1
        resources:
          accelerator:
            count: '1'
            resourceCount: '1'
            resourceType: nvidia.com/gpu
            type: nvidia.com/gpu
          cpuLimit: 1.0
          cpuRequest: 0.2
          memoryLimit: 16.0
          memoryRequest: 1.0
          resourceCpuLimit: '1'
          resourceCpuRequest: 200m
          resourceMemoryLimit: 16G
          resourceMemoryRequest: 1G
pipelineInfo:
  name: inputs-s3in-s3out
root:
  dag:
    tasks:
      compute-batches:
        cachingOptions: {}
        componentRef:
          name: comp-compute-batches
        inputs:
          parameters:
            batch_size:
              componentInputParameter: batch_size
            source:
              componentInputParameter: source
            target:
              componentInputParameter: target
        taskInfo:
          name: compute-batches
      for-loop-1:
        componentRef:
          name: comp-for-loop-1
        dependentTasks:
        - compute-batches
        inputs:
          artifacts:
            pipelinechannel--compute-batches-dataset:
              taskOutputArtifact:
                outputArtifactKey: dataset
                producerTask: compute-batches
          parameters:
            pipelinechannel--compute-batches-batch_indices:
              taskOutputParameter:
                outputParameterKey: batch_indices
                producerTask: compute-batches
            pipelinechannel--convertion_options:
              componentInputParameter: convertion_options
            pipelinechannel--source:
              componentInputParameter: source
            pipelinechannel--target:
              componentInputParameter: target
        iteratorPolicy:
          parallelismLimit: 20
        parameterIterator:
          itemInput: pipelinechannel--compute-batches-batch_indices-loop-item
          items:
            inputParameter: pipelinechannel--compute-batches-batch_indices
        taskInfo:
          name: for-loop-1
  inputDefinitions:
    parameters:
      batch_size:
        defaultValue: 20.0
        isOptional: true
        parameterType: NUMBER_INTEGER
      convertion_options:
        defaultValue:
          abort_on_error: false
          do_code_enrichment: false
          do_formula_enrichment: false
          do_ocr: true
          do_picture_classification: false
          do_picture_description: false
          do_table_structure: true
          force_ocr: false
          from_formats:
          - docx
          - pptx
          - html
          - image
          - pdf
          - asciidoc
          - md
          - xlsx
          - xml_uspto
          - xml_jats
          - json_docling
          generate_picture_images: false
          image_export_mode: placeholder
          images_scale: 2.0
          include_images: true
          ocr_engine: easyocr
          ocr_lang: []
          pdf_backend: dlparse_v2
          return_as_file: false
          table_mode: accurate
          to_formats:
          - md
          - json
          - html
          - text
          - doctags
        isOptional: true
        parameterType: STRUCT
      source:
        defaultValue:
          access_key: '123454321'
          bucket: source-bucket
          endpoint: s3.eu-de.cloud-object-storage.appdomain.cloud
          key_prefix: my-docs
          secret_key: secretsecret
          verify_ssl: true
        isOptional: true
        parameterType: STRUCT
      target:
        defaultValue:
          access_key: '123454321'
          bucket: target-bucket
          endpoint: s3.eu-de.cloud-object-storage.appdomain.cloud
          key_prefix: my-docs
          secret_key: secretsecret
          verify_ssl: true
        isOptional: true
        parameterType: STRUCT
schemaVersion: 2.1.0
sdkVersion: kfp-2.13.0
---
platforms:
  kubernetes:
    deploymentSpec:
      executors:
        exec-convert-payload:
          pvcMount:
          - constant: docling-model-cache-pvc-multi
            mountPath: /modelcache
            pvcNameParameter:
              runtimeValue:
                constant: docling-model-cache-pvc-multi
          tolerations:
          - effect: NoSchedule
            key: key1
            operator: Equal
            value: mcad
