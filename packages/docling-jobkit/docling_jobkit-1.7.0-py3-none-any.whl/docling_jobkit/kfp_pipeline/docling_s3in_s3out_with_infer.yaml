# PIPELINE DEFINITION
# Name: docling-s3in-s3out-with-infer
# Inputs:
#    batch_size: int [Default: 20.0]
#    convertion_options: dict [Default: {'force_ocr': False, 'do_code_enrichment': False, 'do_formula_enrichment': False, 'do_picture_classification': False, 'to_formats': ['md', 'json', 'html', 'text', 'doctags'], 'return_as_file': False, 'include_images': True, 'abort_on_error': False, 'ocr_lang': [], 'do_picture_description': False, 'ocr_engine': 'easyocr', 'table_mode': 'accurate', 'images_scale': 2.0, 'image_export_mode': 'placeholder', 'generate_picture_images': False, 'do_ocr': True, 'from_formats': ['docx', 'pptx', 'html', 'image', 'pdf', 'asciidoc', 'md', 'xlsx', 'xml_uspto', 'xml_jats', 'json_docling'], 'pdf_backend': 'dlparse_v2', 'do_table_structure': True}]
#    source: dict [Default: {'endpoint': 's3.eu-de.cloud-object-storage.appdomain.cloud', 'access_key': '123454321', 'verify_ssl': True, 'key_prefix': 'my-docs', 'bucket': 'source-bucket', 'secret_key': 'secretsecret'}]
#    target: dict [Default: {'endpoint': 's3.eu-de.cloud-object-storage.appdomain.cloud', 'access_key': '123454321', 'verify_ssl': True, 'key_prefix': 'my-docs', 'bucket': 'target-bucket', 'secret_key': 'secretsecret'}]
components:
  comp-compute-batches:
    executorLabel: exec-compute-batches
    inputDefinitions:
      parameters:
        batch_size:
          defaultValue: 10.0
          isOptional: true
          parameterType: NUMBER_INTEGER
        source:
          parameterType: STRUCT
        target:
          parameterType: STRUCT
    outputDefinitions:
      artifacts:
        dataset:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
      parameters:
        batch_indices:
          parameterType: LIST
  comp-convert-payload:
    executorLabel: exec-convert-payload
    inputDefinitions:
      artifacts:
        dataset:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
      parameters:
        batch_index:
          parameterType: NUMBER_INTEGER
        options:
          parameterType: STRUCT
        source:
          parameterType: STRUCT
        target:
          parameterType: STRUCT
    outputDefinitions:
      parameters:
        Output:
          parameterType: LIST
  comp-for-loop-1:
    dag:
      tasks:
        convert-payload:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-convert-payload
          inputs:
            artifacts:
              dataset:
                componentInputArtifact: pipelinechannel--compute-batches-dataset
            parameters:
              batch_index:
                componentInputParameter: pipelinechannel--compute-batches-batch_indices-loop-item
              options:
                componentInputParameter: pipelinechannel--convertion_options
              source:
                componentInputParameter: pipelinechannel--source
              target:
                componentInputParameter: pipelinechannel--target
          taskInfo:
            name: convert-payload
    inputDefinitions:
      artifacts:
        pipelinechannel--compute-batches-dataset:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
      parameters:
        pipelinechannel--compute-batches-batch_indices:
          parameterType: LIST
        pipelinechannel--compute-batches-batch_indices-loop-item:
          parameterType: NUMBER_INTEGER
        pipelinechannel--convertion_options:
          parameterType: STRUCT
        pipelinechannel--source:
          parameterType: STRUCT
        pipelinechannel--target:
          parameterType: STRUCT
deploymentSpec:
  executors:
    exec-compute-batches:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - compute_batches
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.13.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'pydantic' 'boto3~=1.35.36'\
          \ 'git+https://github.com/docling-project/docling-jobkit@main' && \"$0\"\
          \ \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef compute_batches(\n    source: dict,\n    target: dict,\n    dataset:\
          \ Output[Dataset],\n    batch_size: int = 10,\n) -> NamedTuple(\"outputs\"\
          , [(\"batch_indices\", List[int])]):  # type: ignore[valid-type]\n    import\
          \ json\n    from typing import NamedTuple\n\n    from docling_jobkit.connectors.s3_helper\
          \ import (\n        check_target_has_source_converted,\n        generate_batch_keys,\n\
          \        get_s3_connection,\n        get_source_files,\n    )\n    from\
          \ docling_jobkit.datamodel.s3_coords import S3Coordinates\n\n    # validate\
          \ inputs\n    s3_coords_source = S3Coordinates.model_validate(source)\n\
          \    s3_target_coords = S3Coordinates.model_validate(target)\n\n    s3_source_client,\
          \ s3_source_resource = get_s3_connection(s3_coords_source)\n    source_objects_list\
          \ = get_source_files(\n        s3_source_client, s3_source_resource, s3_coords_source\n\
          \    )\n    filtered_source_keys = check_target_has_source_converted(\n\
          \        s3_target_coords, source_objects_list, s3_coords_source.key_prefix\n\
          \    )\n    batch_keys = generate_batch_keys(\n        filtered_source_keys,\n\
          \        batch_size=batch_size,\n    )\n\n    # store batches on s3 for\
          \ debugging\n    s3_target_client, s3_target_resource = get_s3_connection(s3_target_coords)\n\
          \    try:\n        s3_target_client.put_object(\n            Body=json.dumps(batch_keys),\n\
          \            Bucket=s3_target_coords.bucket,\n            Key=f\"{s3_target_coords.key_prefix}/buckets.json\"\
          ,\n            ContentType=\"application/json\",\n        )\n    except\
          \ Exception as e:\n        print(\"Uploading batch.json to s3 failed\",\
          \ exc_info=e)\n\n    with open(dataset.path, \"w\") as out_batches:\n  \
          \      json.dump(batch_keys, out_batches)\n\n    batch_indices = list(range(len(batch_keys)))\n\
          \    outputs = NamedTuple(\"outputs\", [(\"batch_indices\", List[int])])\n\
          \    print(f\"Batches to convert: {len(batch_keys)}\")\n    return outputs(batch_indices)\n\
          \n"
        image: ghcr.io/docling-project/docling-serve-cpu:v1.5.1
    exec-convert-payload:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - convert_payload
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.13.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'docling==2.50.0'\
          \ 'boto3~=1.35.36' 'git+https://github.com/docling-project/docling-jobkit@main'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef convert_payload(\n    options: dict,\n    source: dict,\n   \
          \ target: dict,\n    batch_index: int,\n    # source_keys: List[str],\n\
          \    dataset: Input[Dataset],\n) -> list:\n    import json\n    import logging\n\
          \    from pathlib import Path\n\n    from docling.datamodel.base_models\
          \ import ConversionStatus, InputFormat\n    from docling.datamodel.pipeline_options\
          \ import (\n        VlmPipelineOptions,\n    )\n    from docling.datamodel.pipeline_options_vlm_model\
          \ import (\n        ApiVlmOptions,\n        ResponseFormat,\n    )\n   \
          \ from docling.datamodel.settings import settings\n    from docling.document_converter\
          \ import DocumentConverter, PdfFormatOption\n    from docling.pipeline.vlm_pipeline\
          \ import VlmPipeline\n    from docling_core.types.doc.base import ImageRefMode\n\
          \n    from docling_jobkit.connectors.s3_helper import (\n        generate_presign_url,\n\
          \        get_s3_connection,\n    )\n    from docling_jobkit.convert.manager\
          \ import (\n        DoclingConverterManager,\n        DoclingConverterManagerConfig,\n\
          \    )\n    from docling_jobkit.datamodel.convert import ConvertDocumentsOptions\n\
          \    from docling_jobkit.datamodel.s3_coords import S3Coordinates\n\n  \
          \  def openai_compatible_vlm_options(\n        model: str,\n        prompt:\
          \ str,\n        format: ResponseFormat,\n        hostname_and_port,\n  \
          \      temperature: float = 0.7,\n        max_tokens: int = 4096,\n    \
          \    api_key: str = \"\",\n        skip_special_tokens=False,\n    ):\n\
          \        headers = {}\n        if api_key:\n            headers[\"Authorization\"\
          ] = f\"Bearer {api_key}\"\n\n        options = ApiVlmOptions(\n        \
          \    url=f\"http://{hostname_and_port}/v1/chat/completions\",  # LM studio\
          \ defaults to port 1234, VLLM to 8000\n            params = {\n        \
          \        \"model\": model,\n                \"max_tokens\": max_tokens,\n\
          \                \"skip_special_tokens\": skip_special_tokens  # needed\
          \ for VLLM\n            },\n            headers=headers,\n            prompt=prompt,\n\
          \            timeout=90,\n            scale=2.0,\n            temperature=temperature,\n\
          \            response_format=format,\n        )\n        return options\n\
          \n    logging.basicConfig(level=logging.INFO)\n    settings.debug.profile_pipeline_timings\
          \ = True\n\n    # validate coords\n    s3_coords_source = S3Coordinates.model_validate(source)\n\
          \    target_s3_coords = S3Coordinates.model_validate(target)\n    s3_source_client,\
          \ s3_source_resource = get_s3_connection(s3_coords_source)\n    s3_target_client,\
          \ s3_target_resource = get_s3_connection(target_s3_coords)\n\n\n    with\
          \ open(dataset.path) as f:\n        batches = json.load(f)\n    source_keys\
          \ = batches[batch_index]\n\n    presign_filtered_source_keys = [\n     \
          \   generate_presign_url(s3_source_client, key, s3_coords_source.bucket)\n\
          \        for key in source_keys\n    ]\n\n    pipeline_options = VlmPipelineOptions(\n\
          \        enable_remote_services=True  # required when calling remote VLM\
          \ endpoints\n    )\n\n    pipeline_options.vlm_options = openai_compatible_vlm_options(\n\
          \        model=\"ibm-granite/granite-docling-258M\",  # For VLLM use \"\
          ibm-granite/granite-docling-258M\"\n        hostname_and_port=\"vllm-gpu.deep-search.svc.cluster.local:8000\"\
          ,  # LM studio defaults to port 1234, VLLM to 8000\n        prompt=\"Convert\
          \ this page to docling.\",\n        format=ResponseFormat.DOCTAGS,\n   \
          \     api_key=\"\",\n    )\n\n    doc_converter = DocumentConverter(\n \
          \       format_options={\n            InputFormat.PDF: PdfFormatOption(\n\
          \                pipeline_options=pipeline_options,\n                pipeline_cls=VlmPipeline,\n\
          \            )\n        }\n    )\n\n    for item in presign_filtered_source_keys:\n\
          \        conv_res = doc_converter.convert(item)\n        if conv_res.status\
          \ == ConversionStatus.SUCCESS:\n            filename = conv_res.input.document_hash\n\
          \n            # store doc\n            temp_folder = Path(\"/modelcache/docling-temp\"\
          )\n            temp_folder.mkdir(parents=True, exist_ok=True)\n        \
          \    temp_json_file = temp_folder / f\"{filename}.temp.json\"\n        \
          \    conv_res.document.save_as_json(\n                filename=temp_json_file,\n\
          \                image_mode=ImageRefMode.EMBEDDED,\n            )\n    \
          \        kwargs = {}\n            kwargs[\"ContentType\"] = \"application/json\"\
          \n            try:\n                s3_target_client.upload_file(\n    \
          \                Filename=temp_json_file,\n                    Bucket=target_s3_coords.bucket,\n\
          \                    Key=f\"{target_s3_coords.key_prefix}/{filename}.json\"\
          ,\n                    ExtraArgs={**kwargs},\n                )\n      \
          \          # s3_target_client.put_object(\n                #     Body=json.dumps(conv_res.document.export_to_dict()),\n\
          \                #     Bucket=target_s3_coords.bucket,\n               \
          \ #     Key=f\"{target_s3_coords.key_prefix}/{filename}.json\",\n      \
          \          #     ContentType='\"application/json\"'\n                # )\n\
          \            except Exception as e:\n                print(\"Uploading document\
          \ to s3 failed\", exc_info=e)\n\n            # clean-up\n            temp_json_file.unlink()\n\
          \n            # store timings\n            timings = {}\n            for\
          \ key in conv_res.timings.keys():\n                timings[key] = {\n  \
          \                  \"scope\": conv_res.timings[key].scope.name,\n      \
          \              \"count\": conv_res.timings[key].count,\n               \
          \     \"times\": conv_res.timings[key].times,\n                }\n     \
          \       try:\n                s3_target_client.put_object(\n           \
          \         Body=json.dumps(timings),\n                    Bucket=target_s3_coords.bucket,\n\
          \                    Key=f\"{target_s3_coords.key_prefix}/{filename}.timings.json\"\
          ,\n                    ContentType=\"application/json\",\n             \
          \   )\n            except Exception as e:\n                print(\"Uploading\
          \ timings to s3 failed\", exc_info=e)\n\n    return []\n\n"
        image: ghcr.io/docling-project/docling-serve-cpu:v1.5.1
        resources:
          cpuLimit: 1.0
          cpuRequest: 0.2
          memoryLimit: 16.0
          memoryRequest: 1.0
          resourceCpuLimit: '1'
          resourceCpuRequest: 200m
          resourceMemoryLimit: 16G
          resourceMemoryRequest: 1G
pipelineInfo:
  name: docling-s3in-s3out-with-infer
root:
  dag:
    tasks:
      compute-batches:
        cachingOptions: {}
        componentRef:
          name: comp-compute-batches
        inputs:
          parameters:
            batch_size:
              componentInputParameter: batch_size
            source:
              componentInputParameter: source
            target:
              componentInputParameter: target
        taskInfo:
          name: compute-batches
      for-loop-1:
        componentRef:
          name: comp-for-loop-1
        dependentTasks:
        - compute-batches
        inputs:
          artifacts:
            pipelinechannel--compute-batches-dataset:
              taskOutputArtifact:
                outputArtifactKey: dataset
                producerTask: compute-batches
          parameters:
            pipelinechannel--compute-batches-batch_indices:
              taskOutputParameter:
                outputParameterKey: batch_indices
                producerTask: compute-batches
            pipelinechannel--convertion_options:
              componentInputParameter: convertion_options
            pipelinechannel--source:
              componentInputParameter: source
            pipelinechannel--target:
              componentInputParameter: target
        iteratorPolicy:
          parallelismLimit: 20
        parameterIterator:
          itemInput: pipelinechannel--compute-batches-batch_indices-loop-item
          items:
            inputParameter: pipelinechannel--compute-batches-batch_indices
        taskInfo:
          name: for-loop-1
  inputDefinitions:
    parameters:
      batch_size:
        defaultValue: 20.0
        isOptional: true
        parameterType: NUMBER_INTEGER
      convertion_options:
        defaultValue:
          abort_on_error: false
          do_code_enrichment: false
          do_formula_enrichment: false
          do_ocr: true
          do_picture_classification: false
          do_picture_description: false
          do_table_structure: true
          force_ocr: false
          from_formats:
          - docx
          - pptx
          - html
          - image
          - pdf
          - asciidoc
          - md
          - xlsx
          - xml_uspto
          - xml_jats
          - json_docling
          generate_picture_images: false
          image_export_mode: placeholder
          images_scale: 2.0
          include_images: true
          ocr_engine: easyocr
          ocr_lang: []
          pdf_backend: dlparse_v2
          return_as_file: false
          table_mode: accurate
          to_formats:
          - md
          - json
          - html
          - text
          - doctags
        isOptional: true
        parameterType: STRUCT
      source:
        defaultValue:
          access_key: '123454321'
          bucket: source-bucket
          endpoint: s3.eu-de.cloud-object-storage.appdomain.cloud
          key_prefix: my-docs
          secret_key: secretsecret
          verify_ssl: true
        isOptional: true
        parameterType: STRUCT
      target:
        defaultValue:
          access_key: '123454321'
          bucket: target-bucket
          endpoint: s3.eu-de.cloud-object-storage.appdomain.cloud
          key_prefix: my-docs
          secret_key: secretsecret
          verify_ssl: true
        isOptional: true
        parameterType: STRUCT
schemaVersion: 2.1.0
sdkVersion: kfp-2.13.0
---
platforms:
  kubernetes:
    deploymentSpec:
      executors:
        exec-convert-payload:
          pvcMount:
          - constant: docling-model-cache-pvc-multi
            mountPath: /modelcache
            pvcNameParameter:
              runtimeValue:
                constant: docling-model-cache-pvc-multi
          tolerations:
          - effect: NoSchedule
            key: key1
            operator: Equal
            value: mcad
