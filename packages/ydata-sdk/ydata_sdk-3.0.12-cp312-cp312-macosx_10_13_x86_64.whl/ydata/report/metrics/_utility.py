from itertools import chain, combinations
from typing import List

from matplotlib.pyplot import subplots as plt_subplots
from matplotlib.pyplot import subplots_adjust as plt_subplots_adjust
from matplotlib.pyplot import tight_layout as plt_tight_layout
from numpy import amax as np_amax
from numpy import amin as np_amin
from numpy import mean as np_mean
from numpy import sum as np_sum
from numpy.random import choice, randint
from pandas import DataFrame as pdDataFrame
from pandas import Index as pdIndex
from pandas import Series as pdSeries
from pandas import concat as pd_concat
from seaborn import barplot as sns_barplot
from seaborn import despine as sns_despine
from seaborn import set_context as sns_set_context
from seaborn import set_style as sns_set_style
from sklearn.compose import ColumnTransformer
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.feature_selection import SelectKBest, chi2
from sklearn.inspection import permutation_importance
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.preprocessing import KBinsDiscretizer, OrdinalEncoder

from ydata.metadata import Metadata
from ydata.report.logger import logger
from ydata.report.metrics import MetricType
from ydata.report.metrics.base_metric import BaseMetric
from ydata.report.metrics.predictive_models import ModelsList, PredictiveModel, Task
from ydata.report.styles.html import StyleHTML
from ydata.utils.data_types import CATEGORICAL_DTYPES, DataType
from ydata.utils.metadata import get_types

tstr_mapper = {
    "randomforest": "Random Forest",
    "gaussiannb": "Gaussian Naive Bayes",
    "bernoullinb": "Bernoulli Naive Bayes",
    "svmlin": "Linear Support Vector",
    "extra_trees": "Extra Trees",
    "lda": "Linear Discriminant Analysis",
    "adaboost": "AdaBoost",
    "bagging": "Bagging",
    "gbm": "Gradient Boosting",
    "nn": "Multi-layer Perceptron",
    "linear": "Linear Regression",
    "mlp": "Multi-layer Perceptron",
    "tree_r": "Decision Tree",
    "ridge": "Ridge",
    "lasso": "Lasso",
    "svr": "Linear Support Vector",
}


DECIMAL_PRECISION = 2


def _mode(serie):
    mode = serie.mode()
    if isinstance(mode, pdSeries):
        mode = mode[0]
    return mode


def _last(serie):
    return serie.iloc[-1]


class PredictiveScore(BaseMetric):
    def __init__(self, formatter=StyleHTML, exclude_entity_col: bool = True) -> None:
        super().__init__(formatter, exclude_entity_col)

    @staticmethod
    def _get_description(formatter):
        description = "The prediction metrics highlight the AUROC performance of several \
            machine learning (ML) models that are trained separately on both the synthetic \
            and original datasets. The score is calculated by testing the models on withheld \
            data from the original dataset.\nThe Train Synthetic Test Real (TSTR) score measures \
            the performance of an estimator trained on synthetic data and later evaluated \
            on real data. Train Real Test Real (TRTR) provides the estimator score expected \
            if actual data was available.\nSuppose the TSTR and TRTR scores are comparable. \
            In that case, the data generated by the synthesizer has a similar predictive \
            performance as the original. This score is calculated using (TSTR/TRTR) and for that \
            values between [0, +inf[ are expected. Values >1 mean that the synthetic data had a better \
            performance than the original data. For Machine Learning use, a predictive score above 0.8 is recommended."

        return description

    def _get_task(self, data_types: dict, target: str) -> Task:
        if DataType(data_types[target]) == DataType.NUMERICAL:
            return Task.REGRESSION
        elif DataType(data_types[target]) in CATEGORICAL_DTYPES:
            return Task.CLASSIFICATION
        else:
            raise Exception(
                f"Provided target variable ({target}) not of numerical, bool or category type."
            )

    def _get_metric(self, task: Task) -> str:
        if task == Task.CLASSIFICATION:
            return "auc"
        else:
            return "mape"

    def _split_dataset(
        self, df: pdDataFrame, target: str, task: Task, test_ratio: float = 0.3
    ):
        y = df[target]
        X = df.drop(target, axis=1)
        if test_ratio == 0:
            return (X, y)

        if task == Task.CLASSIFICATION:
            sss = StratifiedShuffleSplit(n_splits=1)
            train, test = [(train, test) for train, test in sss.split(X, y)][0]
            return (X.iloc[train], y.iloc[train]), (X.iloc[test], y.iloc[test])
        else:
            pivot = int(len(df) * test_ratio)
            return (X[:pivot], y[:pivot]), (X[:pivot], y[:pivot])

    def _calculate_scores(
        self, real_train, real_test, synth_data, models, metric, task
    ):
        performance_trtr = []
        performance_tstr = []
        detailed = {"Estimators": [], "Real Data": [], "Synth Data": []}

        for model_name in models:
            real_model = PredictiveModel(model_name, task)
            synth_model = PredictiveModel(model_name, task)
            real_score = real_model.evaluate(*real_train, *real_test, metric)
            synth_score = synth_model.evaluate(*synth_data, *real_test, metric)
            detailed["Estimators"].append(tstr_mapper[model_name.lower()])
            detailed["Real Data"].append(round(real_score, DECIMAL_PRECISION))
            detailed["Synth Data"].append(
                round(synth_score, DECIMAL_PRECISION))
            performance_tstr.append(synth_score)
            performance_trtr.append(real_score)

        average_tstr = np_mean(performance_tstr)
        average_trtr = np_mean(performance_trtr)
        return average_tstr, average_trtr, detailed

    @property
    def name(self) -> str:
        return "Train on Synthetic Test on Real"


class TSTR(PredictiveScore):
    def __init__(self, formatter=StyleHTML) -> None:
        super().__init__(formatter)

    def _evaluate(self, source, synthetic, **kwargs):
        return self._tstr(
            source, synthetic, data_types=kwargs["data_types"], target=kwargs["target"]
        )

    def _tstr(
        self, df_real: pdDataFrame, df_synth: pdDataFrame, data_types: dict, target: str
    ):
        """Train on Synthetic Test on Real."""
        task = self._get_task(data_types, target)
        models = ModelsList.elements_by_task(task)
        metric = self._get_metric(task)

        real_train, real_test = self._split_dataset(df_real, target, task)
        # FIXME if both datasets are of the same size we train the synth one with more data since we dont split
        synth_data = self._split_dataset(df_synth, target, task, test_ratio=0)
        average_tstr, average_trtr, detailed = self._calculate_scores(
            real_train, real_test, synth_data, models, metric, task
        )
        return {
            "target_variable": target,
            "task": task.value,
            "score_tstr": average_tstr,
            "score_trtr": average_trtr,
            "metric": metric,
            "detailed": detailed,
        }


class TSTRTimeseries(PredictiveScore):
    def __init__(self, formatter=StyleHTML, exclude_entity_col: bool = True) -> None:
        super().__init__(formatter, exclude_entity_col)

    def _evaluate(self, source, synthetic, **kwargs):
        return self._tstr_timeseries(
            source,
            synthetic,
            metadata=kwargs["metadata"],
            entity_data=kwargs["entity_data"],
            data_types=kwargs["data_types"],
            target=kwargs["target"]
        )

    def _get_sample_sequence_start(self, source_length: int, sample_lenght: int) -> int:
        if source_length <= sample_lenght:
            return 0
        return randint(0, source_length - sample_lenght)

    def _get_columns_map(self, columns: List[str], metadata: Metadata, target: str, entity_cols: list[str] | None = None) -> dict:
        columns_map = {
            col: "mean" if col in metadata.numerical_vars else _mode for col in columns if col not in entity_cols
        }
        columns_map[target] = _last

        return columns_map

    def _sample_sequences(
        self,
        dataframe: pdDataFrame,
        sample_size: int,
        sequence_length: int,
        metadata: Metadata,
        task: Task,
        target: str,
        entity_data: pdDataFrame
    ):
        columns_map = self._get_columns_map(
            dataframe.columns, metadata, target, list(entity_data.columns))

        if not entity_data.empty:
            sequences = []
            entities = entity_data.apply(
                lambda x: tuple(x), axis=1).unique()
            for _ in range(sample_size):
                entity = choice(entities)
                entity_filtered_data = entity_data[(
                    entity_data == entity).all(axis=1)]
                df = dataframe.loc[entity_filtered_data.index].reset_index(
                    drop=True)
                seq_start = self._get_sample_sequence_start(
                    len(df), sequence_length)
                sequences.append(
                    df.loc[seq_start: (seq_start + sequence_length - 1)].agg(
                        columns_map
                    )
                )

            sequences = pdDataFrame(sequences)
        else:
            seq_start = randint(0, len(dataframe) -
                                sequence_length, sample_size)
            sequences = [
                dataframe.iloc[i: (i + sequence_length - 1)].agg(columns_map)
                for i in seq_start
            ]
            sequences = pdDataFrame(sequences)

        if task == Task.CLASSIFICATION:
            sequences = self._complementary_sample(
                sequences, dataframe, target, entity_data, sequence_length, metadata
            )

        return sequences

    def _complementary_sample(
        self,
        current_sample: pdDataFrame,
        dataframe: pdDataFrame,
        target: str,
        entity_data: pdDataFrame,
        sequence_length: int,
        metadata: Metadata,
    ):
        unsampled_class = set(dataframe[target].unique()) - set(
            current_sample[target].unique()
        )
        if len(unsampled_class) == 0:
            return current_sample

        columns_map = self._get_columns_map(
            dataframe.columns, metadata, target)
        sequences = []
        min_entries = 10
        for _class in unsampled_class:
            # if there are less entries of the class than min_entries there will be repeated values in the sample
            for _ in range(min_entries):
                idx = choice(dataframe[dataframe[target] == _class].index)
                if not entity_data.empty:
                    entity = tuple(entity_data.loc[idx])
                    entity_filtered_data = entity_data[(
                        entity_data == entity).all(axis=1)]
                    df = dataframe.loc[entity_filtered_data.index]
                    sequences.append(
                        df.loc[:idx][-sequence_length:].agg(columns_map))
                else:
                    sequences.append(
                        dataframe.loc[:idx][-sequence_length:].agg(columns_map)
                    )

        return pd_concat([current_sample, pdDataFrame(sequences)]).reset_index(
            drop=True
        )

    def _tstr_timeseries(
        self,
        df_real: pdDataFrame,
        df_synth: pdDataFrame,
        metadata: Metadata,
        entity_data: dict,
        target: str = None,
        sequence_lenght: int = 10,
        data_types=None,
    ):
        if data_types is None:
            data_types = get_types(df_real)

        task = self._get_task(data_types, target)
        sample_size = 3_000
        df_real = self._sample_sequences(
            df_real, sample_size, sequence_lenght, metadata, task, target, entity_data["real"]
        )
        df_synth = self._sample_sequences(
            df_synth, sample_size, sequence_lenght, metadata, task, target, entity_data[
                "synth"]
        )

        models = ModelsList.elements_by_task(task)
        metric = self._get_metric(task)

        # data preparation
        logger.info(f"[PROFILEREPORT] - [TSTRTimeSeries] task {task}")
        real_train, real_test = self._split_dataset(df_real, target, task)
        synth_data = self._split_dataset(df_synth, target, task, test_ratio=0)

        average_tstr, average_trtr, detailed = self._calculate_scores(
            real_train, real_test, synth_data, models, metric, task
        )

        return {
            "target_variable": target,
            "task": task.value,
            "score_tstr": average_tstr,
            "score_trtr": average_trtr,
            "metric": metric,
            "detailed": detailed,
        }


def feature_importance(df_real, df_synth, target, visualize=False):
    """
    target- temporary
    Extract information about the features of both dataset. To see if they have the same importance.
    Outputs:
        feat_scores: pdDataframe containing best features per dataset with relative score.
    """
    best_feat = SelectKBest(score_func=chi2, k=10)
    fit_real = best_feat.fit(df_real.drop(target, axis=1), df_real[target])
    best_feat = SelectKBest(score_func=chi2, k=10)
    fit_synth = best_feat.fit(df_synth.drop(target, axis=1), df_synth[target])
    score_real = pdSeries(fit_real.scores_)
    score_real = score_real / np_sum(score_real)
    score_synth = pdSeries(fit_synth.scores_)
    score_synth = score_synth / np_sum(score_synth)
    columns_real = pdSeries(df_real.drop(target, axis=1).columns)
    columns_synth = pdSeries(df_synth.drop(target, axis=1).columns)
    real = pdSeries(["real"] * len(columns_real))
    synth = pdSeries(["synth"] * len(columns_synth))

    if visualize:
        a = pd_concat([columns_real, score_real, real], axis=1)
        a.columns = ["columns", "score", "type"]
        b = pd_concat([columns_synth, score_synth, synth], axis=1)
        b.columns = ["columns", "score", "type"]
        real_synth_viz_df = a.append(b, ignore_index=True)

    feat_scores = pd_concat(
        [columns_real, score_real, columns_synth, score_synth], axis=1
    )
    feat_scores.columns = ["real_columns",
                           "score_real", "synth_columns", "score_synth"]
    feat_scores.sort_values(
        by="score_real", inplace=True, ascending=False, ignore_index=True
    )
    feat_scores.iloc[:, 2:] = feat_scores.iloc[:, 2:].sort_values(
        by="score_synth", ascending=False, ignore_index=True
    )
    top_feat_scores = feat_scores.iloc[:10, :]
    results = (top_feat_scores,
               real_synth_viz_df) if visualize else top_feat_scores
    return results


class QScore(BaseMetric):
    """QScore returns The QScore measures the downstream performance of the
    synthetic data by running many random aggregation-based queries on both the
    synthetic and original datasets and then scoring the similarity of their
    returns.

    Args:
    max_cols (int): number of columns to group by.
    n_queries (int): number of random queries to run.
    n_bins (int): number of bins to discretize numerical columns. Defaults to 100.
    compute_penalty (bool): enables the penalty computation. Defaults to False.
    """

    def __init__(
        self,
        formatter=StyleHTML,
        max_cols: int = 2,
        n_queries: int = 1000,
        n_bins: int = 100,
        compute_penalty: bool = False,
    ):
        super().__init__(formatter)

        self.max_cols = max_cols
        self.n_queries = n_queries
        self.n_bins = n_bins
        self.max_categories = n_bins
        self.compute_penalty = compute_penalty

    @staticmethod
    def _get_description(formatter):
        description = f"{formatter.bold('QScore')} measures the utility of the synthetic \
            data by comparing the returns of random aggregation-based queries on both the \
            synthetic and original datasets. The QScore ensures that future queries \
            performed on the synthetic data would return the same statistical characteristics \
            as those on the original data. Score between [0, 1]. \n \
            It is recommended a QScore above 0.8 if leveraging the synthetic data for BI \
            activities or unsupervised learning."

        return description

    def _evaluate(self, source, synthetic, **kwargs):
        return self._score(
            source,
            synthetic,
            data_types=kwargs["data_types"],
        )

    @property
    def name(self) -> str:
        return "QScore"

    @staticmethod
    def penalty_score(matched_df, real_prematch, synth_prematch):
        penalty = len(matched_df) / \
            max(len(real_prematch), len(synth_prematch))
        return penalty

    def _discretize(
        self,
        source: pdDataFrame,
        synthetic: pdDataFrame,
        data_types: dict,
        encode: str = "ordinal",
        strategy: str = "uniform",
    ):
        discretizer = KBinsDiscretizer(
            n_bins=self.n_bins, encode=encode, strategy=strategy
        )
        cat_encoder = OrdinalEncoder(max_categories=self.max_categories)
        num_cols = [
            col for col in source.columns if DataType(data_types[col]) == DataType.NUMERICAL and col in synthetic.columns
        ]
        cat_cols = [
            col for col in source.columns if DataType(data_types[col]) in [DataType.CATEGORICAL, DataType.STR] and col in synthetic.columns
        ]
        transformer = ColumnTransformer(
            [
                ("discretizer", discretizer, pdIndex(num_cols)),
                ("categorical_encoder", cat_encoder, pdIndex(cat_cols)),
            ],
            remainder="passthrough"
        )
        transformer.fit(pd_concat([source, synthetic], axis=0))
        columns = num_cols + cat_cols + \
            [col for col in source.columns if col not in (num_cols + cat_cols)]

        return (
            pdDataFrame(transformer.transform(source), columns=columns),
            pdDataFrame(transformer.transform(
                synthetic), columns=columns),
        )

    def _sample_columns(self, columns):
        # Randomly select the number of columns to query
        past_queries = set()
        queries = []
        for _ in range(self.n_queries):
            query_n_cols = randint(low=1, high=min(
                len(columns) + 1, self.max_cols+1))
            query = choice(columns, query_n_cols, replace=False).tolist()
            qname = "|".join(map(str, sorted(query)))
            if qname not in past_queries:
                queries.append(query)
                past_queries.add(qname)

        return queries

    def _powerset(self, columns: list) -> List[List[str]]:
        "powerset([1,2,3]) --> (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)"
        column_combinations = chain.from_iterable(
            combinations(columns, r)
            for r in range(1, min(len(columns), self.max_cols)+1)
        )
        return list(map(list, column_combinations))

    def _get_queries(self, columns):
        if 2 ** len(columns) < self.n_queries:
            return self._powerset(columns)
        else:
            return self._sample_columns(columns)

    def _query_datasource(self, dataframe: pdDataFrame, columns: List[str]):
        # Aggregate the data on the randomly selected columns
        query = dataframe.groupby(columns).size()
        query /= dataframe.shape[0]
        return query.fillna(0)

    def _penalty_score(
        self,
        source_query: pdDataFrame,
        synth_query: pdDataFrame,
        joined_query: pdDataFrame,
    ):
        return len(joined_query) / max(len(source_query), len(synth_query))

    def _query_columns(
        self, source: pdDataFrame, synthetic: pdDataFrame, columns: List[str]
    ):
        source_query = self._query_datasource(source, columns)
        synth_query = self._query_datasource(synthetic, columns)
        joined = pd_concat([source_query, synth_query], axis=1, join="inner")
        penalty = 1
        if self.compute_penalty:
            penalty = self._penalty_score(source_query, synth_query, joined)

        return joined.to_numpy(), penalty

    def _compute_query_score(self, queries, penalty):
        # Compare the rows of the synthetic and original aggregated columns.
        # This divides the total of the smallest aggregated values by the largest
        # aggregated values irrespective of whether they are from the synthetic or
        # the orginal datasets
        scores = []

        max_list = np_amax(queries, axis=1)
        min_list = np_amin(queries, axis=1)
        # ensure no zero division error
        if len(min_list) > 0 and sum(max_list) > 0:
            query_score = sum(min_list) / sum(max_list)
            # incorporate the penalty score which penalises the final score for not including all categories
            scores.append(query_score * penalty)
        return scores

    def _score(self, source, synthetic, data_types):
        source, synthetic = self._discretize(
            source, synthetic, data_types)

        query_scores = []

        queries = self._get_queries(source.columns)
        for query_cols in queries:
            query_results, penalty = self._query_columns(
                source, synthetic, query_cols)
            score = self._compute_query_score(query_results, penalty)
            query_scores.extend(score)
        score = np_mean(query_scores)

        return score


class FeatureImportance(BaseMetric):
    def __init__(self, formatter=StyleHTML, include_plot=True) -> None:
        super().__init__(formatter)
        self._include_plot = include_plot

    @property
    def name(self) -> str:
        return "Feature Importance"

    @property
    def type(self) -> MetricType:
        return MetricType.VISUAL

    @staticmethod
    def _get_description(formatter):
        description = "The feature importance score extends the prediction score by \
            highlighting the differences in the importance of the most critical features \
            for a downstream model trained on the synthetic data and the same downstream \
            model trained on the original data. Synthetic data of excellent quality shall \
            preserve an approximate order for variables with higher importance."

        return description

    def _evaluate(self, source, synthetic, **kwargs):
        target = kwargs["target"]
        data_types = kwargs["data_types"]
        source_feat_importance = self._get_feat_importances(
            source, target, data_types)
        synth_feat_importance = self._get_feat_importances(
            synthetic, target, data_types
        )

        importance_score = self.feat_importance_score(
            source_feat_importance, synth_feat_importance
        )

        if self._include_plot:
            plot = self._compare_feat_plots(
                source_feat_importance,
                synth_feat_importance,
            )
            return {"plot": plot, "score": importance_score}
        return importance_score

    def _get_feat_importances(
        self,
        dataframe: pdDataFrame,
        target: str,
        data_types: dict,
        n_repeats: int = 5,
        n_jobs: int = 1
    ):
        """
        Args:
            dataframe: dataset
            target: target feature
            data_types: features data types
            n_repeats: number of rounds of permutation
            n_jobs: number of jobs to run in parallel
        Returns:
            feature_importances (array): a descending list of feature importances.
        """
        if data_types[target] in CATEGORICAL_DTYPES:
            model = GradientBoostingClassifier()
            scoring = "neg_log_loss"
        else:
            model = LinearRegression()
            scoring = "neg_mean_absolute_error"

        X = dataframe.drop([target], axis=1)
        y = dataframe[target]
        model.fit(X, y)
        result = permutation_importance(
            model, X, y, n_repeats=n_repeats, n_jobs=n_jobs, scoring=scoring
        )
        feature_importances = pdSeries(
            result.importances_mean, index=X.columns)
        return feature_importances.sort_values(ascending=False)

    def feat_importance_score(self, importance_real, importance_synth, max_features=10):
        combined = pd_concat(
            [importance_real.head(max_features).rename("real"),
             importance_synth.rename("synth")], join="inner", axis=1
        )
        global_min = combined.min().min()
        global_max = combined.max().max()
        r = global_max - global_min
        sim_per_feature = (
            1 - (abs(combined.synth - combined.real) / (r if r != 0 else 1)))
        return sim_per_feature.sum() / combined.shape[0]

    def _compare_feat_plots(self, real_importance, synth_importance, max_features=10):
        """
        Args:
            real_importance: descending list of feature importances from one dataset
            synth_importance: descending list of feature importances from another dataset
        Returns:
            a figure with two plots of feature importance next to each other.
        """
        fig, ax = plt_subplots(figsize=(8, 4))
        primary_color = "#166FF5"
        secondary_color = "#E32212"

        ax.set_title("Permutation Feature Importance")
        plt_subplots_adjust(top=0.85)

        real_importance = real_importance.head(max_features)
        synth_importance = synth_importance[real_importance.index]
        importance = pd_concat([real_importance, synth_importance])
        importance = importance.reset_index()
        importance["data source"] = ["original"] * \
            len(real_importance) + ["synthetic"] * len(real_importance)
        importance.columns = ["feature", "importance", "data source"]

        def format_column_name(col: str, max_length: int = 15) -> str:
            if len(col) < max_length:
                return col
            return col[:max_length-3] + "..."

        importance["feature"] = [format_column_name(
            x) for x in importance["feature"].values]
        sns_barplot(
            y="feature",
            x="importance",
            hue="data source",
            alpha=0.95,
            ax=ax,
            data=importance,
            palette=[primary_color, secondary_color],
        )
        ax.set_xlabel("Average decrease in score without the feature")
        ax.set_ylabel("Feature")
        ax.tick_params("y", labelrotation=30)
        sns_set_style("white")
        sns_despine()
        sns_set_context("paper")
        plt_tight_layout()

        return fig
