from _typeshed import Incomplete
from enum import Enum

def mean_absolute_percentage_error(y_true, y_pred):
    """Mean absolute percentage error regression loss.

    Examples
    --------
    >>> y_true = [3, -0.5, 2, 7]
    >>> y_pred = [2.5, 0.0, 2, 8]
    >>> mean_absolute_percentage_error(y_true, y_pred) # doctest: +ELLIPSIS
    0.3273...

    >>> y_true = [[0.5, 1], [-1, 1], [7, -6]]
    >>> y_pred = [[0, 2], [-1, 2], [8, -5]]
    >>> mean_absolute_percentage_error(y_true, y_pred) # doctest: +ELLIPSIS
    0.5515...
    """

class Task(Enum):
    CLASSIFICATION = 'classification'
    REGRESSION = 'regression'

class _Model:
    name: Incomplete
    model_class: Incomplete
    task: Incomplete
    parameters: Incomplete
    def __init__(self, name: str, model_class, task: Task = ..., **parameters) -> None: ...
    def fit(self, X, y): ...
    def predict_proba(self, test_x): ...

class ModelsList(Enum):
    LINEAR = ...
    MLP = ...
    TREE_R = ...
    RIDGE = ...
    LASSO = ...
    SVR = ...
    NN = ...
    RANDOMFOREST = ...
    GAUSSIANNB = ...
    BERNOULLINB = ...
    GBM = ...
    EXTRA_TREES = ...
    LDA = ...
    ADABOOST = ...
    BAGGING = ...
    @classmethod
    def elements(cls) -> list[str]: ...
    @classmethod
    def elements_by_task(cls, task: Task) -> list[str]: ...

class Score:
    name: Incomplete
    function: Incomplete
    parameters: Incomplete
    def __init__(self, name, function, **parameters) -> None: ...
    def score(self, *args): ...

class ScoreList(Enum):
    AUC = ...
    APR = ...
    MAE = ...
    MAPE = ...
    RMSE = ...
    F1 = ...
    ACCURACY = ...
    @classmethod
    def elements(cls) -> list[str]: ...

class PredictiveModel:
    def __init__(self, model_name: str, task: Task) -> None: ...
    def evaluate(self, train_x, train_y, test_x, test_y, metric_name: str):
        """Evaluate predictive model performance.

        Args:
        - y_true: original testing labels
        - y_hat: prediction on testing data
        - metric_name: selected metric

        Returns:
        - score: performance of the predictive model
        """
