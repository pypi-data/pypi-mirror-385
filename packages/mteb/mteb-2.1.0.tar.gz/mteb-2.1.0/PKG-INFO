Metadata-Version: 2.4
Name: mteb
Version: 2.1.0
Summary: Massive Text Embedding Benchmark
Author-email: MTEB Contributors <niklas@huggingface.co>, Kenneth Enevoldsen <kenneth.enevoldsen@cas.au.dk>, Nouamane Tazi <nouamane@huggingface.co>, Nils Reimers <info@nils-reimers.de>
Maintainer-email: Kenneth Enevoldsen <kenneth.enevoldsen@cas.au.dk>, Roman Solomatin <risolomatin@gmail.com>, Isaac Chung <chungisaac1217@gmail.com>
License-Expression: Apache-2.0
Project-URL: Homepage, https://github.com/embeddings-benchmark/mteb
Project-URL: Documentation, https://embeddings-benchmark.github.io/mteb/
Project-URL: Repository, https://github.com/embeddings-benchmark/mteb
Project-URL: Hugging Face Organization, https://huggingface.co/mteb
Keywords: deep learning,text embeddings,embeddings,multimodal,benchmark,retrieval,information retrieval
Classifier: Development Status :: 5 - Production/Stable
Classifier: Environment :: Console
Classifier: Intended Audience :: Developers
Classifier: Intended Audience :: Information Technology
Classifier: Operating System :: OS Independent
Classifier: Programming Language :: Python
Requires-Python: <3.14,>=3.10
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: datasets>=2.19.0
Requires-Dist: numpy<3.0.0,>=1.0.0
Requires-Dist: requests>=2.26.0
Requires-Dist: scikit-learn>=1.4.0
Requires-Dist: scipy>=0.0.0
Requires-Dist: sentence_transformers>=3.0.0
Requires-Dist: typing-extensions>=4.5.0
Requires-Dist: torch>1.0.0
Requires-Dist: tqdm>1.0.0
Requires-Dist: rich>=0.0.0
Requires-Dist: pytrec-eval-terrier>=0.5.6
Requires-Dist: pydantic>=2.0.0
Requires-Dist: polars>=0.20.22
Provides-Extra: image
Requires-Dist: torchvision>0.2.1; extra == "image"
Provides-Extra: codecarbon
Requires-Dist: codecarbon<3.0.0,>=2.0.0; extra == "codecarbon"
Provides-Extra: leaderboard
Requires-Dist: gradio==5.35.0; extra == "leaderboard"
Requires-Dist: plotly<6.0.0,>=5.24.0; extra == "leaderboard"
Requires-Dist: cachetools>=5.2.0; extra == "leaderboard"
Requires-Dist: matplotlib>=3.9.4; extra == "leaderboard"
Provides-Extra: peft
Requires-Dist: peft>=0.11.0; extra == "peft"
Provides-Extra: flagembedding
Requires-Dist: FlagEmbedding==1.3.4; extra == "flagembedding"
Provides-Extra: jina
Requires-Dist: einops>=0.8.0; extra == "jina"
Provides-Extra: jina-v4
Requires-Dist: peft>=0.15.2; extra == "jina-v4"
Requires-Dist: transformers>=4.52.0; extra == "jina-v4"
Requires-Dist: torchvision>=0.22.1; extra == "jina-v4"
Provides-Extra: flash-attention
Requires-Dist: flash-attn>=2.6.3; extra == "flash-attention"
Provides-Extra: openai
Requires-Dist: openai>=1.41.0; extra == "openai"
Requires-Dist: tiktoken>=0.8.0; extra == "openai"
Provides-Extra: model2vec
Requires-Dist: model2vec>=0.3.0; extra == "model2vec"
Provides-Extra: pylate
Requires-Dist: pylate>=1.3.1; python_version < "3.13" and extra == "pylate"
Provides-Extra: bm25s
Requires-Dist: bm25s>=0.2.6; extra == "bm25s"
Requires-Dist: PyStemmer>=2.2.0.3; extra == "bm25s"
Provides-Extra: gritlm
Requires-Dist: gritlm>=1.0.2; extra == "gritlm"
Provides-Extra: xformers
Requires-Dist: xformers>=0.0.29; extra == "xformers"
Provides-Extra: blip2
Requires-Dist: salesforce-lavis>=1.0.2; extra == "blip2"
Provides-Extra: voyageai
Requires-Dist: voyageai<2.0.0,>0.3.0; extra == "voyageai"
Provides-Extra: voyage-v
Requires-Dist: voyageai<2.0.0,>0.3.0; extra == "voyage-v"
Requires-Dist: tenacity>9.0.0; extra == "voyage-v"
Provides-Extra: cohere
Requires-Dist: cohere==5.14.0; extra == "cohere"
Provides-Extra: vertexai
Requires-Dist: vertexai==1.71.1; extra == "vertexai"
Provides-Extra: llm2vec
Requires-Dist: llm2vec<0.3.0,>=0.2.3; extra == "llm2vec"
Provides-Extra: timm
Requires-Dist: timm<1.1.0,>=1.0.15; extra == "timm"
Provides-Extra: open-clip-torch
Requires-Dist: open_clip_torch==2.31.0; extra == "open-clip-torch"
Provides-Extra: nomic
Requires-Dist: einops>=0.8.1; extra == "nomic"
Provides-Extra: ark
Requires-Dist: volcengine-python-sdk[ark]==3.0.2; extra == "ark"
Requires-Dist: tiktoken>=0.8.0; extra == "ark"
Provides-Extra: colpali-engine
Requires-Dist: colpali_engine>=0.3.12; extra == "colpali-engine"
Provides-Extra: xet
Requires-Dist: huggingface_hub>=0.32.0; extra == "xet"
Provides-Extra: youtu
Requires-Dist: tencentcloud-sdk-python-common>=3.0.1454; extra == "youtu"
Requires-Dist: tencentcloud-sdk-python-lkeap>=3.0.1451; extra == "youtu"
Provides-Extra: llama-embed-nemotron
Requires-Dist: transformers==4.51.0; extra == "llama-embed-nemotron"
Provides-Extra: faiss-cpu
Requires-Dist: faiss-cpu>=1.12.0; extra == "faiss-cpu"
Dynamic: license-file

<h1 align="center">
  <img src="docs/images/logos/mteb_logo/dots-icon.png" alt="MTEB" width="28" style="vertical-align: middle; margin-right: 10px;"/> MTEB
</h1>

<h3 align="center" style="border-bottom: none;">Multimodal toolbox for evaluating embeddings and retrieval systems</h3>

<p align="center">
    <a href="https://github.com/embeddings-benchmark/mteb/releases">
        <img alt="GitHub release" src="https://img.shields.io/github/release/embeddings-benchmark/mteb.svg">
    </a>
    <a href="https://github.com/embeddings-benchmark/mteb/blob/master/LICENSE">
        <img alt="License" src="https://img.shields.io/github/license/embeddings-benchmark/mteb.svg?color=green">
    </a>
    <a href="https://pepy.tech/project/mteb">
        <img alt="Downloads" src="https://static.pepy.tech/personalized-badge/mteb?period=total&units=international_system&left_color=grey&right_color=orange&left_text=Downloads">
    </a>
</p>

<h4 align="center">
    <p>
        <a href="https://embeddings-benchmark.github.io/mteb/installation/">Installation</a> |
        <a href="https://embeddings-benchmark.github.io/mteb/">Usage</a> |
        <a href="https://huggingface.co/spaces/mteb/leaderboard">Leaderboard</a> |
        <a href="https://embeddings-benchmark.github.io/mteb/">Documentation</a> |
        <a href="#citing">Citing</a>
    </p>
</h4>


<h3 align="center">
    <a href="https://huggingface.co/spaces/mteb/leaderboard"><img style="float: middle; padding: 10px 10px 10px 10px;" width="60" height="55" src="./docs/images/logos/hf_logo.png" /></a>
</h3>


## Installation

You can install mteb simply using pip. For more on installation please see the [documentation](https://embeddings-benchmark.github.io/mteb/installation/).

```bash
pip install mteb
```


## Example Usage

Below we present a simple use-case example. For more information, see the [documentation](https://embeddings-benchmark.github.io/mteb/).

```python
import mteb
from sentence_transformers import SentenceTransformer

# Select model
model_name = "sentence-transformers/all-MiniLM-L6-v2"
model = mteb.get_model(model_name) # if the model is not implemented in MTEB it will be eq. to SentenceTransformer(model_name)

# Select tasks
tasks = mteb.get_tasks(tasks=["Banking77Classification.v2"])

# evaluate
results = mteb.evaluate(model, tasks=tasks)
```

You can also run it using the CLI:

```bash
mteb run \
    -m sentence-transformers/all-MiniLM-L6-v2 \
    -t "Banking77Classification.v2" \
    --output-folder results
```

For more on how to use the CLI check out the [related documentation](https://embeddings-benchmark.github.io/mteb/usage/cli/).

## Overview

| Overview                       |                                                                                      |
|--------------------------------|--------------------------------------------------------------------------------------|
| üìà [Leaderboard]               | The interactive leaderboard of the benchmark                                         |
| **Get Started**.               |                                                                                      |
| üèÉ [Get Started]               | Overview of how to use mteb                                                          |
| ü§ñ [Defining Models]           | How to use existing model and define custom ones                                     |
| üìã [Selecting tasks]           | How to select tasks, benchmarks, splits etc.                                         |
| üè≠ [Running Evaluation]        | How to run the evaluations, including cache management, speeding up evaluations etc. |
| üìä [Loading Results]           | How to load and work with existing model results                                     |
| **Overview**.                  |                                                                                      |
| üìã [Tasks]                     | Overview of available tasks                                                          |
| üìê [Benchmarks]                | Overview of available benchmarks                                                     |
| ü§ñ [Models]                    | Overview of available Models                                                         |
| **Contributing**               |                                                                                      |
| ü§ñ [Adding a model]            | How to submit a model to MTEB and to the leaderboard                                 |
| üë©‚Äçüíª [Adding a dataset]          | How to add a new task/dataset to MTEB                                                |
| üë©‚Äçüíª [Adding a benchmark]        | How to add a new benchmark to MTEB and to the leaderboard                            |
| ü§ù [Contributing]              | How to contribute to MTEB and set it up for development                              |

[Get Started]: https://embeddings-benchmark.github.io/mteb/usage/get_started/
[Defining Models]: https://embeddings-benchmark.github.io/mteb/usage/defining_the_model/
[Selecting tasks]: https://embeddings-benchmark.github.io/mteb/usage/selecting_tasks/
[Running Evaluation]: https://embeddings-benchmark.github.io/mteb/usage/running_the_evaluation/
[Loading Results]: https://embeddings-benchmark.github.io/mteb/usage/loading_results/
[Tasks]: https://embeddings-benchmark.github.io/mteb/overview/available_tasks/any2anymultilingualretrieval/
[Benchmarks]: https://embeddings-benchmark.github.io/mteb/overview/available_benchmarks/
[Models]: https://embeddings-benchmark.github.io/mteb/overview/available_models/text/
[Contributing]: docs/CONTRIBUTING.md
[Adding a model]: docs/contributing/adding_a_model.md
[Adding a dataset]: docs/contributing/adding_a_dataset.md
[Adding a benchmark]: docs/contributing/adding_a_benchmark.md
[Leaderboard]: https://huggingface.co/spaces/mteb/leaderboard

## Citing

MTEB was introduced in "[MTEB: Massive Text Embedding Benchmark](https://arxiv.org/abs/2210.07316)", and heavily expanded in "[MMTEB: Massive Multilingual Text Embedding Benchmark](https://arxiv.org/abs/2502.13595)". When using `mteb`, we recommend that you cite both articles.

<details>
  <summary> Bibtex Citation (click to unfold) </summary>


```bibtex
@article{muennighoff2022mteb,
  author = {Muennighoff, Niklas and Tazi, Nouamane and Magne, Lo√Øc and Reimers, Nils},
  title = {MTEB: Massive Text Embedding Benchmark},
  publisher = {arXiv},
  journal={arXiv preprint arXiv:2210.07316},
  year = {2022}
  url = {https://arxiv.org/abs/2210.07316},
  doi = {10.48550/ARXIV.2210.07316},
}

@article{enevoldsen2025mmtebmassivemultilingualtext,
  title={MMTEB: Massive Multilingual Text Embedding Benchmark},
  author={Kenneth Enevoldsen and Isaac Chung and Imene Kerboua and M√°rton Kardos and Ashwin Mathur and David Stap and Jay Gala and Wissam Siblini and Dominik Krzemi≈Ñski and Genta Indra Winata and Saba Sturua and Saiteja Utpala and Mathieu Ciancone and Marion Schaeffer and Gabriel Sequeira and Diganta Misra and Shreeya Dhakal and Jonathan Rystr√∏m and Roman Solomatin and √ñmer √áaƒüatan and Akash Kundu and Martin Bernstorff and Shitao Xiao and Akshita Sukhlecha and Bhavish Pahwa and Rafa≈Ç Po≈õwiata and Kranthi Kiran GV and Shawon Ashraf and Daniel Auras and Bj√∂rn Pl√ºster and Jan Philipp Harries and Lo√Øc Magne and Isabelle Mohr and Mariya Hendriksen and Dawei Zhu and Hippolyte Gisserot-Boukhlef and Tom Aarsen and Jan Kostkan and Konrad Wojtasik and Taemin Lee and Marek ≈†uppa and Crystina Zhang and Roberta Rocca and Mohammed Hamdy and Andrianos Michail and John Yang and Manuel Faysse and Aleksei Vatolin and Nandan Thakur and Manan Dey and Dipam Vasani and Pranjal Chitale and Simone Tedeschi and Nguyen Tai and Artem Snegirev and Michael G√ºnther and Mengzhou Xia and Weijia Shi and Xing Han L√π and Jordan Clive and Gayatri Krishnakumar and Anna Maksimova and Silvan Wehrli and Maria Tikhonova and Henil Panchal and Aleksandr Abramov and Malte Ostendorff and Zheng Liu and Simon Clematide and Lester James Miranda and Alena Fenogenova and Guangyu Song and Ruqiya Bin Safi and Wen-Ding Li and Alessia Borghini and Federico Cassano and Hongjin Su and Jimmy Lin and Howard Yen and Lasse Hansen and Sara Hooker and Chenghao Xiao and Vaibhav Adlakha and Orion Weller and Siva Reddy and Niklas Muennighoff},
  publisher = {arXiv},
  journal={arXiv preprint arXiv:2502.13595},
  year={2025},
  url={https://arxiv.org/abs/2502.13595},
  doi = {10.48550/arXiv.2502.13595},
}
```
</details>


If you use any of the specific benchmarks, we also recommend that you cite the authors of both the benchmark and its tasks:

```py
benchmark = mteb.get_benchmark("MTEB(eng, v2)")
benchmark.citation # get citation for a specific benchmark

# you can also create a table of the task for the appendix using:
benchmark.tasks.to_latex()
```
