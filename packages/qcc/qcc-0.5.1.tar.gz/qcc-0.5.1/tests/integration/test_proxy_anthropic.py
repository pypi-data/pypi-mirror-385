#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯• QCC ä»£ç†æœåŠ¡å™¨çš„ Anthropic åè®®æ”¯æŒ
éªŒè¯ä»£ç†æœåŠ¡å™¨æ˜¯å¦æ­£ç¡®è½¬å‘ Anthropic /v1/messages è¯·æ±‚
"""

import httpx
import json
import sys
import io
import asyncio

if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

# æµ‹è¯•é…ç½®
PROXY_URL = "http://127.0.0.1:7860"  # QCC ä»£ç†åœ°å€
TEST_API_KEY = "sk-2EQrynW6WnwhebbW95Ym8uyiezKAETsxtAkboJHJyzH64OfD"  # ç”¨äºæµ‹è¯•çš„ API Key

def print_section(title: str):
    """æ‰“å°åˆ†éš”çº¿æ ‡é¢˜"""
    print(f"\n{'='*60}")
    print(f"  {title}")
    print(f"{'='*60}\n")

async def test_messages_endpoint():
    """æµ‹è¯• /v1/messages ç«¯ç‚¹é€šè¿‡ä»£ç†"""
    print_section("æµ‹è¯• 1: /v1/messages é€šè¿‡ä»£ç†")

    headers = {
        "x-api-key": TEST_API_KEY,
        "anthropic-version": "2023-06-01",
        "content-type": "application/json"
    }

    payload = {
        "model": "claude-3-5-haiku-20241022",
        "max_tokens": 100,
        "messages": [
            {"role": "user", "content": "ç”¨ä¸€ä¸ªè¯å›ç­”ï¼šå¤©ç©ºæ˜¯ä»€ä¹ˆé¢œè‰²ï¼Ÿ"}
        ]
    }

    try:
        async with httpx.AsyncClient(timeout=30.0) as client:
            print(f"å‘é€è¯·æ±‚åˆ°ä»£ç†: {PROXY_URL}/v1/messages")
            print(f"ä½¿ç”¨æ¨¡å‹: {payload['model']}")

            response = await client.post(
                f"{PROXY_URL}/v1/messages",
                headers=headers,
                json=payload
            )

            print(f"\nçŠ¶æ€ç : {response.status_code}")

            if response.status_code == 200:
                data = response.json()
                print("\nâœ“ æˆåŠŸï¼")
                print(f"æ¨¡å‹: {data.get('model', 'N/A')}")

                if data.get('content'):
                    content = data['content'][0].get('text', '')
                    print(f"å›å¤: {content}")

                usage = data.get('usage', {})
                print(f"Tokens: è¾“å…¥={usage.get('input_tokens')}, è¾“å‡º={usage.get('output_tokens')}")

                return True
            else:
                print(f"\nâœ— å¤±è´¥")
                print(f"å“åº”: {response.text}")
                return False

    except Exception as e:
        print(f"\nâœ— å¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()
        return False

async def test_streaming_messages():
    """æµ‹è¯•æµå¼å“åº”é€šè¿‡ä»£ç†"""
    print_section("æµ‹è¯• 2: æµå¼ /v1/messages é€šè¿‡ä»£ç†")

    headers = {
        "x-api-key": TEST_API_KEY,
        "anthropic-version": "2023-06-01",
        "content-type": "application/json"
    }

    payload = {
        "model": "claude-3-5-haiku-20241022",
        "max_tokens": 50,
        "messages": [
            {"role": "user", "content": "æ•°åˆ°5"}
        ],
        "stream": True
    }

    try:
        async with httpx.AsyncClient(timeout=30.0) as client:
            print(f"å‘é€æµå¼è¯·æ±‚åˆ°ä»£ç†: {PROXY_URL}/v1/messages")

            async with client.stream(
                "POST",
                f"{PROXY_URL}/v1/messages",
                headers=headers,
                json=payload
            ) as response:
                print(f"çŠ¶æ€ç : {response.status_code}")

                if response.status_code == 200:
                    print("\nâœ“ æµå¼æ•°æ®æ¥æ”¶ä¸­...")
                    chunk_count = 0

                    async for line in response.aiter_lines():
                        if line.strip():
                            chunk_count += 1
                            if chunk_count <= 5:  # åªæ˜¾ç¤ºå‰5ä¸ª chunk
                                print(f"  Chunk {chunk_count}: {line[:100]}")

                    print(f"\nâœ“ å…±æ¥æ”¶ {chunk_count} ä¸ªæ•°æ®å—")
                    return True
                else:
                    body = await response.aread()
                    print(f"\nâœ— å¤±è´¥: {body.decode()}")
                    return False

    except Exception as e:
        print(f"\nâœ— å¼‚å¸¸: {e}")
        return False

async def test_all_available_models():
    """æµ‹è¯•æ‰€æœ‰å¯ç”¨çš„ Claude æ¨¡å‹"""
    print_section("æµ‹è¯• 3: æµ‹è¯•å¯ç”¨çš„ Claude æ¨¡å‹")

    models_to_test = [
        "claude-3-5-haiku-20241022",
        "claude-haiku-4-5-20251001",
        # å…¶ä»–æ¨¡å‹å¯èƒ½å› è´Ÿè½½é™åˆ¶è€Œä¸å¯ç”¨
    ]

    headers = {
        "x-api-key": TEST_API_KEY,
        "anthropic-version": "2023-06-01",
        "content-type": "application/json"
    }

    results = {}

    for model in models_to_test:
        print(f"\næµ‹è¯•æ¨¡å‹: {model}")

        payload = {
            "model": model,
            "max_tokens": 10,
            "messages": [
                {"role": "user", "content": "Hi"}
            ]
        }

        try:
            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.post(
                    f"{PROXY_URL}/v1/messages",
                    headers=headers,
                    json=payload
                )

                if response.status_code == 200:
                    print(f"  âœ“ å¯ç”¨")
                    results[model] = "å¯ç”¨"
                else:
                    error_text = response.text[:100]
                    print(f"  âœ— çŠ¶æ€ç : {response.status_code}")
                    print(f"  âœ— é”™è¯¯: {error_text}")
                    results[model] = f"ä¸å¯ç”¨ ({response.status_code})"

        except Exception as e:
            print(f"  âœ— å¼‚å¸¸: {e}")
            results[model] = f"å¼‚å¸¸ ({str(e)[:50]})"

    print("\nç»“æœæ±‡æ€»:")
    for model, status in results.items():
        print(f"  {model}: {status}")

    return results

async def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("\n" + "="*60)
    print("  QCC ä»£ç†æœåŠ¡å™¨ Anthropic åè®®æµ‹è¯•")
    print(f"  ä»£ç†åœ°å€: {PROXY_URL}")
    print("="*60)

    # æ£€æŸ¥ä»£ç†æœåŠ¡å™¨æ˜¯å¦è¿è¡Œ
    try:
        async with httpx.AsyncClient(timeout=5.0) as client:
            response = await client.get(f"{PROXY_URL}/")
            print(f"\nâœ“ ä»£ç†æœåŠ¡å™¨è¿è¡Œä¸­")
    except Exception:
        print(f"\nâœ— ä»£ç†æœåŠ¡å™¨æœªè¿è¡Œï¼Œè¯·å…ˆå¯åŠ¨: uvx qcc proxy start")
        return

    # è¿è¡Œæµ‹è¯•
    test1_result = await test_messages_endpoint()
    test2_result = await test_streaming_messages()
    test3_results = await test_all_available_models()

    # æ€»ç»“
    print_section("æµ‹è¯•æ€»ç»“")

    if test1_result:
        print("âœ“ åŸºç¡€ /v1/messages è¯·æ±‚: é€šè¿‡")
    else:
        print("âœ— åŸºç¡€ /v1/messages è¯·æ±‚: å¤±è´¥")

    if test2_result:
        print("âœ“ æµå¼ /v1/messages è¯·æ±‚: é€šè¿‡")
    else:
        print("âœ— æµå¼ /v1/messages è¯·æ±‚: å¤±è´¥")

    available_models = [m for m, s in test3_results.items() if s == "å¯ç”¨"]
    print(f"\nå¯ç”¨æ¨¡å‹: {len(available_models)}/{len(test3_results)}")
    for model in available_models:
        print(f"  - {model}")

    if test1_result and test2_result and len(available_models) > 0:
        print("\nğŸ‰ æ‰€æœ‰å…³é”®æµ‹è¯•é€šè¿‡ï¼QCC ä»£ç†æœåŠ¡å™¨ Anthropic åè®®æ”¯æŒæ­£å¸¸ï¼")
    else:
        print("\nâš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®å’Œæ—¥å¿—")

if __name__ == "__main__":
    asyncio.run(main())
