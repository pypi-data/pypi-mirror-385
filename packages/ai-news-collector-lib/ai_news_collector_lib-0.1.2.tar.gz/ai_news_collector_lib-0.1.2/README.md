# AI News Collector Library

ä¸€ä¸ªç”¨äºæ”¶é›†AIç›¸å…³æ–°é—»çš„Pythonåº“ï¼Œæ”¯æŒå¤šç§æœç´¢æºå’Œé«˜çº§åŠŸèƒ½ã€‚

## ğŸš€ ç‰¹æ€§

- **å¤šæºæœç´¢**: æ”¯æŒHackerNewsã€ArXivã€DuckDuckGoã€NewsAPIç­‰
- **å†…å®¹æå–**: è‡ªåŠ¨æå–ç½‘é¡µå†…å®¹
- **å…³é”®è¯åˆ†æ**: æ™ºèƒ½æå–å…³é”®è¯
- **ç»“æœç¼“å­˜**: æ”¯æŒç»“æœç¼“å­˜ï¼Œæé«˜æ•ˆç‡
- **å®šæ—¶ä»»åŠ¡**: æ”¯æŒå®šæ—¶è‡ªåŠ¨æ”¶é›†
- **æŠ¥å‘Šç”Ÿæˆ**: ç”Ÿæˆå¤šç§æ ¼å¼çš„æŠ¥å‘Š
- **æ˜“äºé›†æˆ**: ç®€å•çš„APIæ¥å£

## ğŸ“ é¡¹ç›®ç»“æ„

```
ai_news_collector_lib/
â”œâ”€â”€ __init__.py          # ä¸»æ¨¡å—å…¥å£
â”œâ”€â”€ cli.py              # å‘½ä»¤è¡Œæ¥å£
â”œâ”€â”€ config/             # é…ç½®æ¨¡å—
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ settings.py     # æœç´¢é…ç½®
â”‚   â””â”€â”€ api_keys.py     # APIå¯†é’¥ç®¡ç†
â”œâ”€â”€ core/               # æ ¸å¿ƒåŠŸèƒ½
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ collector.py    # åŸºç¡€æ”¶é›†å™¨
â”‚   â””â”€â”€ advanced_collector.py  # é«˜çº§æ”¶é›†å™¨
â”œâ”€â”€ models/             # æ•°æ®æ¨¡å‹
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ article.py      # æ–‡ç« æ¨¡å‹
â”‚   â””â”€â”€ result.py       # ç»“æœæ¨¡å‹
â”œâ”€â”€ tools/              # æœç´¢å·¥å…·
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ search_tools.py # å„ç§æœç´¢å·¥å…·
â”œâ”€â”€ utils/              # å·¥å…·å‡½æ•°
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ cache.py        # ç¼“å­˜ç®¡ç†
â”‚   â”œâ”€â”€ content_extractor.py  # å†…å®¹æå–
â”‚   â”œâ”€â”€ keyword_extractor.py # å…³é”®è¯æå–
â”‚   â”œâ”€â”€ reporter.py     # æŠ¥å‘Šç”Ÿæˆ
â”‚   â””â”€â”€ scheduler.py    # ä»»åŠ¡è°ƒåº¦
â”œâ”€â”€ tests/              # æµ‹è¯•æ–‡ä»¶
â”œâ”€â”€ examples/           # ä½¿ç”¨ç¤ºä¾‹
â”œâ”€â”€ scripts/            # æ„å»ºè„šæœ¬
â”œâ”€â”€ setup.py           # å®‰è£…é…ç½®
â”œâ”€â”€ pyproject.toml     # é¡¹ç›®é…ç½®
â””â”€â”€ README.md          # é¡¹ç›®è¯´æ˜
```

## ğŸ“¦ å®‰è£…

### åŸºç¡€å®‰è£…

```bash
pip install ai-news-collector-lib
```

### é«˜çº§åŠŸèƒ½å®‰è£…

```bash
pip install ai-news-collector-lib[advanced]
```

### å¼€å‘å®‰è£…

```bash
git clone https://github.com/ai-news-collector/ai-news-collector-lib.git
cd ai-news-collector-lib
pip install -e .
```

## ğŸ”§ å¿«é€Ÿå¼€å§‹

### åŸºç¡€ä½¿ç”¨

```python
import asyncio
from ai_news_collector_lib import AINewsCollector, SearchConfig

# åˆ›å»ºé…ç½®
config = SearchConfig(
    enable_hackernews=True,
    enable_arxiv=True,
    enable_duckduckgo=True,
    max_articles_per_source=10
)

# åˆ›å»ºæœé›†å™¨
collector = AINewsCollector(config)

# æ”¶é›†æ–°é—»
async def main():
    result = await collector.collect_news("artificial intelligence")
    print(f"æ”¶é›†åˆ° {result.total_articles} ç¯‡æ–‡ç« ")
    return result.articles

# è¿è¡Œ
articles = asyncio.run(main())
```

### é«˜çº§ä½¿ç”¨

```python
from ai_news_collector_lib import AdvancedAINewsCollector, AdvancedSearchConfig

# åˆ›å»ºé«˜çº§é…ç½®
config = AdvancedSearchConfig(
    enable_hackernews=True,
    enable_arxiv=True,
    enable_duckduckgo=True,
    enable_content_extraction=True,
    enable_keyword_extraction=True,
    cache_results=True
)

# åˆ›å»ºé«˜çº§æœé›†å™¨
collector = AdvancedAINewsCollector(config)

# æ”¶é›†å¢å¼ºæ–°é—»
async def main():
    result = await collector.collect_news_advanced("machine learning")
    
    # åˆ†æç»“æœ
    total_words = sum(article['word_count'] for article in result['articles'])
    print(f"æ€»å­—æ•°: {total_words}")
    
    return result

# è¿è¡Œ
enhanced_result = asyncio.run(main())
```

## ğŸ“Š æ”¯æŒçš„æœç´¢æº

### å…è´¹æº

- ğŸ”¥ **HackerNews** - æŠ€æœ¯ç¤¾åŒºè®¨è®º
- ğŸ“š **ArXiv** - å­¦æœ¯è®ºæ–‡å’Œé¢„å°æœ¬
- ğŸ¦† **DuckDuckGo** - éšç§ä¿æŠ¤çš„ç½‘é¡µæœç´¢

### ä»˜è´¹æº (éœ€è¦APIå¯†é’¥)

- ğŸ“¡ **NewsAPI** - å¤šæºæ–°é—»èšåˆ
- ğŸ” **Tavily** - AIé©±åŠ¨çš„æœç´¢API
- ğŸŒ **Google Search** - Googleè‡ªå®šä¹‰æœç´¢API
- ğŸ”µ **Bing Search** - å¾®è½¯Bingæœç´¢API
- âš¡ **Serper** - å¿«é€ŸGoogleæœç´¢API
- ğŸ¦ **Brave Search** - ç‹¬ç«‹éšç§æœç´¢API
- ğŸ”¬ **MetaSota Search** - åŸºäºMCPåè®®çš„æ™ºèƒ½æœç´¢æœåŠ¡

## âš™ï¸ é…ç½®

### ç¯å¢ƒå˜é‡

```bash
# APIå¯†é’¥
NEWS_API_KEY=your_newsapi_key
TAVILY_API_KEY=your_tavily_key
GOOGLE_SEARCH_API_KEY=your_google_key
GOOGLE_SEARCH_ENGINE_ID=your_engine_id
BING_SEARCH_API_KEY=your_bing_key
SERPER_API_KEY=your_serper_key
BRAVE_SEARCH_API_KEY=your_brave_key
METASOSEARCH_API_KEY=your_metasota_key
```

### é…ç½®æ–‡ä»¶

```python
from ai_news_collector_lib import SearchConfig

config = SearchConfig(
    # ä¼ ç»Ÿæº
    enable_hackernews=True,
    enable_arxiv=True,
    enable_newsapi=False,
    enable_rss_feeds=True,
    
    # æœç´¢å¼•æ“æº
    enable_duckduckgo=True,
    enable_tavily=False,
    enable_google_search=False,
    enable_bing_search=False,
    enable_serper=False,
    enable_brave_search=False,
    enable_metasota_search=False,
    
    # æœç´¢å‚æ•°
    max_articles_per_source=10,
    days_back=7,
    similarity_threshold=0.85
)
```

## ğŸ› ï¸ é«˜çº§åŠŸèƒ½

### å®šæ—¶ä»»åŠ¡

```python
from ai_news_collector_lib import DailyScheduler

# åˆ›å»ºè°ƒåº¦å™¨
scheduler = DailyScheduler(
    collector_func=collect_news,
    schedule_time="09:00",
    timezone="Asia/Shanghai"
)

# å¯åŠ¨è°ƒåº¦å™¨
scheduler.start()
```

### ç¼“å­˜ç®¡ç†

```python
from ai_news_collector_lib import CacheManager

# åˆ›å»ºç¼“å­˜ç®¡ç†å™¨
cache = CacheManager(cache_dir="./cache", default_ttl_hours=24)

# æ£€æŸ¥ç¼“å­˜
cache_key = cache.get_cache_key("ai news", ["hackernews", "arxiv"])
cached_result = cache.get_cached_result(cache_key)

if cached_result:
    print("ä½¿ç”¨ç¼“å­˜ç»“æœ")
else:
    # æ‰§è¡Œæœç´¢å¹¶ç¼“å­˜ç»“æœ
    result = await collector.collect_news("ai news")
    cache.cache_result(cache_key, result)
```

### æŠ¥å‘Šç”Ÿæˆ

```python
from ai_news_collector_lib import ReportGenerator

# åˆ›å»ºæŠ¥å‘Šç”Ÿæˆå™¨
reporter = ReportGenerator(output_dir="./reports")

# ç”ŸæˆæŠ¥å‘Š
report = reporter.generate_daily_report(result, format="markdown")
reporter.save_report(result, filename="daily_report.md")
```

## ğŸ“ˆ ä½¿ç”¨ç¤ºä¾‹

### æ¯æ—¥æ”¶é›†è„šæœ¬

```python
#!/usr/bin/env python3
import asyncio
from ai_news_collector_lib import AdvancedAINewsCollector, AdvancedSearchConfig

async def daily_collection():
    # é…ç½®
    config = AdvancedSearchConfig(
        enable_hackernews=True,
        enable_arxiv=True,
        enable_duckduckgo=True,
        enable_content_extraction=True,
        cache_results=True
    )
    
    # åˆ›å»ºæœé›†å™¨
    collector = AdvancedAINewsCollector(config)
    
    # æ”¶é›†å¤šä¸ªä¸»é¢˜
    topics = ["artificial intelligence", "machine learning", "deep learning"]
    result = await collector.collect_multiple_topics(topics)
    
    print(f"æ”¶é›†å®Œæˆ: {result['unique_articles']} ç¯‡ç‹¬ç‰¹æ–‡ç« ")
    return result

if __name__ == "__main__":
    asyncio.run(daily_collection())
```

### Web APIé›†æˆ

```python
from fastapi import FastAPI
from ai_news_collector_lib import AINewsCollector, SearchConfig

app = FastAPI()
collector = AINewsCollector(SearchConfig())

@app.get("/ai-news")
async def get_ai_news(query: str = "artificial intelligence"):
    result = await collector.collect_news(query)
    return {
        "total": result.total_articles,
        "unique": result.unique_articles,
        "articles": [article.to_dict() for article in result.articles]
    }
```

## ğŸ§ª æµ‹è¯•

```bash
# è¿è¡Œæµ‹è¯•
pytest

# è¿è¡Œå¼‚æ­¥æµ‹è¯•
pytest -v

# è¿è¡Œç‰¹å®šæµ‹è¯•
pytest tests/test_collector.py
```

## ğŸ—“ï¸ ArXiv æ—¥æœŸè§£æä¸å›é€€

- é»˜è®¤é‡‡ç”¨ `BeautifulSoup` çš„ XML è§£æè·å– `published` å­—æ®µï¼›è‹¥è§£æå¼‚å¸¸åˆ™å›é€€åˆ° `feedparser`ã€‚
- åœ¨ `feedparser` åˆ†æ”¯ä¸­ï¼Œæ—¥æœŸå­—æ®µå¯èƒ½ä»…å­˜åœ¨å…¶ä¸€ï¼š`published_parsed` æˆ– `updated_parsed`ï¼Œä¸¤è€…ç±»å‹å‡ä¸º `time.struct_time`ã€‚
- å›é€€é¡ºåºä¸ºï¼š`published_parsed` â†’ `updated_parsed` â†’ `datetime.now()`ï¼Œä»¥å°½é‡ä¿æŒæ¡ç›®çš„æ—¶é—´æ¥è¿‘çœŸå®å‘å¸ƒæ—¶é—´ã€‚
- å°† `struct_time` è½¬æ¢ä¸º `datetime` æ—¶ä»…å–åˆ°ç§’ä½ï¼š`datetime(*entry.published_parsed[:6])` æˆ– `datetime(*entry.updated_parsed[:6])`ã€‚
- æ—¶åŒºè¯´æ˜ï¼šAtom ä¸­å°¾éƒ¨ `Z` è¡¨ç¤º UTCã€‚BS4 åˆ†æ”¯ä½¿ç”¨ `published_str.replace('Z', '+00:00')` åé€šè¿‡ `datetime.fromisoformat` è§£æï¼›`feedparser` åˆ†æ”¯ç›´æ¥ç”± `struct_time` æ„å»º `datetime`ã€‚

å®ç°èŠ‚é€‰ï¼ˆä½äº `ai_news_collector_lib/tools/search_tools.py` çš„ `ArxivTool`ï¼‰ï¼š

```python
feed = feedparser.parse(response.content)
for entry in feed.entries:
    # è¯´æ˜ï¼šfeedparser å¯èƒ½ä»…æä¾› published_parsed æˆ– updated_parsed
    # å›é€€é¡ºåºï¼špublished_parsed > updated_parsed > å½“å‰æ—¶é—´
    try:
        if hasattr(entry, 'published_parsed') and entry.published_parsed:
            published_date = datetime(*entry.published_parsed[:6])
        elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:
            published_date = datetime(*entry.updated_parsed[:6])
        else:
            published_date = datetime.now()
    except Exception:
        published_date = datetime.now()
```

æœ€å°éªŒè¯è„šæœ¬ï¼š`scripts/min_check_feedparser_fallback.py`

```bash
python scripts/min_check_feedparser_fallback.py
```

è¯¥è„šæœ¬åˆ†åˆ«æ„é€  RSS (`pubDate`) ä¸ Atom (`updated`) çš„ç¤ºä¾‹ï¼Œåœ¨ä»…å­˜åœ¨å…¶ä¸­ä¸€ä¸ªæ—¥æœŸå­—æ®µæ—¶éªŒè¯å›é€€é€»è¾‘èƒ½å¤Ÿæ­£å¸¸è¿è¡Œä¸”ä¸æŠ›å¼‚å¸¸ã€‚

## ğŸ“š æ–‡æ¡£

- [å®Œæ•´æ–‡æ¡£](https://ai-news-collector-lib.readthedocs.io/)
- [APIå‚è€ƒ](https://ai-news-collector-lib.readthedocs.io/api/)
- [ç¤ºä¾‹ä»£ç ](https://github.com/ai-news-collector/ai-news-collector-lib/tree/main/examples)

## ğŸ¤ è´¡çŒ®

æ¬¢è¿è´¡çŒ®ä»£ç ï¼è¯·æŸ¥çœ‹ [è´¡çŒ®æŒ‡å—](CONTRIBUTING.md) äº†è§£è¯¦ç»†ä¿¡æ¯ã€‚

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ã€‚æŸ¥çœ‹ [LICENSE](LICENSE) æ–‡ä»¶äº†è§£è¯¦ç»†ä¿¡æ¯ã€‚

## ğŸ†˜ æ”¯æŒ

- [é—®é¢˜æŠ¥å‘Š](https://github.com/ai-news-collector/ai-news-collector-lib/issues)
- [è®¨è®ºåŒº](https://github.com/ai-news-collector/ai-news-collector-lib/discussions)
- [é‚®ä»¶æ”¯æŒ](mailto:support@ai-news-collector.com)

## ğŸ”„ æ›´æ–°æ—¥å¿—

### v0.1.0 (2025-10-07)

- åˆå§‹é¢„å‘å¸ƒç‰ˆæœ¬
- æ”¯æŒåŸºç¡€æœç´¢åŠŸèƒ½
- æ”¯æŒå¤šç§æœç´¢æº
- æ”¯æŒé«˜çº§åŠŸèƒ½ï¼ˆå†…å®¹æå–ã€å…³é”®è¯åˆ†æã€ç¼“å­˜ç­‰ï¼‰
- âš ï¸ æ³¨æ„ï¼šè¿™æ˜¯é¢„å‘å¸ƒç‰ˆæœ¬ï¼ŒåŠŸèƒ½å¯èƒ½ä¸ç¨³å®š

---

**ç¥ä½ ä½¿ç”¨æ„‰å¿«ï¼** ğŸ‰
