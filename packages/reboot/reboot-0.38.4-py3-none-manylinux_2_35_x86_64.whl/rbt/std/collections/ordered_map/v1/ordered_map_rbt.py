# yapf: disable
# isort: skip_file
# ruff: noqa



# To not generate code where imported names might get shadowed when a user
# specifies some name in their proto file to be the same as one of our imported
# names, (for example: a request field named `uuid`) we bind all imports to
# names that are forbidden in 'proto' and therefore can never collide.

# Standard imports.
from __future__ import annotations as IMPORT_future_annotations

# The following MUST appear before the rest of the imports, since those imports
# may be invalid (broken) if the generated code is mismatched with the installed
# libraries.
import rebootdev.versioning as IMPORT_reboot_versioning
IMPORT_reboot_versioning.check_generated_code_compatible("0.38.4")

# ATTENTION: no types in this file should be imported with their unqualified
#            name (e.g. `from typing import Any`). That would cause clashes
#            with user-defined methods that have the same name. Use
#            fully-qualified names (e.g. `IMPORT_typing.Any`) instead.
import asyncio as IMPORT_asyncio
import builtins as IMPORT_builtins
import contextvars as IMPORT_contextvars
import dataclasses as IMPORT_dataclasses
import google.protobuf.descriptor as IMPORT_google_protobuf_descriptor
import google.protobuf.json_format as IMPORT_google_protobuf_json_format
import google.protobuf.message as IMPORT_google_protobuf_message
import grpc as IMPORT_grpc
import grpc_status._async as IMPORT_rpc_status_async
from grpc_status import rpc_status as IMPORT_rpc_status_sync
import json as IMPORT_json
import os as IMPORT_os
import traceback as IMPORT_traceback
import uuid as IMPORT_uuid
import pickle as IMPORT_pickle
import rebootdev as IMPORT_rebootdev
import log.log as IMPORT_log_log   # type: ignore[import]
import typing as IMPORT_typing
import rebootdev.aio.backoff as IMPORT_reboot_aio_backoff
import functools as IMPORT_functools
from abc import abstractmethod as IMPORT_abc_abstractmethod
from datetime import datetime as IMPORT_datetime_datetime
from datetime import timedelta as IMPORT_datetime_timedelta
from datetime import timezone as IMPORT_datetime_timezone
from google.protobuf import timestamp_pb2 as IMPORT_google_protobuf_timestamp_pb2
from google.protobuf import wrappers_pb2 as IMPORT_google_protobuf_wrappers_pb2
import rebootdev.aio.tracing as IMPORT_reboot_aio_tracing
from google.rpc import status_pb2 as IMPORT_google_rpc_status_pb2
from tzlocal import get_localzone as IMPORT_tzlocal_get_localzone
import rebootdev.aio.call as IMPORT_reboot_aio_call
import rebootdev.aio.contexts as IMPORT_reboot_aio_contexts
import rebootdev.aio.headers as IMPORT_reboot_aio_headers
import rebootdev.aio.idempotency as IMPORT_reboot_aio_idempotency
import rebootdev.aio.internals.channel_manager as IMPORT_reboot_aio_internals_channel_manager
import rebootdev.aio.internals.middleware as IMPORT_reboot_aio_internals_middleware
import rebootdev.aio.internals.tasks_cache as IMPORT_reboot_aio_internals_tasks_cache
import rebootdev.aio.internals.tasks_dispatcher as IMPORT_reboot_aio_internals_tasks_dispatcher
import rebootdev.aio.placement as IMPORT_reboot_aio_placement
import rebootdev.aio.servicers as IMPORT_reboot_aio_servicers
import rebootdev.aio.state_managers as IMPORT_reboot_aio_state_managers
import rebootdev.aio.stubs as IMPORT_reboot_aio_stubs
import rebootdev.aio.tasks as IMPORT_reboot_aio_tasks
import rebootdev.aio.types as IMPORT_reboot_aio_types
import rebootdev.aio.external as IMPORT_reboot_aio_external
import rebootdev.aio.workflows as IMPORT_reboot_aio_workflows
import rebootdev.settings as IMPORT_reboot_settings
import rebootdev.nodejs.python as IMPORT_reboot_nodejs_python
from rebootdev.time import DateTimeWithTimeZone as IMPORT_reboot_time_DateTimeWithTimeZone
import rbt.v1alpha1 as IMPORT_rbt_v1alpha1
import rbt.v1alpha1.nodejs_pb2 as IMPORT_rbt_v1alpha1_nodejs_pb2
import google.protobuf.any_pb2 as IMPORT_google_protobuf_any_pb2

# User defined or referenced imports.
import google.protobuf.any_pb2
import google.protobuf.descriptor_pb2
import google.protobuf.struct_pb2
import rbt.std.collections.ordered_map.v1.ordered_map_pb2
import rbt.std.collections.ordered_map.v1.ordered_map_pb2_grpc
import rbt.v1alpha1.options_pb2
# Additionally re-export all messages and enums from the pb2 module.
from rbt.std.collections.ordered_map.v1.ordered_map_pb2 import (
    Entry,
    InvalidRangeError,
    NodeCreateRequest,
    NodeCreateResponse,
    NodeEntry,
    NodeInsertRequest,
    NodeInsertResponse,
    NodeRangeRequest,
    NodeRangeResponse,
    NodeRemoveRequest,
    NodeRemoveResponse,
    NodeReverseRangeRequest,
    NodeReverseRangeResponse,
    NodeSearchRequest,
    NodeSearchResponse,
    NodeStringifyRequest,
    NodeStringifyResponse,
    OrderedMapCreateRequest,
    OrderedMapCreateResponse,
    OrderedMapInsertRequest,
    OrderedMapInsertResponse,
    OrderedMapRangeRequest,
    OrderedMapRangeResponse,
    OrderedMapRemoveRequest,
    OrderedMapRemoveResponse,
    OrderedMapReverseRangeRequest,
    OrderedMapReverseRangeResponse,
    OrderedMapSearchRequest,
    OrderedMapSearchResponse,
    OrderedMapStringifyRequest,
    OrderedMapStringifyResponse,
    Value,
)

logger = IMPORT_log_log.get_logger(__name__)

############################ Legacy gRPC Servicers ############################
# This section is relevant (only) for servicers that implement a legacy gRPC
# service in a Reboot context. It is irrelevant to clients.

def MakeLegacyGrpcServiceable(
    # A legacy gRPC servicer type can't be more specific than `type`,
    # because legacy gRPC servicers (as generated by the gRPC `protoc`
    # plugin) do not share any common base class other than `object`.
    servicer_type: type
) -> IMPORT_reboot_aio_servicers.Serviceable:
    raise ValueError(f"Unknown legacy gRPC servicer type '{servicer_type}'")



############################ Reboot Servicer Middlewares ############################
# This section is relevant (only) for servicers implementing a Reboot servicer. It
# is irrelevant to clients, except for the fact that some clients are _also_ such
# servicers.

# For internal calls, we can use a magic token to bypass token verification and
# authorization checks. The token provides no auth information (e.g.,
# `context.auth is None`).
__internal_magic_token__: str = f'internal-{IMPORT_uuid.uuid4()}'

class NodeServicerMiddleware(IMPORT_reboot_aio_internals_middleware.Middleware):

    def __init__(
        self,
        *,
        servicer: NodeBaseServicer,
        application_id: IMPORT_reboot_aio_types.ApplicationId,
        consensus_id: IMPORT_reboot_aio_types.ConsensusId,
        state_manager: IMPORT_reboot_aio_state_managers.StateManager,
        placement_client: IMPORT_reboot_aio_placement.PlacementClient,
        channel_manager: IMPORT_reboot_aio_internals_channel_manager._ChannelManager,
        tasks_cache: IMPORT_reboot_aio_internals_tasks_cache.TasksCache,
        token_verifier: IMPORT_typing.Optional[IMPORT_rebootdev.aio.auth.token_verifiers.TokenVerifier],
        effect_validation: IMPORT_reboot_aio_contexts.EffectValidation,
        app_internal_api_key_secret: str,
        ready: IMPORT_asyncio.Event,
    ):
        super().__init__(
            application_id=application_id,
            consensus_id=consensus_id,
            state_type_name = IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.Node'),
            service_names = [
                IMPORT_reboot_aio_types.ServiceName("rbt.std.collections.ordered_map.v1.NodeMethods"),
            ],
            placement_client=placement_client,
            channel_manager=channel_manager,
            effect_validation=effect_validation,
            app_internal_api_key_secret=app_internal_api_key_secret,
        )

        self._servicer = servicer
        self._state_manager = state_manager
        self.tasks_dispatcher = IMPORT_reboot_aio_internals_tasks_dispatcher.TasksDispatcher(
            application_id=application_id,
            dispatch=self.dispatch,
            tasks_cache=tasks_cache,
            ready=ready,
            complete_task=self._state_manager.complete_task,
        )

        # Store the type of each method's request so that stored requests can be
        # deserialized into the correct type.
        self.request_type_by_method_name: dict[str, type[IMPORT_google_protobuf_message.Message]] = {
            'Create': rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateRequest,
            'Search': rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeSearchRequest,
            'Insert': rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeInsertRequest,
            'Remove': rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRemoveRequest,
            'Range': rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRangeRequest,
            'ReverseRange': rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeReverseRangeRequest,
            'Stringify': rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeStringifyRequest,
        }

        # Get authorizer, if any, converting from a rule if necessary.
        def convert_authorizer_rule_if_necessary(
            authorizer_or_rule: IMPORT_typing.Optional[
                IMPORT_rebootdev.aio.auth.authorizers.Authorizer | IMPORT_rebootdev.aio.auth.authorizers.AuthorizerRule
            ]
        ) -> IMPORT_rebootdev.aio.auth.authorizers.Authorizer:

            # If no authorizer or rule is provided, return the default
            # authorizer which allows if app internal or allows if in
            # dev mode (and logs some warnings to help the user
            # realize where they are missing authorization).
            if authorizer_or_rule is None:
                return IMPORT_rebootdev.aio.auth.authorizers.DefaultAuthorizer(
                    'Node'
                )

            if isinstance(authorizer_or_rule, IMPORT_rebootdev.aio.auth.authorizers.AuthorizerRule):
                return NodeAuthorizer(
                    _default=authorizer_or_rule
                )

            return authorizer_or_rule

        self._authorizer = convert_authorizer_rule_if_necessary(
            servicer.authorizer()
        )

        # Create token verifier.
        self._token_verifier: IMPORT_typing.Optional[IMPORT_rebootdev.aio.auth.token_verifiers.TokenVerifier] = (
            servicer.token_verifier() or token_verifier
        )

        # Since users specify errors as proto messages they can't raise them
        # directly - to do so they have to use the `Aborted` wrapper, which will
        # hold the original proto message. On errors we'll need to check whether
        # such wrappers hold a proto message for a specified error, so we can
        # avoid retrying tasks that complete with a specified error.
        self._specified_errors_by_service_method_name: dict[str, list[str]] = {
            'rbt.std.collections.ordered_map.v1.NodeMethods.Create': [
                'rbt.v1alpha1.errors_pb2.StateAlreadyConstructed',
            ],
            'rbt.std.collections.ordered_map.v1.NodeMethods.Range': [
                'rbt.std.collections.ordered_map.v1.ordered_map_pb2.InvalidRangeError',
            ],
            'rbt.std.collections.ordered_map.v1.NodeMethods.ReverseRange': [
                'rbt.std.collections.ordered_map.v1.ordered_map_pb2.InvalidRangeError',
            ],
        }


    def add_to_server(self, server: IMPORT_grpc.aio.Server) -> None:
        rbt.std.collections.ordered_map.v1.ordered_map_pb2_grpc.add_NodeMethodsServicer_to_server(
            self, server
        )

    async def inspect(self, state_ref: IMPORT_reboot_aio_types.StateRef) -> IMPORT_typing.AsyncIterator[IMPORT_google_protobuf_message.Message]:
        """Implementation of `Middleware.inspect()`."""
        context = self.create_context(
            headers=IMPORT_reboot_aio_headers.Headers(
                application_id=self.application_id,
                state_ref=state_ref,
            ),
            state_type_name = IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.Node'),
            method="inspect",
            context_type=IMPORT_reboot_aio_contexts.ReaderContext,
        )

        async with self._state_manager.streaming_reader_idempotency_key(
            context,
            self._servicer.__state_type__,
            authorize=None,
        ) as states:
            async for (state, idempotency_key) in states:
                yield state

    async def react_query(
        self,
        headers: IMPORT_reboot_aio_headers.Headers,
        method: str,
        request_bytes: bytes,
    ) -> IMPORT_typing.AsyncIterator[tuple[IMPORT_typing.Optional[IMPORT_google_protobuf_message.Message], list[IMPORT_uuid.UUID]]]:
        """Returns the response of calling 'method' given a message
        deserialized from the provided 'request_bytes' for each state
        update that creates a different response.

        # The caller (react.py) should have already ensured that this consensus
        # is authoritative for this traffic.
        assert self.placement_client.consensus_for_actor(
            headers.application_id,
            headers.state_ref,
        ) == self._consensus_id

        NOTE: only unary reader methods are supported."""
        # Need to define these up here since we can only do that once.
        last_response: IMPORT_typing.Optional[IMPORT_google_protobuf_message.Message] = None
        aggregated_idempotency_keys: list[IMPORT_uuid.UUID] = []
        if method == 'Create':
            # Invariant here is that users should not have called this
            # directly but only through code generated React
            # components which should not have been generated except
            # for valid method candidates.
            logger.warning(
                "Got a React query request with an invalid method name: "
                f"Method '{method}' is invalid for servicer Node."
                "\n"
                "Do you have a browser tab open for an older version "
                "of this application, or for a different application all together?"
            )
            raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                IMPORT_rbt_v1alpha1.errors_pb2.InvalidMethod(),
                message=
                    f"Method '{method}' is invalid"
            )
            yield  # Necessary for type checking.
        elif method == 'Search':

            context = self.create_context(
                headers=headers,
                state_type_name = IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.Node'),
                method='Search',
                context_type=IMPORT_reboot_aio_contexts.ReaderContext,
            )

            with IMPORT_reboot_aio_tracing.span(
                state_name=f"{context.state_type_name}('{context.state_id}')",
                span_name="Search() (reactively)",
                # The naming above matches Python, but not TypeScript.
                python_specific=True,
                level=IMPORT_reboot_aio_tracing.TraceLevel.CUSTOMER,
            ):
                context.auth = await self._maybe_verify_token(
                    headers=headers, method='Search'
                )

                request = rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeSearchRequest()
                request.ParseFromString(request_bytes)

                async with self._state_manager.reactively(
                    context,
                    self._servicer.__state_type__,
                    authorize=self._maybe_authorize(
                        method_name='rbt.std.collections.ordered_map.v1.NodeMethods.Search',
                        headers=headers,
                        auth=context.auth,
                        request=request,
                    ),
                ) as states:
                    async for (state, idempotency_keys) in states:

                        aggregated_idempotency_keys.extend(idempotency_keys)

                        # Note: This does not do any defensive copying currently:
                        # see https://github.com/reboot-dev/respect/issues/2636.
                        @IMPORT_reboot_aio_internals_middleware.maybe_run_function_twice_to_validate_effects
                        async def run__Search(validating_effects: bool) -> IMPORT_google_protobuf_message.Message:
                            return await self.__Search(
                                context,
                                state,
                                request,
                                validating_effects=validating_effects,
                            )

                        response = await run__Search()

                        if last_response != response:
                            yield (response, aggregated_idempotency_keys)
                            last_response = response
                        else:
                            yield (None, aggregated_idempotency_keys)

                        aggregated_idempotency_keys.clear()
        elif method == 'Insert':
            # Invariant here is that users should not have called this
            # directly but only through code generated React
            # components which should not have been generated except
            # for valid method candidates.
            logger.warning(
                "Got a React query request with an invalid method name: "
                f"Method '{method}' is invalid for servicer Node."
                "\n"
                "Do you have a browser tab open for an older version "
                "of this application, or for a different application all together?"
            )
            raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                IMPORT_rbt_v1alpha1.errors_pb2.InvalidMethod(),
                message=
                    f"Method '{method}' is invalid"
            )
            yield  # Necessary for type checking.
        elif method == 'Remove':
            # Invariant here is that users should not have called this
            # directly but only through code generated React
            # components which should not have been generated except
            # for valid method candidates.
            logger.warning(
                "Got a React query request with an invalid method name: "
                f"Method '{method}' is invalid for servicer Node."
                "\n"
                "Do you have a browser tab open for an older version "
                "of this application, or for a different application all together?"
            )
            raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                IMPORT_rbt_v1alpha1.errors_pb2.InvalidMethod(),
                message=
                    f"Method '{method}' is invalid"
            )
            yield  # Necessary for type checking.
        elif method == 'Range':

            context = self.create_context(
                headers=headers,
                state_type_name = IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.Node'),
                method='Range',
                context_type=IMPORT_reboot_aio_contexts.ReaderContext,
            )

            with IMPORT_reboot_aio_tracing.span(
                state_name=f"{context.state_type_name}('{context.state_id}')",
                span_name="Range() (reactively)",
                # The naming above matches Python, but not TypeScript.
                python_specific=True,
                level=IMPORT_reboot_aio_tracing.TraceLevel.CUSTOMER,
            ):
                context.auth = await self._maybe_verify_token(
                    headers=headers, method='Range'
                )

                request = rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRangeRequest()
                request.ParseFromString(request_bytes)

                async with self._state_manager.reactively(
                    context,
                    self._servicer.__state_type__,
                    authorize=self._maybe_authorize(
                        method_name='rbt.std.collections.ordered_map.v1.NodeMethods.Range',
                        headers=headers,
                        auth=context.auth,
                        request=request,
                    ),
                ) as states:
                    async for (state, idempotency_keys) in states:

                        aggregated_idempotency_keys.extend(idempotency_keys)

                        # Note: This does not do any defensive copying currently:
                        # see https://github.com/reboot-dev/respect/issues/2636.
                        @IMPORT_reboot_aio_internals_middleware.maybe_run_function_twice_to_validate_effects
                        async def run__Range(validating_effects: bool) -> IMPORT_google_protobuf_message.Message:
                            return await self.__Range(
                                context,
                                state,
                                request,
                                validating_effects=validating_effects,
                            )

                        response = await run__Range()

                        if last_response != response:
                            yield (response, aggregated_idempotency_keys)
                            last_response = response
                        else:
                            yield (None, aggregated_idempotency_keys)

                        aggregated_idempotency_keys.clear()
        elif method == 'ReverseRange':

            context = self.create_context(
                headers=headers,
                state_type_name = IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.Node'),
                method='ReverseRange',
                context_type=IMPORT_reboot_aio_contexts.ReaderContext,
            )

            with IMPORT_reboot_aio_tracing.span(
                state_name=f"{context.state_type_name}('{context.state_id}')",
                span_name="ReverseRange() (reactively)",
                # The naming above matches Python, but not TypeScript.
                python_specific=True,
                level=IMPORT_reboot_aio_tracing.TraceLevel.CUSTOMER,
            ):
                context.auth = await self._maybe_verify_token(
                    headers=headers, method='ReverseRange'
                )

                request = rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeReverseRangeRequest()
                request.ParseFromString(request_bytes)

                async with self._state_manager.reactively(
                    context,
                    self._servicer.__state_type__,
                    authorize=self._maybe_authorize(
                        method_name='rbt.std.collections.ordered_map.v1.NodeMethods.ReverseRange',
                        headers=headers,
                        auth=context.auth,
                        request=request,
                    ),
                ) as states:
                    async for (state, idempotency_keys) in states:

                        aggregated_idempotency_keys.extend(idempotency_keys)

                        # Note: This does not do any defensive copying currently:
                        # see https://github.com/reboot-dev/respect/issues/2636.
                        @IMPORT_reboot_aio_internals_middleware.maybe_run_function_twice_to_validate_effects
                        async def run__ReverseRange(validating_effects: bool) -> IMPORT_google_protobuf_message.Message:
                            return await self.__ReverseRange(
                                context,
                                state,
                                request,
                                validating_effects=validating_effects,
                            )

                        response = await run__ReverseRange()

                        if last_response != response:
                            yield (response, aggregated_idempotency_keys)
                            last_response = response
                        else:
                            yield (None, aggregated_idempotency_keys)

                        aggregated_idempotency_keys.clear()
        elif method == 'Stringify':

            context = self.create_context(
                headers=headers,
                state_type_name = IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.Node'),
                method='Stringify',
                context_type=IMPORT_reboot_aio_contexts.ReaderContext,
            )

            with IMPORT_reboot_aio_tracing.span(
                state_name=f"{context.state_type_name}('{context.state_id}')",
                span_name="Stringify() (reactively)",
                # The naming above matches Python, but not TypeScript.
                python_specific=True,
                level=IMPORT_reboot_aio_tracing.TraceLevel.CUSTOMER,
            ):
                context.auth = await self._maybe_verify_token(
                    headers=headers, method='Stringify'
                )

                request = rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeStringifyRequest()
                request.ParseFromString(request_bytes)

                async with self._state_manager.reactively(
                    context,
                    self._servicer.__state_type__,
                    authorize=self._maybe_authorize(
                        method_name='rbt.std.collections.ordered_map.v1.NodeMethods.Stringify',
                        headers=headers,
                        auth=context.auth,
                        request=request,
                    ),
                ) as states:
                    async for (state, idempotency_keys) in states:

                        aggregated_idempotency_keys.extend(idempotency_keys)

                        # Note: This does not do any defensive copying currently:
                        # see https://github.com/reboot-dev/respect/issues/2636.
                        @IMPORT_reboot_aio_internals_middleware.maybe_run_function_twice_to_validate_effects
                        async def run__Stringify(validating_effects: bool) -> IMPORT_google_protobuf_message.Message:
                            return await self.__Stringify(
                                context,
                                state,
                                request,
                                validating_effects=validating_effects,
                            )

                        response = await run__Stringify()

                        if last_response != response:
                            yield (response, aggregated_idempotency_keys)
                            last_response = response
                        else:
                            yield (None, aggregated_idempotency_keys)

                        aggregated_idempotency_keys.clear()
        else:
            logger.warning(
                "Got a React query request with an invalid method name: "
                f"Method '{method}' is invalid for servicer Node."
                "\n"
                "Do you have a browser tab open for an older version "
                "of this application, or for a different application all together?"
            )
            raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                IMPORT_rbt_v1alpha1.errors_pb2.InvalidMethod(),
                message=
                    f"Method '{method}' not found"
            )
            yield  # Unreachable but necessary for mypy.

    async def react_mutate(
        self,
        headers: IMPORT_reboot_aio_headers.Headers,
        method: str,
        request_bytes: bytes,
    ) -> IMPORT_google_protobuf_message.Message:
        """Returns the response of calling 'method' given a message
        deserialized from the provided 'request_bytes'."""
        if method == 'Create':
            request = rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateRequest()
            request.ParseFromString(request_bytes)

            # NOTE: we automatically retry mutations that come through
            # React when we get a `IMPORT_grpc.StatusCode.UNAVAILABLE` to
            # match the retry logic we do in the React code generated
            # to handle lack/loss of connectivity.
            #
            # TODO(benh): revisit this decision if we ever see reason
            # to call `react_mutate()` from any place other than where
            # we're executing React (e.g., browser, next.js server
            # component, etc).
            call_backoff = IMPORT_reboot_aio_backoff.Backoff()
            while True:
                # We make a full-fledged gRPC call, so that if this traffic
                # was misrouted (i.e. this consensus is not authoritative
                # for the state), it will now go to the right place. The
                # receiving middleware will handle things like effect
                # validation and so forth.
                assert headers.application_id is not None  # Guaranteed by `Headers`.
                stub = rbt.std.collections.ordered_map.v1.ordered_map_pb2_grpc.NodeMethodsStub(
                    self.channel_manager.get_channel_to(
                        self.placement_client.address_for_actor(
                            headers.application_id,
                            headers.state_ref,
                        )
                    )
                )
                call = stub.Create(
                    request=request,
                    metadata=headers.to_grpc_metadata(),
                )
                try:
                    return await call
                except IMPORT_grpc.aio.AioRpcError as error:
                    if error.code() == IMPORT_grpc.StatusCode.UNAVAILABLE:
                        await call_backoff()
                        continue

                    # Reconstitute the error that the server threw, if it was a declared error.
                    status = await IMPORT_rpc_status_async.from_call(call)
                    if status is not None:
                        raise Node.CreateAborted.from_status(
                            status
                        ) from None
                    raise Node.CreateAborted.from_grpc_aio_rpc_error(
                        error
                     ) from None

        elif method == 'Search':
            # Invariant here is that users should not have called this
            # directly but only through code generated React
            # components which should not have been generated except
            # for valid method candidates.
            logger.warning(
                "Got a react mutate request with an invalid method name: "
                "Method 'Search' is invalid for servicer Node."
                "\n"
                "Do you have an old browser tab still open for an older version "
                "of this application, or a different application all together?"
            )
            raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                IMPORT_rbt_v1alpha1.errors_pb2.InvalidMethod(),
                message=f"Method '{method}' is invalid"
            )
        elif method == 'Insert':
            request = rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeInsertRequest()
            request.ParseFromString(request_bytes)

            # NOTE: we automatically retry mutations that come through
            # React when we get a `IMPORT_grpc.StatusCode.UNAVAILABLE` to
            # match the retry logic we do in the React code generated
            # to handle lack/loss of connectivity.
            #
            # TODO(benh): revisit this decision if we ever see reason
            # to call `react_mutate()` from any place other than where
            # we're executing React (e.g., browser, next.js server
            # component, etc).
            call_backoff = IMPORT_reboot_aio_backoff.Backoff()
            while True:
                # We make a full-fledged gRPC call, so that if this traffic
                # was misrouted (i.e. this consensus is not authoritative
                # for the state), it will now go to the right place. The
                # receiving middleware will handle things like effect
                # validation and so forth.
                assert headers.application_id is not None  # Guaranteed by `Headers`.
                stub = rbt.std.collections.ordered_map.v1.ordered_map_pb2_grpc.NodeMethodsStub(
                    self.channel_manager.get_channel_to(
                        self.placement_client.address_for_actor(
                            headers.application_id,
                            headers.state_ref,
                        )
                    )
                )
                call = stub.Insert(
                    request=request,
                    metadata=headers.to_grpc_metadata(),
                )
                try:
                    return await call
                except IMPORT_grpc.aio.AioRpcError as error:
                    if error.code() == IMPORT_grpc.StatusCode.UNAVAILABLE:
                        await call_backoff()
                        continue

                    # Reconstitute the error that the server threw, if it was a declared error.
                    status = await IMPORT_rpc_status_async.from_call(call)
                    if status is not None:
                        raise Node.InsertAborted.from_status(
                            status
                        ) from None
                    raise Node.InsertAborted.from_grpc_aio_rpc_error(
                        error
                     ) from None

        elif method == 'Remove':
            request = rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRemoveRequest()
            request.ParseFromString(request_bytes)

            # NOTE: we automatically retry mutations that come through
            # React when we get a `IMPORT_grpc.StatusCode.UNAVAILABLE` to
            # match the retry logic we do in the React code generated
            # to handle lack/loss of connectivity.
            #
            # TODO(benh): revisit this decision if we ever see reason
            # to call `react_mutate()` from any place other than where
            # we're executing React (e.g., browser, next.js server
            # component, etc).
            call_backoff = IMPORT_reboot_aio_backoff.Backoff()
            while True:
                # We make a full-fledged gRPC call, so that if this traffic
                # was misrouted (i.e. this consensus is not authoritative
                # for the state), it will now go to the right place. The
                # receiving middleware will handle things like effect
                # validation and so forth.
                assert headers.application_id is not None  # Guaranteed by `Headers`.
                stub = rbt.std.collections.ordered_map.v1.ordered_map_pb2_grpc.NodeMethodsStub(
                    self.channel_manager.get_channel_to(
                        self.placement_client.address_for_actor(
                            headers.application_id,
                            headers.state_ref,
                        )
                    )
                )
                call = stub.Remove(
                    request=request,
                    metadata=headers.to_grpc_metadata(),
                )
                try:
                    return await call
                except IMPORT_grpc.aio.AioRpcError as error:
                    if error.code() == IMPORT_grpc.StatusCode.UNAVAILABLE:
                        await call_backoff()
                        continue

                    # Reconstitute the error that the server threw, if it was a declared error.
                    status = await IMPORT_rpc_status_async.from_call(call)
                    if status is not None:
                        raise Node.RemoveAborted.from_status(
                            status
                        ) from None
                    raise Node.RemoveAborted.from_grpc_aio_rpc_error(
                        error
                     ) from None

        elif method == 'Range':
            # Invariant here is that users should not have called this
            # directly but only through code generated React
            # components which should not have been generated except
            # for valid method candidates.
            logger.warning(
                "Got a react mutate request with an invalid method name: "
                "Method 'Range' is invalid for servicer Node."
                "\n"
                "Do you have an old browser tab still open for an older version "
                "of this application, or a different application all together?"
            )
            raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                IMPORT_rbt_v1alpha1.errors_pb2.InvalidMethod(),
                message=f"Method '{method}' is invalid"
            )
        elif method == 'ReverseRange':
            # Invariant here is that users should not have called this
            # directly but only through code generated React
            # components which should not have been generated except
            # for valid method candidates.
            logger.warning(
                "Got a react mutate request with an invalid method name: "
                "Method 'ReverseRange' is invalid for servicer Node."
                "\n"
                "Do you have an old browser tab still open for an older version "
                "of this application, or a different application all together?"
            )
            raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                IMPORT_rbt_v1alpha1.errors_pb2.InvalidMethod(),
                message=f"Method '{method}' is invalid"
            )
        elif method == 'Stringify':
            # Invariant here is that users should not have called this
            # directly but only through code generated React
            # components which should not have been generated except
            # for valid method candidates.
            logger.warning(
                "Got a react mutate request with an invalid method name: "
                "Method 'Stringify' is invalid for servicer Node."
                "\n"
                "Do you have an old browser tab still open for an older version "
                "of this application, or a different application all together?"
            )
            raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                IMPORT_rbt_v1alpha1.errors_pb2.InvalidMethod(),
                message=f"Method '{method}' is invalid"
            )
        else:
            logger.warning(
                "Got a react mutate request with an invalid method name: "
                f"Method '{method}' is invalid for servicer Node."
                "\n"
                "Do you have an old browser tab still open for an older version "
                "of this application, or a different application all together?"
            )
            raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                IMPORT_rbt_v1alpha1.errors_pb2.InvalidMethod(),
                message=
                    f"Method '{method}' not found"
            )

    async def dispatch(
        self,
        task: IMPORT_reboot_aio_tasks.TaskEffect,
        *,
        only_validate: bool = False,
        on_loop_iteration: IMPORT_reboot_aio_internals_tasks_dispatcher.OnLoopIterationCallable = (lambda iteration, next_iteration_schedule: None),
    ) -> IMPORT_reboot_aio_internals_tasks_dispatcher.TaskResponseOrError:
        """Dispatches the tasks to execute unless 'only_validate' is set to
        true, in which case just ensures that the task actually exists.
        Note that this function will be called *by* tasks_dispatcher; it will
        not itself call into tasks_dispatcher."""

        if 'Create' == task.method_name:
            if only_validate:
                # TODO(benh): validate 'task.request' is correct type.
                return rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateResponse()

            # Use an inline method to create a new scope, so that we can use
            # variable names like `context` and `effects` in multiple branches
            # in this code (notably when there are multiple task types) without
            # hitting a mypy error that the variable's type is not consistent.
            async def run_Create(
                context: IMPORT_reboot_aio_contexts.WorkflowContext,
                *,
                validating_effects: bool = False,
            ):
                async with self._state_manager.task_workflow(
                    context,
                    task,
                    on_loop_iteration=on_loop_iteration,
                    validating_effects=validating_effects,
                ) as complete:
                    try:
                        response = await (NodeWorkflowStub(
                            context=context,
                            state_ref=context._state_ref,
                        ).Create(
                            IMPORT_typing.cast(rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateRequest, task.request),
                            bearer_token=__internal_magic_token__,
                            idempotency=IMPORT_reboot_aio_idempotency.Idempotency(
                                alias=f'Task {IMPORT_uuid.UUID(bytes=task.task_id.task_uuid)}',
                            ),
                        ))
                        await complete(task, (response, None))
                        return (response, None)
                    except IMPORT_asyncio.CancelledError:
                        # Check if the task was cancelled by a TasksServicer.
                        if self.tasks_dispatcher.is_task_cancelled(task.task_id.task_uuid):
                            # The running task was cancelled by a TasksServicer.
                            await complete(task, (None, IMPORT_rbt_v1alpha1.tasks_pb2.TaskCancelledError()))
                            return (None, IMPORT_rbt_v1alpha1.tasks_pb2.TaskCancelledError())
                        else:
                            raise
                    except IMPORT_rebootdev.aio.aborted.Aborted as aborted:
                        error_type = f'{aborted.error.__class__.__module__}.{aborted.error.__class__.__qualname__}'
                        # Do not retry a task if the error was specified in the
                        # proto file.
                        if error_type in self._specified_errors_by_service_method_name.get('rbt.std.collections.ordered_map.v1.NodeMethods.Create', []):
                            await complete(task, (None, aborted.error))
                            return (None, aborted.error)
                        raise


            return await run_Create(
                self.create_context(
                    headers=IMPORT_reboot_aio_headers.Headers(
                        application_id=self.application_id,
                        state_ref=IMPORT_reboot_aio_types.StateRef(task.task_id.state_ref),
                    ),
                    state_type_name = IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.Node'),
                    method='Create',
                    context_type=IMPORT_reboot_aio_contexts.WorkflowContext,
                    task=task,
                )
            )
        elif 'Search' == task.method_name:
            if only_validate:
                # TODO(benh): validate 'task.request' is correct type.
                return rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeSearchResponse()

            # Use an inline method to create a new scope, so that we can use
            # variable names like `context` and `effects` in multiple branches
            # in this code (notably when there are multiple task types) without
            # hitting a mypy error that the variable's type is not consistent.
            async def run_Search(
                context: IMPORT_reboot_aio_contexts.WorkflowContext,
                *,
                validating_effects: bool = False,
            ):
                async with self._state_manager.task_workflow(
                    context,
                    task,
                    on_loop_iteration=on_loop_iteration,
                    validating_effects=validating_effects,
                ) as complete:
                    try:
                        response = await (NodeWorkflowStub(
                            context=context,
                            state_ref=context._state_ref,
                        ).Search(
                            IMPORT_typing.cast(rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeSearchRequest, task.request),
                            bearer_token=__internal_magic_token__,
                        ))
                        await complete(task, (response, None))
                        return (response, None)
                    except IMPORT_asyncio.CancelledError:
                        # Check if the task was cancelled by a TasksServicer.
                        if self.tasks_dispatcher.is_task_cancelled(task.task_id.task_uuid):
                            # The running task was cancelled by a TasksServicer.
                            await complete(task, (None, IMPORT_rbt_v1alpha1.tasks_pb2.TaskCancelledError()))
                            return (None, IMPORT_rbt_v1alpha1.tasks_pb2.TaskCancelledError())
                        else:
                            raise
                    except IMPORT_rebootdev.aio.aborted.Aborted as aborted:
                        error_type = f'{aborted.error.__class__.__module__}.{aborted.error.__class__.__qualname__}'
                        # Do not retry a task if the error was specified in the
                        # proto file.
                        if error_type in self._specified_errors_by_service_method_name.get('rbt.std.collections.ordered_map.v1.NodeMethods.Search', []):
                            await complete(task, (None, aborted.error))
                            return (None, aborted.error)
                        raise


            return await run_Search(
                self.create_context(
                    headers=IMPORT_reboot_aio_headers.Headers(
                        application_id=self.application_id,
                        state_ref=IMPORT_reboot_aio_types.StateRef(task.task_id.state_ref),
                    ),
                    state_type_name = IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.Node'),
                    method='Search',
                    context_type=IMPORT_reboot_aio_contexts.WorkflowContext,
                    task=task,
                )
            )
        elif 'Insert' == task.method_name:
            if only_validate:
                # TODO(benh): validate 'task.request' is correct type.
                return rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeInsertResponse()

            # Use an inline method to create a new scope, so that we can use
            # variable names like `context` and `effects` in multiple branches
            # in this code (notably when there are multiple task types) without
            # hitting a mypy error that the variable's type is not consistent.
            async def run_Insert(
                context: IMPORT_reboot_aio_contexts.WorkflowContext,
                *,
                validating_effects: bool = False,
            ):
                async with self._state_manager.task_workflow(
                    context,
                    task,
                    on_loop_iteration=on_loop_iteration,
                    validating_effects=validating_effects,
                ) as complete:
                    try:
                        response = await (NodeWorkflowStub(
                            context=context,
                            state_ref=context._state_ref,
                        ).Insert(
                            IMPORT_typing.cast(rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeInsertRequest, task.request),
                            bearer_token=__internal_magic_token__,
                            idempotency=IMPORT_reboot_aio_idempotency.Idempotency(
                                alias=f'Task {IMPORT_uuid.UUID(bytes=task.task_id.task_uuid)}',
                            ),
                        ))
                        await complete(task, (response, None))
                        return (response, None)
                    except IMPORT_asyncio.CancelledError:
                        # Check if the task was cancelled by a TasksServicer.
                        if self.tasks_dispatcher.is_task_cancelled(task.task_id.task_uuid):
                            # The running task was cancelled by a TasksServicer.
                            await complete(task, (None, IMPORT_rbt_v1alpha1.tasks_pb2.TaskCancelledError()))
                            return (None, IMPORT_rbt_v1alpha1.tasks_pb2.TaskCancelledError())
                        else:
                            raise
                    except IMPORT_rebootdev.aio.aborted.Aborted as aborted:
                        error_type = f'{aborted.error.__class__.__module__}.{aborted.error.__class__.__qualname__}'
                        # Do not retry a task if the error was specified in the
                        # proto file.
                        if error_type in self._specified_errors_by_service_method_name.get('rbt.std.collections.ordered_map.v1.NodeMethods.Insert', []):
                            await complete(task, (None, aborted.error))
                            return (None, aborted.error)
                        raise


            return await run_Insert(
                self.create_context(
                    headers=IMPORT_reboot_aio_headers.Headers(
                        application_id=self.application_id,
                        state_ref=IMPORT_reboot_aio_types.StateRef(task.task_id.state_ref),
                    ),
                    state_type_name = IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.Node'),
                    method='Insert',
                    context_type=IMPORT_reboot_aio_contexts.WorkflowContext,
                    task=task,
                )
            )
        elif 'Remove' == task.method_name:
            if only_validate:
                # TODO(benh): validate 'task.request' is correct type.
                return rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRemoveResponse()

            # Use an inline method to create a new scope, so that we can use
            # variable names like `context` and `effects` in multiple branches
            # in this code (notably when there are multiple task types) without
            # hitting a mypy error that the variable's type is not consistent.
            async def run_Remove(
                context: IMPORT_reboot_aio_contexts.WorkflowContext,
                *,
                validating_effects: bool = False,
            ):
                async with self._state_manager.task_workflow(
                    context,
                    task,
                    on_loop_iteration=on_loop_iteration,
                    validating_effects=validating_effects,
                ) as complete:
                    try:
                        response = await (NodeWorkflowStub(
                            context=context,
                            state_ref=context._state_ref,
                        ).Remove(
                            IMPORT_typing.cast(rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRemoveRequest, task.request),
                            bearer_token=__internal_magic_token__,
                            idempotency=IMPORT_reboot_aio_idempotency.Idempotency(
                                alias=f'Task {IMPORT_uuid.UUID(bytes=task.task_id.task_uuid)}',
                            ),
                        ))
                        await complete(task, (response, None))
                        return (response, None)
                    except IMPORT_asyncio.CancelledError:
                        # Check if the task was cancelled by a TasksServicer.
                        if self.tasks_dispatcher.is_task_cancelled(task.task_id.task_uuid):
                            # The running task was cancelled by a TasksServicer.
                            await complete(task, (None, IMPORT_rbt_v1alpha1.tasks_pb2.TaskCancelledError()))
                            return (None, IMPORT_rbt_v1alpha1.tasks_pb2.TaskCancelledError())
                        else:
                            raise
                    except IMPORT_rebootdev.aio.aborted.Aborted as aborted:
                        error_type = f'{aborted.error.__class__.__module__}.{aborted.error.__class__.__qualname__}'
                        # Do not retry a task if the error was specified in the
                        # proto file.
                        if error_type in self._specified_errors_by_service_method_name.get('rbt.std.collections.ordered_map.v1.NodeMethods.Remove', []):
                            await complete(task, (None, aborted.error))
                            return (None, aborted.error)
                        raise


            return await run_Remove(
                self.create_context(
                    headers=IMPORT_reboot_aio_headers.Headers(
                        application_id=self.application_id,
                        state_ref=IMPORT_reboot_aio_types.StateRef(task.task_id.state_ref),
                    ),
                    state_type_name = IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.Node'),
                    method='Remove',
                    context_type=IMPORT_reboot_aio_contexts.WorkflowContext,
                    task=task,
                )
            )
        elif 'Range' == task.method_name:
            if only_validate:
                # TODO(benh): validate 'task.request' is correct type.
                return rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRangeResponse()

            # Use an inline method to create a new scope, so that we can use
            # variable names like `context` and `effects` in multiple branches
            # in this code (notably when there are multiple task types) without
            # hitting a mypy error that the variable's type is not consistent.
            async def run_Range(
                context: IMPORT_reboot_aio_contexts.WorkflowContext,
                *,
                validating_effects: bool = False,
            ):
                async with self._state_manager.task_workflow(
                    context,
                    task,
                    on_loop_iteration=on_loop_iteration,
                    validating_effects=validating_effects,
                ) as complete:
                    try:
                        response = await (NodeWorkflowStub(
                            context=context,
                            state_ref=context._state_ref,
                        ).Range(
                            IMPORT_typing.cast(rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRangeRequest, task.request),
                            bearer_token=__internal_magic_token__,
                        ))
                        await complete(task, (response, None))
                        return (response, None)
                    except IMPORT_asyncio.CancelledError:
                        # Check if the task was cancelled by a TasksServicer.
                        if self.tasks_dispatcher.is_task_cancelled(task.task_id.task_uuid):
                            # The running task was cancelled by a TasksServicer.
                            await complete(task, (None, IMPORT_rbt_v1alpha1.tasks_pb2.TaskCancelledError()))
                            return (None, IMPORT_rbt_v1alpha1.tasks_pb2.TaskCancelledError())
                        else:
                            raise
                    except IMPORT_rebootdev.aio.aborted.Aborted as aborted:
                        error_type = f'{aborted.error.__class__.__module__}.{aborted.error.__class__.__qualname__}'
                        # Do not retry a task if the error was specified in the
                        # proto file.
                        if error_type in self._specified_errors_by_service_method_name.get('rbt.std.collections.ordered_map.v1.NodeMethods.Range', []):
                            await complete(task, (None, aborted.error))
                            return (None, aborted.error)
                        raise


            return await run_Range(
                self.create_context(
                    headers=IMPORT_reboot_aio_headers.Headers(
                        application_id=self.application_id,
                        state_ref=IMPORT_reboot_aio_types.StateRef(task.task_id.state_ref),
                    ),
                    state_type_name = IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.Node'),
                    method='Range',
                    context_type=IMPORT_reboot_aio_contexts.WorkflowContext,
                    task=task,
                )
            )
        elif 'ReverseRange' == task.method_name:
            if only_validate:
                # TODO(benh): validate 'task.request' is correct type.
                return rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeReverseRangeResponse()

            # Use an inline method to create a new scope, so that we can use
            # variable names like `context` and `effects` in multiple branches
            # in this code (notably when there are multiple task types) without
            # hitting a mypy error that the variable's type is not consistent.
            async def run_ReverseRange(
                context: IMPORT_reboot_aio_contexts.WorkflowContext,
                *,
                validating_effects: bool = False,
            ):
                async with self._state_manager.task_workflow(
                    context,
                    task,
                    on_loop_iteration=on_loop_iteration,
                    validating_effects=validating_effects,
                ) as complete:
                    try:
                        response = await (NodeWorkflowStub(
                            context=context,
                            state_ref=context._state_ref,
                        ).ReverseRange(
                            IMPORT_typing.cast(rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeReverseRangeRequest, task.request),
                            bearer_token=__internal_magic_token__,
                        ))
                        await complete(task, (response, None))
                        return (response, None)
                    except IMPORT_asyncio.CancelledError:
                        # Check if the task was cancelled by a TasksServicer.
                        if self.tasks_dispatcher.is_task_cancelled(task.task_id.task_uuid):
                            # The running task was cancelled by a TasksServicer.
                            await complete(task, (None, IMPORT_rbt_v1alpha1.tasks_pb2.TaskCancelledError()))
                            return (None, IMPORT_rbt_v1alpha1.tasks_pb2.TaskCancelledError())
                        else:
                            raise
                    except IMPORT_rebootdev.aio.aborted.Aborted as aborted:
                        error_type = f'{aborted.error.__class__.__module__}.{aborted.error.__class__.__qualname__}'
                        # Do not retry a task if the error was specified in the
                        # proto file.
                        if error_type in self._specified_errors_by_service_method_name.get('rbt.std.collections.ordered_map.v1.NodeMethods.ReverseRange', []):
                            await complete(task, (None, aborted.error))
                            return (None, aborted.error)
                        raise


            return await run_ReverseRange(
                self.create_context(
                    headers=IMPORT_reboot_aio_headers.Headers(
                        application_id=self.application_id,
                        state_ref=IMPORT_reboot_aio_types.StateRef(task.task_id.state_ref),
                    ),
                    state_type_name = IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.Node'),
                    method='ReverseRange',
                    context_type=IMPORT_reboot_aio_contexts.WorkflowContext,
                    task=task,
                )
            )
        elif 'Stringify' == task.method_name:
            if only_validate:
                # TODO(benh): validate 'task.request' is correct type.
                return rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeStringifyResponse()

            # Use an inline method to create a new scope, so that we can use
            # variable names like `context` and `effects` in multiple branches
            # in this code (notably when there are multiple task types) without
            # hitting a mypy error that the variable's type is not consistent.
            async def run_Stringify(
                context: IMPORT_reboot_aio_contexts.WorkflowContext,
                *,
                validating_effects: bool = False,
            ):
                async with self._state_manager.task_workflow(
                    context,
                    task,
                    on_loop_iteration=on_loop_iteration,
                    validating_effects=validating_effects,
                ) as complete:
                    try:
                        response = await (NodeWorkflowStub(
                            context=context,
                            state_ref=context._state_ref,
                        ).Stringify(
                            IMPORT_typing.cast(rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeStringifyRequest, task.request),
                            bearer_token=__internal_magic_token__,
                        ))
                        await complete(task, (response, None))
                        return (response, None)
                    except IMPORT_asyncio.CancelledError:
                        # Check if the task was cancelled by a TasksServicer.
                        if self.tasks_dispatcher.is_task_cancelled(task.task_id.task_uuid):
                            # The running task was cancelled by a TasksServicer.
                            await complete(task, (None, IMPORT_rbt_v1alpha1.tasks_pb2.TaskCancelledError()))
                            return (None, IMPORT_rbt_v1alpha1.tasks_pb2.TaskCancelledError())
                        else:
                            raise
                    except IMPORT_rebootdev.aio.aborted.Aborted as aborted:
                        error_type = f'{aborted.error.__class__.__module__}.{aborted.error.__class__.__qualname__}'
                        # Do not retry a task if the error was specified in the
                        # proto file.
                        if error_type in self._specified_errors_by_service_method_name.get('rbt.std.collections.ordered_map.v1.NodeMethods.Stringify', []):
                            await complete(task, (None, aborted.error))
                            return (None, aborted.error)
                        raise


            return await run_Stringify(
                self.create_context(
                    headers=IMPORT_reboot_aio_headers.Headers(
                        application_id=self.application_id,
                        state_ref=IMPORT_reboot_aio_types.StateRef(task.task_id.state_ref),
                    ),
                    state_type_name = IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.Node'),
                    method='Stringify',
                    context_type=IMPORT_reboot_aio_contexts.WorkflowContext,
                    task=task,
                )
            )

        # There are no tasks for this service.
        start_or_validate = "start" if not only_validate else "validate"
        raise RuntimeError(
            f"Attempted to {start_or_validate} task '{task.method_name}' "
            f"on 'Node' which does not exist"
        )

    # Node specific methods:
    async def __Create(
        self,
        context: IMPORT_reboot_aio_contexts.WriterContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateRequest,
        *,
        validating_effects: bool,
    ) -> Node.CreateEffects:
        try:
            response = (
                await self._servicer._Create(
                    context=context,
                    state=state,
                    request=request
                )
            )
            IMPORT_reboot_aio_types.assert_type(
                response,
                [rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateResponse],
            )
            self.maybe_raise_effect_validation_retry(
                logger=logger,
                idempotency_manager=context,
                method_name='Node.Create',
                validating_effects=validating_effects,
                context=context,
            )
            return Node.CreateEffects(
                state=state,
                response=response,
                tasks=context._tasks,
                _colocated_upserts=context._colocated_upserts,
            )
        except IMPORT_reboot_aio_contexts.RetryReactively:
            # Retrying reactively, just let this propagate.
            raise
        except IMPORT_reboot_aio_contexts.EffectValidationRetry:
            # Doing effect validation, just let this propagate.
            raise
        except IMPORT_rebootdev.aio.aborted.Aborted as aborted:
            # If the caller aborted due to a retryable error, just
            # propagate the aborted instead of propagating `Unknown`
            # so that a client can transparently retry.
            if IMPORT_rebootdev.aio.aborted.is_retryable(aborted):
                raise aborted
            # Log any _unhandled_ abort stack traces to make it
            # easier for debugging.
            #
            # NOTE: we don't log if we're a task as it will be logged
            # in `public/rebootdev/aio/internals/tasks_dispatcher.py` instead.
            aborted_type: IMPORT_typing.Optional[type] = None
            aborted_type = Node.CreateAborted
            if isinstance(aborted, IMPORT_rebootdev.aio.aborted.SystemAborted):
                # Not logging when within `node` as we already log there.
                if IMPORT_reboot_nodejs_python.should_print_stacktrace():
                    logger.warning(
                        f"Unhandled (in 'rbt.std.collections.ordered_map.v1.Node.Create') {aborted}; propagating as 'Unknown'\n" +
                        ''.join(IMPORT_traceback.format_exception(aborted))
                    )
                raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                    IMPORT_rbt_v1alpha1.errors_pb2.Unknown(),
                    # TODO(benh): consider whether or not we want to
                    # include the 'package.service.method' which may
                    # get concatenated together forming a kind of
                    # "stack trace"; while it's super helpful for
                    # debugging, it does expose implementation
                    # information.
                    message=f"unhandled (in 'rbt.std.collections.ordered_map.v1.Node.Create') {aborted}"
                )
            else:
                if (
                    aborted_type is not None and
                    not isinstance(aborted, aborted_type) and
                    aborted_type.is_declared_error(aborted.error)
                ):
                    # We propagate declared errors that might have
                    # come from another call, i.e., we might have an
                    # `Aborted` but not for this method but the
                    # `Aborted` that we have has an error that this
                    # method declared. This allows a developer to
                    # simply add the declared error to their `.proto`
                    # file rather than having to catch and re-raise
                    # the error with their own aborted type.
                    if context.task is None:
                        logger.warning(
                            f"Propagating unhandled but declared error (in 'rbt.std.collections.ordered_map.v1.Node.Create') {aborted}"
                        )
                elif (
                    aborted_type is None or
                    not isinstance(aborted, aborted_type)
                ):
                    # Not logging when within `node` as we already log there.
                    if IMPORT_reboot_nodejs_python.should_print_stacktrace():
                        logger.warning(
                            f"Unhandled (in 'rbt.std.collections.ordered_map.v1.Node.Create') {aborted}; propagating as 'Unknown'\n" +
                            ''.join(IMPORT_traceback.format_exception(aborted))
                        )
                    # If this wasn't a declared error than we
                    # propagate it as `Unknown`.
                    raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                        IMPORT_rbt_v1alpha1.errors_pb2.Unknown(),
                        # TODO(benh): consider whether or not we want to
                        # include the 'package.service.method' which may
                        # get concatenated together forming a kind of
                        # "stack trace"; while it's super helpful for
                        # debugging, it does expose implementation
                        # information.
                        message=f"unhandled (in 'rbt.std.collections.ordered_map.v1.Node.Create') {aborted}"
                    )

            raise
        except IMPORT_asyncio.CancelledError:
            # It's pretty normal for an RPC to be cancelled; it's not useful to
            # print a stack trace.
            raise
        except IMPORT_google_protobuf_message.DecodeError as decode_error:
            # We usually see this error when we are trying to construct a proto
            # message which is too deeply nested: protobuf has a limit of 100
            # nested messages. See the limits here:
            #   https://protobuf.dev/programming-guides/proto-limits/

            if IMPORT_reboot_nodejs_python.should_print_stacktrace():
                logger.warning(
                    "Unhandled (in 'rbt.std.collections.ordered_map.v1.Node.Create') "
                    f"{type(decode_error).__name__}{': ' + str(decode_error) if len(str(decode_error)) > 0 else ''}; "
                    "This is usually caused by a deeply nested protobuf message, which is not supported by protobuf.\n"
                    "See the limits here: https://protobuf.dev/programming-guides/proto-limits/" +
                    ''.join(IMPORT_traceback.format_exception(decode_error))
                )
            raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                IMPORT_rbt_v1alpha1.errors_pb2.Unknown(),
                message=f"unhandled (in 'rbt.std.collections.ordered_map.v1.Node.Create') {decode_error}; "
                        "This is usually caused by a deeply nested protobuf message, which is not supported by protobuf.\n"
                        "See the limits here: https://protobuf.dev/programming-guides/proto-limits/"
            )
        except BaseException as exception:
            # Not logging when within `node` as we already log there.
            if IMPORT_reboot_nodejs_python.should_print_stacktrace():
                logger.warning(
                    "Unhandled (in 'rbt.std.collections.ordered_map.v1.Node.Create') "
                    f"{type(exception).__name__}{': ' + str(exception) if len(str(exception)) > 0 else ''}; "
                    "propagating as 'Unknown'\n" +
                    ''.join(IMPORT_traceback.format_exception(exception))
                )
            raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                IMPORT_rbt_v1alpha1.errors_pb2.Unknown(),
                # TODO(benh): consider whether or not we want to
                # include the 'package.service.method' which may
                # get concatenated together forming a kind of
                # "stack trace"; while it's super helpful for
                # debugging, it does expose implementation
                # information.
                message=f"unhandled (in 'rbt.std.collections.ordered_map.v1.Node.Create') {type(exception).__name__}: {exception}"
            )

    @IMPORT_reboot_aio_tracing.function_span(
        # We expect an `EffectValidationRetry` exception; that's not an error.
        set_status_on_exception=False
    )
    async def _Create(
        self,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateRequest,
        context: IMPORT_reboot_aio_contexts.WriterContext,
        *,
        validating_effects: bool,
        grpc_context: IMPORT_typing.Optional[IMPORT_grpc.aio.ServicerContext] = None,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateResponse:
        # Try to verify the token if a token verifier exists.
        context.auth = await self._maybe_verify_token(
            headers=context._headers, method='Create'
        )

        # Check if we already have performed this mutation!
        #
        # We do this _before_ calling 'transactionally()' because
        # if this call is for a transaction method _and_ we've
        # already performed the transaction then we don't want to
        # become a transaction participant (again) we just want to
        # return the transaction's response.
        idempotent_mutation = self._state_manager.check_for_idempotent_mutation(
            context
        )

        if idempotent_mutation is not None:
            response = rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateResponse()
            response.ParseFromString(idempotent_mutation.response)
            return response

        async with self._state_manager.transactionally(
            context,
            self.tasks_dispatcher,
            aborted_type=Node.CreateAborted,
        ) as transaction:
            if transaction is not None:
                context.participants.add(
                    self._servicer.__state_type_name__, context._state_ref
                )
            async with self._state_manager.writer(
                context,
                self._servicer.__state_type__,
                self.tasks_dispatcher,
                authorize=self._maybe_authorize(
                    method_name='rbt.std.collections.ordered_map.v1.NodeMethods.Create',
                    headers=context._headers,
                    auth=context.auth,
                    request=request,
                ),
                transaction=transaction,
                from_constructor=False,
                requires_constructor=False,
            ) as (state, writer):

                effects = await self.__Create(
                    context,
                    state,
                    request,
                    validating_effects=validating_effects,
                )

                await writer.complete(effects)

                # TODO: We need a single `Effects` superclass for all methods, so we
                # would need to make it "partially" generic (with per-method subclasses
                # filling out the rest of the generic parameters) in order to fix this.
                return effects.response  # type: ignore[return-value]

    async def _schedule_Create(
        self,
        *,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateRequest,
        headers: IMPORT_reboot_aio_headers.Headers,
        grpc_context: IMPORT_grpc.aio.ServicerContext,
    ) -> tuple[IMPORT_reboot_aio_contexts.WriterContext, rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateResponse]:
        context: IMPORT_reboot_aio_contexts.WriterContext = self.create_context(
            headers=headers,
            state_type_name = IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.Node'),
            method='Create',
            context_type=IMPORT_reboot_aio_contexts.WriterContext,
        )
        response = rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateResponse()

        # Check if we already have performed this mutation!
        #
        # We do this _before_ calling 'transactionally()' because
        # if this call is for a transaction method _and_ we've
        # already performed the transaction then we don't want to
        # become a transaction participant (again) we just want to
        # return the transaction's response.
        idempotent_mutation = self._state_manager.check_for_idempotent_mutation(
            context
        )

        if idempotent_mutation is not None:
            response.ParseFromString(idempotent_mutation.response)

            # We should have only scheduled a single task!
            assert len(idempotent_mutation.task_ids) == 1
            assert grpc_context is not None
            grpc_context.set_trailing_metadata(
                grpc_context.trailing_metadata() +
                (
                    (
                        IMPORT_reboot_aio_headers.TASK_ID_UUID,
                        str(IMPORT_uuid.UUID(bytes=idempotent_mutation.task_ids[0].task_uuid))
                    ),
                )
            )

            return context, response

        async with self._state_manager.transactionally(
            context,
            self.tasks_dispatcher,
            aborted_type=Node.CreateAborted,
        ) as transaction:
            if transaction is not None:
                context.participants.add(
                    self._servicer.__state_type_name__, context._state_ref
                )

            # Try to verify the token if a token verifier exists.
            context.auth = await self._maybe_verify_token(
                headers=headers, method='Create'
            )

            async with self._state_manager.writer(
                context,
                self._servicer.__state_type__,
                self.tasks_dispatcher,
                transaction=transaction,
                authorize=self._maybe_authorize(
                    method_name='rbt.std.collections.ordered_map.v1.NodeMethods.Create',
                    headers=context._headers,
                    auth=context.auth,
                    request=request,
                ),
                from_constructor=False,
                requires_constructor=False
            ) as (state, writer):

                task = await NodeServicerTasks(
                    context=context,
                    state_ref=context._state_ref,
                ).Create(
                    request,
                    schedule=context._headers.task_schedule,
                )

                effects = IMPORT_reboot_aio_state_managers.Effects(
                    response=response,
                    state=state,
                    tasks=[task],
                )

                assert effects.tasks is not None

                await writer.complete(effects)

                assert grpc_context is not None

                grpc_context.set_trailing_metadata(
                    grpc_context.trailing_metadata() +
                    (
                        (
                            IMPORT_reboot_aio_headers.TASK_ID_UUID,
                            str(IMPORT_uuid.UUID(bytes=task.task_id.task_uuid))
                        ),
                    )
                )

                return context, response

        return context, response


    # Entrypoint for non-reactive network calls (i.e. typical gRPC calls).
    async def Create(
        self,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateRequest,
        grpc_context: IMPORT_grpc.aio.ServicerContext,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateResponse:
        headers = IMPORT_reboot_aio_headers.Headers.from_grpc_context(grpc_context)
        assert headers.application_id is not None  # Guaranteed by `Headers`.

        # Confirm whether this is the right consensus to be serving this
        # request.
        authoritative_consensus = self.placement_client.consensus_for_actor(
            headers.application_id,
            headers.state_ref,
        )
        if authoritative_consensus != self.consensus_id:
            # This is NOT the correct consensus. Fail.
            await grpc_context.abort(
                IMPORT_grpc.StatusCode.UNAVAILABLE,
                f"Consensus '{self.consensus_id}' is not authoritative for this "
                f"request; consensus '{authoritative_consensus}' is.",
            )
            raise  # Unreachable but necessary for mypy.

        @IMPORT_reboot_aio_internals_middleware.maybe_run_function_twice_to_validate_effects
        async def _run(
            validating_effects: bool,
        ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateResponse:
            context: IMPORT_typing.Optional[IMPORT_reboot_aio_contexts.Context] = None
            try:
                if headers.task_schedule is not None:
                    context, response = await self._schedule_Create(
                        headers=headers,
                        request=request,
                        grpc_context=grpc_context,
                    )
                    return response

                context = self.create_context(
                    headers=headers,
                    state_type_name = IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.Node'),
                    method='Create',
                    context_type=IMPORT_reboot_aio_contexts.WriterContext,
                )
                assert context is not None

                return await self._Create(
                    request,
                    context,
                    validating_effects=validating_effects,
                    grpc_context=grpc_context,
                )
            except IMPORT_reboot_aio_contexts.EffectValidationRetry:
                # Doing effect validation, just let this propagate.
                raise
            except IMPORT_rebootdev.aio.aborted.Aborted as aborted:
                status = IMPORT_rpc_status_sync.to_status(aborted.to_status())
                # Need to add transaction participants here because
                # calling `grpc_context.abort_with_status()` will
                # ignore any other trailing metadata.
                if context is not None and context.transaction_id is not None:
                    status = status._replace(
                        trailing_metadata=status.trailing_metadata + context.participants.to_grpc_metadata()
                    )
                await grpc_context.abort_with_status(status)
                raise  # Unreachable but necessary for mypy.
            except IMPORT_asyncio.CancelledError:
                # It's pretty normal for an RPC to be cancelled; it's not useful to
                # print a stack trace.
                raise
            except BaseException as exception:
                # Print the exception stack trace for easier debugging. Note
                # that we don't include the stack trace in an error message
                # for the same reason that gRPC doesn't do so by default,
                # see https://github.com/grpc/grpc/issues/14897, but since this
                # should only get logged on the server side it is safe.
                logger.warning(
                    'Unhandled exception\n' +
                    ''.join(IMPORT_traceback.format_exc() if IMPORT_reboot_nodejs_python.should_print_stacktrace() else [f"{type(exception).__name__}: {exception}"])
                )

                # Re-raise the exception for gRPC to handle!
                #
                # TODO: gRPC will print a stack trace from this
                # exception which we don't want if we're executing via
                # Node.js.
                raise
            finally:
                if context is not None and context.transaction_id is not None:
                    # Propagate transaction participants.
                    grpc_context.set_trailing_metadata(
                        grpc_context.trailing_metadata() +
                        context.participants.to_grpc_metadata()
                    )

        with IMPORT_reboot_aio_tracing.context_from_headers(headers):
            return await _run()

    async def __Search(
        self,
        context: IMPORT_reboot_aio_contexts.ReaderContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeSearchRequest,
        *,
        validating_effects: bool,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeSearchResponse:
        try:
            response = (
                await self._servicer._Search(
                    context=context,
                    state=state,
                    request=request
                )
            )
            IMPORT_reboot_aio_types.assert_type(
                response,
                [rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeSearchResponse],
            )
            self.maybe_raise_effect_validation_retry(
                logger=logger,
                idempotency_manager=context,
                method_name='Node.Search',
                validating_effects=validating_effects,
                context=context,
            )
            return response
        except IMPORT_reboot_aio_contexts.RetryReactively:
            # Retrying reactively, just let this propagate.
            raise
        except IMPORT_reboot_aio_contexts.EffectValidationRetry:
            # Doing effect validation, just let this propagate.
            raise
        except IMPORT_rebootdev.aio.aborted.Aborted as aborted:
            # If the caller aborted due to a retryable error, just
            # propagate the aborted instead of propagating `Unknown`
            # so that a client can transparently retry.
            if IMPORT_rebootdev.aio.aborted.is_retryable(aborted):
                raise aborted
            # Log any _unhandled_ abort stack traces to make it
            # easier for debugging.
            #
            # NOTE: we don't log if we're a task as it will be logged
            # in `public/rebootdev/aio/internals/tasks_dispatcher.py` instead.
            aborted_type: IMPORT_typing.Optional[type] = None
            aborted_type = Node.SearchAborted
            if isinstance(aborted, IMPORT_rebootdev.aio.aborted.SystemAborted):
                # Not logging when within `node` as we already log there.
                if IMPORT_reboot_nodejs_python.should_print_stacktrace():
                    logger.warning(
                        f"Unhandled (in 'rbt.std.collections.ordered_map.v1.Node.Search') {aborted}; propagating as 'Unknown'\n" +
                        ''.join(IMPORT_traceback.format_exception(aborted))
                    )
                raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                    IMPORT_rbt_v1alpha1.errors_pb2.Unknown(),
                    # TODO(benh): consider whether or not we want to
                    # include the 'package.service.method' which may
                    # get concatenated together forming a kind of
                    # "stack trace"; while it's super helpful for
                    # debugging, it does expose implementation
                    # information.
                    message=f"unhandled (in 'rbt.std.collections.ordered_map.v1.Node.Search') {aborted}"
                )
            else:
                if (
                    aborted_type is not None and
                    not isinstance(aborted, aborted_type) and
                    aborted_type.is_declared_error(aborted.error)
                ):
                    # We propagate declared errors that might have
                    # come from another call, i.e., we might have an
                    # `Aborted` but not for this method but the
                    # `Aborted` that we have has an error that this
                    # method declared. This allows a developer to
                    # simply add the declared error to their `.proto`
                    # file rather than having to catch and re-raise
                    # the error with their own aborted type.
                    if context.task is None:
                        logger.warning(
                            f"Propagating unhandled but declared error (in 'rbt.std.collections.ordered_map.v1.Node.Search') {aborted}"
                        )
                elif (
                    aborted_type is None or
                    not isinstance(aborted, aborted_type)
                ):
                    # Not logging when within `node` as we already log there.
                    if IMPORT_reboot_nodejs_python.should_print_stacktrace():
                        logger.warning(
                            f"Unhandled (in 'rbt.std.collections.ordered_map.v1.Node.Search') {aborted}; propagating as 'Unknown'\n" +
                            ''.join(IMPORT_traceback.format_exception(aborted))
                        )
                    # If this wasn't a declared error than we
                    # propagate it as `Unknown`.
                    raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                        IMPORT_rbt_v1alpha1.errors_pb2.Unknown(),
                        # TODO(benh): consider whether or not we want to
                        # include the 'package.service.method' which may
                        # get concatenated together forming a kind of
                        # "stack trace"; while it's super helpful for
                        # debugging, it does expose implementation
                        # information.
                        message=f"unhandled (in 'rbt.std.collections.ordered_map.v1.Node.Search') {aborted}"
                    )

            raise
        except IMPORT_asyncio.CancelledError:
            # It's pretty normal for an RPC to be cancelled; it's not useful to
            # print a stack trace.
            raise
        except IMPORT_google_protobuf_message.DecodeError as decode_error:
            # We usually see this error when we are trying to construct a proto
            # message which is too deeply nested: protobuf has a limit of 100
            # nested messages. See the limits here:
            #   https://protobuf.dev/programming-guides/proto-limits/

            if IMPORT_reboot_nodejs_python.should_print_stacktrace():
                logger.warning(
                    "Unhandled (in 'rbt.std.collections.ordered_map.v1.Node.Search') "
                    f"{type(decode_error).__name__}{': ' + str(decode_error) if len(str(decode_error)) > 0 else ''}; "
                    "This is usually caused by a deeply nested protobuf message, which is not supported by protobuf.\n"
                    "See the limits here: https://protobuf.dev/programming-guides/proto-limits/" +
                    ''.join(IMPORT_traceback.format_exception(decode_error))
                )
            raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                IMPORT_rbt_v1alpha1.errors_pb2.Unknown(),
                message=f"unhandled (in 'rbt.std.collections.ordered_map.v1.Node.Search') {decode_error}; "
                        "This is usually caused by a deeply nested protobuf message, which is not supported by protobuf.\n"
                        "See the limits here: https://protobuf.dev/programming-guides/proto-limits/"
            )
        except BaseException as exception:
            # Not logging when within `node` as we already log there.
            if IMPORT_reboot_nodejs_python.should_print_stacktrace():
                logger.warning(
                    "Unhandled (in 'rbt.std.collections.ordered_map.v1.Node.Search') "
                    f"{type(exception).__name__}{': ' + str(exception) if len(str(exception)) > 0 else ''}; "
                    "propagating as 'Unknown'\n" +
                    ''.join(IMPORT_traceback.format_exception(exception))
                )
            raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                IMPORT_rbt_v1alpha1.errors_pb2.Unknown(),
                # TODO(benh): consider whether or not we want to
                # include the 'package.service.method' which may
                # get concatenated together forming a kind of
                # "stack trace"; while it's super helpful for
                # debugging, it does expose implementation
                # information.
                message=f"unhandled (in 'rbt.std.collections.ordered_map.v1.Node.Search') {type(exception).__name__}: {exception}"
            )

    @IMPORT_reboot_aio_tracing.function_span(
        # We expect an `EffectValidationRetry` exception; that's not an error.
        set_status_on_exception=False
    )
    async def _Search(
        self,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeSearchRequest,
        context: IMPORT_reboot_aio_contexts.ReaderContext,
        *,
        validating_effects: bool,
        grpc_context: IMPORT_typing.Optional[IMPORT_grpc.aio.ServicerContext] = None,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeSearchResponse:
        # Try to verify the token if a token verifier exists.
        context.auth = await self._maybe_verify_token(
            headers=context._headers, method='Search'
        )


        async with self._state_manager.transactionally(
            context,
            self.tasks_dispatcher,
            aborted_type=Node.SearchAborted,
        ) as transaction:
            if transaction is not None:
                context.participants.add(
                    self._servicer.__state_type_name__, context._state_ref
                )
            authorizer = self._maybe_authorize(
                method_name='rbt.std.collections.ordered_map.v1.NodeMethods.Search',
                headers=context._headers,
                auth=context.auth,
                request=request,
            )
            async with self._state_manager.reader(
                context,
                self._servicer.__state_type__,
                authorize=authorizer,
            ) as state:
                response = await self.__Search(
                    context,
                    state,
                    request,
                    validating_effects=validating_effects,
                )
                return response

    async def _schedule_Search(
        self,
        *,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeSearchRequest,
        headers: IMPORT_reboot_aio_headers.Headers,
        grpc_context: IMPORT_grpc.aio.ServicerContext,
    ) -> tuple[IMPORT_reboot_aio_contexts.WriterContext, rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeSearchResponse]:
        context: IMPORT_reboot_aio_contexts.WriterContext = self.create_context(
            headers=headers,
            state_type_name = IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.Node'),
            method='Search',
            context_type=IMPORT_reboot_aio_contexts.WriterContext,
        )
        response = rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeSearchResponse()


        async with self._state_manager.transactionally(
            context,
            self.tasks_dispatcher,
            aborted_type=Node.SearchAborted,
        ) as transaction:
            if transaction is not None:
                context.participants.add(
                    self._servicer.__state_type_name__, context._state_ref
                )

            # Try to verify the token if a token verifier exists.
            context.auth = await self._maybe_verify_token(
                headers=headers, method='Search'
            )

            async with self._state_manager.writer(
                context,
                self._servicer.__state_type__,
                self.tasks_dispatcher,
                transaction=transaction,
                authorize=self._maybe_authorize(
                    method_name='rbt.std.collections.ordered_map.v1.NodeMethods.Search',
                    headers=context._headers,
                    auth=context.auth,
                    request=request,
                ),
                from_constructor=False,
                requires_constructor=False
            ) as (state, writer):

                task = await NodeServicerTasks(
                    context=context,
                    state_ref=context._state_ref,
                ).Search(
                    request,
                    schedule=context._headers.task_schedule,
                )

                effects = IMPORT_reboot_aio_state_managers.Effects(
                    response=response,
                    state=state,
                    tasks=[task],
                )

                assert effects.tasks is not None

                await writer.complete(effects)

                assert grpc_context is not None

                grpc_context.set_trailing_metadata(
                    grpc_context.trailing_metadata() +
                    (
                        (
                            IMPORT_reboot_aio_headers.TASK_ID_UUID,
                            str(IMPORT_uuid.UUID(bytes=task.task_id.task_uuid))
                        ),
                    )
                )

                return context, response

        return context, response


    # Entrypoint for non-reactive network calls (i.e. typical gRPC calls).
    async def Search(
        self,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeSearchRequest,
        grpc_context: IMPORT_grpc.aio.ServicerContext,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeSearchResponse:
        headers = IMPORT_reboot_aio_headers.Headers.from_grpc_context(grpc_context)
        assert headers.application_id is not None  # Guaranteed by `Headers`.

        # Confirm whether this is the right consensus to be serving this
        # request.
        authoritative_consensus = self.placement_client.consensus_for_actor(
            headers.application_id,
            headers.state_ref,
        )
        if authoritative_consensus != self.consensus_id:
            # This is NOT the correct consensus. Fail.
            await grpc_context.abort(
                IMPORT_grpc.StatusCode.UNAVAILABLE,
                f"Consensus '{self.consensus_id}' is not authoritative for this "
                f"request; consensus '{authoritative_consensus}' is.",
            )
            raise  # Unreachable but necessary for mypy.

        @IMPORT_reboot_aio_internals_middleware.maybe_run_function_twice_to_validate_effects
        async def _run(
            validating_effects: bool,
        ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeSearchResponse:
            context: IMPORT_typing.Optional[IMPORT_reboot_aio_contexts.Context] = None
            try:
                if headers.task_schedule is not None:
                    context, response = await self._schedule_Search(
                        headers=headers,
                        request=request,
                        grpc_context=grpc_context,
                    )
                    return response

                context = self.create_context(
                    headers=headers,
                    state_type_name = IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.Node'),
                    method='Search',
                    context_type=IMPORT_reboot_aio_contexts.ReaderContext,
                )
                assert context is not None

                return await self._Search(
                    request,
                    context,
                    validating_effects=validating_effects,
                    grpc_context=grpc_context,
                )
            except IMPORT_reboot_aio_contexts.EffectValidationRetry:
                # Doing effect validation, just let this propagate.
                raise
            except IMPORT_rebootdev.aio.aborted.Aborted as aborted:
                status = IMPORT_rpc_status_sync.to_status(aborted.to_status())
                # Need to add transaction participants here because
                # calling `grpc_context.abort_with_status()` will
                # ignore any other trailing metadata.
                if context is not None and context.transaction_id is not None:
                    status = status._replace(
                        trailing_metadata=status.trailing_metadata + context.participants.to_grpc_metadata()
                    )
                await grpc_context.abort_with_status(status)
                raise  # Unreachable but necessary for mypy.
            except IMPORT_asyncio.CancelledError:
                # It's pretty normal for an RPC to be cancelled; it's not useful to
                # print a stack trace.
                raise
            except BaseException as exception:
                # Print the exception stack trace for easier debugging. Note
                # that we don't include the stack trace in an error message
                # for the same reason that gRPC doesn't do so by default,
                # see https://github.com/grpc/grpc/issues/14897, but since this
                # should only get logged on the server side it is safe.
                logger.warning(
                    'Unhandled exception\n' +
                    ''.join(IMPORT_traceback.format_exc() if IMPORT_reboot_nodejs_python.should_print_stacktrace() else [f"{type(exception).__name__}: {exception}"])
                )

                # Re-raise the exception for gRPC to handle!
                #
                # TODO: gRPC will print a stack trace from this
                # exception which we don't want if we're executing via
                # Node.js.
                raise
            finally:
                if context is not None and context.transaction_id is not None:
                    # Propagate transaction participants.
                    grpc_context.set_trailing_metadata(
                        grpc_context.trailing_metadata() +
                        context.participants.to_grpc_metadata()
                    )

        with IMPORT_reboot_aio_tracing.context_from_headers(headers):
            return await _run()

    async def __Insert(
        self,
        context: IMPORT_reboot_aio_contexts.TransactionContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeInsertRequest,
        *,
        validating_effects: bool,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeInsertResponse:
        try:
            response = (
                await self._servicer._Insert(
                    context=context,
                    state=state,
                    request=request
                )
            )
            IMPORT_reboot_aio_types.assert_type(
                response,
                [rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeInsertResponse],
            )
            self.maybe_raise_effect_validation_retry(
                logger=logger,
                idempotency_manager=context,
                method_name='Node.Insert',
                validating_effects=validating_effects,
                context=context,
            )
            return response
        except IMPORT_reboot_aio_contexts.RetryReactively:
            # Retrying reactively, just let this propagate.
            raise
        except IMPORT_reboot_aio_contexts.EffectValidationRetry:
            # Doing effect validation, just let this propagate.
            raise
        except IMPORT_rebootdev.aio.aborted.Aborted as aborted:
            # If the caller aborted due to a retryable error, just
            # propagate the aborted instead of propagating `Unknown`
            # so that a client can transparently retry.
            if IMPORT_rebootdev.aio.aborted.is_retryable(aborted):
                raise aborted
            # Log any _unhandled_ abort stack traces to make it
            # easier for debugging.
            #
            # NOTE: we don't log if we're a task as it will be logged
            # in `public/rebootdev/aio/internals/tasks_dispatcher.py` instead.
            aborted_type: IMPORT_typing.Optional[type] = None
            aborted_type = Node.InsertAborted
            if isinstance(aborted, IMPORT_rebootdev.aio.aborted.SystemAborted):
                # Not logging when within `node` as we already log there.
                if IMPORT_reboot_nodejs_python.should_print_stacktrace():
                    logger.warning(
                        f"Unhandled (in 'rbt.std.collections.ordered_map.v1.Node.Insert') {aborted}; propagating as 'Unknown'\n" +
                        ''.join(IMPORT_traceback.format_exception(aborted))
                    )
                raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                    IMPORT_rbt_v1alpha1.errors_pb2.Unknown(),
                    # TODO(benh): consider whether or not we want to
                    # include the 'package.service.method' which may
                    # get concatenated together forming a kind of
                    # "stack trace"; while it's super helpful for
                    # debugging, it does expose implementation
                    # information.
                    message=f"unhandled (in 'rbt.std.collections.ordered_map.v1.Node.Insert') {aborted}"
                )
            else:
                if (
                    aborted_type is not None and
                    not isinstance(aborted, aborted_type) and
                    aborted_type.is_declared_error(aborted.error)
                ):
                    # We propagate declared errors that might have
                    # come from another call, i.e., we might have an
                    # `Aborted` but not for this method but the
                    # `Aborted` that we have has an error that this
                    # method declared. This allows a developer to
                    # simply add the declared error to their `.proto`
                    # file rather than having to catch and re-raise
                    # the error with their own aborted type.
                    if context.task is None:
                        logger.warning(
                            f"Propagating unhandled but declared error (in 'rbt.std.collections.ordered_map.v1.Node.Insert') {aborted}"
                        )
                elif (
                    aborted_type is None or
                    not isinstance(aborted, aborted_type)
                ):
                    # Not logging when within `node` as we already log there.
                    if IMPORT_reboot_nodejs_python.should_print_stacktrace():
                        logger.warning(
                            f"Unhandled (in 'rbt.std.collections.ordered_map.v1.Node.Insert') {aborted}; propagating as 'Unknown'\n" +
                            ''.join(IMPORT_traceback.format_exception(aborted))
                        )
                    # If this wasn't a declared error than we
                    # propagate it as `Unknown`.
                    raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                        IMPORT_rbt_v1alpha1.errors_pb2.Unknown(),
                        # TODO(benh): consider whether or not we want to
                        # include the 'package.service.method' which may
                        # get concatenated together forming a kind of
                        # "stack trace"; while it's super helpful for
                        # debugging, it does expose implementation
                        # information.
                        message=f"unhandled (in 'rbt.std.collections.ordered_map.v1.Node.Insert') {aborted}"
                    )

            raise
        except IMPORT_asyncio.CancelledError:
            # It's pretty normal for an RPC to be cancelled; it's not useful to
            # print a stack trace.
            raise
        except IMPORT_google_protobuf_message.DecodeError as decode_error:
            # We usually see this error when we are trying to construct a proto
            # message which is too deeply nested: protobuf has a limit of 100
            # nested messages. See the limits here:
            #   https://protobuf.dev/programming-guides/proto-limits/

            if IMPORT_reboot_nodejs_python.should_print_stacktrace():
                logger.warning(
                    "Unhandled (in 'rbt.std.collections.ordered_map.v1.Node.Insert') "
                    f"{type(decode_error).__name__}{': ' + str(decode_error) if len(str(decode_error)) > 0 else ''}; "
                    "This is usually caused by a deeply nested protobuf message, which is not supported by protobuf.\n"
                    "See the limits here: https://protobuf.dev/programming-guides/proto-limits/" +
                    ''.join(IMPORT_traceback.format_exception(decode_error))
                )
            raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                IMPORT_rbt_v1alpha1.errors_pb2.Unknown(),
                message=f"unhandled (in 'rbt.std.collections.ordered_map.v1.Node.Insert') {decode_error}; "
                        "This is usually caused by a deeply nested protobuf message, which is not supported by protobuf.\n"
                        "See the limits here: https://protobuf.dev/programming-guides/proto-limits/"
            )
        except BaseException as exception:
            # Not logging when within `node` as we already log there.
            if IMPORT_reboot_nodejs_python.should_print_stacktrace():
                logger.warning(
                    "Unhandled (in 'rbt.std.collections.ordered_map.v1.Node.Insert') "
                    f"{type(exception).__name__}{': ' + str(exception) if len(str(exception)) > 0 else ''}; "
                    "propagating as 'Unknown'\n" +
                    ''.join(IMPORT_traceback.format_exception(exception))
                )
            raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                IMPORT_rbt_v1alpha1.errors_pb2.Unknown(),
                # TODO(benh): consider whether or not we want to
                # include the 'package.service.method' which may
                # get concatenated together forming a kind of
                # "stack trace"; while it's super helpful for
                # debugging, it does expose implementation
                # information.
                message=f"unhandled (in 'rbt.std.collections.ordered_map.v1.Node.Insert') {type(exception).__name__}: {exception}"
            )

    @IMPORT_reboot_aio_tracing.function_span(
        # We expect an `EffectValidationRetry` exception; that's not an error.
        set_status_on_exception=False
    )
    async def _Insert(
        self,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeInsertRequest,
        context: IMPORT_reboot_aio_contexts.TransactionContext,
        *,
        validating_effects: bool,
        grpc_context: IMPORT_typing.Optional[IMPORT_grpc.aio.ServicerContext] = None,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeInsertResponse:
        # Try to verify the token if a token verifier exists.
        context.auth = await self._maybe_verify_token(
            headers=context._headers, method='Insert'
        )

        # Check if we already have performed this mutation!
        #
        # We do this _before_ calling 'transactionally()' because
        # if this call is for a transaction method _and_ we've
        # already performed the transaction then we don't want to
        # become a transaction participant (again) we just want to
        # return the transaction's response.
        idempotent_mutation = self._state_manager.check_for_idempotent_mutation(
            context
        )

        if idempotent_mutation is not None:
            response = rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeInsertResponse()
            response.ParseFromString(idempotent_mutation.response)
            return response

        async with self._state_manager.transactionally(
            context,
            self.tasks_dispatcher,
            aborted_type=Node.InsertAborted,
        ) as transaction:
            if transaction is not None:
                context.participants.add(
                    self._servicer.__state_type_name__, context._state_ref
                )
            assert transaction is not None
            async with self._state_manager.transaction(
                context,
                self._servicer.__state_type__,
                transaction,
                authorize=self._maybe_authorize(
                    method_name='rbt.std.collections.ordered_map.v1.NodeMethods.Insert',
                    headers=context._headers,
                    auth=context.auth,
                    request=request,
                ),
                from_constructor=False,
                requires_constructor=False
            ) as (state, complete):

                response = await self.__Insert(
                    context,
                    state,
                    request,
                    validating_effects=validating_effects,
                )

                await complete(
                    IMPORT_reboot_aio_state_managers.Effects(
                        state=state,
                        response=response,
                    )
                )
                return response

    async def _schedule_Insert(
        self,
        *,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeInsertRequest,
        headers: IMPORT_reboot_aio_headers.Headers,
        grpc_context: IMPORT_grpc.aio.ServicerContext,
    ) -> tuple[IMPORT_reboot_aio_contexts.WriterContext, rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeInsertResponse]:
        context: IMPORT_reboot_aio_contexts.WriterContext = self.create_context(
            headers=headers,
            state_type_name = IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.Node'),
            method='Insert',
            context_type=IMPORT_reboot_aio_contexts.WriterContext,
        )
        response = rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeInsertResponse()

        # Check if we already have performed this mutation!
        #
        # We do this _before_ calling 'transactionally()' because
        # if this call is for a transaction method _and_ we've
        # already performed the transaction then we don't want to
        # become a transaction participant (again) we just want to
        # return the transaction's response.
        idempotent_mutation = self._state_manager.check_for_idempotent_mutation(
            context
        )

        if idempotent_mutation is not None:
            response.ParseFromString(idempotent_mutation.response)

            # We should have only scheduled a single task!
            assert len(idempotent_mutation.task_ids) == 1
            assert grpc_context is not None
            grpc_context.set_trailing_metadata(
                grpc_context.trailing_metadata() +
                (
                    (
                        IMPORT_reboot_aio_headers.TASK_ID_UUID,
                        str(IMPORT_uuid.UUID(bytes=idempotent_mutation.task_ids[0].task_uuid))
                    ),
                )
            )

            return context, response

        async with self._state_manager.transactionally(
            context,
            self.tasks_dispatcher,
            aborted_type=Node.InsertAborted,
        ) as transaction:
            if transaction is not None:
                context.participants.add(
                    self._servicer.__state_type_name__, context._state_ref
                )

            # Try to verify the token if a token verifier exists.
            context.auth = await self._maybe_verify_token(
                headers=headers, method='Insert'
            )

            async with self._state_manager.writer(
                context,
                self._servicer.__state_type__,
                self.tasks_dispatcher,
                transaction=transaction,
                authorize=self._maybe_authorize(
                    method_name='rbt.std.collections.ordered_map.v1.NodeMethods.Insert',
                    headers=context._headers,
                    auth=context.auth,
                    request=request,
                ),
                from_constructor=False,
                requires_constructor=False
            ) as (state, writer):

                task = await NodeServicerTasks(
                    context=context,
                    state_ref=context._state_ref,
                ).Insert(
                    request,
                    schedule=context._headers.task_schedule,
                )

                effects = IMPORT_reboot_aio_state_managers.Effects(
                    response=response,
                    state=state,
                    tasks=[task],
                )

                assert effects.tasks is not None

                await writer.complete(effects)

                assert grpc_context is not None

                grpc_context.set_trailing_metadata(
                    grpc_context.trailing_metadata() +
                    (
                        (
                            IMPORT_reboot_aio_headers.TASK_ID_UUID,
                            str(IMPORT_uuid.UUID(bytes=task.task_id.task_uuid))
                        ),
                    )
                )

                return context, response

        return context, response


    # Entrypoint for non-reactive network calls (i.e. typical gRPC calls).
    async def Insert(
        self,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeInsertRequest,
        grpc_context: IMPORT_grpc.aio.ServicerContext,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeInsertResponse:
        headers = IMPORT_reboot_aio_headers.Headers.from_grpc_context(grpc_context)
        assert headers.application_id is not None  # Guaranteed by `Headers`.

        # Confirm whether this is the right consensus to be serving this
        # request.
        authoritative_consensus = self.placement_client.consensus_for_actor(
            headers.application_id,
            headers.state_ref,
        )
        if authoritative_consensus != self.consensus_id:
            # This is NOT the correct consensus. Fail.
            await grpc_context.abort(
                IMPORT_grpc.StatusCode.UNAVAILABLE,
                f"Consensus '{self.consensus_id}' is not authoritative for this "
                f"request; consensus '{authoritative_consensus}' is.",
            )
            raise  # Unreachable but necessary for mypy.

        @IMPORT_reboot_aio_internals_middleware.maybe_run_function_twice_to_validate_effects
        async def _run(
            validating_effects: bool,
        ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeInsertResponse:
            context: IMPORT_typing.Optional[IMPORT_reboot_aio_contexts.Context] = None
            try:
                if headers.task_schedule is not None:
                    context, response = await self._schedule_Insert(
                        headers=headers,
                        request=request,
                        grpc_context=grpc_context,
                    )
                    return response

                context = self.create_context(
                    headers=headers,
                    state_type_name = IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.Node'),
                    method='Insert',
                    context_type=IMPORT_reboot_aio_contexts.TransactionContext,
                )
                assert context is not None

                return await self._Insert(
                    request,
                    context,
                    validating_effects=validating_effects,
                    grpc_context=grpc_context,
                )
            except IMPORT_reboot_aio_contexts.EffectValidationRetry:
                # Doing effect validation, just let this propagate.
                raise
            except IMPORT_rebootdev.aio.aborted.Aborted as aborted:
                status = IMPORT_rpc_status_sync.to_status(aborted.to_status())
                # Need to add transaction participants here because
                # calling `grpc_context.abort_with_status()` will
                # ignore any other trailing metadata.
                if context is not None and context.transaction_id is not None:
                    status = status._replace(
                        trailing_metadata=status.trailing_metadata + context.participants.to_grpc_metadata()
                    )
                await grpc_context.abort_with_status(status)
                raise  # Unreachable but necessary for mypy.
            except IMPORT_asyncio.CancelledError:
                # It's pretty normal for an RPC to be cancelled; it's not useful to
                # print a stack trace.
                raise
            except BaseException as exception:
                # Print the exception stack trace for easier debugging. Note
                # that we don't include the stack trace in an error message
                # for the same reason that gRPC doesn't do so by default,
                # see https://github.com/grpc/grpc/issues/14897, but since this
                # should only get logged on the server side it is safe.
                logger.warning(
                    'Unhandled exception\n' +
                    ''.join(IMPORT_traceback.format_exc() if IMPORT_reboot_nodejs_python.should_print_stacktrace() else [f"{type(exception).__name__}: {exception}"])
                )

                # Re-raise the exception for gRPC to handle!
                #
                # TODO: gRPC will print a stack trace from this
                # exception which we don't want if we're executing via
                # Node.js.
                raise
            finally:
                if context is not None and context.transaction_id is not None:
                    # Propagate transaction participants.
                    grpc_context.set_trailing_metadata(
                        grpc_context.trailing_metadata() +
                        context.participants.to_grpc_metadata()
                    )

        with IMPORT_reboot_aio_tracing.context_from_headers(headers):
            return await _run()

    async def __Remove(
        self,
        context: IMPORT_reboot_aio_contexts.TransactionContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRemoveRequest,
        *,
        validating_effects: bool,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRemoveResponse:
        try:
            response = (
                await self._servicer._Remove(
                    context=context,
                    state=state,
                    request=request
                )
            )
            IMPORT_reboot_aio_types.assert_type(
                response,
                [rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRemoveResponse],
            )
            self.maybe_raise_effect_validation_retry(
                logger=logger,
                idempotency_manager=context,
                method_name='Node.Remove',
                validating_effects=validating_effects,
                context=context,
            )
            return response
        except IMPORT_reboot_aio_contexts.RetryReactively:
            # Retrying reactively, just let this propagate.
            raise
        except IMPORT_reboot_aio_contexts.EffectValidationRetry:
            # Doing effect validation, just let this propagate.
            raise
        except IMPORT_rebootdev.aio.aborted.Aborted as aborted:
            # If the caller aborted due to a retryable error, just
            # propagate the aborted instead of propagating `Unknown`
            # so that a client can transparently retry.
            if IMPORT_rebootdev.aio.aborted.is_retryable(aborted):
                raise aborted
            # Log any _unhandled_ abort stack traces to make it
            # easier for debugging.
            #
            # NOTE: we don't log if we're a task as it will be logged
            # in `public/rebootdev/aio/internals/tasks_dispatcher.py` instead.
            aborted_type: IMPORT_typing.Optional[type] = None
            aborted_type = Node.RemoveAborted
            if isinstance(aborted, IMPORT_rebootdev.aio.aborted.SystemAborted):
                # Not logging when within `node` as we already log there.
                if IMPORT_reboot_nodejs_python.should_print_stacktrace():
                    logger.warning(
                        f"Unhandled (in 'rbt.std.collections.ordered_map.v1.Node.Remove') {aborted}; propagating as 'Unknown'\n" +
                        ''.join(IMPORT_traceback.format_exception(aborted))
                    )
                raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                    IMPORT_rbt_v1alpha1.errors_pb2.Unknown(),
                    # TODO(benh): consider whether or not we want to
                    # include the 'package.service.method' which may
                    # get concatenated together forming a kind of
                    # "stack trace"; while it's super helpful for
                    # debugging, it does expose implementation
                    # information.
                    message=f"unhandled (in 'rbt.std.collections.ordered_map.v1.Node.Remove') {aborted}"
                )
            else:
                if (
                    aborted_type is not None and
                    not isinstance(aborted, aborted_type) and
                    aborted_type.is_declared_error(aborted.error)
                ):
                    # We propagate declared errors that might have
                    # come from another call, i.e., we might have an
                    # `Aborted` but not for this method but the
                    # `Aborted` that we have has an error that this
                    # method declared. This allows a developer to
                    # simply add the declared error to their `.proto`
                    # file rather than having to catch and re-raise
                    # the error with their own aborted type.
                    if context.task is None:
                        logger.warning(
                            f"Propagating unhandled but declared error (in 'rbt.std.collections.ordered_map.v1.Node.Remove') {aborted}"
                        )
                elif (
                    aborted_type is None or
                    not isinstance(aborted, aborted_type)
                ):
                    # Not logging when within `node` as we already log there.
                    if IMPORT_reboot_nodejs_python.should_print_stacktrace():
                        logger.warning(
                            f"Unhandled (in 'rbt.std.collections.ordered_map.v1.Node.Remove') {aborted}; propagating as 'Unknown'\n" +
                            ''.join(IMPORT_traceback.format_exception(aborted))
                        )
                    # If this wasn't a declared error than we
                    # propagate it as `Unknown`.
                    raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                        IMPORT_rbt_v1alpha1.errors_pb2.Unknown(),
                        # TODO(benh): consider whether or not we want to
                        # include the 'package.service.method' which may
                        # get concatenated together forming a kind of
                        # "stack trace"; while it's super helpful for
                        # debugging, it does expose implementation
                        # information.
                        message=f"unhandled (in 'rbt.std.collections.ordered_map.v1.Node.Remove') {aborted}"
                    )

            raise
        except IMPORT_asyncio.CancelledError:
            # It's pretty normal for an RPC to be cancelled; it's not useful to
            # print a stack trace.
            raise
        except IMPORT_google_protobuf_message.DecodeError as decode_error:
            # We usually see this error when we are trying to construct a proto
            # message which is too deeply nested: protobuf has a limit of 100
            # nested messages. See the limits here:
            #   https://protobuf.dev/programming-guides/proto-limits/

            if IMPORT_reboot_nodejs_python.should_print_stacktrace():
                logger.warning(
                    "Unhandled (in 'rbt.std.collections.ordered_map.v1.Node.Remove') "
                    f"{type(decode_error).__name__}{': ' + str(decode_error) if len(str(decode_error)) > 0 else ''}; "
                    "This is usually caused by a deeply nested protobuf message, which is not supported by protobuf.\n"
                    "See the limits here: https://protobuf.dev/programming-guides/proto-limits/" +
                    ''.join(IMPORT_traceback.format_exception(decode_error))
                )
            raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                IMPORT_rbt_v1alpha1.errors_pb2.Unknown(),
                message=f"unhandled (in 'rbt.std.collections.ordered_map.v1.Node.Remove') {decode_error}; "
                        "This is usually caused by a deeply nested protobuf message, which is not supported by protobuf.\n"
                        "See the limits here: https://protobuf.dev/programming-guides/proto-limits/"
            )
        except BaseException as exception:
            # Not logging when within `node` as we already log there.
            if IMPORT_reboot_nodejs_python.should_print_stacktrace():
                logger.warning(
                    "Unhandled (in 'rbt.std.collections.ordered_map.v1.Node.Remove') "
                    f"{type(exception).__name__}{': ' + str(exception) if len(str(exception)) > 0 else ''}; "
                    "propagating as 'Unknown'\n" +
                    ''.join(IMPORT_traceback.format_exception(exception))
                )
            raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                IMPORT_rbt_v1alpha1.errors_pb2.Unknown(),
                # TODO(benh): consider whether or not we want to
                # include the 'package.service.method' which may
                # get concatenated together forming a kind of
                # "stack trace"; while it's super helpful for
                # debugging, it does expose implementation
                # information.
                message=f"unhandled (in 'rbt.std.collections.ordered_map.v1.Node.Remove') {type(exception).__name__}: {exception}"
            )

    @IMPORT_reboot_aio_tracing.function_span(
        # We expect an `EffectValidationRetry` exception; that's not an error.
        set_status_on_exception=False
    )
    async def _Remove(
        self,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRemoveRequest,
        context: IMPORT_reboot_aio_contexts.TransactionContext,
        *,
        validating_effects: bool,
        grpc_context: IMPORT_typing.Optional[IMPORT_grpc.aio.ServicerContext] = None,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRemoveResponse:
        # Try to verify the token if a token verifier exists.
        context.auth = await self._maybe_verify_token(
            headers=context._headers, method='Remove'
        )

        # Check if we already have performed this mutation!
        #
        # We do this _before_ calling 'transactionally()' because
        # if this call is for a transaction method _and_ we've
        # already performed the transaction then we don't want to
        # become a transaction participant (again) we just want to
        # return the transaction's response.
        idempotent_mutation = self._state_manager.check_for_idempotent_mutation(
            context
        )

        if idempotent_mutation is not None:
            response = rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRemoveResponse()
            response.ParseFromString(idempotent_mutation.response)
            return response

        async with self._state_manager.transactionally(
            context,
            self.tasks_dispatcher,
            aborted_type=Node.RemoveAborted,
        ) as transaction:
            if transaction is not None:
                context.participants.add(
                    self._servicer.__state_type_name__, context._state_ref
                )
            assert transaction is not None
            async with self._state_manager.transaction(
                context,
                self._servicer.__state_type__,
                transaction,
                authorize=self._maybe_authorize(
                    method_name='rbt.std.collections.ordered_map.v1.NodeMethods.Remove',
                    headers=context._headers,
                    auth=context.auth,
                    request=request,
                ),
                from_constructor=False,
                requires_constructor=False
            ) as (state, complete):

                response = await self.__Remove(
                    context,
                    state,
                    request,
                    validating_effects=validating_effects,
                )

                await complete(
                    IMPORT_reboot_aio_state_managers.Effects(
                        state=state,
                        response=response,
                    )
                )
                return response

    async def _schedule_Remove(
        self,
        *,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRemoveRequest,
        headers: IMPORT_reboot_aio_headers.Headers,
        grpc_context: IMPORT_grpc.aio.ServicerContext,
    ) -> tuple[IMPORT_reboot_aio_contexts.WriterContext, rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRemoveResponse]:
        context: IMPORT_reboot_aio_contexts.WriterContext = self.create_context(
            headers=headers,
            state_type_name = IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.Node'),
            method='Remove',
            context_type=IMPORT_reboot_aio_contexts.WriterContext,
        )
        response = rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRemoveResponse()

        # Check if we already have performed this mutation!
        #
        # We do this _before_ calling 'transactionally()' because
        # if this call is for a transaction method _and_ we've
        # already performed the transaction then we don't want to
        # become a transaction participant (again) we just want to
        # return the transaction's response.
        idempotent_mutation = self._state_manager.check_for_idempotent_mutation(
            context
        )

        if idempotent_mutation is not None:
            response.ParseFromString(idempotent_mutation.response)

            # We should have only scheduled a single task!
            assert len(idempotent_mutation.task_ids) == 1
            assert grpc_context is not None
            grpc_context.set_trailing_metadata(
                grpc_context.trailing_metadata() +
                (
                    (
                        IMPORT_reboot_aio_headers.TASK_ID_UUID,
                        str(IMPORT_uuid.UUID(bytes=idempotent_mutation.task_ids[0].task_uuid))
                    ),
                )
            )

            return context, response

        async with self._state_manager.transactionally(
            context,
            self.tasks_dispatcher,
            aborted_type=Node.RemoveAborted,
        ) as transaction:
            if transaction is not None:
                context.participants.add(
                    self._servicer.__state_type_name__, context._state_ref
                )

            # Try to verify the token if a token verifier exists.
            context.auth = await self._maybe_verify_token(
                headers=headers, method='Remove'
            )

            async with self._state_manager.writer(
                context,
                self._servicer.__state_type__,
                self.tasks_dispatcher,
                transaction=transaction,
                authorize=self._maybe_authorize(
                    method_name='rbt.std.collections.ordered_map.v1.NodeMethods.Remove',
                    headers=context._headers,
                    auth=context.auth,
                    request=request,
                ),
                from_constructor=False,
                requires_constructor=False
            ) as (state, writer):

                task = await NodeServicerTasks(
                    context=context,
                    state_ref=context._state_ref,
                ).Remove(
                    request,
                    schedule=context._headers.task_schedule,
                )

                effects = IMPORT_reboot_aio_state_managers.Effects(
                    response=response,
                    state=state,
                    tasks=[task],
                )

                assert effects.tasks is not None

                await writer.complete(effects)

                assert grpc_context is not None

                grpc_context.set_trailing_metadata(
                    grpc_context.trailing_metadata() +
                    (
                        (
                            IMPORT_reboot_aio_headers.TASK_ID_UUID,
                            str(IMPORT_uuid.UUID(bytes=task.task_id.task_uuid))
                        ),
                    )
                )

                return context, response

        return context, response


    # Entrypoint for non-reactive network calls (i.e. typical gRPC calls).
    async def Remove(
        self,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRemoveRequest,
        grpc_context: IMPORT_grpc.aio.ServicerContext,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRemoveResponse:
        headers = IMPORT_reboot_aio_headers.Headers.from_grpc_context(grpc_context)
        assert headers.application_id is not None  # Guaranteed by `Headers`.

        # Confirm whether this is the right consensus to be serving this
        # request.
        authoritative_consensus = self.placement_client.consensus_for_actor(
            headers.application_id,
            headers.state_ref,
        )
        if authoritative_consensus != self.consensus_id:
            # This is NOT the correct consensus. Fail.
            await grpc_context.abort(
                IMPORT_grpc.StatusCode.UNAVAILABLE,
                f"Consensus '{self.consensus_id}' is not authoritative for this "
                f"request; consensus '{authoritative_consensus}' is.",
            )
            raise  # Unreachable but necessary for mypy.

        @IMPORT_reboot_aio_internals_middleware.maybe_run_function_twice_to_validate_effects
        async def _run(
            validating_effects: bool,
        ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRemoveResponse:
            context: IMPORT_typing.Optional[IMPORT_reboot_aio_contexts.Context] = None
            try:
                if headers.task_schedule is not None:
                    context, response = await self._schedule_Remove(
                        headers=headers,
                        request=request,
                        grpc_context=grpc_context,
                    )
                    return response

                context = self.create_context(
                    headers=headers,
                    state_type_name = IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.Node'),
                    method='Remove',
                    context_type=IMPORT_reboot_aio_contexts.TransactionContext,
                )
                assert context is not None

                return await self._Remove(
                    request,
                    context,
                    validating_effects=validating_effects,
                    grpc_context=grpc_context,
                )
            except IMPORT_reboot_aio_contexts.EffectValidationRetry:
                # Doing effect validation, just let this propagate.
                raise
            except IMPORT_rebootdev.aio.aborted.Aborted as aborted:
                status = IMPORT_rpc_status_sync.to_status(aborted.to_status())
                # Need to add transaction participants here because
                # calling `grpc_context.abort_with_status()` will
                # ignore any other trailing metadata.
                if context is not None and context.transaction_id is not None:
                    status = status._replace(
                        trailing_metadata=status.trailing_metadata + context.participants.to_grpc_metadata()
                    )
                await grpc_context.abort_with_status(status)
                raise  # Unreachable but necessary for mypy.
            except IMPORT_asyncio.CancelledError:
                # It's pretty normal for an RPC to be cancelled; it's not useful to
                # print a stack trace.
                raise
            except BaseException as exception:
                # Print the exception stack trace for easier debugging. Note
                # that we don't include the stack trace in an error message
                # for the same reason that gRPC doesn't do so by default,
                # see https://github.com/grpc/grpc/issues/14897, but since this
                # should only get logged on the server side it is safe.
                logger.warning(
                    'Unhandled exception\n' +
                    ''.join(IMPORT_traceback.format_exc() if IMPORT_reboot_nodejs_python.should_print_stacktrace() else [f"{type(exception).__name__}: {exception}"])
                )

                # Re-raise the exception for gRPC to handle!
                #
                # TODO: gRPC will print a stack trace from this
                # exception which we don't want if we're executing via
                # Node.js.
                raise
            finally:
                if context is not None and context.transaction_id is not None:
                    # Propagate transaction participants.
                    grpc_context.set_trailing_metadata(
                        grpc_context.trailing_metadata() +
                        context.participants.to_grpc_metadata()
                    )

        with IMPORT_reboot_aio_tracing.context_from_headers(headers):
            return await _run()

    async def __Range(
        self,
        context: IMPORT_reboot_aio_contexts.ReaderContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRangeRequest,
        *,
        validating_effects: bool,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRangeResponse:
        try:
            response = (
                await self._servicer._Range(
                    context=context,
                    state=state,
                    request=request
                )
            )
            IMPORT_reboot_aio_types.assert_type(
                response,
                [rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRangeResponse],
            )
            self.maybe_raise_effect_validation_retry(
                logger=logger,
                idempotency_manager=context,
                method_name='Node.Range',
                validating_effects=validating_effects,
                context=context,
            )
            return response
        except IMPORT_reboot_aio_contexts.RetryReactively:
            # Retrying reactively, just let this propagate.
            raise
        except IMPORT_reboot_aio_contexts.EffectValidationRetry:
            # Doing effect validation, just let this propagate.
            raise
        except IMPORT_rebootdev.aio.aborted.Aborted as aborted:
            # If the caller aborted due to a retryable error, just
            # propagate the aborted instead of propagating `Unknown`
            # so that a client can transparently retry.
            if IMPORT_rebootdev.aio.aborted.is_retryable(aborted):
                raise aborted
            # Log any _unhandled_ abort stack traces to make it
            # easier for debugging.
            #
            # NOTE: we don't log if we're a task as it will be logged
            # in `public/rebootdev/aio/internals/tasks_dispatcher.py` instead.
            aborted_type: IMPORT_typing.Optional[type] = None
            aborted_type = Node.RangeAborted
            if isinstance(aborted, IMPORT_rebootdev.aio.aborted.SystemAborted):
                # Not logging when within `node` as we already log there.
                if IMPORT_reboot_nodejs_python.should_print_stacktrace():
                    logger.warning(
                        f"Unhandled (in 'rbt.std.collections.ordered_map.v1.Node.Range') {aborted}; propagating as 'Unknown'\n" +
                        ''.join(IMPORT_traceback.format_exception(aborted))
                    )
                raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                    IMPORT_rbt_v1alpha1.errors_pb2.Unknown(),
                    # TODO(benh): consider whether or not we want to
                    # include the 'package.service.method' which may
                    # get concatenated together forming a kind of
                    # "stack trace"; while it's super helpful for
                    # debugging, it does expose implementation
                    # information.
                    message=f"unhandled (in 'rbt.std.collections.ordered_map.v1.Node.Range') {aborted}"
                )
            else:
                if (
                    aborted_type is not None and
                    not isinstance(aborted, aborted_type) and
                    aborted_type.is_declared_error(aborted.error)
                ):
                    # We propagate declared errors that might have
                    # come from another call, i.e., we might have an
                    # `Aborted` but not for this method but the
                    # `Aborted` that we have has an error that this
                    # method declared. This allows a developer to
                    # simply add the declared error to their `.proto`
                    # file rather than having to catch and re-raise
                    # the error with their own aborted type.
                    if context.task is None:
                        logger.warning(
                            f"Propagating unhandled but declared error (in 'rbt.std.collections.ordered_map.v1.Node.Range') {aborted}"
                        )
                elif (
                    aborted_type is None or
                    not isinstance(aborted, aborted_type)
                ):
                    # Not logging when within `node` as we already log there.
                    if IMPORT_reboot_nodejs_python.should_print_stacktrace():
                        logger.warning(
                            f"Unhandled (in 'rbt.std.collections.ordered_map.v1.Node.Range') {aborted}; propagating as 'Unknown'\n" +
                            ''.join(IMPORT_traceback.format_exception(aborted))
                        )
                    # If this wasn't a declared error than we
                    # propagate it as `Unknown`.
                    raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                        IMPORT_rbt_v1alpha1.errors_pb2.Unknown(),
                        # TODO(benh): consider whether or not we want to
                        # include the 'package.service.method' which may
                        # get concatenated together forming a kind of
                        # "stack trace"; while it's super helpful for
                        # debugging, it does expose implementation
                        # information.
                        message=f"unhandled (in 'rbt.std.collections.ordered_map.v1.Node.Range') {aborted}"
                    )

            raise
        except IMPORT_asyncio.CancelledError:
            # It's pretty normal for an RPC to be cancelled; it's not useful to
            # print a stack trace.
            raise
        except IMPORT_google_protobuf_message.DecodeError as decode_error:
            # We usually see this error when we are trying to construct a proto
            # message which is too deeply nested: protobuf has a limit of 100
            # nested messages. See the limits here:
            #   https://protobuf.dev/programming-guides/proto-limits/

            if IMPORT_reboot_nodejs_python.should_print_stacktrace():
                logger.warning(
                    "Unhandled (in 'rbt.std.collections.ordered_map.v1.Node.Range') "
                    f"{type(decode_error).__name__}{': ' + str(decode_error) if len(str(decode_error)) > 0 else ''}; "
                    "This is usually caused by a deeply nested protobuf message, which is not supported by protobuf.\n"
                    "See the limits here: https://protobuf.dev/programming-guides/proto-limits/" +
                    ''.join(IMPORT_traceback.format_exception(decode_error))
                )
            raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                IMPORT_rbt_v1alpha1.errors_pb2.Unknown(),
                message=f"unhandled (in 'rbt.std.collections.ordered_map.v1.Node.Range') {decode_error}; "
                        "This is usually caused by a deeply nested protobuf message, which is not supported by protobuf.\n"
                        "See the limits here: https://protobuf.dev/programming-guides/proto-limits/"
            )
        except BaseException as exception:
            # Not logging when within `node` as we already log there.
            if IMPORT_reboot_nodejs_python.should_print_stacktrace():
                logger.warning(
                    "Unhandled (in 'rbt.std.collections.ordered_map.v1.Node.Range') "
                    f"{type(exception).__name__}{': ' + str(exception) if len(str(exception)) > 0 else ''}; "
                    "propagating as 'Unknown'\n" +
                    ''.join(IMPORT_traceback.format_exception(exception))
                )
            raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                IMPORT_rbt_v1alpha1.errors_pb2.Unknown(),
                # TODO(benh): consider whether or not we want to
                # include the 'package.service.method' which may
                # get concatenated together forming a kind of
                # "stack trace"; while it's super helpful for
                # debugging, it does expose implementation
                # information.
                message=f"unhandled (in 'rbt.std.collections.ordered_map.v1.Node.Range') {type(exception).__name__}: {exception}"
            )

    @IMPORT_reboot_aio_tracing.function_span(
        # We expect an `EffectValidationRetry` exception; that's not an error.
        set_status_on_exception=False
    )
    async def _Range(
        self,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRangeRequest,
        context: IMPORT_reboot_aio_contexts.ReaderContext,
        *,
        validating_effects: bool,
        grpc_context: IMPORT_typing.Optional[IMPORT_grpc.aio.ServicerContext] = None,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRangeResponse:
        # Try to verify the token if a token verifier exists.
        context.auth = await self._maybe_verify_token(
            headers=context._headers, method='Range'
        )


        async with self._state_manager.transactionally(
            context,
            self.tasks_dispatcher,
            aborted_type=Node.RangeAborted,
        ) as transaction:
            if transaction is not None:
                context.participants.add(
                    self._servicer.__state_type_name__, context._state_ref
                )
            authorizer = self._maybe_authorize(
                method_name='rbt.std.collections.ordered_map.v1.NodeMethods.Range',
                headers=context._headers,
                auth=context.auth,
                request=request,
            )
            async with self._state_manager.reader(
                context,
                self._servicer.__state_type__,
                authorize=authorizer,
            ) as state:
                response = await self.__Range(
                    context,
                    state,
                    request,
                    validating_effects=validating_effects,
                )
                return response

    async def _schedule_Range(
        self,
        *,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRangeRequest,
        headers: IMPORT_reboot_aio_headers.Headers,
        grpc_context: IMPORT_grpc.aio.ServicerContext,
    ) -> tuple[IMPORT_reboot_aio_contexts.WriterContext, rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRangeResponse]:
        context: IMPORT_reboot_aio_contexts.WriterContext = self.create_context(
            headers=headers,
            state_type_name = IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.Node'),
            method='Range',
            context_type=IMPORT_reboot_aio_contexts.WriterContext,
        )
        response = rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRangeResponse()


        async with self._state_manager.transactionally(
            context,
            self.tasks_dispatcher,
            aborted_type=Node.RangeAborted,
        ) as transaction:
            if transaction is not None:
                context.participants.add(
                    self._servicer.__state_type_name__, context._state_ref
                )

            # Try to verify the token if a token verifier exists.
            context.auth = await self._maybe_verify_token(
                headers=headers, method='Range'
            )

            async with self._state_manager.writer(
                context,
                self._servicer.__state_type__,
                self.tasks_dispatcher,
                transaction=transaction,
                authorize=self._maybe_authorize(
                    method_name='rbt.std.collections.ordered_map.v1.NodeMethods.Range',
                    headers=context._headers,
                    auth=context.auth,
                    request=request,
                ),
                from_constructor=False,
                requires_constructor=False
            ) as (state, writer):

                task = await NodeServicerTasks(
                    context=context,
                    state_ref=context._state_ref,
                ).Range(
                    request,
                    schedule=context._headers.task_schedule,
                )

                effects = IMPORT_reboot_aio_state_managers.Effects(
                    response=response,
                    state=state,
                    tasks=[task],
                )

                assert effects.tasks is not None

                await writer.complete(effects)

                assert grpc_context is not None

                grpc_context.set_trailing_metadata(
                    grpc_context.trailing_metadata() +
                    (
                        (
                            IMPORT_reboot_aio_headers.TASK_ID_UUID,
                            str(IMPORT_uuid.UUID(bytes=task.task_id.task_uuid))
                        ),
                    )
                )

                return context, response

        return context, response


    # Entrypoint for non-reactive network calls (i.e. typical gRPC calls).
    async def Range(
        self,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRangeRequest,
        grpc_context: IMPORT_grpc.aio.ServicerContext,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRangeResponse:
        headers = IMPORT_reboot_aio_headers.Headers.from_grpc_context(grpc_context)
        assert headers.application_id is not None  # Guaranteed by `Headers`.

        # Confirm whether this is the right consensus to be serving this
        # request.
        authoritative_consensus = self.placement_client.consensus_for_actor(
            headers.application_id,
            headers.state_ref,
        )
        if authoritative_consensus != self.consensus_id:
            # This is NOT the correct consensus. Fail.
            await grpc_context.abort(
                IMPORT_grpc.StatusCode.UNAVAILABLE,
                f"Consensus '{self.consensus_id}' is not authoritative for this "
                f"request; consensus '{authoritative_consensus}' is.",
            )
            raise  # Unreachable but necessary for mypy.

        @IMPORT_reboot_aio_internals_middleware.maybe_run_function_twice_to_validate_effects
        async def _run(
            validating_effects: bool,
        ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRangeResponse:
            context: IMPORT_typing.Optional[IMPORT_reboot_aio_contexts.Context] = None
            try:
                if headers.task_schedule is not None:
                    context, response = await self._schedule_Range(
                        headers=headers,
                        request=request,
                        grpc_context=grpc_context,
                    )
                    return response

                context = self.create_context(
                    headers=headers,
                    state_type_name = IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.Node'),
                    method='Range',
                    context_type=IMPORT_reboot_aio_contexts.ReaderContext,
                )
                assert context is not None

                return await self._Range(
                    request,
                    context,
                    validating_effects=validating_effects,
                    grpc_context=grpc_context,
                )
            except IMPORT_reboot_aio_contexts.EffectValidationRetry:
                # Doing effect validation, just let this propagate.
                raise
            except IMPORT_rebootdev.aio.aborted.Aborted as aborted:
                status = IMPORT_rpc_status_sync.to_status(aborted.to_status())
                # Need to add transaction participants here because
                # calling `grpc_context.abort_with_status()` will
                # ignore any other trailing metadata.
                if context is not None and context.transaction_id is not None:
                    status = status._replace(
                        trailing_metadata=status.trailing_metadata + context.participants.to_grpc_metadata()
                    )
                await grpc_context.abort_with_status(status)
                raise  # Unreachable but necessary for mypy.
            except IMPORT_asyncio.CancelledError:
                # It's pretty normal for an RPC to be cancelled; it's not useful to
                # print a stack trace.
                raise
            except BaseException as exception:
                # Print the exception stack trace for easier debugging. Note
                # that we don't include the stack trace in an error message
                # for the same reason that gRPC doesn't do so by default,
                # see https://github.com/grpc/grpc/issues/14897, but since this
                # should only get logged on the server side it is safe.
                logger.warning(
                    'Unhandled exception\n' +
                    ''.join(IMPORT_traceback.format_exc() if IMPORT_reboot_nodejs_python.should_print_stacktrace() else [f"{type(exception).__name__}: {exception}"])
                )

                # Re-raise the exception for gRPC to handle!
                #
                # TODO: gRPC will print a stack trace from this
                # exception which we don't want if we're executing via
                # Node.js.
                raise
            finally:
                if context is not None and context.transaction_id is not None:
                    # Propagate transaction participants.
                    grpc_context.set_trailing_metadata(
                        grpc_context.trailing_metadata() +
                        context.participants.to_grpc_metadata()
                    )

        with IMPORT_reboot_aio_tracing.context_from_headers(headers):
            return await _run()

    async def __ReverseRange(
        self,
        context: IMPORT_reboot_aio_contexts.ReaderContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeReverseRangeRequest,
        *,
        validating_effects: bool,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeReverseRangeResponse:
        try:
            response = (
                await self._servicer._ReverseRange(
                    context=context,
                    state=state,
                    request=request
                )
            )
            IMPORT_reboot_aio_types.assert_type(
                response,
                [rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeReverseRangeResponse],
            )
            self.maybe_raise_effect_validation_retry(
                logger=logger,
                idempotency_manager=context,
                method_name='Node.ReverseRange',
                validating_effects=validating_effects,
                context=context,
            )
            return response
        except IMPORT_reboot_aio_contexts.RetryReactively:
            # Retrying reactively, just let this propagate.
            raise
        except IMPORT_reboot_aio_contexts.EffectValidationRetry:
            # Doing effect validation, just let this propagate.
            raise
        except IMPORT_rebootdev.aio.aborted.Aborted as aborted:
            # If the caller aborted due to a retryable error, just
            # propagate the aborted instead of propagating `Unknown`
            # so that a client can transparently retry.
            if IMPORT_rebootdev.aio.aborted.is_retryable(aborted):
                raise aborted
            # Log any _unhandled_ abort stack traces to make it
            # easier for debugging.
            #
            # NOTE: we don't log if we're a task as it will be logged
            # in `public/rebootdev/aio/internals/tasks_dispatcher.py` instead.
            aborted_type: IMPORT_typing.Optional[type] = None
            aborted_type = Node.ReverseRangeAborted
            if isinstance(aborted, IMPORT_rebootdev.aio.aborted.SystemAborted):
                # Not logging when within `node` as we already log there.
                if IMPORT_reboot_nodejs_python.should_print_stacktrace():
                    logger.warning(
                        f"Unhandled (in 'rbt.std.collections.ordered_map.v1.Node.ReverseRange') {aborted}; propagating as 'Unknown'\n" +
                        ''.join(IMPORT_traceback.format_exception(aborted))
                    )
                raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                    IMPORT_rbt_v1alpha1.errors_pb2.Unknown(),
                    # TODO(benh): consider whether or not we want to
                    # include the 'package.service.method' which may
                    # get concatenated together forming a kind of
                    # "stack trace"; while it's super helpful for
                    # debugging, it does expose implementation
                    # information.
                    message=f"unhandled (in 'rbt.std.collections.ordered_map.v1.Node.ReverseRange') {aborted}"
                )
            else:
                if (
                    aborted_type is not None and
                    not isinstance(aborted, aborted_type) and
                    aborted_type.is_declared_error(aborted.error)
                ):
                    # We propagate declared errors that might have
                    # come from another call, i.e., we might have an
                    # `Aborted` but not for this method but the
                    # `Aborted` that we have has an error that this
                    # method declared. This allows a developer to
                    # simply add the declared error to their `.proto`
                    # file rather than having to catch and re-raise
                    # the error with their own aborted type.
                    if context.task is None:
                        logger.warning(
                            f"Propagating unhandled but declared error (in 'rbt.std.collections.ordered_map.v1.Node.ReverseRange') {aborted}"
                        )
                elif (
                    aborted_type is None or
                    not isinstance(aborted, aborted_type)
                ):
                    # Not logging when within `node` as we already log there.
                    if IMPORT_reboot_nodejs_python.should_print_stacktrace():
                        logger.warning(
                            f"Unhandled (in 'rbt.std.collections.ordered_map.v1.Node.ReverseRange') {aborted}; propagating as 'Unknown'\n" +
                            ''.join(IMPORT_traceback.format_exception(aborted))
                        )
                    # If this wasn't a declared error than we
                    # propagate it as `Unknown`.
                    raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                        IMPORT_rbt_v1alpha1.errors_pb2.Unknown(),
                        # TODO(benh): consider whether or not we want to
                        # include the 'package.service.method' which may
                        # get concatenated together forming a kind of
                        # "stack trace"; while it's super helpful for
                        # debugging, it does expose implementation
                        # information.
                        message=f"unhandled (in 'rbt.std.collections.ordered_map.v1.Node.ReverseRange') {aborted}"
                    )

            raise
        except IMPORT_asyncio.CancelledError:
            # It's pretty normal for an RPC to be cancelled; it's not useful to
            # print a stack trace.
            raise
        except IMPORT_google_protobuf_message.DecodeError as decode_error:
            # We usually see this error when we are trying to construct a proto
            # message which is too deeply nested: protobuf has a limit of 100
            # nested messages. See the limits here:
            #   https://protobuf.dev/programming-guides/proto-limits/

            if IMPORT_reboot_nodejs_python.should_print_stacktrace():
                logger.warning(
                    "Unhandled (in 'rbt.std.collections.ordered_map.v1.Node.ReverseRange') "
                    f"{type(decode_error).__name__}{': ' + str(decode_error) if len(str(decode_error)) > 0 else ''}; "
                    "This is usually caused by a deeply nested protobuf message, which is not supported by protobuf.\n"
                    "See the limits here: https://protobuf.dev/programming-guides/proto-limits/" +
                    ''.join(IMPORT_traceback.format_exception(decode_error))
                )
            raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                IMPORT_rbt_v1alpha1.errors_pb2.Unknown(),
                message=f"unhandled (in 'rbt.std.collections.ordered_map.v1.Node.ReverseRange') {decode_error}; "
                        "This is usually caused by a deeply nested protobuf message, which is not supported by protobuf.\n"
                        "See the limits here: https://protobuf.dev/programming-guides/proto-limits/"
            )
        except BaseException as exception:
            # Not logging when within `node` as we already log there.
            if IMPORT_reboot_nodejs_python.should_print_stacktrace():
                logger.warning(
                    "Unhandled (in 'rbt.std.collections.ordered_map.v1.Node.ReverseRange') "
                    f"{type(exception).__name__}{': ' + str(exception) if len(str(exception)) > 0 else ''}; "
                    "propagating as 'Unknown'\n" +
                    ''.join(IMPORT_traceback.format_exception(exception))
                )
            raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                IMPORT_rbt_v1alpha1.errors_pb2.Unknown(),
                # TODO(benh): consider whether or not we want to
                # include the 'package.service.method' which may
                # get concatenated together forming a kind of
                # "stack trace"; while it's super helpful for
                # debugging, it does expose implementation
                # information.
                message=f"unhandled (in 'rbt.std.collections.ordered_map.v1.Node.ReverseRange') {type(exception).__name__}: {exception}"
            )

    @IMPORT_reboot_aio_tracing.function_span(
        # We expect an `EffectValidationRetry` exception; that's not an error.
        set_status_on_exception=False
    )
    async def _ReverseRange(
        self,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeReverseRangeRequest,
        context: IMPORT_reboot_aio_contexts.ReaderContext,
        *,
        validating_effects: bool,
        grpc_context: IMPORT_typing.Optional[IMPORT_grpc.aio.ServicerContext] = None,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeReverseRangeResponse:
        # Try to verify the token if a token verifier exists.
        context.auth = await self._maybe_verify_token(
            headers=context._headers, method='ReverseRange'
        )


        async with self._state_manager.transactionally(
            context,
            self.tasks_dispatcher,
            aborted_type=Node.ReverseRangeAborted,
        ) as transaction:
            if transaction is not None:
                context.participants.add(
                    self._servicer.__state_type_name__, context._state_ref
                )
            authorizer = self._maybe_authorize(
                method_name='rbt.std.collections.ordered_map.v1.NodeMethods.ReverseRange',
                headers=context._headers,
                auth=context.auth,
                request=request,
            )
            async with self._state_manager.reader(
                context,
                self._servicer.__state_type__,
                authorize=authorizer,
            ) as state:
                response = await self.__ReverseRange(
                    context,
                    state,
                    request,
                    validating_effects=validating_effects,
                )
                return response

    async def _schedule_ReverseRange(
        self,
        *,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeReverseRangeRequest,
        headers: IMPORT_reboot_aio_headers.Headers,
        grpc_context: IMPORT_grpc.aio.ServicerContext,
    ) -> tuple[IMPORT_reboot_aio_contexts.WriterContext, rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeReverseRangeResponse]:
        context: IMPORT_reboot_aio_contexts.WriterContext = self.create_context(
            headers=headers,
            state_type_name = IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.Node'),
            method='ReverseRange',
            context_type=IMPORT_reboot_aio_contexts.WriterContext,
        )
        response = rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeReverseRangeResponse()


        async with self._state_manager.transactionally(
            context,
            self.tasks_dispatcher,
            aborted_type=Node.ReverseRangeAborted,
        ) as transaction:
            if transaction is not None:
                context.participants.add(
                    self._servicer.__state_type_name__, context._state_ref
                )

            # Try to verify the token if a token verifier exists.
            context.auth = await self._maybe_verify_token(
                headers=headers, method='ReverseRange'
            )

            async with self._state_manager.writer(
                context,
                self._servicer.__state_type__,
                self.tasks_dispatcher,
                transaction=transaction,
                authorize=self._maybe_authorize(
                    method_name='rbt.std.collections.ordered_map.v1.NodeMethods.ReverseRange',
                    headers=context._headers,
                    auth=context.auth,
                    request=request,
                ),
                from_constructor=False,
                requires_constructor=False
            ) as (state, writer):

                task = await NodeServicerTasks(
                    context=context,
                    state_ref=context._state_ref,
                ).ReverseRange(
                    request,
                    schedule=context._headers.task_schedule,
                )

                effects = IMPORT_reboot_aio_state_managers.Effects(
                    response=response,
                    state=state,
                    tasks=[task],
                )

                assert effects.tasks is not None

                await writer.complete(effects)

                assert grpc_context is not None

                grpc_context.set_trailing_metadata(
                    grpc_context.trailing_metadata() +
                    (
                        (
                            IMPORT_reboot_aio_headers.TASK_ID_UUID,
                            str(IMPORT_uuid.UUID(bytes=task.task_id.task_uuid))
                        ),
                    )
                )

                return context, response

        return context, response


    # Entrypoint for non-reactive network calls (i.e. typical gRPC calls).
    async def ReverseRange(
        self,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeReverseRangeRequest,
        grpc_context: IMPORT_grpc.aio.ServicerContext,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeReverseRangeResponse:
        headers = IMPORT_reboot_aio_headers.Headers.from_grpc_context(grpc_context)
        assert headers.application_id is not None  # Guaranteed by `Headers`.

        # Confirm whether this is the right consensus to be serving this
        # request.
        authoritative_consensus = self.placement_client.consensus_for_actor(
            headers.application_id,
            headers.state_ref,
        )
        if authoritative_consensus != self.consensus_id:
            # This is NOT the correct consensus. Fail.
            await grpc_context.abort(
                IMPORT_grpc.StatusCode.UNAVAILABLE,
                f"Consensus '{self.consensus_id}' is not authoritative for this "
                f"request; consensus '{authoritative_consensus}' is.",
            )
            raise  # Unreachable but necessary for mypy.

        @IMPORT_reboot_aio_internals_middleware.maybe_run_function_twice_to_validate_effects
        async def _run(
            validating_effects: bool,
        ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeReverseRangeResponse:
            context: IMPORT_typing.Optional[IMPORT_reboot_aio_contexts.Context] = None
            try:
                if headers.task_schedule is not None:
                    context, response = await self._schedule_ReverseRange(
                        headers=headers,
                        request=request,
                        grpc_context=grpc_context,
                    )
                    return response

                context = self.create_context(
                    headers=headers,
                    state_type_name = IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.Node'),
                    method='ReverseRange',
                    context_type=IMPORT_reboot_aio_contexts.ReaderContext,
                )
                assert context is not None

                return await self._ReverseRange(
                    request,
                    context,
                    validating_effects=validating_effects,
                    grpc_context=grpc_context,
                )
            except IMPORT_reboot_aio_contexts.EffectValidationRetry:
                # Doing effect validation, just let this propagate.
                raise
            except IMPORT_rebootdev.aio.aborted.Aborted as aborted:
                status = IMPORT_rpc_status_sync.to_status(aborted.to_status())
                # Need to add transaction participants here because
                # calling `grpc_context.abort_with_status()` will
                # ignore any other trailing metadata.
                if context is not None and context.transaction_id is not None:
                    status = status._replace(
                        trailing_metadata=status.trailing_metadata + context.participants.to_grpc_metadata()
                    )
                await grpc_context.abort_with_status(status)
                raise  # Unreachable but necessary for mypy.
            except IMPORT_asyncio.CancelledError:
                # It's pretty normal for an RPC to be cancelled; it's not useful to
                # print a stack trace.
                raise
            except BaseException as exception:
                # Print the exception stack trace for easier debugging. Note
                # that we don't include the stack trace in an error message
                # for the same reason that gRPC doesn't do so by default,
                # see https://github.com/grpc/grpc/issues/14897, but since this
                # should only get logged on the server side it is safe.
                logger.warning(
                    'Unhandled exception\n' +
                    ''.join(IMPORT_traceback.format_exc() if IMPORT_reboot_nodejs_python.should_print_stacktrace() else [f"{type(exception).__name__}: {exception}"])
                )

                # Re-raise the exception for gRPC to handle!
                #
                # TODO: gRPC will print a stack trace from this
                # exception which we don't want if we're executing via
                # Node.js.
                raise
            finally:
                if context is not None and context.transaction_id is not None:
                    # Propagate transaction participants.
                    grpc_context.set_trailing_metadata(
                        grpc_context.trailing_metadata() +
                        context.participants.to_grpc_metadata()
                    )

        with IMPORT_reboot_aio_tracing.context_from_headers(headers):
            return await _run()

    async def __Stringify(
        self,
        context: IMPORT_reboot_aio_contexts.ReaderContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeStringifyRequest,
        *,
        validating_effects: bool,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeStringifyResponse:
        try:
            response = (
                await self._servicer._Stringify(
                    context=context,
                    state=state,
                    request=request
                )
            )
            IMPORT_reboot_aio_types.assert_type(
                response,
                [rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeStringifyResponse],
            )
            self.maybe_raise_effect_validation_retry(
                logger=logger,
                idempotency_manager=context,
                method_name='Node.Stringify',
                validating_effects=validating_effects,
                context=context,
            )
            return response
        except IMPORT_reboot_aio_contexts.RetryReactively:
            # Retrying reactively, just let this propagate.
            raise
        except IMPORT_reboot_aio_contexts.EffectValidationRetry:
            # Doing effect validation, just let this propagate.
            raise
        except IMPORT_rebootdev.aio.aborted.Aborted as aborted:
            # If the caller aborted due to a retryable error, just
            # propagate the aborted instead of propagating `Unknown`
            # so that a client can transparently retry.
            if IMPORT_rebootdev.aio.aborted.is_retryable(aborted):
                raise aborted
            # Log any _unhandled_ abort stack traces to make it
            # easier for debugging.
            #
            # NOTE: we don't log if we're a task as it will be logged
            # in `public/rebootdev/aio/internals/tasks_dispatcher.py` instead.
            aborted_type: IMPORT_typing.Optional[type] = None
            aborted_type = Node.StringifyAborted
            if isinstance(aborted, IMPORT_rebootdev.aio.aborted.SystemAborted):
                # Not logging when within `node` as we already log there.
                if IMPORT_reboot_nodejs_python.should_print_stacktrace():
                    logger.warning(
                        f"Unhandled (in 'rbt.std.collections.ordered_map.v1.Node.Stringify') {aborted}; propagating as 'Unknown'\n" +
                        ''.join(IMPORT_traceback.format_exception(aborted))
                    )
                raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                    IMPORT_rbt_v1alpha1.errors_pb2.Unknown(),
                    # TODO(benh): consider whether or not we want to
                    # include the 'package.service.method' which may
                    # get concatenated together forming a kind of
                    # "stack trace"; while it's super helpful for
                    # debugging, it does expose implementation
                    # information.
                    message=f"unhandled (in 'rbt.std.collections.ordered_map.v1.Node.Stringify') {aborted}"
                )
            else:
                if (
                    aborted_type is not None and
                    not isinstance(aborted, aborted_type) and
                    aborted_type.is_declared_error(aborted.error)
                ):
                    # We propagate declared errors that might have
                    # come from another call, i.e., we might have an
                    # `Aborted` but not for this method but the
                    # `Aborted` that we have has an error that this
                    # method declared. This allows a developer to
                    # simply add the declared error to their `.proto`
                    # file rather than having to catch and re-raise
                    # the error with their own aborted type.
                    if context.task is None:
                        logger.warning(
                            f"Propagating unhandled but declared error (in 'rbt.std.collections.ordered_map.v1.Node.Stringify') {aborted}"
                        )
                elif (
                    aborted_type is None or
                    not isinstance(aborted, aborted_type)
                ):
                    # Not logging when within `node` as we already log there.
                    if IMPORT_reboot_nodejs_python.should_print_stacktrace():
                        logger.warning(
                            f"Unhandled (in 'rbt.std.collections.ordered_map.v1.Node.Stringify') {aborted}; propagating as 'Unknown'\n" +
                            ''.join(IMPORT_traceback.format_exception(aborted))
                        )
                    # If this wasn't a declared error than we
                    # propagate it as `Unknown`.
                    raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                        IMPORT_rbt_v1alpha1.errors_pb2.Unknown(),
                        # TODO(benh): consider whether or not we want to
                        # include the 'package.service.method' which may
                        # get concatenated together forming a kind of
                        # "stack trace"; while it's super helpful for
                        # debugging, it does expose implementation
                        # information.
                        message=f"unhandled (in 'rbt.std.collections.ordered_map.v1.Node.Stringify') {aborted}"
                    )

            raise
        except IMPORT_asyncio.CancelledError:
            # It's pretty normal for an RPC to be cancelled; it's not useful to
            # print a stack trace.
            raise
        except IMPORT_google_protobuf_message.DecodeError as decode_error:
            # We usually see this error when we are trying to construct a proto
            # message which is too deeply nested: protobuf has a limit of 100
            # nested messages. See the limits here:
            #   https://protobuf.dev/programming-guides/proto-limits/

            if IMPORT_reboot_nodejs_python.should_print_stacktrace():
                logger.warning(
                    "Unhandled (in 'rbt.std.collections.ordered_map.v1.Node.Stringify') "
                    f"{type(decode_error).__name__}{': ' + str(decode_error) if len(str(decode_error)) > 0 else ''}; "
                    "This is usually caused by a deeply nested protobuf message, which is not supported by protobuf.\n"
                    "See the limits here: https://protobuf.dev/programming-guides/proto-limits/" +
                    ''.join(IMPORT_traceback.format_exception(decode_error))
                )
            raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                IMPORT_rbt_v1alpha1.errors_pb2.Unknown(),
                message=f"unhandled (in 'rbt.std.collections.ordered_map.v1.Node.Stringify') {decode_error}; "
                        "This is usually caused by a deeply nested protobuf message, which is not supported by protobuf.\n"
                        "See the limits here: https://protobuf.dev/programming-guides/proto-limits/"
            )
        except BaseException as exception:
            # Not logging when within `node` as we already log there.
            if IMPORT_reboot_nodejs_python.should_print_stacktrace():
                logger.warning(
                    "Unhandled (in 'rbt.std.collections.ordered_map.v1.Node.Stringify') "
                    f"{type(exception).__name__}{': ' + str(exception) if len(str(exception)) > 0 else ''}; "
                    "propagating as 'Unknown'\n" +
                    ''.join(IMPORT_traceback.format_exception(exception))
                )
            raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                IMPORT_rbt_v1alpha1.errors_pb2.Unknown(),
                # TODO(benh): consider whether or not we want to
                # include the 'package.service.method' which may
                # get concatenated together forming a kind of
                # "stack trace"; while it's super helpful for
                # debugging, it does expose implementation
                # information.
                message=f"unhandled (in 'rbt.std.collections.ordered_map.v1.Node.Stringify') {type(exception).__name__}: {exception}"
            )

    @IMPORT_reboot_aio_tracing.function_span(
        # We expect an `EffectValidationRetry` exception; that's not an error.
        set_status_on_exception=False
    )
    async def _Stringify(
        self,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeStringifyRequest,
        context: IMPORT_reboot_aio_contexts.ReaderContext,
        *,
        validating_effects: bool,
        grpc_context: IMPORT_typing.Optional[IMPORT_grpc.aio.ServicerContext] = None,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeStringifyResponse:
        # Try to verify the token if a token verifier exists.
        context.auth = await self._maybe_verify_token(
            headers=context._headers, method='Stringify'
        )


        async with self._state_manager.transactionally(
            context,
            self.tasks_dispatcher,
            aborted_type=Node.StringifyAborted,
        ) as transaction:
            if transaction is not None:
                context.participants.add(
                    self._servicer.__state_type_name__, context._state_ref
                )
            authorizer = self._maybe_authorize(
                method_name='rbt.std.collections.ordered_map.v1.NodeMethods.Stringify',
                headers=context._headers,
                auth=context.auth,
                request=request,
            )
            async with self._state_manager.reader(
                context,
                self._servicer.__state_type__,
                authorize=authorizer,
            ) as state:
                response = await self.__Stringify(
                    context,
                    state,
                    request,
                    validating_effects=validating_effects,
                )
                return response

    async def _schedule_Stringify(
        self,
        *,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeStringifyRequest,
        headers: IMPORT_reboot_aio_headers.Headers,
        grpc_context: IMPORT_grpc.aio.ServicerContext,
    ) -> tuple[IMPORT_reboot_aio_contexts.WriterContext, rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeStringifyResponse]:
        context: IMPORT_reboot_aio_contexts.WriterContext = self.create_context(
            headers=headers,
            state_type_name = IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.Node'),
            method='Stringify',
            context_type=IMPORT_reboot_aio_contexts.WriterContext,
        )
        response = rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeStringifyResponse()


        async with self._state_manager.transactionally(
            context,
            self.tasks_dispatcher,
            aborted_type=Node.StringifyAborted,
        ) as transaction:
            if transaction is not None:
                context.participants.add(
                    self._servicer.__state_type_name__, context._state_ref
                )

            # Try to verify the token if a token verifier exists.
            context.auth = await self._maybe_verify_token(
                headers=headers, method='Stringify'
            )

            async with self._state_manager.writer(
                context,
                self._servicer.__state_type__,
                self.tasks_dispatcher,
                transaction=transaction,
                authorize=self._maybe_authorize(
                    method_name='rbt.std.collections.ordered_map.v1.NodeMethods.Stringify',
                    headers=context._headers,
                    auth=context.auth,
                    request=request,
                ),
                from_constructor=False,
                requires_constructor=False
            ) as (state, writer):

                task = await NodeServicerTasks(
                    context=context,
                    state_ref=context._state_ref,
                ).Stringify(
                    request,
                    schedule=context._headers.task_schedule,
                )

                effects = IMPORT_reboot_aio_state_managers.Effects(
                    response=response,
                    state=state,
                    tasks=[task],
                )

                assert effects.tasks is not None

                await writer.complete(effects)

                assert grpc_context is not None

                grpc_context.set_trailing_metadata(
                    grpc_context.trailing_metadata() +
                    (
                        (
                            IMPORT_reboot_aio_headers.TASK_ID_UUID,
                            str(IMPORT_uuid.UUID(bytes=task.task_id.task_uuid))
                        ),
                    )
                )

                return context, response

        return context, response


    # Entrypoint for non-reactive network calls (i.e. typical gRPC calls).
    async def Stringify(
        self,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeStringifyRequest,
        grpc_context: IMPORT_grpc.aio.ServicerContext,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeStringifyResponse:
        headers = IMPORT_reboot_aio_headers.Headers.from_grpc_context(grpc_context)
        assert headers.application_id is not None  # Guaranteed by `Headers`.

        # Confirm whether this is the right consensus to be serving this
        # request.
        authoritative_consensus = self.placement_client.consensus_for_actor(
            headers.application_id,
            headers.state_ref,
        )
        if authoritative_consensus != self.consensus_id:
            # This is NOT the correct consensus. Fail.
            await grpc_context.abort(
                IMPORT_grpc.StatusCode.UNAVAILABLE,
                f"Consensus '{self.consensus_id}' is not authoritative for this "
                f"request; consensus '{authoritative_consensus}' is.",
            )
            raise  # Unreachable but necessary for mypy.

        @IMPORT_reboot_aio_internals_middleware.maybe_run_function_twice_to_validate_effects
        async def _run(
            validating_effects: bool,
        ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeStringifyResponse:
            context: IMPORT_typing.Optional[IMPORT_reboot_aio_contexts.Context] = None
            try:
                if headers.task_schedule is not None:
                    context, response = await self._schedule_Stringify(
                        headers=headers,
                        request=request,
                        grpc_context=grpc_context,
                    )
                    return response

                context = self.create_context(
                    headers=headers,
                    state_type_name = IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.Node'),
                    method='Stringify',
                    context_type=IMPORT_reboot_aio_contexts.ReaderContext,
                )
                assert context is not None

                return await self._Stringify(
                    request,
                    context,
                    validating_effects=validating_effects,
                    grpc_context=grpc_context,
                )
            except IMPORT_reboot_aio_contexts.EffectValidationRetry:
                # Doing effect validation, just let this propagate.
                raise
            except IMPORT_rebootdev.aio.aborted.Aborted as aborted:
                status = IMPORT_rpc_status_sync.to_status(aborted.to_status())
                # Need to add transaction participants here because
                # calling `grpc_context.abort_with_status()` will
                # ignore any other trailing metadata.
                if context is not None and context.transaction_id is not None:
                    status = status._replace(
                        trailing_metadata=status.trailing_metadata + context.participants.to_grpc_metadata()
                    )
                await grpc_context.abort_with_status(status)
                raise  # Unreachable but necessary for mypy.
            except IMPORT_asyncio.CancelledError:
                # It's pretty normal for an RPC to be cancelled; it's not useful to
                # print a stack trace.
                raise
            except BaseException as exception:
                # Print the exception stack trace for easier debugging. Note
                # that we don't include the stack trace in an error message
                # for the same reason that gRPC doesn't do so by default,
                # see https://github.com/grpc/grpc/issues/14897, but since this
                # should only get logged on the server side it is safe.
                logger.warning(
                    'Unhandled exception\n' +
                    ''.join(IMPORT_traceback.format_exc() if IMPORT_reboot_nodejs_python.should_print_stacktrace() else [f"{type(exception).__name__}: {exception}"])
                )

                # Re-raise the exception for gRPC to handle!
                #
                # TODO: gRPC will print a stack trace from this
                # exception which we don't want if we're executing via
                # Node.js.
                raise
            finally:
                if context is not None and context.transaction_id is not None:
                    # Propagate transaction participants.
                    grpc_context.set_trailing_metadata(
                        grpc_context.trailing_metadata() +
                        context.participants.to_grpc_metadata()
                    )

        with IMPORT_reboot_aio_tracing.context_from_headers(headers):
            return await _run()

    def _maybe_authorize(
        self,
        *,
        method_name: str,
        headers: IMPORT_reboot_aio_headers.Headers,
        auth: IMPORT_typing.Optional[IMPORT_rebootdev.aio.auth.Auth],
        request: IMPORT_typing.Optional[NodeRequestTypes] = None,
    ) -> IMPORT_typing.Optional[IMPORT_typing.Callable[[IMPORT_typing.Optional[NodeStateType]], IMPORT_typing.Awaitable[None]]]:
        """Returns a function to check authorization for the given method.

        Raises `PermissionDenied` in case Authorizer is present but the request
        is not authorized.
        """
        # To authorize internal calls, we use an internal magic token.
        if headers.bearer_token == __internal_magic_token__:
            return None

        assert self._authorizer is not None

        async def authorize(state: IMPORT_typing.Optional[NodeStateType]) -> None:
            # Create context for the authorizer. This is a `ReaderContext`
            # independently of the calling context.
            with self.use_context(
                headers=(
                    # Get headers suitable for doing authorization.
                    headers.copy_for_token_verification_and_authorization()
                ),
                state_type_name = IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.Node'),
                method=method_name,
                context_type=IMPORT_reboot_aio_contexts.ReaderContext,
            ) as context:
                context.auth = auth

                # Get the authorizer decision.
                authorization_decision = await self._authorizer.authorize(
                    method_name=method_name,
                    context=context,
                    state=state,
                    request=request,
                )

            # Enforce correct authorizer decision type.
            try:
                IMPORT_reboot_aio_types.assert_type(
                    authorization_decision,
                    [
                        IMPORT_rbt_v1alpha1.errors_pb2.Ok,
                        IMPORT_rbt_v1alpha1.errors_pb2.Unauthenticated,
                        IMPORT_rbt_v1alpha1.errors_pb2.PermissionDenied,
                    ]
                )
            except TypeError as e:
                # Retyping.cast the exception to provide more context.
                authorizer_type = f"{type(self._authorizer).__module__}.{type(self._authorizer).__name__}"
                raise TypeError(
                    f"Authorizer '{authorizer_type}' "
                    f"returned unexpected type '{type(authorization_decision).__name__}' "
                    f"for method '{method_name}' on "
                    f"`rbt.std.collections.ordered_map.v1.Node('{headers.state_ref.id}')`"
                ) from e

            # If the decision is not `True`, raise a `SystemAborted` with either a
            # `PermissionDenied` error (in case of `False`) or an `Unauthenticated`
            # error.
            if not isinstance(authorization_decision, IMPORT_rbt_v1alpha1.errors_pb2.Ok):
                if isinstance(authorization_decision, IMPORT_rbt_v1alpha1.errors_pb2.Unauthenticated):
                    logger.warning(
                        f"Unauthenticated call to '{method_name}' on "
                        f"`rbt.std.collections.ordered_map.v1.Node('{headers.state_ref.id}')`"
                    )

                raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                    authorization_decision,
                    message=
                    f"You are not authorized to call '{method_name}' on "
                    f"`rbt.std.collections.ordered_map.v1.Node('{headers.state_ref.id}')`"
                )

        return authorize

    async def _maybe_verify_token(
        self,
        *,
        headers: IMPORT_reboot_aio_headers.Headers,
        method: str,
    ) -> IMPORT_typing.Optional[IMPORT_rebootdev.aio.auth.Auth]:
        """Verify the bearer token and if a token verifier is present.

        Returns the (optional) `rebootdev.aio.auth.Auth` object
        produced by the token verifier if the token can be verified.
        """
        if self._token_verifier is not None:
            if headers.bearer_token == __internal_magic_token__:
                return None

            with self.use_context(
                headers=(
                    # Get headers suitable for doing token verification.
                    headers.copy_for_token_verification_and_authorization()
                ),
                state_type_name = IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.Node'),
                method=method,
                context_type=IMPORT_reboot_aio_contexts.ReaderContext,
            ) as context:
                return await self._token_verifier.verify_token(
                    context=context,
                    token=headers.bearer_token,
                )

        return None

class OrderedMapServicerMiddleware(IMPORT_reboot_aio_internals_middleware.Middleware):

    def __init__(
        self,
        *,
        servicer: OrderedMapBaseServicer,
        application_id: IMPORT_reboot_aio_types.ApplicationId,
        consensus_id: IMPORT_reboot_aio_types.ConsensusId,
        state_manager: IMPORT_reboot_aio_state_managers.StateManager,
        placement_client: IMPORT_reboot_aio_placement.PlacementClient,
        channel_manager: IMPORT_reboot_aio_internals_channel_manager._ChannelManager,
        tasks_cache: IMPORT_reboot_aio_internals_tasks_cache.TasksCache,
        token_verifier: IMPORT_typing.Optional[IMPORT_rebootdev.aio.auth.token_verifiers.TokenVerifier],
        effect_validation: IMPORT_reboot_aio_contexts.EffectValidation,
        app_internal_api_key_secret: str,
        ready: IMPORT_asyncio.Event,
    ):
        super().__init__(
            application_id=application_id,
            consensus_id=consensus_id,
            state_type_name = IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.OrderedMap'),
            service_names = [
                IMPORT_reboot_aio_types.ServiceName("rbt.std.collections.ordered_map.v1.OrderedMapMethods"),
            ],
            placement_client=placement_client,
            channel_manager=channel_manager,
            effect_validation=effect_validation,
            app_internal_api_key_secret=app_internal_api_key_secret,
        )

        self._servicer = servicer
        self._state_manager = state_manager
        self.tasks_dispatcher = IMPORT_reboot_aio_internals_tasks_dispatcher.TasksDispatcher(
            application_id=application_id,
            dispatch=self.dispatch,
            tasks_cache=tasks_cache,
            ready=ready,
            complete_task=self._state_manager.complete_task,
        )

        # Store the type of each method's request so that stored requests can be
        # deserialized into the correct type.
        self.request_type_by_method_name: dict[str, type[IMPORT_google_protobuf_message.Message]] = {
            'Create': rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapCreateRequest,
            'Search': rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapSearchRequest,
            'Insert': rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapInsertRequest,
            'Remove': rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRemoveRequest,
            'Range': rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRangeRequest,
            'ReverseRange': rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapReverseRangeRequest,
            'Stringify': rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapStringifyRequest,
        }

        # Get authorizer, if any, converting from a rule if necessary.
        def convert_authorizer_rule_if_necessary(
            authorizer_or_rule: IMPORT_typing.Optional[
                IMPORT_rebootdev.aio.auth.authorizers.Authorizer | IMPORT_rebootdev.aio.auth.authorizers.AuthorizerRule
            ]
        ) -> IMPORT_rebootdev.aio.auth.authorizers.Authorizer:

            # If no authorizer or rule is provided, return the default
            # authorizer which allows if app internal or allows if in
            # dev mode (and logs some warnings to help the user
            # realize where they are missing authorization).
            if authorizer_or_rule is None:
                return IMPORT_rebootdev.aio.auth.authorizers.DefaultAuthorizer(
                    'OrderedMap'
                )

            if isinstance(authorizer_or_rule, IMPORT_rebootdev.aio.auth.authorizers.AuthorizerRule):
                return OrderedMapAuthorizer(
                    _default=authorizer_or_rule
                )

            return authorizer_or_rule

        self._authorizer = convert_authorizer_rule_if_necessary(
            servicer.authorizer()
        )

        # Create token verifier.
        self._token_verifier: IMPORT_typing.Optional[IMPORT_rebootdev.aio.auth.token_verifiers.TokenVerifier] = (
            servicer.token_verifier() or token_verifier
        )

        # Since users specify errors as proto messages they can't raise them
        # directly - to do so they have to use the `Aborted` wrapper, which will
        # hold the original proto message. On errors we'll need to check whether
        # such wrappers hold a proto message for a specified error, so we can
        # avoid retrying tasks that complete with a specified error.
        self._specified_errors_by_service_method_name: dict[str, list[str]] = {
            'rbt.std.collections.ordered_map.v1.OrderedMapMethods.Create': [
                'rbt.v1alpha1.errors_pb2.StateAlreadyConstructed',
            ],
            'rbt.std.collections.ordered_map.v1.OrderedMapMethods.Range': [
                'rbt.std.collections.ordered_map.v1.ordered_map_pb2.InvalidRangeError',
            ],
            'rbt.std.collections.ordered_map.v1.OrderedMapMethods.ReverseRange': [
                'rbt.std.collections.ordered_map.v1.ordered_map_pb2.InvalidRangeError',
            ],
        }


    def add_to_server(self, server: IMPORT_grpc.aio.Server) -> None:
        rbt.std.collections.ordered_map.v1.ordered_map_pb2_grpc.add_OrderedMapMethodsServicer_to_server(
            self, server
        )

    async def inspect(self, state_ref: IMPORT_reboot_aio_types.StateRef) -> IMPORT_typing.AsyncIterator[IMPORT_google_protobuf_message.Message]:
        """Implementation of `Middleware.inspect()`."""
        context = self.create_context(
            headers=IMPORT_reboot_aio_headers.Headers(
                application_id=self.application_id,
                state_ref=state_ref,
            ),
            state_type_name = IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.OrderedMap'),
            method="inspect",
            context_type=IMPORT_reboot_aio_contexts.ReaderContext,
        )

        async with self._state_manager.streaming_reader_idempotency_key(
            context,
            self._servicer.__state_type__,
            authorize=None,
        ) as states:
            async for (state, idempotency_key) in states:
                yield state

    async def react_query(
        self,
        headers: IMPORT_reboot_aio_headers.Headers,
        method: str,
        request_bytes: bytes,
    ) -> IMPORT_typing.AsyncIterator[tuple[IMPORT_typing.Optional[IMPORT_google_protobuf_message.Message], list[IMPORT_uuid.UUID]]]:
        """Returns the response of calling 'method' given a message
        deserialized from the provided 'request_bytes' for each state
        update that creates a different response.

        # The caller (react.py) should have already ensured that this consensus
        # is authoritative for this traffic.
        assert self.placement_client.consensus_for_actor(
            headers.application_id,
            headers.state_ref,
        ) == self._consensus_id

        NOTE: only unary reader methods are supported."""
        # Need to define these up here since we can only do that once.
        last_response: IMPORT_typing.Optional[IMPORT_google_protobuf_message.Message] = None
        aggregated_idempotency_keys: list[IMPORT_uuid.UUID] = []
        if method == 'Create':
            # Invariant here is that users should not have called this
            # directly but only through code generated React
            # components which should not have been generated except
            # for valid method candidates.
            logger.warning(
                "Got a React query request with an invalid method name: "
                f"Method '{method}' is invalid for servicer OrderedMap."
                "\n"
                "Do you have a browser tab open for an older version "
                "of this application, or for a different application all together?"
            )
            raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                IMPORT_rbt_v1alpha1.errors_pb2.InvalidMethod(),
                message=
                    f"Method '{method}' is invalid"
            )
            yield  # Necessary for type checking.
        elif method == 'Search':

            context = self.create_context(
                headers=headers,
                state_type_name = IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.OrderedMap'),
                method='Search',
                context_type=IMPORT_reboot_aio_contexts.ReaderContext,
            )

            with IMPORT_reboot_aio_tracing.span(
                state_name=f"{context.state_type_name}('{context.state_id}')",
                span_name="Search() (reactively)",
                # The naming above matches Python, but not TypeScript.
                python_specific=True,
                level=IMPORT_reboot_aio_tracing.TraceLevel.CUSTOMER,
            ):
                context.auth = await self._maybe_verify_token(
                    headers=headers, method='Search'
                )

                request = rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapSearchRequest()
                request.ParseFromString(request_bytes)

                async with self._state_manager.reactively(
                    context,
                    self._servicer.__state_type__,
                    authorize=self._maybe_authorize(
                        method_name='rbt.std.collections.ordered_map.v1.OrderedMapMethods.Search',
                        headers=headers,
                        auth=context.auth,
                        request=request,
                    ),
                ) as states:
                    async for (state, idempotency_keys) in states:

                        aggregated_idempotency_keys.extend(idempotency_keys)

                        # Note: This does not do any defensive copying currently:
                        # see https://github.com/reboot-dev/respect/issues/2636.
                        @IMPORT_reboot_aio_internals_middleware.maybe_run_function_twice_to_validate_effects
                        async def run__Search(validating_effects: bool) -> IMPORT_google_protobuf_message.Message:
                            return await self.__Search(
                                context,
                                state,
                                request,
                                validating_effects=validating_effects,
                            )

                        response = await run__Search()

                        if last_response != response:
                            yield (response, aggregated_idempotency_keys)
                            last_response = response
                        else:
                            yield (None, aggregated_idempotency_keys)

                        aggregated_idempotency_keys.clear()
        elif method == 'Insert':
            # Invariant here is that users should not have called this
            # directly but only through code generated React
            # components which should not have been generated except
            # for valid method candidates.
            logger.warning(
                "Got a React query request with an invalid method name: "
                f"Method '{method}' is invalid for servicer OrderedMap."
                "\n"
                "Do you have a browser tab open for an older version "
                "of this application, or for a different application all together?"
            )
            raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                IMPORT_rbt_v1alpha1.errors_pb2.InvalidMethod(),
                message=
                    f"Method '{method}' is invalid"
            )
            yield  # Necessary for type checking.
        elif method == 'Remove':
            # Invariant here is that users should not have called this
            # directly but only through code generated React
            # components which should not have been generated except
            # for valid method candidates.
            logger.warning(
                "Got a React query request with an invalid method name: "
                f"Method '{method}' is invalid for servicer OrderedMap."
                "\n"
                "Do you have a browser tab open for an older version "
                "of this application, or for a different application all together?"
            )
            raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                IMPORT_rbt_v1alpha1.errors_pb2.InvalidMethod(),
                message=
                    f"Method '{method}' is invalid"
            )
            yield  # Necessary for type checking.
        elif method == 'Range':

            context = self.create_context(
                headers=headers,
                state_type_name = IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.OrderedMap'),
                method='Range',
                context_type=IMPORT_reboot_aio_contexts.ReaderContext,
            )

            with IMPORT_reboot_aio_tracing.span(
                state_name=f"{context.state_type_name}('{context.state_id}')",
                span_name="Range() (reactively)",
                # The naming above matches Python, but not TypeScript.
                python_specific=True,
                level=IMPORT_reboot_aio_tracing.TraceLevel.CUSTOMER,
            ):
                context.auth = await self._maybe_verify_token(
                    headers=headers, method='Range'
                )

                request = rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRangeRequest()
                request.ParseFromString(request_bytes)

                async with self._state_manager.reactively(
                    context,
                    self._servicer.__state_type__,
                    authorize=self._maybe_authorize(
                        method_name='rbt.std.collections.ordered_map.v1.OrderedMapMethods.Range',
                        headers=headers,
                        auth=context.auth,
                        request=request,
                    ),
                ) as states:
                    async for (state, idempotency_keys) in states:

                        aggregated_idempotency_keys.extend(idempotency_keys)

                        # Note: This does not do any defensive copying currently:
                        # see https://github.com/reboot-dev/respect/issues/2636.
                        @IMPORT_reboot_aio_internals_middleware.maybe_run_function_twice_to_validate_effects
                        async def run__Range(validating_effects: bool) -> IMPORT_google_protobuf_message.Message:
                            return await self.__Range(
                                context,
                                state,
                                request,
                                validating_effects=validating_effects,
                            )

                        response = await run__Range()

                        if last_response != response:
                            yield (response, aggregated_idempotency_keys)
                            last_response = response
                        else:
                            yield (None, aggregated_idempotency_keys)

                        aggregated_idempotency_keys.clear()
        elif method == 'ReverseRange':

            context = self.create_context(
                headers=headers,
                state_type_name = IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.OrderedMap'),
                method='ReverseRange',
                context_type=IMPORT_reboot_aio_contexts.ReaderContext,
            )

            with IMPORT_reboot_aio_tracing.span(
                state_name=f"{context.state_type_name}('{context.state_id}')",
                span_name="ReverseRange() (reactively)",
                # The naming above matches Python, but not TypeScript.
                python_specific=True,
                level=IMPORT_reboot_aio_tracing.TraceLevel.CUSTOMER,
            ):
                context.auth = await self._maybe_verify_token(
                    headers=headers, method='ReverseRange'
                )

                request = rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapReverseRangeRequest()
                request.ParseFromString(request_bytes)

                async with self._state_manager.reactively(
                    context,
                    self._servicer.__state_type__,
                    authorize=self._maybe_authorize(
                        method_name='rbt.std.collections.ordered_map.v1.OrderedMapMethods.ReverseRange',
                        headers=headers,
                        auth=context.auth,
                        request=request,
                    ),
                ) as states:
                    async for (state, idempotency_keys) in states:

                        aggregated_idempotency_keys.extend(idempotency_keys)

                        # Note: This does not do any defensive copying currently:
                        # see https://github.com/reboot-dev/respect/issues/2636.
                        @IMPORT_reboot_aio_internals_middleware.maybe_run_function_twice_to_validate_effects
                        async def run__ReverseRange(validating_effects: bool) -> IMPORT_google_protobuf_message.Message:
                            return await self.__ReverseRange(
                                context,
                                state,
                                request,
                                validating_effects=validating_effects,
                            )

                        response = await run__ReverseRange()

                        if last_response != response:
                            yield (response, aggregated_idempotency_keys)
                            last_response = response
                        else:
                            yield (None, aggregated_idempotency_keys)

                        aggregated_idempotency_keys.clear()
        elif method == 'Stringify':

            context = self.create_context(
                headers=headers,
                state_type_name = IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.OrderedMap'),
                method='Stringify',
                context_type=IMPORT_reboot_aio_contexts.ReaderContext,
            )

            with IMPORT_reboot_aio_tracing.span(
                state_name=f"{context.state_type_name}('{context.state_id}')",
                span_name="Stringify() (reactively)",
                # The naming above matches Python, but not TypeScript.
                python_specific=True,
                level=IMPORT_reboot_aio_tracing.TraceLevel.CUSTOMER,
            ):
                context.auth = await self._maybe_verify_token(
                    headers=headers, method='Stringify'
                )

                request = rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapStringifyRequest()
                request.ParseFromString(request_bytes)

                async with self._state_manager.reactively(
                    context,
                    self._servicer.__state_type__,
                    authorize=self._maybe_authorize(
                        method_name='rbt.std.collections.ordered_map.v1.OrderedMapMethods.Stringify',
                        headers=headers,
                        auth=context.auth,
                        request=request,
                    ),
                ) as states:
                    async for (state, idempotency_keys) in states:

                        aggregated_idempotency_keys.extend(idempotency_keys)

                        # Note: This does not do any defensive copying currently:
                        # see https://github.com/reboot-dev/respect/issues/2636.
                        @IMPORT_reboot_aio_internals_middleware.maybe_run_function_twice_to_validate_effects
                        async def run__Stringify(validating_effects: bool) -> IMPORT_google_protobuf_message.Message:
                            return await self.__Stringify(
                                context,
                                state,
                                request,
                                validating_effects=validating_effects,
                            )

                        response = await run__Stringify()

                        if last_response != response:
                            yield (response, aggregated_idempotency_keys)
                            last_response = response
                        else:
                            yield (None, aggregated_idempotency_keys)

                        aggregated_idempotency_keys.clear()
        else:
            logger.warning(
                "Got a React query request with an invalid method name: "
                f"Method '{method}' is invalid for servicer OrderedMap."
                "\n"
                "Do you have a browser tab open for an older version "
                "of this application, or for a different application all together?"
            )
            raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                IMPORT_rbt_v1alpha1.errors_pb2.InvalidMethod(),
                message=
                    f"Method '{method}' not found"
            )
            yield  # Unreachable but necessary for mypy.

    async def react_mutate(
        self,
        headers: IMPORT_reboot_aio_headers.Headers,
        method: str,
        request_bytes: bytes,
    ) -> IMPORT_google_protobuf_message.Message:
        """Returns the response of calling 'method' given a message
        deserialized from the provided 'request_bytes'."""
        if method == 'Create':
            request = rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapCreateRequest()
            request.ParseFromString(request_bytes)

            # NOTE: we automatically retry mutations that come through
            # React when we get a `IMPORT_grpc.StatusCode.UNAVAILABLE` to
            # match the retry logic we do in the React code generated
            # to handle lack/loss of connectivity.
            #
            # TODO(benh): revisit this decision if we ever see reason
            # to call `react_mutate()` from any place other than where
            # we're executing React (e.g., browser, next.js server
            # component, etc).
            call_backoff = IMPORT_reboot_aio_backoff.Backoff()
            while True:
                # We make a full-fledged gRPC call, so that if this traffic
                # was misrouted (i.e. this consensus is not authoritative
                # for the state), it will now go to the right place. The
                # receiving middleware will handle things like effect
                # validation and so forth.
                assert headers.application_id is not None  # Guaranteed by `Headers`.
                stub = rbt.std.collections.ordered_map.v1.ordered_map_pb2_grpc.OrderedMapMethodsStub(
                    self.channel_manager.get_channel_to(
                        self.placement_client.address_for_actor(
                            headers.application_id,
                            headers.state_ref,
                        )
                    )
                )
                call = stub.Create(
                    request=request,
                    metadata=headers.to_grpc_metadata(),
                )
                try:
                    return await call
                except IMPORT_grpc.aio.AioRpcError as error:
                    if error.code() == IMPORT_grpc.StatusCode.UNAVAILABLE:
                        await call_backoff()
                        continue

                    # Reconstitute the error that the server threw, if it was a declared error.
                    status = await IMPORT_rpc_status_async.from_call(call)
                    if status is not None:
                        raise OrderedMap.CreateAborted.from_status(
                            status
                        ) from None
                    raise OrderedMap.CreateAborted.from_grpc_aio_rpc_error(
                        error
                     ) from None

        elif method == 'Search':
            # Invariant here is that users should not have called this
            # directly but only through code generated React
            # components which should not have been generated except
            # for valid method candidates.
            logger.warning(
                "Got a react mutate request with an invalid method name: "
                "Method 'Search' is invalid for servicer OrderedMap."
                "\n"
                "Do you have an old browser tab still open for an older version "
                "of this application, or a different application all together?"
            )
            raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                IMPORT_rbt_v1alpha1.errors_pb2.InvalidMethod(),
                message=f"Method '{method}' is invalid"
            )
        elif method == 'Insert':
            request = rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapInsertRequest()
            request.ParseFromString(request_bytes)

            # NOTE: we automatically retry mutations that come through
            # React when we get a `IMPORT_grpc.StatusCode.UNAVAILABLE` to
            # match the retry logic we do in the React code generated
            # to handle lack/loss of connectivity.
            #
            # TODO(benh): revisit this decision if we ever see reason
            # to call `react_mutate()` from any place other than where
            # we're executing React (e.g., browser, next.js server
            # component, etc).
            call_backoff = IMPORT_reboot_aio_backoff.Backoff()
            while True:
                # We make a full-fledged gRPC call, so that if this traffic
                # was misrouted (i.e. this consensus is not authoritative
                # for the state), it will now go to the right place. The
                # receiving middleware will handle things like effect
                # validation and so forth.
                assert headers.application_id is not None  # Guaranteed by `Headers`.
                stub = rbt.std.collections.ordered_map.v1.ordered_map_pb2_grpc.OrderedMapMethodsStub(
                    self.channel_manager.get_channel_to(
                        self.placement_client.address_for_actor(
                            headers.application_id,
                            headers.state_ref,
                        )
                    )
                )
                call = stub.Insert(
                    request=request,
                    metadata=headers.to_grpc_metadata(),
                )
                try:
                    return await call
                except IMPORT_grpc.aio.AioRpcError as error:
                    if error.code() == IMPORT_grpc.StatusCode.UNAVAILABLE:
                        await call_backoff()
                        continue

                    # Reconstitute the error that the server threw, if it was a declared error.
                    status = await IMPORT_rpc_status_async.from_call(call)
                    if status is not None:
                        raise OrderedMap.InsertAborted.from_status(
                            status
                        ) from None
                    raise OrderedMap.InsertAborted.from_grpc_aio_rpc_error(
                        error
                     ) from None

        elif method == 'Remove':
            request = rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRemoveRequest()
            request.ParseFromString(request_bytes)

            # NOTE: we automatically retry mutations that come through
            # React when we get a `IMPORT_grpc.StatusCode.UNAVAILABLE` to
            # match the retry logic we do in the React code generated
            # to handle lack/loss of connectivity.
            #
            # TODO(benh): revisit this decision if we ever see reason
            # to call `react_mutate()` from any place other than where
            # we're executing React (e.g., browser, next.js server
            # component, etc).
            call_backoff = IMPORT_reboot_aio_backoff.Backoff()
            while True:
                # We make a full-fledged gRPC call, so that if this traffic
                # was misrouted (i.e. this consensus is not authoritative
                # for the state), it will now go to the right place. The
                # receiving middleware will handle things like effect
                # validation and so forth.
                assert headers.application_id is not None  # Guaranteed by `Headers`.
                stub = rbt.std.collections.ordered_map.v1.ordered_map_pb2_grpc.OrderedMapMethodsStub(
                    self.channel_manager.get_channel_to(
                        self.placement_client.address_for_actor(
                            headers.application_id,
                            headers.state_ref,
                        )
                    )
                )
                call = stub.Remove(
                    request=request,
                    metadata=headers.to_grpc_metadata(),
                )
                try:
                    return await call
                except IMPORT_grpc.aio.AioRpcError as error:
                    if error.code() == IMPORT_grpc.StatusCode.UNAVAILABLE:
                        await call_backoff()
                        continue

                    # Reconstitute the error that the server threw, if it was a declared error.
                    status = await IMPORT_rpc_status_async.from_call(call)
                    if status is not None:
                        raise OrderedMap.RemoveAborted.from_status(
                            status
                        ) from None
                    raise OrderedMap.RemoveAborted.from_grpc_aio_rpc_error(
                        error
                     ) from None

        elif method == 'Range':
            # Invariant here is that users should not have called this
            # directly but only through code generated React
            # components which should not have been generated except
            # for valid method candidates.
            logger.warning(
                "Got a react mutate request with an invalid method name: "
                "Method 'Range' is invalid for servicer OrderedMap."
                "\n"
                "Do you have an old browser tab still open for an older version "
                "of this application, or a different application all together?"
            )
            raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                IMPORT_rbt_v1alpha1.errors_pb2.InvalidMethod(),
                message=f"Method '{method}' is invalid"
            )
        elif method == 'ReverseRange':
            # Invariant here is that users should not have called this
            # directly but only through code generated React
            # components which should not have been generated except
            # for valid method candidates.
            logger.warning(
                "Got a react mutate request with an invalid method name: "
                "Method 'ReverseRange' is invalid for servicer OrderedMap."
                "\n"
                "Do you have an old browser tab still open for an older version "
                "of this application, or a different application all together?"
            )
            raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                IMPORT_rbt_v1alpha1.errors_pb2.InvalidMethod(),
                message=f"Method '{method}' is invalid"
            )
        elif method == 'Stringify':
            # Invariant here is that users should not have called this
            # directly but only through code generated React
            # components which should not have been generated except
            # for valid method candidates.
            logger.warning(
                "Got a react mutate request with an invalid method name: "
                "Method 'Stringify' is invalid for servicer OrderedMap."
                "\n"
                "Do you have an old browser tab still open for an older version "
                "of this application, or a different application all together?"
            )
            raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                IMPORT_rbt_v1alpha1.errors_pb2.InvalidMethod(),
                message=f"Method '{method}' is invalid"
            )
        else:
            logger.warning(
                "Got a react mutate request with an invalid method name: "
                f"Method '{method}' is invalid for servicer OrderedMap."
                "\n"
                "Do you have an old browser tab still open for an older version "
                "of this application, or a different application all together?"
            )
            raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                IMPORT_rbt_v1alpha1.errors_pb2.InvalidMethod(),
                message=
                    f"Method '{method}' not found"
            )

    async def dispatch(
        self,
        task: IMPORT_reboot_aio_tasks.TaskEffect,
        *,
        only_validate: bool = False,
        on_loop_iteration: IMPORT_reboot_aio_internals_tasks_dispatcher.OnLoopIterationCallable = (lambda iteration, next_iteration_schedule: None),
    ) -> IMPORT_reboot_aio_internals_tasks_dispatcher.TaskResponseOrError:
        """Dispatches the tasks to execute unless 'only_validate' is set to
        true, in which case just ensures that the task actually exists.
        Note that this function will be called *by* tasks_dispatcher; it will
        not itself call into tasks_dispatcher."""

        if 'Create' == task.method_name:
            if only_validate:
                # TODO(benh): validate 'task.request' is correct type.
                return rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapCreateResponse()

            # Use an inline method to create a new scope, so that we can use
            # variable names like `context` and `effects` in multiple branches
            # in this code (notably when there are multiple task types) without
            # hitting a mypy error that the variable's type is not consistent.
            async def run_Create(
                context: IMPORT_reboot_aio_contexts.WorkflowContext,
                *,
                validating_effects: bool = False,
            ):
                async with self._state_manager.task_workflow(
                    context,
                    task,
                    on_loop_iteration=on_loop_iteration,
                    validating_effects=validating_effects,
                ) as complete:
                    try:
                        response = await (OrderedMapWorkflowStub(
                            context=context,
                            state_ref=context._state_ref,
                        ).Create(
                            IMPORT_typing.cast(rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapCreateRequest, task.request),
                            bearer_token=__internal_magic_token__,
                            idempotency=IMPORT_reboot_aio_idempotency.Idempotency(
                                alias=f'Task {IMPORT_uuid.UUID(bytes=task.task_id.task_uuid)}',
                            ),
                        ))
                        await complete(task, (response, None))
                        return (response, None)
                    except IMPORT_asyncio.CancelledError:
                        # Check if the task was cancelled by a TasksServicer.
                        if self.tasks_dispatcher.is_task_cancelled(task.task_id.task_uuid):
                            # The running task was cancelled by a TasksServicer.
                            await complete(task, (None, IMPORT_rbt_v1alpha1.tasks_pb2.TaskCancelledError()))
                            return (None, IMPORT_rbt_v1alpha1.tasks_pb2.TaskCancelledError())
                        else:
                            raise
                    except IMPORT_rebootdev.aio.aborted.Aborted as aborted:
                        error_type = f'{aborted.error.__class__.__module__}.{aborted.error.__class__.__qualname__}'
                        # Do not retry a task if the error was specified in the
                        # proto file.
                        if error_type in self._specified_errors_by_service_method_name.get('rbt.std.collections.ordered_map.v1.OrderedMapMethods.Create', []):
                            await complete(task, (None, aborted.error))
                            return (None, aborted.error)
                        raise


            return await run_Create(
                self.create_context(
                    headers=IMPORT_reboot_aio_headers.Headers(
                        application_id=self.application_id,
                        state_ref=IMPORT_reboot_aio_types.StateRef(task.task_id.state_ref),
                    ),
                    state_type_name = IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.OrderedMap'),
                    method='Create',
                    context_type=IMPORT_reboot_aio_contexts.WorkflowContext,
                    task=task,
                )
            )
        elif 'Search' == task.method_name:
            if only_validate:
                # TODO(benh): validate 'task.request' is correct type.
                return rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapSearchResponse()

            # Use an inline method to create a new scope, so that we can use
            # variable names like `context` and `effects` in multiple branches
            # in this code (notably when there are multiple task types) without
            # hitting a mypy error that the variable's type is not consistent.
            async def run_Search(
                context: IMPORT_reboot_aio_contexts.WorkflowContext,
                *,
                validating_effects: bool = False,
            ):
                async with self._state_manager.task_workflow(
                    context,
                    task,
                    on_loop_iteration=on_loop_iteration,
                    validating_effects=validating_effects,
                ) as complete:
                    try:
                        response = await (OrderedMapWorkflowStub(
                            context=context,
                            state_ref=context._state_ref,
                        ).Search(
                            IMPORT_typing.cast(rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapSearchRequest, task.request),
                            bearer_token=__internal_magic_token__,
                        ))
                        await complete(task, (response, None))
                        return (response, None)
                    except IMPORT_asyncio.CancelledError:
                        # Check if the task was cancelled by a TasksServicer.
                        if self.tasks_dispatcher.is_task_cancelled(task.task_id.task_uuid):
                            # The running task was cancelled by a TasksServicer.
                            await complete(task, (None, IMPORT_rbt_v1alpha1.tasks_pb2.TaskCancelledError()))
                            return (None, IMPORT_rbt_v1alpha1.tasks_pb2.TaskCancelledError())
                        else:
                            raise
                    except IMPORT_rebootdev.aio.aborted.Aborted as aborted:
                        error_type = f'{aborted.error.__class__.__module__}.{aborted.error.__class__.__qualname__}'
                        # Do not retry a task if the error was specified in the
                        # proto file.
                        if error_type in self._specified_errors_by_service_method_name.get('rbt.std.collections.ordered_map.v1.OrderedMapMethods.Search', []):
                            await complete(task, (None, aborted.error))
                            return (None, aborted.error)
                        raise


            return await run_Search(
                self.create_context(
                    headers=IMPORT_reboot_aio_headers.Headers(
                        application_id=self.application_id,
                        state_ref=IMPORT_reboot_aio_types.StateRef(task.task_id.state_ref),
                    ),
                    state_type_name = IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.OrderedMap'),
                    method='Search',
                    context_type=IMPORT_reboot_aio_contexts.WorkflowContext,
                    task=task,
                )
            )
        elif 'Insert' == task.method_name:
            if only_validate:
                # TODO(benh): validate 'task.request' is correct type.
                return rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapInsertResponse()

            # Use an inline method to create a new scope, so that we can use
            # variable names like `context` and `effects` in multiple branches
            # in this code (notably when there are multiple task types) without
            # hitting a mypy error that the variable's type is not consistent.
            async def run_Insert(
                context: IMPORT_reboot_aio_contexts.WorkflowContext,
                *,
                validating_effects: bool = False,
            ):
                async with self._state_manager.task_workflow(
                    context,
                    task,
                    on_loop_iteration=on_loop_iteration,
                    validating_effects=validating_effects,
                ) as complete:
                    try:
                        response = await (OrderedMapWorkflowStub(
                            context=context,
                            state_ref=context._state_ref,
                        ).Insert(
                            IMPORT_typing.cast(rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapInsertRequest, task.request),
                            bearer_token=__internal_magic_token__,
                            idempotency=IMPORT_reboot_aio_idempotency.Idempotency(
                                alias=f'Task {IMPORT_uuid.UUID(bytes=task.task_id.task_uuid)}',
                            ),
                        ))
                        await complete(task, (response, None))
                        return (response, None)
                    except IMPORT_asyncio.CancelledError:
                        # Check if the task was cancelled by a TasksServicer.
                        if self.tasks_dispatcher.is_task_cancelled(task.task_id.task_uuid):
                            # The running task was cancelled by a TasksServicer.
                            await complete(task, (None, IMPORT_rbt_v1alpha1.tasks_pb2.TaskCancelledError()))
                            return (None, IMPORT_rbt_v1alpha1.tasks_pb2.TaskCancelledError())
                        else:
                            raise
                    except IMPORT_rebootdev.aio.aborted.Aborted as aborted:
                        error_type = f'{aborted.error.__class__.__module__}.{aborted.error.__class__.__qualname__}'
                        # Do not retry a task if the error was specified in the
                        # proto file.
                        if error_type in self._specified_errors_by_service_method_name.get('rbt.std.collections.ordered_map.v1.OrderedMapMethods.Insert', []):
                            await complete(task, (None, aborted.error))
                            return (None, aborted.error)
                        raise


            return await run_Insert(
                self.create_context(
                    headers=IMPORT_reboot_aio_headers.Headers(
                        application_id=self.application_id,
                        state_ref=IMPORT_reboot_aio_types.StateRef(task.task_id.state_ref),
                    ),
                    state_type_name = IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.OrderedMap'),
                    method='Insert',
                    context_type=IMPORT_reboot_aio_contexts.WorkflowContext,
                    task=task,
                )
            )
        elif 'Remove' == task.method_name:
            if only_validate:
                # TODO(benh): validate 'task.request' is correct type.
                return rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRemoveResponse()

            # Use an inline method to create a new scope, so that we can use
            # variable names like `context` and `effects` in multiple branches
            # in this code (notably when there are multiple task types) without
            # hitting a mypy error that the variable's type is not consistent.
            async def run_Remove(
                context: IMPORT_reboot_aio_contexts.WorkflowContext,
                *,
                validating_effects: bool = False,
            ):
                async with self._state_manager.task_workflow(
                    context,
                    task,
                    on_loop_iteration=on_loop_iteration,
                    validating_effects=validating_effects,
                ) as complete:
                    try:
                        response = await (OrderedMapWorkflowStub(
                            context=context,
                            state_ref=context._state_ref,
                        ).Remove(
                            IMPORT_typing.cast(rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRemoveRequest, task.request),
                            bearer_token=__internal_magic_token__,
                            idempotency=IMPORT_reboot_aio_idempotency.Idempotency(
                                alias=f'Task {IMPORT_uuid.UUID(bytes=task.task_id.task_uuid)}',
                            ),
                        ))
                        await complete(task, (response, None))
                        return (response, None)
                    except IMPORT_asyncio.CancelledError:
                        # Check if the task was cancelled by a TasksServicer.
                        if self.tasks_dispatcher.is_task_cancelled(task.task_id.task_uuid):
                            # The running task was cancelled by a TasksServicer.
                            await complete(task, (None, IMPORT_rbt_v1alpha1.tasks_pb2.TaskCancelledError()))
                            return (None, IMPORT_rbt_v1alpha1.tasks_pb2.TaskCancelledError())
                        else:
                            raise
                    except IMPORT_rebootdev.aio.aborted.Aborted as aborted:
                        error_type = f'{aborted.error.__class__.__module__}.{aborted.error.__class__.__qualname__}'
                        # Do not retry a task if the error was specified in the
                        # proto file.
                        if error_type in self._specified_errors_by_service_method_name.get('rbt.std.collections.ordered_map.v1.OrderedMapMethods.Remove', []):
                            await complete(task, (None, aborted.error))
                            return (None, aborted.error)
                        raise


            return await run_Remove(
                self.create_context(
                    headers=IMPORT_reboot_aio_headers.Headers(
                        application_id=self.application_id,
                        state_ref=IMPORT_reboot_aio_types.StateRef(task.task_id.state_ref),
                    ),
                    state_type_name = IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.OrderedMap'),
                    method='Remove',
                    context_type=IMPORT_reboot_aio_contexts.WorkflowContext,
                    task=task,
                )
            )
        elif 'Range' == task.method_name:
            if only_validate:
                # TODO(benh): validate 'task.request' is correct type.
                return rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRangeResponse()

            # Use an inline method to create a new scope, so that we can use
            # variable names like `context` and `effects` in multiple branches
            # in this code (notably when there are multiple task types) without
            # hitting a mypy error that the variable's type is not consistent.
            async def run_Range(
                context: IMPORT_reboot_aio_contexts.WorkflowContext,
                *,
                validating_effects: bool = False,
            ):
                async with self._state_manager.task_workflow(
                    context,
                    task,
                    on_loop_iteration=on_loop_iteration,
                    validating_effects=validating_effects,
                ) as complete:
                    try:
                        response = await (OrderedMapWorkflowStub(
                            context=context,
                            state_ref=context._state_ref,
                        ).Range(
                            IMPORT_typing.cast(rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRangeRequest, task.request),
                            bearer_token=__internal_magic_token__,
                        ))
                        await complete(task, (response, None))
                        return (response, None)
                    except IMPORT_asyncio.CancelledError:
                        # Check if the task was cancelled by a TasksServicer.
                        if self.tasks_dispatcher.is_task_cancelled(task.task_id.task_uuid):
                            # The running task was cancelled by a TasksServicer.
                            await complete(task, (None, IMPORT_rbt_v1alpha1.tasks_pb2.TaskCancelledError()))
                            return (None, IMPORT_rbt_v1alpha1.tasks_pb2.TaskCancelledError())
                        else:
                            raise
                    except IMPORT_rebootdev.aio.aborted.Aborted as aborted:
                        error_type = f'{aborted.error.__class__.__module__}.{aborted.error.__class__.__qualname__}'
                        # Do not retry a task if the error was specified in the
                        # proto file.
                        if error_type in self._specified_errors_by_service_method_name.get('rbt.std.collections.ordered_map.v1.OrderedMapMethods.Range', []):
                            await complete(task, (None, aborted.error))
                            return (None, aborted.error)
                        raise


            return await run_Range(
                self.create_context(
                    headers=IMPORT_reboot_aio_headers.Headers(
                        application_id=self.application_id,
                        state_ref=IMPORT_reboot_aio_types.StateRef(task.task_id.state_ref),
                    ),
                    state_type_name = IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.OrderedMap'),
                    method='Range',
                    context_type=IMPORT_reboot_aio_contexts.WorkflowContext,
                    task=task,
                )
            )
        elif 'ReverseRange' == task.method_name:
            if only_validate:
                # TODO(benh): validate 'task.request' is correct type.
                return rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapReverseRangeResponse()

            # Use an inline method to create a new scope, so that we can use
            # variable names like `context` and `effects` in multiple branches
            # in this code (notably when there are multiple task types) without
            # hitting a mypy error that the variable's type is not consistent.
            async def run_ReverseRange(
                context: IMPORT_reboot_aio_contexts.WorkflowContext,
                *,
                validating_effects: bool = False,
            ):
                async with self._state_manager.task_workflow(
                    context,
                    task,
                    on_loop_iteration=on_loop_iteration,
                    validating_effects=validating_effects,
                ) as complete:
                    try:
                        response = await (OrderedMapWorkflowStub(
                            context=context,
                            state_ref=context._state_ref,
                        ).ReverseRange(
                            IMPORT_typing.cast(rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapReverseRangeRequest, task.request),
                            bearer_token=__internal_magic_token__,
                        ))
                        await complete(task, (response, None))
                        return (response, None)
                    except IMPORT_asyncio.CancelledError:
                        # Check if the task was cancelled by a TasksServicer.
                        if self.tasks_dispatcher.is_task_cancelled(task.task_id.task_uuid):
                            # The running task was cancelled by a TasksServicer.
                            await complete(task, (None, IMPORT_rbt_v1alpha1.tasks_pb2.TaskCancelledError()))
                            return (None, IMPORT_rbt_v1alpha1.tasks_pb2.TaskCancelledError())
                        else:
                            raise
                    except IMPORT_rebootdev.aio.aborted.Aborted as aborted:
                        error_type = f'{aborted.error.__class__.__module__}.{aborted.error.__class__.__qualname__}'
                        # Do not retry a task if the error was specified in the
                        # proto file.
                        if error_type in self._specified_errors_by_service_method_name.get('rbt.std.collections.ordered_map.v1.OrderedMapMethods.ReverseRange', []):
                            await complete(task, (None, aborted.error))
                            return (None, aborted.error)
                        raise


            return await run_ReverseRange(
                self.create_context(
                    headers=IMPORT_reboot_aio_headers.Headers(
                        application_id=self.application_id,
                        state_ref=IMPORT_reboot_aio_types.StateRef(task.task_id.state_ref),
                    ),
                    state_type_name = IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.OrderedMap'),
                    method='ReverseRange',
                    context_type=IMPORT_reboot_aio_contexts.WorkflowContext,
                    task=task,
                )
            )
        elif 'Stringify' == task.method_name:
            if only_validate:
                # TODO(benh): validate 'task.request' is correct type.
                return rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapStringifyResponse()

            # Use an inline method to create a new scope, so that we can use
            # variable names like `context` and `effects` in multiple branches
            # in this code (notably when there are multiple task types) without
            # hitting a mypy error that the variable's type is not consistent.
            async def run_Stringify(
                context: IMPORT_reboot_aio_contexts.WorkflowContext,
                *,
                validating_effects: bool = False,
            ):
                async with self._state_manager.task_workflow(
                    context,
                    task,
                    on_loop_iteration=on_loop_iteration,
                    validating_effects=validating_effects,
                ) as complete:
                    try:
                        response = await (OrderedMapWorkflowStub(
                            context=context,
                            state_ref=context._state_ref,
                        ).Stringify(
                            IMPORT_typing.cast(rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapStringifyRequest, task.request),
                            bearer_token=__internal_magic_token__,
                        ))
                        await complete(task, (response, None))
                        return (response, None)
                    except IMPORT_asyncio.CancelledError:
                        # Check if the task was cancelled by a TasksServicer.
                        if self.tasks_dispatcher.is_task_cancelled(task.task_id.task_uuid):
                            # The running task was cancelled by a TasksServicer.
                            await complete(task, (None, IMPORT_rbt_v1alpha1.tasks_pb2.TaskCancelledError()))
                            return (None, IMPORT_rbt_v1alpha1.tasks_pb2.TaskCancelledError())
                        else:
                            raise
                    except IMPORT_rebootdev.aio.aborted.Aborted as aborted:
                        error_type = f'{aborted.error.__class__.__module__}.{aborted.error.__class__.__qualname__}'
                        # Do not retry a task if the error was specified in the
                        # proto file.
                        if error_type in self._specified_errors_by_service_method_name.get('rbt.std.collections.ordered_map.v1.OrderedMapMethods.Stringify', []):
                            await complete(task, (None, aborted.error))
                            return (None, aborted.error)
                        raise


            return await run_Stringify(
                self.create_context(
                    headers=IMPORT_reboot_aio_headers.Headers(
                        application_id=self.application_id,
                        state_ref=IMPORT_reboot_aio_types.StateRef(task.task_id.state_ref),
                    ),
                    state_type_name = IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.OrderedMap'),
                    method='Stringify',
                    context_type=IMPORT_reboot_aio_contexts.WorkflowContext,
                    task=task,
                )
            )

        # There are no tasks for this service.
        start_or_validate = "start" if not only_validate else "validate"
        raise RuntimeError(
            f"Attempted to {start_or_validate} task '{task.method_name}' "
            f"on 'OrderedMap' which does not exist"
        )

    # OrderedMap specific methods:
    async def __Create(
        self,
        context: IMPORT_reboot_aio_contexts.TransactionContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapCreateRequest,
        *,
        validating_effects: bool,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapCreateResponse:
        try:
            response = (
                await self._servicer._Create(
                    context=context,
                    state=state,
                    request=request
                )
            )
            IMPORT_reboot_aio_types.assert_type(
                response,
                [rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapCreateResponse],
            )
            self.maybe_raise_effect_validation_retry(
                logger=logger,
                idempotency_manager=context,
                method_name='OrderedMap.Create',
                validating_effects=validating_effects,
                context=context,
            )
            return response
        except IMPORT_reboot_aio_contexts.RetryReactively:
            # Retrying reactively, just let this propagate.
            raise
        except IMPORT_reboot_aio_contexts.EffectValidationRetry:
            # Doing effect validation, just let this propagate.
            raise
        except IMPORT_rebootdev.aio.aborted.Aborted as aborted:
            # If the caller aborted due to a retryable error, just
            # propagate the aborted instead of propagating `Unknown`
            # so that a client can transparently retry.
            if IMPORT_rebootdev.aio.aborted.is_retryable(aborted):
                raise aborted
            # Log any _unhandled_ abort stack traces to make it
            # easier for debugging.
            #
            # NOTE: we don't log if we're a task as it will be logged
            # in `public/rebootdev/aio/internals/tasks_dispatcher.py` instead.
            aborted_type: IMPORT_typing.Optional[type] = None
            aborted_type = OrderedMap.CreateAborted
            if isinstance(aborted, IMPORT_rebootdev.aio.aborted.SystemAborted):
                # Not logging when within `node` as we already log there.
                if IMPORT_reboot_nodejs_python.should_print_stacktrace():
                    logger.warning(
                        f"Unhandled (in 'rbt.std.collections.ordered_map.v1.OrderedMap.Create') {aborted}; propagating as 'Unknown'\n" +
                        ''.join(IMPORT_traceback.format_exception(aborted))
                    )
                raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                    IMPORT_rbt_v1alpha1.errors_pb2.Unknown(),
                    # TODO(benh): consider whether or not we want to
                    # include the 'package.service.method' which may
                    # get concatenated together forming a kind of
                    # "stack trace"; while it's super helpful for
                    # debugging, it does expose implementation
                    # information.
                    message=f"unhandled (in 'rbt.std.collections.ordered_map.v1.OrderedMap.Create') {aborted}"
                )
            else:
                if (
                    aborted_type is not None and
                    not isinstance(aborted, aborted_type) and
                    aborted_type.is_declared_error(aborted.error)
                ):
                    # We propagate declared errors that might have
                    # come from another call, i.e., we might have an
                    # `Aborted` but not for this method but the
                    # `Aborted` that we have has an error that this
                    # method declared. This allows a developer to
                    # simply add the declared error to their `.proto`
                    # file rather than having to catch and re-raise
                    # the error with their own aborted type.
                    if context.task is None:
                        logger.warning(
                            f"Propagating unhandled but declared error (in 'rbt.std.collections.ordered_map.v1.OrderedMap.Create') {aborted}"
                        )
                elif (
                    aborted_type is None or
                    not isinstance(aborted, aborted_type)
                ):
                    # Not logging when within `node` as we already log there.
                    if IMPORT_reboot_nodejs_python.should_print_stacktrace():
                        logger.warning(
                            f"Unhandled (in 'rbt.std.collections.ordered_map.v1.OrderedMap.Create') {aborted}; propagating as 'Unknown'\n" +
                            ''.join(IMPORT_traceback.format_exception(aborted))
                        )
                    # If this wasn't a declared error than we
                    # propagate it as `Unknown`.
                    raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                        IMPORT_rbt_v1alpha1.errors_pb2.Unknown(),
                        # TODO(benh): consider whether or not we want to
                        # include the 'package.service.method' which may
                        # get concatenated together forming a kind of
                        # "stack trace"; while it's super helpful for
                        # debugging, it does expose implementation
                        # information.
                        message=f"unhandled (in 'rbt.std.collections.ordered_map.v1.OrderedMap.Create') {aborted}"
                    )

            raise
        except IMPORT_asyncio.CancelledError:
            # It's pretty normal for an RPC to be cancelled; it's not useful to
            # print a stack trace.
            raise
        except IMPORT_google_protobuf_message.DecodeError as decode_error:
            # We usually see this error when we are trying to construct a proto
            # message which is too deeply nested: protobuf has a limit of 100
            # nested messages. See the limits here:
            #   https://protobuf.dev/programming-guides/proto-limits/

            if IMPORT_reboot_nodejs_python.should_print_stacktrace():
                logger.warning(
                    "Unhandled (in 'rbt.std.collections.ordered_map.v1.OrderedMap.Create') "
                    f"{type(decode_error).__name__}{': ' + str(decode_error) if len(str(decode_error)) > 0 else ''}; "
                    "This is usually caused by a deeply nested protobuf message, which is not supported by protobuf.\n"
                    "See the limits here: https://protobuf.dev/programming-guides/proto-limits/" +
                    ''.join(IMPORT_traceback.format_exception(decode_error))
                )
            raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                IMPORT_rbt_v1alpha1.errors_pb2.Unknown(),
                message=f"unhandled (in 'rbt.std.collections.ordered_map.v1.OrderedMap.Create') {decode_error}; "
                        "This is usually caused by a deeply nested protobuf message, which is not supported by protobuf.\n"
                        "See the limits here: https://protobuf.dev/programming-guides/proto-limits/"
            )
        except BaseException as exception:
            # Not logging when within `node` as we already log there.
            if IMPORT_reboot_nodejs_python.should_print_stacktrace():
                logger.warning(
                    "Unhandled (in 'rbt.std.collections.ordered_map.v1.OrderedMap.Create') "
                    f"{type(exception).__name__}{': ' + str(exception) if len(str(exception)) > 0 else ''}; "
                    "propagating as 'Unknown'\n" +
                    ''.join(IMPORT_traceback.format_exception(exception))
                )
            raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                IMPORT_rbt_v1alpha1.errors_pb2.Unknown(),
                # TODO(benh): consider whether or not we want to
                # include the 'package.service.method' which may
                # get concatenated together forming a kind of
                # "stack trace"; while it's super helpful for
                # debugging, it does expose implementation
                # information.
                message=f"unhandled (in 'rbt.std.collections.ordered_map.v1.OrderedMap.Create') {type(exception).__name__}: {exception}"
            )

    @IMPORT_reboot_aio_tracing.function_span(
        # We expect an `EffectValidationRetry` exception; that's not an error.
        set_status_on_exception=False
    )
    async def _Create(
        self,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapCreateRequest,
        context: IMPORT_reboot_aio_contexts.TransactionContext,
        *,
        validating_effects: bool,
        grpc_context: IMPORT_typing.Optional[IMPORT_grpc.aio.ServicerContext] = None,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapCreateResponse:
        # Try to verify the token if a token verifier exists.
        context.auth = await self._maybe_verify_token(
            headers=context._headers, method='Create'
        )

        # Check if we already have performed this mutation!
        #
        # We do this _before_ calling 'transactionally()' because
        # if this call is for a transaction method _and_ we've
        # already performed the transaction then we don't want to
        # become a transaction participant (again) we just want to
        # return the transaction's response.
        idempotent_mutation = self._state_manager.check_for_idempotent_mutation(
            context
        )

        if idempotent_mutation is not None:
            response = rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapCreateResponse()
            response.ParseFromString(idempotent_mutation.response)
            return response

        async with self._state_manager.transactionally(
            context,
            self.tasks_dispatcher,
            aborted_type=OrderedMap.CreateAborted,
        ) as transaction:
            if transaction is not None:
                context.participants.add(
                    self._servicer.__state_type_name__, context._state_ref
                )
            assert transaction is not None
            async with self._state_manager.transaction(
                context,
                self._servicer.__state_type__,
                transaction,
                authorize=self._maybe_authorize(
                    method_name='rbt.std.collections.ordered_map.v1.OrderedMapMethods.Create',
                    headers=context._headers,
                    auth=context.auth,
                    request=request,
                ),
                from_constructor=False,
                requires_constructor=False
            ) as (state, complete):

                response = await self.__Create(
                    context,
                    state,
                    request,
                    validating_effects=validating_effects,
                )

                await complete(
                    IMPORT_reboot_aio_state_managers.Effects(
                        state=state,
                        response=response,
                    )
                )
                return response

    async def _schedule_Create(
        self,
        *,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapCreateRequest,
        headers: IMPORT_reboot_aio_headers.Headers,
        grpc_context: IMPORT_grpc.aio.ServicerContext,
    ) -> tuple[IMPORT_reboot_aio_contexts.WriterContext, rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapCreateResponse]:
        context: IMPORT_reboot_aio_contexts.WriterContext = self.create_context(
            headers=headers,
            state_type_name = IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.OrderedMap'),
            method='Create',
            context_type=IMPORT_reboot_aio_contexts.WriterContext,
        )
        response = rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapCreateResponse()

        # Check if we already have performed this mutation!
        #
        # We do this _before_ calling 'transactionally()' because
        # if this call is for a transaction method _and_ we've
        # already performed the transaction then we don't want to
        # become a transaction participant (again) we just want to
        # return the transaction's response.
        idempotent_mutation = self._state_manager.check_for_idempotent_mutation(
            context
        )

        if idempotent_mutation is not None:
            response.ParseFromString(idempotent_mutation.response)

            # We should have only scheduled a single task!
            assert len(idempotent_mutation.task_ids) == 1
            assert grpc_context is not None
            grpc_context.set_trailing_metadata(
                grpc_context.trailing_metadata() +
                (
                    (
                        IMPORT_reboot_aio_headers.TASK_ID_UUID,
                        str(IMPORT_uuid.UUID(bytes=idempotent_mutation.task_ids[0].task_uuid))
                    ),
                )
            )

            return context, response

        async with self._state_manager.transactionally(
            context,
            self.tasks_dispatcher,
            aborted_type=OrderedMap.CreateAborted,
        ) as transaction:
            if transaction is not None:
                context.participants.add(
                    self._servicer.__state_type_name__, context._state_ref
                )

            # Try to verify the token if a token verifier exists.
            context.auth = await self._maybe_verify_token(
                headers=headers, method='Create'
            )

            async with self._state_manager.writer(
                context,
                self._servicer.__state_type__,
                self.tasks_dispatcher,
                transaction=transaction,
                authorize=self._maybe_authorize(
                    method_name='rbt.std.collections.ordered_map.v1.OrderedMapMethods.Create',
                    headers=context._headers,
                    auth=context.auth,
                    request=request,
                ),
                from_constructor=False,
                requires_constructor=False
            ) as (state, writer):

                task = await OrderedMapServicerTasks(
                    context=context,
                    state_ref=context._state_ref,
                ).Create(
                    request,
                    schedule=context._headers.task_schedule,
                )

                effects = IMPORT_reboot_aio_state_managers.Effects(
                    response=response,
                    state=state,
                    tasks=[task],
                )

                assert effects.tasks is not None

                await writer.complete(effects)

                assert grpc_context is not None

                grpc_context.set_trailing_metadata(
                    grpc_context.trailing_metadata() +
                    (
                        (
                            IMPORT_reboot_aio_headers.TASK_ID_UUID,
                            str(IMPORT_uuid.UUID(bytes=task.task_id.task_uuid))
                        ),
                    )
                )

                return context, response

        return context, response


    # Entrypoint for non-reactive network calls (i.e. typical gRPC calls).
    async def Create(
        self,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapCreateRequest,
        grpc_context: IMPORT_grpc.aio.ServicerContext,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapCreateResponse:
        headers = IMPORT_reboot_aio_headers.Headers.from_grpc_context(grpc_context)
        assert headers.application_id is not None  # Guaranteed by `Headers`.

        # Confirm whether this is the right consensus to be serving this
        # request.
        authoritative_consensus = self.placement_client.consensus_for_actor(
            headers.application_id,
            headers.state_ref,
        )
        if authoritative_consensus != self.consensus_id:
            # This is NOT the correct consensus. Fail.
            await grpc_context.abort(
                IMPORT_grpc.StatusCode.UNAVAILABLE,
                f"Consensus '{self.consensus_id}' is not authoritative for this "
                f"request; consensus '{authoritative_consensus}' is.",
            )
            raise  # Unreachable but necessary for mypy.

        @IMPORT_reboot_aio_internals_middleware.maybe_run_function_twice_to_validate_effects
        async def _run(
            validating_effects: bool,
        ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapCreateResponse:
            context: IMPORT_typing.Optional[IMPORT_reboot_aio_contexts.Context] = None
            try:
                if headers.task_schedule is not None:
                    context, response = await self._schedule_Create(
                        headers=headers,
                        request=request,
                        grpc_context=grpc_context,
                    )
                    return response

                context = self.create_context(
                    headers=headers,
                    state_type_name = IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.OrderedMap'),
                    method='Create',
                    context_type=IMPORT_reboot_aio_contexts.TransactionContext,
                )
                assert context is not None

                return await self._Create(
                    request,
                    context,
                    validating_effects=validating_effects,
                    grpc_context=grpc_context,
                )
            except IMPORT_reboot_aio_contexts.EffectValidationRetry:
                # Doing effect validation, just let this propagate.
                raise
            except IMPORT_rebootdev.aio.aborted.Aborted as aborted:
                status = IMPORT_rpc_status_sync.to_status(aborted.to_status())
                # Need to add transaction participants here because
                # calling `grpc_context.abort_with_status()` will
                # ignore any other trailing metadata.
                if context is not None and context.transaction_id is not None:
                    status = status._replace(
                        trailing_metadata=status.trailing_metadata + context.participants.to_grpc_metadata()
                    )
                await grpc_context.abort_with_status(status)
                raise  # Unreachable but necessary for mypy.
            except IMPORT_asyncio.CancelledError:
                # It's pretty normal for an RPC to be cancelled; it's not useful to
                # print a stack trace.
                raise
            except BaseException as exception:
                # Print the exception stack trace for easier debugging. Note
                # that we don't include the stack trace in an error message
                # for the same reason that gRPC doesn't do so by default,
                # see https://github.com/grpc/grpc/issues/14897, but since this
                # should only get logged on the server side it is safe.
                logger.warning(
                    'Unhandled exception\n' +
                    ''.join(IMPORT_traceback.format_exc() if IMPORT_reboot_nodejs_python.should_print_stacktrace() else [f"{type(exception).__name__}: {exception}"])
                )

                # Re-raise the exception for gRPC to handle!
                #
                # TODO: gRPC will print a stack trace from this
                # exception which we don't want if we're executing via
                # Node.js.
                raise
            finally:
                if context is not None and context.transaction_id is not None:
                    # Propagate transaction participants.
                    grpc_context.set_trailing_metadata(
                        grpc_context.trailing_metadata() +
                        context.participants.to_grpc_metadata()
                    )

        with IMPORT_reboot_aio_tracing.context_from_headers(headers):
            return await _run()

    async def __Search(
        self,
        context: IMPORT_reboot_aio_contexts.ReaderContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapSearchRequest,
        *,
        validating_effects: bool,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapSearchResponse:
        try:
            response = (
                await self._servicer._Search(
                    context=context,
                    state=state,
                    request=request
                )
            )
            IMPORT_reboot_aio_types.assert_type(
                response,
                [rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapSearchResponse],
            )
            self.maybe_raise_effect_validation_retry(
                logger=logger,
                idempotency_manager=context,
                method_name='OrderedMap.Search',
                validating_effects=validating_effects,
                context=context,
            )
            return response
        except IMPORT_reboot_aio_contexts.RetryReactively:
            # Retrying reactively, just let this propagate.
            raise
        except IMPORT_reboot_aio_contexts.EffectValidationRetry:
            # Doing effect validation, just let this propagate.
            raise
        except IMPORT_rebootdev.aio.aborted.Aborted as aborted:
            # If the caller aborted due to a retryable error, just
            # propagate the aborted instead of propagating `Unknown`
            # so that a client can transparently retry.
            if IMPORT_rebootdev.aio.aborted.is_retryable(aborted):
                raise aborted
            # Log any _unhandled_ abort stack traces to make it
            # easier for debugging.
            #
            # NOTE: we don't log if we're a task as it will be logged
            # in `public/rebootdev/aio/internals/tasks_dispatcher.py` instead.
            aborted_type: IMPORT_typing.Optional[type] = None
            aborted_type = OrderedMap.SearchAborted
            if isinstance(aborted, IMPORT_rebootdev.aio.aborted.SystemAborted):
                # Not logging when within `node` as we already log there.
                if IMPORT_reboot_nodejs_python.should_print_stacktrace():
                    logger.warning(
                        f"Unhandled (in 'rbt.std.collections.ordered_map.v1.OrderedMap.Search') {aborted}; propagating as 'Unknown'\n" +
                        ''.join(IMPORT_traceback.format_exception(aborted))
                    )
                raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                    IMPORT_rbt_v1alpha1.errors_pb2.Unknown(),
                    # TODO(benh): consider whether or not we want to
                    # include the 'package.service.method' which may
                    # get concatenated together forming a kind of
                    # "stack trace"; while it's super helpful for
                    # debugging, it does expose implementation
                    # information.
                    message=f"unhandled (in 'rbt.std.collections.ordered_map.v1.OrderedMap.Search') {aborted}"
                )
            else:
                if (
                    aborted_type is not None and
                    not isinstance(aborted, aborted_type) and
                    aborted_type.is_declared_error(aborted.error)
                ):
                    # We propagate declared errors that might have
                    # come from another call, i.e., we might have an
                    # `Aborted` but not for this method but the
                    # `Aborted` that we have has an error that this
                    # method declared. This allows a developer to
                    # simply add the declared error to their `.proto`
                    # file rather than having to catch and re-raise
                    # the error with their own aborted type.
                    if context.task is None:
                        logger.warning(
                            f"Propagating unhandled but declared error (in 'rbt.std.collections.ordered_map.v1.OrderedMap.Search') {aborted}"
                        )
                elif (
                    aborted_type is None or
                    not isinstance(aborted, aborted_type)
                ):
                    # Not logging when within `node` as we already log there.
                    if IMPORT_reboot_nodejs_python.should_print_stacktrace():
                        logger.warning(
                            f"Unhandled (in 'rbt.std.collections.ordered_map.v1.OrderedMap.Search') {aborted}; propagating as 'Unknown'\n" +
                            ''.join(IMPORT_traceback.format_exception(aborted))
                        )
                    # If this wasn't a declared error than we
                    # propagate it as `Unknown`.
                    raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                        IMPORT_rbt_v1alpha1.errors_pb2.Unknown(),
                        # TODO(benh): consider whether or not we want to
                        # include the 'package.service.method' which may
                        # get concatenated together forming a kind of
                        # "stack trace"; while it's super helpful for
                        # debugging, it does expose implementation
                        # information.
                        message=f"unhandled (in 'rbt.std.collections.ordered_map.v1.OrderedMap.Search') {aborted}"
                    )

            raise
        except IMPORT_asyncio.CancelledError:
            # It's pretty normal for an RPC to be cancelled; it's not useful to
            # print a stack trace.
            raise
        except IMPORT_google_protobuf_message.DecodeError as decode_error:
            # We usually see this error when we are trying to construct a proto
            # message which is too deeply nested: protobuf has a limit of 100
            # nested messages. See the limits here:
            #   https://protobuf.dev/programming-guides/proto-limits/

            if IMPORT_reboot_nodejs_python.should_print_stacktrace():
                logger.warning(
                    "Unhandled (in 'rbt.std.collections.ordered_map.v1.OrderedMap.Search') "
                    f"{type(decode_error).__name__}{': ' + str(decode_error) if len(str(decode_error)) > 0 else ''}; "
                    "This is usually caused by a deeply nested protobuf message, which is not supported by protobuf.\n"
                    "See the limits here: https://protobuf.dev/programming-guides/proto-limits/" +
                    ''.join(IMPORT_traceback.format_exception(decode_error))
                )
            raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                IMPORT_rbt_v1alpha1.errors_pb2.Unknown(),
                message=f"unhandled (in 'rbt.std.collections.ordered_map.v1.OrderedMap.Search') {decode_error}; "
                        "This is usually caused by a deeply nested protobuf message, which is not supported by protobuf.\n"
                        "See the limits here: https://protobuf.dev/programming-guides/proto-limits/"
            )
        except BaseException as exception:
            # Not logging when within `node` as we already log there.
            if IMPORT_reboot_nodejs_python.should_print_stacktrace():
                logger.warning(
                    "Unhandled (in 'rbt.std.collections.ordered_map.v1.OrderedMap.Search') "
                    f"{type(exception).__name__}{': ' + str(exception) if len(str(exception)) > 0 else ''}; "
                    "propagating as 'Unknown'\n" +
                    ''.join(IMPORT_traceback.format_exception(exception))
                )
            raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                IMPORT_rbt_v1alpha1.errors_pb2.Unknown(),
                # TODO(benh): consider whether or not we want to
                # include the 'package.service.method' which may
                # get concatenated together forming a kind of
                # "stack trace"; while it's super helpful for
                # debugging, it does expose implementation
                # information.
                message=f"unhandled (in 'rbt.std.collections.ordered_map.v1.OrderedMap.Search') {type(exception).__name__}: {exception}"
            )

    @IMPORT_reboot_aio_tracing.function_span(
        # We expect an `EffectValidationRetry` exception; that's not an error.
        set_status_on_exception=False
    )
    async def _Search(
        self,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapSearchRequest,
        context: IMPORT_reboot_aio_contexts.ReaderContext,
        *,
        validating_effects: bool,
        grpc_context: IMPORT_typing.Optional[IMPORT_grpc.aio.ServicerContext] = None,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapSearchResponse:
        # Try to verify the token if a token verifier exists.
        context.auth = await self._maybe_verify_token(
            headers=context._headers, method='Search'
        )


        async with self._state_manager.transactionally(
            context,
            self.tasks_dispatcher,
            aborted_type=OrderedMap.SearchAborted,
        ) as transaction:
            if transaction is not None:
                context.participants.add(
                    self._servicer.__state_type_name__, context._state_ref
                )
            authorizer = self._maybe_authorize(
                method_name='rbt.std.collections.ordered_map.v1.OrderedMapMethods.Search',
                headers=context._headers,
                auth=context.auth,
                request=request,
            )
            async with self._state_manager.reader(
                context,
                self._servicer.__state_type__,
                authorize=authorizer,
            ) as state:
                response = await self.__Search(
                    context,
                    state,
                    request,
                    validating_effects=validating_effects,
                )
                return response

    async def _schedule_Search(
        self,
        *,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapSearchRequest,
        headers: IMPORT_reboot_aio_headers.Headers,
        grpc_context: IMPORT_grpc.aio.ServicerContext,
    ) -> tuple[IMPORT_reboot_aio_contexts.WriterContext, rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapSearchResponse]:
        context: IMPORT_reboot_aio_contexts.WriterContext = self.create_context(
            headers=headers,
            state_type_name = IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.OrderedMap'),
            method='Search',
            context_type=IMPORT_reboot_aio_contexts.WriterContext,
        )
        response = rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapSearchResponse()


        async with self._state_manager.transactionally(
            context,
            self.tasks_dispatcher,
            aborted_type=OrderedMap.SearchAborted,
        ) as transaction:
            if transaction is not None:
                context.participants.add(
                    self._servicer.__state_type_name__, context._state_ref
                )

            # Try to verify the token if a token verifier exists.
            context.auth = await self._maybe_verify_token(
                headers=headers, method='Search'
            )

            async with self._state_manager.writer(
                context,
                self._servicer.__state_type__,
                self.tasks_dispatcher,
                transaction=transaction,
                authorize=self._maybe_authorize(
                    method_name='rbt.std.collections.ordered_map.v1.OrderedMapMethods.Search',
                    headers=context._headers,
                    auth=context.auth,
                    request=request,
                ),
                from_constructor=False,
                requires_constructor=False
            ) as (state, writer):

                task = await OrderedMapServicerTasks(
                    context=context,
                    state_ref=context._state_ref,
                ).Search(
                    request,
                    schedule=context._headers.task_schedule,
                )

                effects = IMPORT_reboot_aio_state_managers.Effects(
                    response=response,
                    state=state,
                    tasks=[task],
                )

                assert effects.tasks is not None

                await writer.complete(effects)

                assert grpc_context is not None

                grpc_context.set_trailing_metadata(
                    grpc_context.trailing_metadata() +
                    (
                        (
                            IMPORT_reboot_aio_headers.TASK_ID_UUID,
                            str(IMPORT_uuid.UUID(bytes=task.task_id.task_uuid))
                        ),
                    )
                )

                return context, response

        return context, response


    # Entrypoint for non-reactive network calls (i.e. typical gRPC calls).
    async def Search(
        self,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapSearchRequest,
        grpc_context: IMPORT_grpc.aio.ServicerContext,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapSearchResponse:
        headers = IMPORT_reboot_aio_headers.Headers.from_grpc_context(grpc_context)
        assert headers.application_id is not None  # Guaranteed by `Headers`.

        # Confirm whether this is the right consensus to be serving this
        # request.
        authoritative_consensus = self.placement_client.consensus_for_actor(
            headers.application_id,
            headers.state_ref,
        )
        if authoritative_consensus != self.consensus_id:
            # This is NOT the correct consensus. Fail.
            await grpc_context.abort(
                IMPORT_grpc.StatusCode.UNAVAILABLE,
                f"Consensus '{self.consensus_id}' is not authoritative for this "
                f"request; consensus '{authoritative_consensus}' is.",
            )
            raise  # Unreachable but necessary for mypy.

        @IMPORT_reboot_aio_internals_middleware.maybe_run_function_twice_to_validate_effects
        async def _run(
            validating_effects: bool,
        ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapSearchResponse:
            context: IMPORT_typing.Optional[IMPORT_reboot_aio_contexts.Context] = None
            try:
                if headers.task_schedule is not None:
                    context, response = await self._schedule_Search(
                        headers=headers,
                        request=request,
                        grpc_context=grpc_context,
                    )
                    return response

                context = self.create_context(
                    headers=headers,
                    state_type_name = IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.OrderedMap'),
                    method='Search',
                    context_type=IMPORT_reboot_aio_contexts.ReaderContext,
                )
                assert context is not None

                return await self._Search(
                    request,
                    context,
                    validating_effects=validating_effects,
                    grpc_context=grpc_context,
                )
            except IMPORT_reboot_aio_contexts.EffectValidationRetry:
                # Doing effect validation, just let this propagate.
                raise
            except IMPORT_rebootdev.aio.aborted.Aborted as aborted:
                status = IMPORT_rpc_status_sync.to_status(aborted.to_status())
                # Need to add transaction participants here because
                # calling `grpc_context.abort_with_status()` will
                # ignore any other trailing metadata.
                if context is not None and context.transaction_id is not None:
                    status = status._replace(
                        trailing_metadata=status.trailing_metadata + context.participants.to_grpc_metadata()
                    )
                await grpc_context.abort_with_status(status)
                raise  # Unreachable but necessary for mypy.
            except IMPORT_asyncio.CancelledError:
                # It's pretty normal for an RPC to be cancelled; it's not useful to
                # print a stack trace.
                raise
            except BaseException as exception:
                # Print the exception stack trace for easier debugging. Note
                # that we don't include the stack trace in an error message
                # for the same reason that gRPC doesn't do so by default,
                # see https://github.com/grpc/grpc/issues/14897, but since this
                # should only get logged on the server side it is safe.
                logger.warning(
                    'Unhandled exception\n' +
                    ''.join(IMPORT_traceback.format_exc() if IMPORT_reboot_nodejs_python.should_print_stacktrace() else [f"{type(exception).__name__}: {exception}"])
                )

                # Re-raise the exception for gRPC to handle!
                #
                # TODO: gRPC will print a stack trace from this
                # exception which we don't want if we're executing via
                # Node.js.
                raise
            finally:
                if context is not None and context.transaction_id is not None:
                    # Propagate transaction participants.
                    grpc_context.set_trailing_metadata(
                        grpc_context.trailing_metadata() +
                        context.participants.to_grpc_metadata()
                    )

        with IMPORT_reboot_aio_tracing.context_from_headers(headers):
            return await _run()

    async def __Insert(
        self,
        context: IMPORT_reboot_aio_contexts.TransactionContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapInsertRequest,
        *,
        validating_effects: bool,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapInsertResponse:
        try:
            response = (
                await self._servicer._Insert(
                    context=context,
                    state=state,
                    request=request
                )
            )
            IMPORT_reboot_aio_types.assert_type(
                response,
                [rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapInsertResponse],
            )
            self.maybe_raise_effect_validation_retry(
                logger=logger,
                idempotency_manager=context,
                method_name='OrderedMap.Insert',
                validating_effects=validating_effects,
                context=context,
            )
            return response
        except IMPORT_reboot_aio_contexts.RetryReactively:
            # Retrying reactively, just let this propagate.
            raise
        except IMPORT_reboot_aio_contexts.EffectValidationRetry:
            # Doing effect validation, just let this propagate.
            raise
        except IMPORT_rebootdev.aio.aborted.Aborted as aborted:
            # If the caller aborted due to a retryable error, just
            # propagate the aborted instead of propagating `Unknown`
            # so that a client can transparently retry.
            if IMPORT_rebootdev.aio.aborted.is_retryable(aborted):
                raise aborted
            # Log any _unhandled_ abort stack traces to make it
            # easier for debugging.
            #
            # NOTE: we don't log if we're a task as it will be logged
            # in `public/rebootdev/aio/internals/tasks_dispatcher.py` instead.
            aborted_type: IMPORT_typing.Optional[type] = None
            aborted_type = OrderedMap.InsertAborted
            if isinstance(aborted, IMPORT_rebootdev.aio.aborted.SystemAborted):
                # Not logging when within `node` as we already log there.
                if IMPORT_reboot_nodejs_python.should_print_stacktrace():
                    logger.warning(
                        f"Unhandled (in 'rbt.std.collections.ordered_map.v1.OrderedMap.Insert') {aborted}; propagating as 'Unknown'\n" +
                        ''.join(IMPORT_traceback.format_exception(aborted))
                    )
                raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                    IMPORT_rbt_v1alpha1.errors_pb2.Unknown(),
                    # TODO(benh): consider whether or not we want to
                    # include the 'package.service.method' which may
                    # get concatenated together forming a kind of
                    # "stack trace"; while it's super helpful for
                    # debugging, it does expose implementation
                    # information.
                    message=f"unhandled (in 'rbt.std.collections.ordered_map.v1.OrderedMap.Insert') {aborted}"
                )
            else:
                if (
                    aborted_type is not None and
                    not isinstance(aborted, aborted_type) and
                    aborted_type.is_declared_error(aborted.error)
                ):
                    # We propagate declared errors that might have
                    # come from another call, i.e., we might have an
                    # `Aborted` but not for this method but the
                    # `Aborted` that we have has an error that this
                    # method declared. This allows a developer to
                    # simply add the declared error to their `.proto`
                    # file rather than having to catch and re-raise
                    # the error with their own aborted type.
                    if context.task is None:
                        logger.warning(
                            f"Propagating unhandled but declared error (in 'rbt.std.collections.ordered_map.v1.OrderedMap.Insert') {aborted}"
                        )
                elif (
                    aborted_type is None or
                    not isinstance(aborted, aborted_type)
                ):
                    # Not logging when within `node` as we already log there.
                    if IMPORT_reboot_nodejs_python.should_print_stacktrace():
                        logger.warning(
                            f"Unhandled (in 'rbt.std.collections.ordered_map.v1.OrderedMap.Insert') {aborted}; propagating as 'Unknown'\n" +
                            ''.join(IMPORT_traceback.format_exception(aborted))
                        )
                    # If this wasn't a declared error than we
                    # propagate it as `Unknown`.
                    raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                        IMPORT_rbt_v1alpha1.errors_pb2.Unknown(),
                        # TODO(benh): consider whether or not we want to
                        # include the 'package.service.method' which may
                        # get concatenated together forming a kind of
                        # "stack trace"; while it's super helpful for
                        # debugging, it does expose implementation
                        # information.
                        message=f"unhandled (in 'rbt.std.collections.ordered_map.v1.OrderedMap.Insert') {aborted}"
                    )

            raise
        except IMPORT_asyncio.CancelledError:
            # It's pretty normal for an RPC to be cancelled; it's not useful to
            # print a stack trace.
            raise
        except IMPORT_google_protobuf_message.DecodeError as decode_error:
            # We usually see this error when we are trying to construct a proto
            # message which is too deeply nested: protobuf has a limit of 100
            # nested messages. See the limits here:
            #   https://protobuf.dev/programming-guides/proto-limits/

            if IMPORT_reboot_nodejs_python.should_print_stacktrace():
                logger.warning(
                    "Unhandled (in 'rbt.std.collections.ordered_map.v1.OrderedMap.Insert') "
                    f"{type(decode_error).__name__}{': ' + str(decode_error) if len(str(decode_error)) > 0 else ''}; "
                    "This is usually caused by a deeply nested protobuf message, which is not supported by protobuf.\n"
                    "See the limits here: https://protobuf.dev/programming-guides/proto-limits/" +
                    ''.join(IMPORT_traceback.format_exception(decode_error))
                )
            raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                IMPORT_rbt_v1alpha1.errors_pb2.Unknown(),
                message=f"unhandled (in 'rbt.std.collections.ordered_map.v1.OrderedMap.Insert') {decode_error}; "
                        "This is usually caused by a deeply nested protobuf message, which is not supported by protobuf.\n"
                        "See the limits here: https://protobuf.dev/programming-guides/proto-limits/"
            )
        except BaseException as exception:
            # Not logging when within `node` as we already log there.
            if IMPORT_reboot_nodejs_python.should_print_stacktrace():
                logger.warning(
                    "Unhandled (in 'rbt.std.collections.ordered_map.v1.OrderedMap.Insert') "
                    f"{type(exception).__name__}{': ' + str(exception) if len(str(exception)) > 0 else ''}; "
                    "propagating as 'Unknown'\n" +
                    ''.join(IMPORT_traceback.format_exception(exception))
                )
            raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                IMPORT_rbt_v1alpha1.errors_pb2.Unknown(),
                # TODO(benh): consider whether or not we want to
                # include the 'package.service.method' which may
                # get concatenated together forming a kind of
                # "stack trace"; while it's super helpful for
                # debugging, it does expose implementation
                # information.
                message=f"unhandled (in 'rbt.std.collections.ordered_map.v1.OrderedMap.Insert') {type(exception).__name__}: {exception}"
            )

    @IMPORT_reboot_aio_tracing.function_span(
        # We expect an `EffectValidationRetry` exception; that's not an error.
        set_status_on_exception=False
    )
    async def _Insert(
        self,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapInsertRequest,
        context: IMPORT_reboot_aio_contexts.TransactionContext,
        *,
        validating_effects: bool,
        grpc_context: IMPORT_typing.Optional[IMPORT_grpc.aio.ServicerContext] = None,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapInsertResponse:
        # Try to verify the token if a token verifier exists.
        context.auth = await self._maybe_verify_token(
            headers=context._headers, method='Insert'
        )

        # Check if we already have performed this mutation!
        #
        # We do this _before_ calling 'transactionally()' because
        # if this call is for a transaction method _and_ we've
        # already performed the transaction then we don't want to
        # become a transaction participant (again) we just want to
        # return the transaction's response.
        idempotent_mutation = self._state_manager.check_for_idempotent_mutation(
            context
        )

        if idempotent_mutation is not None:
            response = rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapInsertResponse()
            response.ParseFromString(idempotent_mutation.response)
            return response

        async with self._state_manager.transactionally(
            context,
            self.tasks_dispatcher,
            aborted_type=OrderedMap.InsertAborted,
        ) as transaction:
            if transaction is not None:
                context.participants.add(
                    self._servicer.__state_type_name__, context._state_ref
                )
            assert transaction is not None
            async with self._state_manager.transaction(
                context,
                self._servicer.__state_type__,
                transaction,
                authorize=self._maybe_authorize(
                    method_name='rbt.std.collections.ordered_map.v1.OrderedMapMethods.Insert',
                    headers=context._headers,
                    auth=context.auth,
                    request=request,
                ),
                from_constructor=False,
                requires_constructor=False
            ) as (state, complete):

                response = await self.__Insert(
                    context,
                    state,
                    request,
                    validating_effects=validating_effects,
                )

                await complete(
                    IMPORT_reboot_aio_state_managers.Effects(
                        state=state,
                        response=response,
                    )
                )
                return response

    async def _schedule_Insert(
        self,
        *,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapInsertRequest,
        headers: IMPORT_reboot_aio_headers.Headers,
        grpc_context: IMPORT_grpc.aio.ServicerContext,
    ) -> tuple[IMPORT_reboot_aio_contexts.WriterContext, rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapInsertResponse]:
        context: IMPORT_reboot_aio_contexts.WriterContext = self.create_context(
            headers=headers,
            state_type_name = IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.OrderedMap'),
            method='Insert',
            context_type=IMPORT_reboot_aio_contexts.WriterContext,
        )
        response = rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapInsertResponse()

        # Check if we already have performed this mutation!
        #
        # We do this _before_ calling 'transactionally()' because
        # if this call is for a transaction method _and_ we've
        # already performed the transaction then we don't want to
        # become a transaction participant (again) we just want to
        # return the transaction's response.
        idempotent_mutation = self._state_manager.check_for_idempotent_mutation(
            context
        )

        if idempotent_mutation is not None:
            response.ParseFromString(idempotent_mutation.response)

            # We should have only scheduled a single task!
            assert len(idempotent_mutation.task_ids) == 1
            assert grpc_context is not None
            grpc_context.set_trailing_metadata(
                grpc_context.trailing_metadata() +
                (
                    (
                        IMPORT_reboot_aio_headers.TASK_ID_UUID,
                        str(IMPORT_uuid.UUID(bytes=idempotent_mutation.task_ids[0].task_uuid))
                    ),
                )
            )

            return context, response

        async with self._state_manager.transactionally(
            context,
            self.tasks_dispatcher,
            aborted_type=OrderedMap.InsertAborted,
        ) as transaction:
            if transaction is not None:
                context.participants.add(
                    self._servicer.__state_type_name__, context._state_ref
                )

            # Try to verify the token if a token verifier exists.
            context.auth = await self._maybe_verify_token(
                headers=headers, method='Insert'
            )

            async with self._state_manager.writer(
                context,
                self._servicer.__state_type__,
                self.tasks_dispatcher,
                transaction=transaction,
                authorize=self._maybe_authorize(
                    method_name='rbt.std.collections.ordered_map.v1.OrderedMapMethods.Insert',
                    headers=context._headers,
                    auth=context.auth,
                    request=request,
                ),
                from_constructor=False,
                requires_constructor=False
            ) as (state, writer):

                task = await OrderedMapServicerTasks(
                    context=context,
                    state_ref=context._state_ref,
                ).Insert(
                    request,
                    schedule=context._headers.task_schedule,
                )

                effects = IMPORT_reboot_aio_state_managers.Effects(
                    response=response,
                    state=state,
                    tasks=[task],
                )

                assert effects.tasks is not None

                await writer.complete(effects)

                assert grpc_context is not None

                grpc_context.set_trailing_metadata(
                    grpc_context.trailing_metadata() +
                    (
                        (
                            IMPORT_reboot_aio_headers.TASK_ID_UUID,
                            str(IMPORT_uuid.UUID(bytes=task.task_id.task_uuid))
                        ),
                    )
                )

                return context, response

        return context, response


    # Entrypoint for non-reactive network calls (i.e. typical gRPC calls).
    async def Insert(
        self,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapInsertRequest,
        grpc_context: IMPORT_grpc.aio.ServicerContext,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapInsertResponse:
        headers = IMPORT_reboot_aio_headers.Headers.from_grpc_context(grpc_context)
        assert headers.application_id is not None  # Guaranteed by `Headers`.

        # Confirm whether this is the right consensus to be serving this
        # request.
        authoritative_consensus = self.placement_client.consensus_for_actor(
            headers.application_id,
            headers.state_ref,
        )
        if authoritative_consensus != self.consensus_id:
            # This is NOT the correct consensus. Fail.
            await grpc_context.abort(
                IMPORT_grpc.StatusCode.UNAVAILABLE,
                f"Consensus '{self.consensus_id}' is not authoritative for this "
                f"request; consensus '{authoritative_consensus}' is.",
            )
            raise  # Unreachable but necessary for mypy.

        @IMPORT_reboot_aio_internals_middleware.maybe_run_function_twice_to_validate_effects
        async def _run(
            validating_effects: bool,
        ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapInsertResponse:
            context: IMPORT_typing.Optional[IMPORT_reboot_aio_contexts.Context] = None
            try:
                if headers.task_schedule is not None:
                    context, response = await self._schedule_Insert(
                        headers=headers,
                        request=request,
                        grpc_context=grpc_context,
                    )
                    return response

                context = self.create_context(
                    headers=headers,
                    state_type_name = IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.OrderedMap'),
                    method='Insert',
                    context_type=IMPORT_reboot_aio_contexts.TransactionContext,
                )
                assert context is not None

                return await self._Insert(
                    request,
                    context,
                    validating_effects=validating_effects,
                    grpc_context=grpc_context,
                )
            except IMPORT_reboot_aio_contexts.EffectValidationRetry:
                # Doing effect validation, just let this propagate.
                raise
            except IMPORT_rebootdev.aio.aborted.Aborted as aborted:
                status = IMPORT_rpc_status_sync.to_status(aborted.to_status())
                # Need to add transaction participants here because
                # calling `grpc_context.abort_with_status()` will
                # ignore any other trailing metadata.
                if context is not None and context.transaction_id is not None:
                    status = status._replace(
                        trailing_metadata=status.trailing_metadata + context.participants.to_grpc_metadata()
                    )
                await grpc_context.abort_with_status(status)
                raise  # Unreachable but necessary for mypy.
            except IMPORT_asyncio.CancelledError:
                # It's pretty normal for an RPC to be cancelled; it's not useful to
                # print a stack trace.
                raise
            except BaseException as exception:
                # Print the exception stack trace for easier debugging. Note
                # that we don't include the stack trace in an error message
                # for the same reason that gRPC doesn't do so by default,
                # see https://github.com/grpc/grpc/issues/14897, but since this
                # should only get logged on the server side it is safe.
                logger.warning(
                    'Unhandled exception\n' +
                    ''.join(IMPORT_traceback.format_exc() if IMPORT_reboot_nodejs_python.should_print_stacktrace() else [f"{type(exception).__name__}: {exception}"])
                )

                # Re-raise the exception for gRPC to handle!
                #
                # TODO: gRPC will print a stack trace from this
                # exception which we don't want if we're executing via
                # Node.js.
                raise
            finally:
                if context is not None and context.transaction_id is not None:
                    # Propagate transaction participants.
                    grpc_context.set_trailing_metadata(
                        grpc_context.trailing_metadata() +
                        context.participants.to_grpc_metadata()
                    )

        with IMPORT_reboot_aio_tracing.context_from_headers(headers):
            return await _run()

    async def __Remove(
        self,
        context: IMPORT_reboot_aio_contexts.TransactionContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRemoveRequest,
        *,
        validating_effects: bool,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRemoveResponse:
        try:
            response = (
                await self._servicer._Remove(
                    context=context,
                    state=state,
                    request=request
                )
            )
            IMPORT_reboot_aio_types.assert_type(
                response,
                [rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRemoveResponse],
            )
            self.maybe_raise_effect_validation_retry(
                logger=logger,
                idempotency_manager=context,
                method_name='OrderedMap.Remove',
                validating_effects=validating_effects,
                context=context,
            )
            return response
        except IMPORT_reboot_aio_contexts.RetryReactively:
            # Retrying reactively, just let this propagate.
            raise
        except IMPORT_reboot_aio_contexts.EffectValidationRetry:
            # Doing effect validation, just let this propagate.
            raise
        except IMPORT_rebootdev.aio.aborted.Aborted as aborted:
            # If the caller aborted due to a retryable error, just
            # propagate the aborted instead of propagating `Unknown`
            # so that a client can transparently retry.
            if IMPORT_rebootdev.aio.aborted.is_retryable(aborted):
                raise aborted
            # Log any _unhandled_ abort stack traces to make it
            # easier for debugging.
            #
            # NOTE: we don't log if we're a task as it will be logged
            # in `public/rebootdev/aio/internals/tasks_dispatcher.py` instead.
            aborted_type: IMPORT_typing.Optional[type] = None
            aborted_type = OrderedMap.RemoveAborted
            if isinstance(aborted, IMPORT_rebootdev.aio.aborted.SystemAborted):
                # Not logging when within `node` as we already log there.
                if IMPORT_reboot_nodejs_python.should_print_stacktrace():
                    logger.warning(
                        f"Unhandled (in 'rbt.std.collections.ordered_map.v1.OrderedMap.Remove') {aborted}; propagating as 'Unknown'\n" +
                        ''.join(IMPORT_traceback.format_exception(aborted))
                    )
                raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                    IMPORT_rbt_v1alpha1.errors_pb2.Unknown(),
                    # TODO(benh): consider whether or not we want to
                    # include the 'package.service.method' which may
                    # get concatenated together forming a kind of
                    # "stack trace"; while it's super helpful for
                    # debugging, it does expose implementation
                    # information.
                    message=f"unhandled (in 'rbt.std.collections.ordered_map.v1.OrderedMap.Remove') {aborted}"
                )
            else:
                if (
                    aborted_type is not None and
                    not isinstance(aborted, aborted_type) and
                    aborted_type.is_declared_error(aborted.error)
                ):
                    # We propagate declared errors that might have
                    # come from another call, i.e., we might have an
                    # `Aborted` but not for this method but the
                    # `Aborted` that we have has an error that this
                    # method declared. This allows a developer to
                    # simply add the declared error to their `.proto`
                    # file rather than having to catch and re-raise
                    # the error with their own aborted type.
                    if context.task is None:
                        logger.warning(
                            f"Propagating unhandled but declared error (in 'rbt.std.collections.ordered_map.v1.OrderedMap.Remove') {aborted}"
                        )
                elif (
                    aborted_type is None or
                    not isinstance(aborted, aborted_type)
                ):
                    # Not logging when within `node` as we already log there.
                    if IMPORT_reboot_nodejs_python.should_print_stacktrace():
                        logger.warning(
                            f"Unhandled (in 'rbt.std.collections.ordered_map.v1.OrderedMap.Remove') {aborted}; propagating as 'Unknown'\n" +
                            ''.join(IMPORT_traceback.format_exception(aborted))
                        )
                    # If this wasn't a declared error than we
                    # propagate it as `Unknown`.
                    raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                        IMPORT_rbt_v1alpha1.errors_pb2.Unknown(),
                        # TODO(benh): consider whether or not we want to
                        # include the 'package.service.method' which may
                        # get concatenated together forming a kind of
                        # "stack trace"; while it's super helpful for
                        # debugging, it does expose implementation
                        # information.
                        message=f"unhandled (in 'rbt.std.collections.ordered_map.v1.OrderedMap.Remove') {aborted}"
                    )

            raise
        except IMPORT_asyncio.CancelledError:
            # It's pretty normal for an RPC to be cancelled; it's not useful to
            # print a stack trace.
            raise
        except IMPORT_google_protobuf_message.DecodeError as decode_error:
            # We usually see this error when we are trying to construct a proto
            # message which is too deeply nested: protobuf has a limit of 100
            # nested messages. See the limits here:
            #   https://protobuf.dev/programming-guides/proto-limits/

            if IMPORT_reboot_nodejs_python.should_print_stacktrace():
                logger.warning(
                    "Unhandled (in 'rbt.std.collections.ordered_map.v1.OrderedMap.Remove') "
                    f"{type(decode_error).__name__}{': ' + str(decode_error) if len(str(decode_error)) > 0 else ''}; "
                    "This is usually caused by a deeply nested protobuf message, which is not supported by protobuf.\n"
                    "See the limits here: https://protobuf.dev/programming-guides/proto-limits/" +
                    ''.join(IMPORT_traceback.format_exception(decode_error))
                )
            raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                IMPORT_rbt_v1alpha1.errors_pb2.Unknown(),
                message=f"unhandled (in 'rbt.std.collections.ordered_map.v1.OrderedMap.Remove') {decode_error}; "
                        "This is usually caused by a deeply nested protobuf message, which is not supported by protobuf.\n"
                        "See the limits here: https://protobuf.dev/programming-guides/proto-limits/"
            )
        except BaseException as exception:
            # Not logging when within `node` as we already log there.
            if IMPORT_reboot_nodejs_python.should_print_stacktrace():
                logger.warning(
                    "Unhandled (in 'rbt.std.collections.ordered_map.v1.OrderedMap.Remove') "
                    f"{type(exception).__name__}{': ' + str(exception) if len(str(exception)) > 0 else ''}; "
                    "propagating as 'Unknown'\n" +
                    ''.join(IMPORT_traceback.format_exception(exception))
                )
            raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                IMPORT_rbt_v1alpha1.errors_pb2.Unknown(),
                # TODO(benh): consider whether or not we want to
                # include the 'package.service.method' which may
                # get concatenated together forming a kind of
                # "stack trace"; while it's super helpful for
                # debugging, it does expose implementation
                # information.
                message=f"unhandled (in 'rbt.std.collections.ordered_map.v1.OrderedMap.Remove') {type(exception).__name__}: {exception}"
            )

    @IMPORT_reboot_aio_tracing.function_span(
        # We expect an `EffectValidationRetry` exception; that's not an error.
        set_status_on_exception=False
    )
    async def _Remove(
        self,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRemoveRequest,
        context: IMPORT_reboot_aio_contexts.TransactionContext,
        *,
        validating_effects: bool,
        grpc_context: IMPORT_typing.Optional[IMPORT_grpc.aio.ServicerContext] = None,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRemoveResponse:
        # Try to verify the token if a token verifier exists.
        context.auth = await self._maybe_verify_token(
            headers=context._headers, method='Remove'
        )

        # Check if we already have performed this mutation!
        #
        # We do this _before_ calling 'transactionally()' because
        # if this call is for a transaction method _and_ we've
        # already performed the transaction then we don't want to
        # become a transaction participant (again) we just want to
        # return the transaction's response.
        idempotent_mutation = self._state_manager.check_for_idempotent_mutation(
            context
        )

        if idempotent_mutation is not None:
            response = rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRemoveResponse()
            response.ParseFromString(idempotent_mutation.response)
            return response

        async with self._state_manager.transactionally(
            context,
            self.tasks_dispatcher,
            aborted_type=OrderedMap.RemoveAborted,
        ) as transaction:
            if transaction is not None:
                context.participants.add(
                    self._servicer.__state_type_name__, context._state_ref
                )
            assert transaction is not None
            async with self._state_manager.transaction(
                context,
                self._servicer.__state_type__,
                transaction,
                authorize=self._maybe_authorize(
                    method_name='rbt.std.collections.ordered_map.v1.OrderedMapMethods.Remove',
                    headers=context._headers,
                    auth=context.auth,
                    request=request,
                ),
                from_constructor=False,
                requires_constructor=False
            ) as (state, complete):

                response = await self.__Remove(
                    context,
                    state,
                    request,
                    validating_effects=validating_effects,
                )

                await complete(
                    IMPORT_reboot_aio_state_managers.Effects(
                        state=state,
                        response=response,
                    )
                )
                return response

    async def _schedule_Remove(
        self,
        *,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRemoveRequest,
        headers: IMPORT_reboot_aio_headers.Headers,
        grpc_context: IMPORT_grpc.aio.ServicerContext,
    ) -> tuple[IMPORT_reboot_aio_contexts.WriterContext, rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRemoveResponse]:
        context: IMPORT_reboot_aio_contexts.WriterContext = self.create_context(
            headers=headers,
            state_type_name = IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.OrderedMap'),
            method='Remove',
            context_type=IMPORT_reboot_aio_contexts.WriterContext,
        )
        response = rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRemoveResponse()

        # Check if we already have performed this mutation!
        #
        # We do this _before_ calling 'transactionally()' because
        # if this call is for a transaction method _and_ we've
        # already performed the transaction then we don't want to
        # become a transaction participant (again) we just want to
        # return the transaction's response.
        idempotent_mutation = self._state_manager.check_for_idempotent_mutation(
            context
        )

        if idempotent_mutation is not None:
            response.ParseFromString(idempotent_mutation.response)

            # We should have only scheduled a single task!
            assert len(idempotent_mutation.task_ids) == 1
            assert grpc_context is not None
            grpc_context.set_trailing_metadata(
                grpc_context.trailing_metadata() +
                (
                    (
                        IMPORT_reboot_aio_headers.TASK_ID_UUID,
                        str(IMPORT_uuid.UUID(bytes=idempotent_mutation.task_ids[0].task_uuid))
                    ),
                )
            )

            return context, response

        async with self._state_manager.transactionally(
            context,
            self.tasks_dispatcher,
            aborted_type=OrderedMap.RemoveAborted,
        ) as transaction:
            if transaction is not None:
                context.participants.add(
                    self._servicer.__state_type_name__, context._state_ref
                )

            # Try to verify the token if a token verifier exists.
            context.auth = await self._maybe_verify_token(
                headers=headers, method='Remove'
            )

            async with self._state_manager.writer(
                context,
                self._servicer.__state_type__,
                self.tasks_dispatcher,
                transaction=transaction,
                authorize=self._maybe_authorize(
                    method_name='rbt.std.collections.ordered_map.v1.OrderedMapMethods.Remove',
                    headers=context._headers,
                    auth=context.auth,
                    request=request,
                ),
                from_constructor=False,
                requires_constructor=False
            ) as (state, writer):

                task = await OrderedMapServicerTasks(
                    context=context,
                    state_ref=context._state_ref,
                ).Remove(
                    request,
                    schedule=context._headers.task_schedule,
                )

                effects = IMPORT_reboot_aio_state_managers.Effects(
                    response=response,
                    state=state,
                    tasks=[task],
                )

                assert effects.tasks is not None

                await writer.complete(effects)

                assert grpc_context is not None

                grpc_context.set_trailing_metadata(
                    grpc_context.trailing_metadata() +
                    (
                        (
                            IMPORT_reboot_aio_headers.TASK_ID_UUID,
                            str(IMPORT_uuid.UUID(bytes=task.task_id.task_uuid))
                        ),
                    )
                )

                return context, response

        return context, response


    # Entrypoint for non-reactive network calls (i.e. typical gRPC calls).
    async def Remove(
        self,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRemoveRequest,
        grpc_context: IMPORT_grpc.aio.ServicerContext,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRemoveResponse:
        headers = IMPORT_reboot_aio_headers.Headers.from_grpc_context(grpc_context)
        assert headers.application_id is not None  # Guaranteed by `Headers`.

        # Confirm whether this is the right consensus to be serving this
        # request.
        authoritative_consensus = self.placement_client.consensus_for_actor(
            headers.application_id,
            headers.state_ref,
        )
        if authoritative_consensus != self.consensus_id:
            # This is NOT the correct consensus. Fail.
            await grpc_context.abort(
                IMPORT_grpc.StatusCode.UNAVAILABLE,
                f"Consensus '{self.consensus_id}' is not authoritative for this "
                f"request; consensus '{authoritative_consensus}' is.",
            )
            raise  # Unreachable but necessary for mypy.

        @IMPORT_reboot_aio_internals_middleware.maybe_run_function_twice_to_validate_effects
        async def _run(
            validating_effects: bool,
        ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRemoveResponse:
            context: IMPORT_typing.Optional[IMPORT_reboot_aio_contexts.Context] = None
            try:
                if headers.task_schedule is not None:
                    context, response = await self._schedule_Remove(
                        headers=headers,
                        request=request,
                        grpc_context=grpc_context,
                    )
                    return response

                context = self.create_context(
                    headers=headers,
                    state_type_name = IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.OrderedMap'),
                    method='Remove',
                    context_type=IMPORT_reboot_aio_contexts.TransactionContext,
                )
                assert context is not None

                return await self._Remove(
                    request,
                    context,
                    validating_effects=validating_effects,
                    grpc_context=grpc_context,
                )
            except IMPORT_reboot_aio_contexts.EffectValidationRetry:
                # Doing effect validation, just let this propagate.
                raise
            except IMPORT_rebootdev.aio.aborted.Aborted as aborted:
                status = IMPORT_rpc_status_sync.to_status(aborted.to_status())
                # Need to add transaction participants here because
                # calling `grpc_context.abort_with_status()` will
                # ignore any other trailing metadata.
                if context is not None and context.transaction_id is not None:
                    status = status._replace(
                        trailing_metadata=status.trailing_metadata + context.participants.to_grpc_metadata()
                    )
                await grpc_context.abort_with_status(status)
                raise  # Unreachable but necessary for mypy.
            except IMPORT_asyncio.CancelledError:
                # It's pretty normal for an RPC to be cancelled; it's not useful to
                # print a stack trace.
                raise
            except BaseException as exception:
                # Print the exception stack trace for easier debugging. Note
                # that we don't include the stack trace in an error message
                # for the same reason that gRPC doesn't do so by default,
                # see https://github.com/grpc/grpc/issues/14897, but since this
                # should only get logged on the server side it is safe.
                logger.warning(
                    'Unhandled exception\n' +
                    ''.join(IMPORT_traceback.format_exc() if IMPORT_reboot_nodejs_python.should_print_stacktrace() else [f"{type(exception).__name__}: {exception}"])
                )

                # Re-raise the exception for gRPC to handle!
                #
                # TODO: gRPC will print a stack trace from this
                # exception which we don't want if we're executing via
                # Node.js.
                raise
            finally:
                if context is not None and context.transaction_id is not None:
                    # Propagate transaction participants.
                    grpc_context.set_trailing_metadata(
                        grpc_context.trailing_metadata() +
                        context.participants.to_grpc_metadata()
                    )

        with IMPORT_reboot_aio_tracing.context_from_headers(headers):
            return await _run()

    async def __Range(
        self,
        context: IMPORT_reboot_aio_contexts.ReaderContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRangeRequest,
        *,
        validating_effects: bool,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRangeResponse:
        try:
            response = (
                await self._servicer._Range(
                    context=context,
                    state=state,
                    request=request
                )
            )
            IMPORT_reboot_aio_types.assert_type(
                response,
                [rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRangeResponse],
            )
            self.maybe_raise_effect_validation_retry(
                logger=logger,
                idempotency_manager=context,
                method_name='OrderedMap.Range',
                validating_effects=validating_effects,
                context=context,
            )
            return response
        except IMPORT_reboot_aio_contexts.RetryReactively:
            # Retrying reactively, just let this propagate.
            raise
        except IMPORT_reboot_aio_contexts.EffectValidationRetry:
            # Doing effect validation, just let this propagate.
            raise
        except IMPORT_rebootdev.aio.aborted.Aborted as aborted:
            # If the caller aborted due to a retryable error, just
            # propagate the aborted instead of propagating `Unknown`
            # so that a client can transparently retry.
            if IMPORT_rebootdev.aio.aborted.is_retryable(aborted):
                raise aborted
            # Log any _unhandled_ abort stack traces to make it
            # easier for debugging.
            #
            # NOTE: we don't log if we're a task as it will be logged
            # in `public/rebootdev/aio/internals/tasks_dispatcher.py` instead.
            aborted_type: IMPORT_typing.Optional[type] = None
            aborted_type = OrderedMap.RangeAborted
            if isinstance(aborted, IMPORT_rebootdev.aio.aborted.SystemAborted):
                # Not logging when within `node` as we already log there.
                if IMPORT_reboot_nodejs_python.should_print_stacktrace():
                    logger.warning(
                        f"Unhandled (in 'rbt.std.collections.ordered_map.v1.OrderedMap.Range') {aborted}; propagating as 'Unknown'\n" +
                        ''.join(IMPORT_traceback.format_exception(aborted))
                    )
                raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                    IMPORT_rbt_v1alpha1.errors_pb2.Unknown(),
                    # TODO(benh): consider whether or not we want to
                    # include the 'package.service.method' which may
                    # get concatenated together forming a kind of
                    # "stack trace"; while it's super helpful for
                    # debugging, it does expose implementation
                    # information.
                    message=f"unhandled (in 'rbt.std.collections.ordered_map.v1.OrderedMap.Range') {aborted}"
                )
            else:
                if (
                    aborted_type is not None and
                    not isinstance(aborted, aborted_type) and
                    aborted_type.is_declared_error(aborted.error)
                ):
                    # We propagate declared errors that might have
                    # come from another call, i.e., we might have an
                    # `Aborted` but not for this method but the
                    # `Aborted` that we have has an error that this
                    # method declared. This allows a developer to
                    # simply add the declared error to their `.proto`
                    # file rather than having to catch and re-raise
                    # the error with their own aborted type.
                    if context.task is None:
                        logger.warning(
                            f"Propagating unhandled but declared error (in 'rbt.std.collections.ordered_map.v1.OrderedMap.Range') {aborted}"
                        )
                elif (
                    aborted_type is None or
                    not isinstance(aborted, aborted_type)
                ):
                    # Not logging when within `node` as we already log there.
                    if IMPORT_reboot_nodejs_python.should_print_stacktrace():
                        logger.warning(
                            f"Unhandled (in 'rbt.std.collections.ordered_map.v1.OrderedMap.Range') {aborted}; propagating as 'Unknown'\n" +
                            ''.join(IMPORT_traceback.format_exception(aborted))
                        )
                    # If this wasn't a declared error than we
                    # propagate it as `Unknown`.
                    raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                        IMPORT_rbt_v1alpha1.errors_pb2.Unknown(),
                        # TODO(benh): consider whether or not we want to
                        # include the 'package.service.method' which may
                        # get concatenated together forming a kind of
                        # "stack trace"; while it's super helpful for
                        # debugging, it does expose implementation
                        # information.
                        message=f"unhandled (in 'rbt.std.collections.ordered_map.v1.OrderedMap.Range') {aborted}"
                    )

            raise
        except IMPORT_asyncio.CancelledError:
            # It's pretty normal for an RPC to be cancelled; it's not useful to
            # print a stack trace.
            raise
        except IMPORT_google_protobuf_message.DecodeError as decode_error:
            # We usually see this error when we are trying to construct a proto
            # message which is too deeply nested: protobuf has a limit of 100
            # nested messages. See the limits here:
            #   https://protobuf.dev/programming-guides/proto-limits/

            if IMPORT_reboot_nodejs_python.should_print_stacktrace():
                logger.warning(
                    "Unhandled (in 'rbt.std.collections.ordered_map.v1.OrderedMap.Range') "
                    f"{type(decode_error).__name__}{': ' + str(decode_error) if len(str(decode_error)) > 0 else ''}; "
                    "This is usually caused by a deeply nested protobuf message, which is not supported by protobuf.\n"
                    "See the limits here: https://protobuf.dev/programming-guides/proto-limits/" +
                    ''.join(IMPORT_traceback.format_exception(decode_error))
                )
            raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                IMPORT_rbt_v1alpha1.errors_pb2.Unknown(),
                message=f"unhandled (in 'rbt.std.collections.ordered_map.v1.OrderedMap.Range') {decode_error}; "
                        "This is usually caused by a deeply nested protobuf message, which is not supported by protobuf.\n"
                        "See the limits here: https://protobuf.dev/programming-guides/proto-limits/"
            )
        except BaseException as exception:
            # Not logging when within `node` as we already log there.
            if IMPORT_reboot_nodejs_python.should_print_stacktrace():
                logger.warning(
                    "Unhandled (in 'rbt.std.collections.ordered_map.v1.OrderedMap.Range') "
                    f"{type(exception).__name__}{': ' + str(exception) if len(str(exception)) > 0 else ''}; "
                    "propagating as 'Unknown'\n" +
                    ''.join(IMPORT_traceback.format_exception(exception))
                )
            raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                IMPORT_rbt_v1alpha1.errors_pb2.Unknown(),
                # TODO(benh): consider whether or not we want to
                # include the 'package.service.method' which may
                # get concatenated together forming a kind of
                # "stack trace"; while it's super helpful for
                # debugging, it does expose implementation
                # information.
                message=f"unhandled (in 'rbt.std.collections.ordered_map.v1.OrderedMap.Range') {type(exception).__name__}: {exception}"
            )

    @IMPORT_reboot_aio_tracing.function_span(
        # We expect an `EffectValidationRetry` exception; that's not an error.
        set_status_on_exception=False
    )
    async def _Range(
        self,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRangeRequest,
        context: IMPORT_reboot_aio_contexts.ReaderContext,
        *,
        validating_effects: bool,
        grpc_context: IMPORT_typing.Optional[IMPORT_grpc.aio.ServicerContext] = None,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRangeResponse:
        # Try to verify the token if a token verifier exists.
        context.auth = await self._maybe_verify_token(
            headers=context._headers, method='Range'
        )


        async with self._state_manager.transactionally(
            context,
            self.tasks_dispatcher,
            aborted_type=OrderedMap.RangeAborted,
        ) as transaction:
            if transaction is not None:
                context.participants.add(
                    self._servicer.__state_type_name__, context._state_ref
                )
            authorizer = self._maybe_authorize(
                method_name='rbt.std.collections.ordered_map.v1.OrderedMapMethods.Range',
                headers=context._headers,
                auth=context.auth,
                request=request,
            )
            async with self._state_manager.reader(
                context,
                self._servicer.__state_type__,
                authorize=authorizer,
            ) as state:
                response = await self.__Range(
                    context,
                    state,
                    request,
                    validating_effects=validating_effects,
                )
                return response

    async def _schedule_Range(
        self,
        *,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRangeRequest,
        headers: IMPORT_reboot_aio_headers.Headers,
        grpc_context: IMPORT_grpc.aio.ServicerContext,
    ) -> tuple[IMPORT_reboot_aio_contexts.WriterContext, rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRangeResponse]:
        context: IMPORT_reboot_aio_contexts.WriterContext = self.create_context(
            headers=headers,
            state_type_name = IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.OrderedMap'),
            method='Range',
            context_type=IMPORT_reboot_aio_contexts.WriterContext,
        )
        response = rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRangeResponse()


        async with self._state_manager.transactionally(
            context,
            self.tasks_dispatcher,
            aborted_type=OrderedMap.RangeAborted,
        ) as transaction:
            if transaction is not None:
                context.participants.add(
                    self._servicer.__state_type_name__, context._state_ref
                )

            # Try to verify the token if a token verifier exists.
            context.auth = await self._maybe_verify_token(
                headers=headers, method='Range'
            )

            async with self._state_manager.writer(
                context,
                self._servicer.__state_type__,
                self.tasks_dispatcher,
                transaction=transaction,
                authorize=self._maybe_authorize(
                    method_name='rbt.std.collections.ordered_map.v1.OrderedMapMethods.Range',
                    headers=context._headers,
                    auth=context.auth,
                    request=request,
                ),
                from_constructor=False,
                requires_constructor=False
            ) as (state, writer):

                task = await OrderedMapServicerTasks(
                    context=context,
                    state_ref=context._state_ref,
                ).Range(
                    request,
                    schedule=context._headers.task_schedule,
                )

                effects = IMPORT_reboot_aio_state_managers.Effects(
                    response=response,
                    state=state,
                    tasks=[task],
                )

                assert effects.tasks is not None

                await writer.complete(effects)

                assert grpc_context is not None

                grpc_context.set_trailing_metadata(
                    grpc_context.trailing_metadata() +
                    (
                        (
                            IMPORT_reboot_aio_headers.TASK_ID_UUID,
                            str(IMPORT_uuid.UUID(bytes=task.task_id.task_uuid))
                        ),
                    )
                )

                return context, response

        return context, response


    # Entrypoint for non-reactive network calls (i.e. typical gRPC calls).
    async def Range(
        self,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRangeRequest,
        grpc_context: IMPORT_grpc.aio.ServicerContext,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRangeResponse:
        headers = IMPORT_reboot_aio_headers.Headers.from_grpc_context(grpc_context)
        assert headers.application_id is not None  # Guaranteed by `Headers`.

        # Confirm whether this is the right consensus to be serving this
        # request.
        authoritative_consensus = self.placement_client.consensus_for_actor(
            headers.application_id,
            headers.state_ref,
        )
        if authoritative_consensus != self.consensus_id:
            # This is NOT the correct consensus. Fail.
            await grpc_context.abort(
                IMPORT_grpc.StatusCode.UNAVAILABLE,
                f"Consensus '{self.consensus_id}' is not authoritative for this "
                f"request; consensus '{authoritative_consensus}' is.",
            )
            raise  # Unreachable but necessary for mypy.

        @IMPORT_reboot_aio_internals_middleware.maybe_run_function_twice_to_validate_effects
        async def _run(
            validating_effects: bool,
        ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRangeResponse:
            context: IMPORT_typing.Optional[IMPORT_reboot_aio_contexts.Context] = None
            try:
                if headers.task_schedule is not None:
                    context, response = await self._schedule_Range(
                        headers=headers,
                        request=request,
                        grpc_context=grpc_context,
                    )
                    return response

                context = self.create_context(
                    headers=headers,
                    state_type_name = IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.OrderedMap'),
                    method='Range',
                    context_type=IMPORT_reboot_aio_contexts.ReaderContext,
                )
                assert context is not None

                return await self._Range(
                    request,
                    context,
                    validating_effects=validating_effects,
                    grpc_context=grpc_context,
                )
            except IMPORT_reboot_aio_contexts.EffectValidationRetry:
                # Doing effect validation, just let this propagate.
                raise
            except IMPORT_rebootdev.aio.aborted.Aborted as aborted:
                status = IMPORT_rpc_status_sync.to_status(aborted.to_status())
                # Need to add transaction participants here because
                # calling `grpc_context.abort_with_status()` will
                # ignore any other trailing metadata.
                if context is not None and context.transaction_id is not None:
                    status = status._replace(
                        trailing_metadata=status.trailing_metadata + context.participants.to_grpc_metadata()
                    )
                await grpc_context.abort_with_status(status)
                raise  # Unreachable but necessary for mypy.
            except IMPORT_asyncio.CancelledError:
                # It's pretty normal for an RPC to be cancelled; it's not useful to
                # print a stack trace.
                raise
            except BaseException as exception:
                # Print the exception stack trace for easier debugging. Note
                # that we don't include the stack trace in an error message
                # for the same reason that gRPC doesn't do so by default,
                # see https://github.com/grpc/grpc/issues/14897, but since this
                # should only get logged on the server side it is safe.
                logger.warning(
                    'Unhandled exception\n' +
                    ''.join(IMPORT_traceback.format_exc() if IMPORT_reboot_nodejs_python.should_print_stacktrace() else [f"{type(exception).__name__}: {exception}"])
                )

                # Re-raise the exception for gRPC to handle!
                #
                # TODO: gRPC will print a stack trace from this
                # exception which we don't want if we're executing via
                # Node.js.
                raise
            finally:
                if context is not None and context.transaction_id is not None:
                    # Propagate transaction participants.
                    grpc_context.set_trailing_metadata(
                        grpc_context.trailing_metadata() +
                        context.participants.to_grpc_metadata()
                    )

        with IMPORT_reboot_aio_tracing.context_from_headers(headers):
            return await _run()

    async def __ReverseRange(
        self,
        context: IMPORT_reboot_aio_contexts.ReaderContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapReverseRangeRequest,
        *,
        validating_effects: bool,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapReverseRangeResponse:
        try:
            response = (
                await self._servicer._ReverseRange(
                    context=context,
                    state=state,
                    request=request
                )
            )
            IMPORT_reboot_aio_types.assert_type(
                response,
                [rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapReverseRangeResponse],
            )
            self.maybe_raise_effect_validation_retry(
                logger=logger,
                idempotency_manager=context,
                method_name='OrderedMap.ReverseRange',
                validating_effects=validating_effects,
                context=context,
            )
            return response
        except IMPORT_reboot_aio_contexts.RetryReactively:
            # Retrying reactively, just let this propagate.
            raise
        except IMPORT_reboot_aio_contexts.EffectValidationRetry:
            # Doing effect validation, just let this propagate.
            raise
        except IMPORT_rebootdev.aio.aborted.Aborted as aborted:
            # If the caller aborted due to a retryable error, just
            # propagate the aborted instead of propagating `Unknown`
            # so that a client can transparently retry.
            if IMPORT_rebootdev.aio.aborted.is_retryable(aborted):
                raise aborted
            # Log any _unhandled_ abort stack traces to make it
            # easier for debugging.
            #
            # NOTE: we don't log if we're a task as it will be logged
            # in `public/rebootdev/aio/internals/tasks_dispatcher.py` instead.
            aborted_type: IMPORT_typing.Optional[type] = None
            aborted_type = OrderedMap.ReverseRangeAborted
            if isinstance(aborted, IMPORT_rebootdev.aio.aborted.SystemAborted):
                # Not logging when within `node` as we already log there.
                if IMPORT_reboot_nodejs_python.should_print_stacktrace():
                    logger.warning(
                        f"Unhandled (in 'rbt.std.collections.ordered_map.v1.OrderedMap.ReverseRange') {aborted}; propagating as 'Unknown'\n" +
                        ''.join(IMPORT_traceback.format_exception(aborted))
                    )
                raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                    IMPORT_rbt_v1alpha1.errors_pb2.Unknown(),
                    # TODO(benh): consider whether or not we want to
                    # include the 'package.service.method' which may
                    # get concatenated together forming a kind of
                    # "stack trace"; while it's super helpful for
                    # debugging, it does expose implementation
                    # information.
                    message=f"unhandled (in 'rbt.std.collections.ordered_map.v1.OrderedMap.ReverseRange') {aborted}"
                )
            else:
                if (
                    aborted_type is not None and
                    not isinstance(aborted, aborted_type) and
                    aborted_type.is_declared_error(aborted.error)
                ):
                    # We propagate declared errors that might have
                    # come from another call, i.e., we might have an
                    # `Aborted` but not for this method but the
                    # `Aborted` that we have has an error that this
                    # method declared. This allows a developer to
                    # simply add the declared error to their `.proto`
                    # file rather than having to catch and re-raise
                    # the error with their own aborted type.
                    if context.task is None:
                        logger.warning(
                            f"Propagating unhandled but declared error (in 'rbt.std.collections.ordered_map.v1.OrderedMap.ReverseRange') {aborted}"
                        )
                elif (
                    aborted_type is None or
                    not isinstance(aborted, aborted_type)
                ):
                    # Not logging when within `node` as we already log there.
                    if IMPORT_reboot_nodejs_python.should_print_stacktrace():
                        logger.warning(
                            f"Unhandled (in 'rbt.std.collections.ordered_map.v1.OrderedMap.ReverseRange') {aborted}; propagating as 'Unknown'\n" +
                            ''.join(IMPORT_traceback.format_exception(aborted))
                        )
                    # If this wasn't a declared error than we
                    # propagate it as `Unknown`.
                    raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                        IMPORT_rbt_v1alpha1.errors_pb2.Unknown(),
                        # TODO(benh): consider whether or not we want to
                        # include the 'package.service.method' which may
                        # get concatenated together forming a kind of
                        # "stack trace"; while it's super helpful for
                        # debugging, it does expose implementation
                        # information.
                        message=f"unhandled (in 'rbt.std.collections.ordered_map.v1.OrderedMap.ReverseRange') {aborted}"
                    )

            raise
        except IMPORT_asyncio.CancelledError:
            # It's pretty normal for an RPC to be cancelled; it's not useful to
            # print a stack trace.
            raise
        except IMPORT_google_protobuf_message.DecodeError as decode_error:
            # We usually see this error when we are trying to construct a proto
            # message which is too deeply nested: protobuf has a limit of 100
            # nested messages. See the limits here:
            #   https://protobuf.dev/programming-guides/proto-limits/

            if IMPORT_reboot_nodejs_python.should_print_stacktrace():
                logger.warning(
                    "Unhandled (in 'rbt.std.collections.ordered_map.v1.OrderedMap.ReverseRange') "
                    f"{type(decode_error).__name__}{': ' + str(decode_error) if len(str(decode_error)) > 0 else ''}; "
                    "This is usually caused by a deeply nested protobuf message, which is not supported by protobuf.\n"
                    "See the limits here: https://protobuf.dev/programming-guides/proto-limits/" +
                    ''.join(IMPORT_traceback.format_exception(decode_error))
                )
            raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                IMPORT_rbt_v1alpha1.errors_pb2.Unknown(),
                message=f"unhandled (in 'rbt.std.collections.ordered_map.v1.OrderedMap.ReverseRange') {decode_error}; "
                        "This is usually caused by a deeply nested protobuf message, which is not supported by protobuf.\n"
                        "See the limits here: https://protobuf.dev/programming-guides/proto-limits/"
            )
        except BaseException as exception:
            # Not logging when within `node` as we already log there.
            if IMPORT_reboot_nodejs_python.should_print_stacktrace():
                logger.warning(
                    "Unhandled (in 'rbt.std.collections.ordered_map.v1.OrderedMap.ReverseRange') "
                    f"{type(exception).__name__}{': ' + str(exception) if len(str(exception)) > 0 else ''}; "
                    "propagating as 'Unknown'\n" +
                    ''.join(IMPORT_traceback.format_exception(exception))
                )
            raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                IMPORT_rbt_v1alpha1.errors_pb2.Unknown(),
                # TODO(benh): consider whether or not we want to
                # include the 'package.service.method' which may
                # get concatenated together forming a kind of
                # "stack trace"; while it's super helpful for
                # debugging, it does expose implementation
                # information.
                message=f"unhandled (in 'rbt.std.collections.ordered_map.v1.OrderedMap.ReverseRange') {type(exception).__name__}: {exception}"
            )

    @IMPORT_reboot_aio_tracing.function_span(
        # We expect an `EffectValidationRetry` exception; that's not an error.
        set_status_on_exception=False
    )
    async def _ReverseRange(
        self,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapReverseRangeRequest,
        context: IMPORT_reboot_aio_contexts.ReaderContext,
        *,
        validating_effects: bool,
        grpc_context: IMPORT_typing.Optional[IMPORT_grpc.aio.ServicerContext] = None,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapReverseRangeResponse:
        # Try to verify the token if a token verifier exists.
        context.auth = await self._maybe_verify_token(
            headers=context._headers, method='ReverseRange'
        )


        async with self._state_manager.transactionally(
            context,
            self.tasks_dispatcher,
            aborted_type=OrderedMap.ReverseRangeAborted,
        ) as transaction:
            if transaction is not None:
                context.participants.add(
                    self._servicer.__state_type_name__, context._state_ref
                )
            authorizer = self._maybe_authorize(
                method_name='rbt.std.collections.ordered_map.v1.OrderedMapMethods.ReverseRange',
                headers=context._headers,
                auth=context.auth,
                request=request,
            )
            async with self._state_manager.reader(
                context,
                self._servicer.__state_type__,
                authorize=authorizer,
            ) as state:
                response = await self.__ReverseRange(
                    context,
                    state,
                    request,
                    validating_effects=validating_effects,
                )
                return response

    async def _schedule_ReverseRange(
        self,
        *,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapReverseRangeRequest,
        headers: IMPORT_reboot_aio_headers.Headers,
        grpc_context: IMPORT_grpc.aio.ServicerContext,
    ) -> tuple[IMPORT_reboot_aio_contexts.WriterContext, rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapReverseRangeResponse]:
        context: IMPORT_reboot_aio_contexts.WriterContext = self.create_context(
            headers=headers,
            state_type_name = IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.OrderedMap'),
            method='ReverseRange',
            context_type=IMPORT_reboot_aio_contexts.WriterContext,
        )
        response = rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapReverseRangeResponse()


        async with self._state_manager.transactionally(
            context,
            self.tasks_dispatcher,
            aborted_type=OrderedMap.ReverseRangeAborted,
        ) as transaction:
            if transaction is not None:
                context.participants.add(
                    self._servicer.__state_type_name__, context._state_ref
                )

            # Try to verify the token if a token verifier exists.
            context.auth = await self._maybe_verify_token(
                headers=headers, method='ReverseRange'
            )

            async with self._state_manager.writer(
                context,
                self._servicer.__state_type__,
                self.tasks_dispatcher,
                transaction=transaction,
                authorize=self._maybe_authorize(
                    method_name='rbt.std.collections.ordered_map.v1.OrderedMapMethods.ReverseRange',
                    headers=context._headers,
                    auth=context.auth,
                    request=request,
                ),
                from_constructor=False,
                requires_constructor=False
            ) as (state, writer):

                task = await OrderedMapServicerTasks(
                    context=context,
                    state_ref=context._state_ref,
                ).ReverseRange(
                    request,
                    schedule=context._headers.task_schedule,
                )

                effects = IMPORT_reboot_aio_state_managers.Effects(
                    response=response,
                    state=state,
                    tasks=[task],
                )

                assert effects.tasks is not None

                await writer.complete(effects)

                assert grpc_context is not None

                grpc_context.set_trailing_metadata(
                    grpc_context.trailing_metadata() +
                    (
                        (
                            IMPORT_reboot_aio_headers.TASK_ID_UUID,
                            str(IMPORT_uuid.UUID(bytes=task.task_id.task_uuid))
                        ),
                    )
                )

                return context, response

        return context, response


    # Entrypoint for non-reactive network calls (i.e. typical gRPC calls).
    async def ReverseRange(
        self,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapReverseRangeRequest,
        grpc_context: IMPORT_grpc.aio.ServicerContext,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapReverseRangeResponse:
        headers = IMPORT_reboot_aio_headers.Headers.from_grpc_context(grpc_context)
        assert headers.application_id is not None  # Guaranteed by `Headers`.

        # Confirm whether this is the right consensus to be serving this
        # request.
        authoritative_consensus = self.placement_client.consensus_for_actor(
            headers.application_id,
            headers.state_ref,
        )
        if authoritative_consensus != self.consensus_id:
            # This is NOT the correct consensus. Fail.
            await grpc_context.abort(
                IMPORT_grpc.StatusCode.UNAVAILABLE,
                f"Consensus '{self.consensus_id}' is not authoritative for this "
                f"request; consensus '{authoritative_consensus}' is.",
            )
            raise  # Unreachable but necessary for mypy.

        @IMPORT_reboot_aio_internals_middleware.maybe_run_function_twice_to_validate_effects
        async def _run(
            validating_effects: bool,
        ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapReverseRangeResponse:
            context: IMPORT_typing.Optional[IMPORT_reboot_aio_contexts.Context] = None
            try:
                if headers.task_schedule is not None:
                    context, response = await self._schedule_ReverseRange(
                        headers=headers,
                        request=request,
                        grpc_context=grpc_context,
                    )
                    return response

                context = self.create_context(
                    headers=headers,
                    state_type_name = IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.OrderedMap'),
                    method='ReverseRange',
                    context_type=IMPORT_reboot_aio_contexts.ReaderContext,
                )
                assert context is not None

                return await self._ReverseRange(
                    request,
                    context,
                    validating_effects=validating_effects,
                    grpc_context=grpc_context,
                )
            except IMPORT_reboot_aio_contexts.EffectValidationRetry:
                # Doing effect validation, just let this propagate.
                raise
            except IMPORT_rebootdev.aio.aborted.Aborted as aborted:
                status = IMPORT_rpc_status_sync.to_status(aborted.to_status())
                # Need to add transaction participants here because
                # calling `grpc_context.abort_with_status()` will
                # ignore any other trailing metadata.
                if context is not None and context.transaction_id is not None:
                    status = status._replace(
                        trailing_metadata=status.trailing_metadata + context.participants.to_grpc_metadata()
                    )
                await grpc_context.abort_with_status(status)
                raise  # Unreachable but necessary for mypy.
            except IMPORT_asyncio.CancelledError:
                # It's pretty normal for an RPC to be cancelled; it's not useful to
                # print a stack trace.
                raise
            except BaseException as exception:
                # Print the exception stack trace for easier debugging. Note
                # that we don't include the stack trace in an error message
                # for the same reason that gRPC doesn't do so by default,
                # see https://github.com/grpc/grpc/issues/14897, but since this
                # should only get logged on the server side it is safe.
                logger.warning(
                    'Unhandled exception\n' +
                    ''.join(IMPORT_traceback.format_exc() if IMPORT_reboot_nodejs_python.should_print_stacktrace() else [f"{type(exception).__name__}: {exception}"])
                )

                # Re-raise the exception for gRPC to handle!
                #
                # TODO: gRPC will print a stack trace from this
                # exception which we don't want if we're executing via
                # Node.js.
                raise
            finally:
                if context is not None and context.transaction_id is not None:
                    # Propagate transaction participants.
                    grpc_context.set_trailing_metadata(
                        grpc_context.trailing_metadata() +
                        context.participants.to_grpc_metadata()
                    )

        with IMPORT_reboot_aio_tracing.context_from_headers(headers):
            return await _run()

    async def __Stringify(
        self,
        context: IMPORT_reboot_aio_contexts.ReaderContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapStringifyRequest,
        *,
        validating_effects: bool,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapStringifyResponse:
        try:
            response = (
                await self._servicer._Stringify(
                    context=context,
                    state=state,
                    request=request
                )
            )
            IMPORT_reboot_aio_types.assert_type(
                response,
                [rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapStringifyResponse],
            )
            self.maybe_raise_effect_validation_retry(
                logger=logger,
                idempotency_manager=context,
                method_name='OrderedMap.Stringify',
                validating_effects=validating_effects,
                context=context,
            )
            return response
        except IMPORT_reboot_aio_contexts.RetryReactively:
            # Retrying reactively, just let this propagate.
            raise
        except IMPORT_reboot_aio_contexts.EffectValidationRetry:
            # Doing effect validation, just let this propagate.
            raise
        except IMPORT_rebootdev.aio.aborted.Aborted as aborted:
            # If the caller aborted due to a retryable error, just
            # propagate the aborted instead of propagating `Unknown`
            # so that a client can transparently retry.
            if IMPORT_rebootdev.aio.aborted.is_retryable(aborted):
                raise aborted
            # Log any _unhandled_ abort stack traces to make it
            # easier for debugging.
            #
            # NOTE: we don't log if we're a task as it will be logged
            # in `public/rebootdev/aio/internals/tasks_dispatcher.py` instead.
            aborted_type: IMPORT_typing.Optional[type] = None
            aborted_type = OrderedMap.StringifyAborted
            if isinstance(aborted, IMPORT_rebootdev.aio.aborted.SystemAborted):
                # Not logging when within `node` as we already log there.
                if IMPORT_reboot_nodejs_python.should_print_stacktrace():
                    logger.warning(
                        f"Unhandled (in 'rbt.std.collections.ordered_map.v1.OrderedMap.Stringify') {aborted}; propagating as 'Unknown'\n" +
                        ''.join(IMPORT_traceback.format_exception(aborted))
                    )
                raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                    IMPORT_rbt_v1alpha1.errors_pb2.Unknown(),
                    # TODO(benh): consider whether or not we want to
                    # include the 'package.service.method' which may
                    # get concatenated together forming a kind of
                    # "stack trace"; while it's super helpful for
                    # debugging, it does expose implementation
                    # information.
                    message=f"unhandled (in 'rbt.std.collections.ordered_map.v1.OrderedMap.Stringify') {aborted}"
                )
            else:
                if (
                    aborted_type is not None and
                    not isinstance(aborted, aborted_type) and
                    aborted_type.is_declared_error(aborted.error)
                ):
                    # We propagate declared errors that might have
                    # come from another call, i.e., we might have an
                    # `Aborted` but not for this method but the
                    # `Aborted` that we have has an error that this
                    # method declared. This allows a developer to
                    # simply add the declared error to their `.proto`
                    # file rather than having to catch and re-raise
                    # the error with their own aborted type.
                    if context.task is None:
                        logger.warning(
                            f"Propagating unhandled but declared error (in 'rbt.std.collections.ordered_map.v1.OrderedMap.Stringify') {aborted}"
                        )
                elif (
                    aborted_type is None or
                    not isinstance(aborted, aborted_type)
                ):
                    # Not logging when within `node` as we already log there.
                    if IMPORT_reboot_nodejs_python.should_print_stacktrace():
                        logger.warning(
                            f"Unhandled (in 'rbt.std.collections.ordered_map.v1.OrderedMap.Stringify') {aborted}; propagating as 'Unknown'\n" +
                            ''.join(IMPORT_traceback.format_exception(aborted))
                        )
                    # If this wasn't a declared error than we
                    # propagate it as `Unknown`.
                    raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                        IMPORT_rbt_v1alpha1.errors_pb2.Unknown(),
                        # TODO(benh): consider whether or not we want to
                        # include the 'package.service.method' which may
                        # get concatenated together forming a kind of
                        # "stack trace"; while it's super helpful for
                        # debugging, it does expose implementation
                        # information.
                        message=f"unhandled (in 'rbt.std.collections.ordered_map.v1.OrderedMap.Stringify') {aborted}"
                    )

            raise
        except IMPORT_asyncio.CancelledError:
            # It's pretty normal for an RPC to be cancelled; it's not useful to
            # print a stack trace.
            raise
        except IMPORT_google_protobuf_message.DecodeError as decode_error:
            # We usually see this error when we are trying to construct a proto
            # message which is too deeply nested: protobuf has a limit of 100
            # nested messages. See the limits here:
            #   https://protobuf.dev/programming-guides/proto-limits/

            if IMPORT_reboot_nodejs_python.should_print_stacktrace():
                logger.warning(
                    "Unhandled (in 'rbt.std.collections.ordered_map.v1.OrderedMap.Stringify') "
                    f"{type(decode_error).__name__}{': ' + str(decode_error) if len(str(decode_error)) > 0 else ''}; "
                    "This is usually caused by a deeply nested protobuf message, which is not supported by protobuf.\n"
                    "See the limits here: https://protobuf.dev/programming-guides/proto-limits/" +
                    ''.join(IMPORT_traceback.format_exception(decode_error))
                )
            raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                IMPORT_rbt_v1alpha1.errors_pb2.Unknown(),
                message=f"unhandled (in 'rbt.std.collections.ordered_map.v1.OrderedMap.Stringify') {decode_error}; "
                        "This is usually caused by a deeply nested protobuf message, which is not supported by protobuf.\n"
                        "See the limits here: https://protobuf.dev/programming-guides/proto-limits/"
            )
        except BaseException as exception:
            # Not logging when within `node` as we already log there.
            if IMPORT_reboot_nodejs_python.should_print_stacktrace():
                logger.warning(
                    "Unhandled (in 'rbt.std.collections.ordered_map.v1.OrderedMap.Stringify') "
                    f"{type(exception).__name__}{': ' + str(exception) if len(str(exception)) > 0 else ''}; "
                    "propagating as 'Unknown'\n" +
                    ''.join(IMPORT_traceback.format_exception(exception))
                )
            raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                IMPORT_rbt_v1alpha1.errors_pb2.Unknown(),
                # TODO(benh): consider whether or not we want to
                # include the 'package.service.method' which may
                # get concatenated together forming a kind of
                # "stack trace"; while it's super helpful for
                # debugging, it does expose implementation
                # information.
                message=f"unhandled (in 'rbt.std.collections.ordered_map.v1.OrderedMap.Stringify') {type(exception).__name__}: {exception}"
            )

    @IMPORT_reboot_aio_tracing.function_span(
        # We expect an `EffectValidationRetry` exception; that's not an error.
        set_status_on_exception=False
    )
    async def _Stringify(
        self,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapStringifyRequest,
        context: IMPORT_reboot_aio_contexts.ReaderContext,
        *,
        validating_effects: bool,
        grpc_context: IMPORT_typing.Optional[IMPORT_grpc.aio.ServicerContext] = None,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapStringifyResponse:
        # Try to verify the token if a token verifier exists.
        context.auth = await self._maybe_verify_token(
            headers=context._headers, method='Stringify'
        )


        async with self._state_manager.transactionally(
            context,
            self.tasks_dispatcher,
            aborted_type=OrderedMap.StringifyAborted,
        ) as transaction:
            if transaction is not None:
                context.participants.add(
                    self._servicer.__state_type_name__, context._state_ref
                )
            authorizer = self._maybe_authorize(
                method_name='rbt.std.collections.ordered_map.v1.OrderedMapMethods.Stringify',
                headers=context._headers,
                auth=context.auth,
                request=request,
            )
            async with self._state_manager.reader(
                context,
                self._servicer.__state_type__,
                authorize=authorizer,
            ) as state:
                response = await self.__Stringify(
                    context,
                    state,
                    request,
                    validating_effects=validating_effects,
                )
                return response

    async def _schedule_Stringify(
        self,
        *,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapStringifyRequest,
        headers: IMPORT_reboot_aio_headers.Headers,
        grpc_context: IMPORT_grpc.aio.ServicerContext,
    ) -> tuple[IMPORT_reboot_aio_contexts.WriterContext, rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapStringifyResponse]:
        context: IMPORT_reboot_aio_contexts.WriterContext = self.create_context(
            headers=headers,
            state_type_name = IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.OrderedMap'),
            method='Stringify',
            context_type=IMPORT_reboot_aio_contexts.WriterContext,
        )
        response = rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapStringifyResponse()


        async with self._state_manager.transactionally(
            context,
            self.tasks_dispatcher,
            aborted_type=OrderedMap.StringifyAborted,
        ) as transaction:
            if transaction is not None:
                context.participants.add(
                    self._servicer.__state_type_name__, context._state_ref
                )

            # Try to verify the token if a token verifier exists.
            context.auth = await self._maybe_verify_token(
                headers=headers, method='Stringify'
            )

            async with self._state_manager.writer(
                context,
                self._servicer.__state_type__,
                self.tasks_dispatcher,
                transaction=transaction,
                authorize=self._maybe_authorize(
                    method_name='rbt.std.collections.ordered_map.v1.OrderedMapMethods.Stringify',
                    headers=context._headers,
                    auth=context.auth,
                    request=request,
                ),
                from_constructor=False,
                requires_constructor=False
            ) as (state, writer):

                task = await OrderedMapServicerTasks(
                    context=context,
                    state_ref=context._state_ref,
                ).Stringify(
                    request,
                    schedule=context._headers.task_schedule,
                )

                effects = IMPORT_reboot_aio_state_managers.Effects(
                    response=response,
                    state=state,
                    tasks=[task],
                )

                assert effects.tasks is not None

                await writer.complete(effects)

                assert grpc_context is not None

                grpc_context.set_trailing_metadata(
                    grpc_context.trailing_metadata() +
                    (
                        (
                            IMPORT_reboot_aio_headers.TASK_ID_UUID,
                            str(IMPORT_uuid.UUID(bytes=task.task_id.task_uuid))
                        ),
                    )
                )

                return context, response

        return context, response


    # Entrypoint for non-reactive network calls (i.e. typical gRPC calls).
    async def Stringify(
        self,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapStringifyRequest,
        grpc_context: IMPORT_grpc.aio.ServicerContext,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapStringifyResponse:
        headers = IMPORT_reboot_aio_headers.Headers.from_grpc_context(grpc_context)
        assert headers.application_id is not None  # Guaranteed by `Headers`.

        # Confirm whether this is the right consensus to be serving this
        # request.
        authoritative_consensus = self.placement_client.consensus_for_actor(
            headers.application_id,
            headers.state_ref,
        )
        if authoritative_consensus != self.consensus_id:
            # This is NOT the correct consensus. Fail.
            await grpc_context.abort(
                IMPORT_grpc.StatusCode.UNAVAILABLE,
                f"Consensus '{self.consensus_id}' is not authoritative for this "
                f"request; consensus '{authoritative_consensus}' is.",
            )
            raise  # Unreachable but necessary for mypy.

        @IMPORT_reboot_aio_internals_middleware.maybe_run_function_twice_to_validate_effects
        async def _run(
            validating_effects: bool,
        ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapStringifyResponse:
            context: IMPORT_typing.Optional[IMPORT_reboot_aio_contexts.Context] = None
            try:
                if headers.task_schedule is not None:
                    context, response = await self._schedule_Stringify(
                        headers=headers,
                        request=request,
                        grpc_context=grpc_context,
                    )
                    return response

                context = self.create_context(
                    headers=headers,
                    state_type_name = IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.OrderedMap'),
                    method='Stringify',
                    context_type=IMPORT_reboot_aio_contexts.ReaderContext,
                )
                assert context is not None

                return await self._Stringify(
                    request,
                    context,
                    validating_effects=validating_effects,
                    grpc_context=grpc_context,
                )
            except IMPORT_reboot_aio_contexts.EffectValidationRetry:
                # Doing effect validation, just let this propagate.
                raise
            except IMPORT_rebootdev.aio.aborted.Aborted as aborted:
                status = IMPORT_rpc_status_sync.to_status(aborted.to_status())
                # Need to add transaction participants here because
                # calling `grpc_context.abort_with_status()` will
                # ignore any other trailing metadata.
                if context is not None and context.transaction_id is not None:
                    status = status._replace(
                        trailing_metadata=status.trailing_metadata + context.participants.to_grpc_metadata()
                    )
                await grpc_context.abort_with_status(status)
                raise  # Unreachable but necessary for mypy.
            except IMPORT_asyncio.CancelledError:
                # It's pretty normal for an RPC to be cancelled; it's not useful to
                # print a stack trace.
                raise
            except BaseException as exception:
                # Print the exception stack trace for easier debugging. Note
                # that we don't include the stack trace in an error message
                # for the same reason that gRPC doesn't do so by default,
                # see https://github.com/grpc/grpc/issues/14897, but since this
                # should only get logged on the server side it is safe.
                logger.warning(
                    'Unhandled exception\n' +
                    ''.join(IMPORT_traceback.format_exc() if IMPORT_reboot_nodejs_python.should_print_stacktrace() else [f"{type(exception).__name__}: {exception}"])
                )

                # Re-raise the exception for gRPC to handle!
                #
                # TODO: gRPC will print a stack trace from this
                # exception which we don't want if we're executing via
                # Node.js.
                raise
            finally:
                if context is not None and context.transaction_id is not None:
                    # Propagate transaction participants.
                    grpc_context.set_trailing_metadata(
                        grpc_context.trailing_metadata() +
                        context.participants.to_grpc_metadata()
                    )

        with IMPORT_reboot_aio_tracing.context_from_headers(headers):
            return await _run()

    def _maybe_authorize(
        self,
        *,
        method_name: str,
        headers: IMPORT_reboot_aio_headers.Headers,
        auth: IMPORT_typing.Optional[IMPORT_rebootdev.aio.auth.Auth],
        request: IMPORT_typing.Optional[OrderedMapRequestTypes] = None,
    ) -> IMPORT_typing.Optional[IMPORT_typing.Callable[[IMPORT_typing.Optional[OrderedMapStateType]], IMPORT_typing.Awaitable[None]]]:
        """Returns a function to check authorization for the given method.

        Raises `PermissionDenied` in case Authorizer is present but the request
        is not authorized.
        """
        # To authorize internal calls, we use an internal magic token.
        if headers.bearer_token == __internal_magic_token__:
            return None

        assert self._authorizer is not None

        async def authorize(state: IMPORT_typing.Optional[OrderedMapStateType]) -> None:
            # Create context for the authorizer. This is a `ReaderContext`
            # independently of the calling context.
            with self.use_context(
                headers=(
                    # Get headers suitable for doing authorization.
                    headers.copy_for_token_verification_and_authorization()
                ),
                state_type_name = IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.OrderedMap'),
                method=method_name,
                context_type=IMPORT_reboot_aio_contexts.ReaderContext,
            ) as context:
                context.auth = auth

                # Get the authorizer decision.
                authorization_decision = await self._authorizer.authorize(
                    method_name=method_name,
                    context=context,
                    state=state,
                    request=request,
                )

            # Enforce correct authorizer decision type.
            try:
                IMPORT_reboot_aio_types.assert_type(
                    authorization_decision,
                    [
                        IMPORT_rbt_v1alpha1.errors_pb2.Ok,
                        IMPORT_rbt_v1alpha1.errors_pb2.Unauthenticated,
                        IMPORT_rbt_v1alpha1.errors_pb2.PermissionDenied,
                    ]
                )
            except TypeError as e:
                # Retyping.cast the exception to provide more context.
                authorizer_type = f"{type(self._authorizer).__module__}.{type(self._authorizer).__name__}"
                raise TypeError(
                    f"Authorizer '{authorizer_type}' "
                    f"returned unexpected type '{type(authorization_decision).__name__}' "
                    f"for method '{method_name}' on "
                    f"`rbt.std.collections.ordered_map.v1.OrderedMap('{headers.state_ref.id}')`"
                ) from e

            # If the decision is not `True`, raise a `SystemAborted` with either a
            # `PermissionDenied` error (in case of `False`) or an `Unauthenticated`
            # error.
            if not isinstance(authorization_decision, IMPORT_rbt_v1alpha1.errors_pb2.Ok):
                if isinstance(authorization_decision, IMPORT_rbt_v1alpha1.errors_pb2.Unauthenticated):
                    logger.warning(
                        f"Unauthenticated call to '{method_name}' on "
                        f"`rbt.std.collections.ordered_map.v1.OrderedMap('{headers.state_ref.id}')`"
                    )

                raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                    authorization_decision,
                    message=
                    f"You are not authorized to call '{method_name}' on "
                    f"`rbt.std.collections.ordered_map.v1.OrderedMap('{headers.state_ref.id}')`"
                )

        return authorize

    async def _maybe_verify_token(
        self,
        *,
        headers: IMPORT_reboot_aio_headers.Headers,
        method: str,
    ) -> IMPORT_typing.Optional[IMPORT_rebootdev.aio.auth.Auth]:
        """Verify the bearer token and if a token verifier is present.

        Returns the (optional) `rebootdev.aio.auth.Auth` object
        produced by the token verifier if the token can be verified.
        """
        if self._token_verifier is not None:
            if headers.bearer_token == __internal_magic_token__:
                return None

            with self.use_context(
                headers=(
                    # Get headers suitable for doing token verification.
                    headers.copy_for_token_verification_and_authorization()
                ),
                state_type_name = IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.OrderedMap'),
                method=method,
                context_type=IMPORT_reboot_aio_contexts.ReaderContext,
            ) as context:
                return await self._token_verifier.verify_token(
                    context=context,
                    token=headers.bearer_token,
                )

        return None


############################ Client Stubs ############################
# This section is relevant for clients accessing a Reboot service. Since
# servicers are themselves often clients also, this code is generated for
# them also.


class _NodeStub(IMPORT_reboot_aio_stubs.Stub):

    __state_type_name__ = IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.Node')

    def __init__(
        self,
        *,
        context: IMPORT_reboot_aio_contexts.Context | IMPORT_reboot_aio_external.ExternalContext,
        state_ref: IMPORT_reboot_aio_types.StateRef,
        bearer_token: IMPORT_typing.Optional[str] = None,
    ):
        # Within a Reboot context we do not pass on the caller's bearer token, as that might
        # have security implications - we cannot simply trust any service we are calling with
        # the user's credentials. Instead, the developer can rely on the default app-internal
        # auth, or override that and set an explicit bearer token.
        #
        # In the case of `ExternalContext`, however, its `bearer_token` was set specifically
        # by the developer for the purpose of making these calls.
        app_internal_authorization: IMPORT_typing.Optional[str] = None
        if isinstance(context, IMPORT_reboot_aio_external.ExternalContext):
            # Note that only `ExternalContext` even has a `bearer_token` field.
            bearer_token = context.bearer_token
            app_internal_authorization = context.app_internal_authorization

        super().__init__(
            channel_manager=context.channel_manager,
            idempotency_manager=context,
            state_ref=state_ref,
            context=context if isinstance(context, IMPORT_reboot_aio_contexts.Context) else None,
            bearer_token=bearer_token,
            app_internal_authorization=app_internal_authorization,
        )

        # All the channels for all services of this state will go to the same
        # place, so we can just get a single channel and share it across all
        # stubs.
        channel = self._channel_manager.get_channel_to_state(
            self.__state_type_name__, state_ref
        )
        self._rbt_std_collections_ordered_map_v1_nodemethods_stub = rbt.std.collections.ordered_map.v1.ordered_map_pb2_grpc.NodeMethodsStub(channel)


class NodeReaderStub(_NodeStub):

    def __init__(
        self,
        context: IMPORT_reboot_aio_contexts.ReaderContext | IMPORT_reboot_aio_contexts.WriterContext | IMPORT_reboot_aio_contexts.TransactionContext | IMPORT_reboot_aio_contexts.WorkflowContext | IMPORT_reboot_aio_external.ExternalContext,
        *,
        state_ref: IMPORT_reboot_aio_types.StateRef,
        bearer_token: IMPORT_typing.Optional[str] = None,
    ):
        IMPORT_reboot_aio_types.assert_type(context, [IMPORT_reboot_aio_contexts.ReaderContext, IMPORT_reboot_aio_contexts.WriterContext, IMPORT_reboot_aio_contexts.TransactionContext, IMPORT_reboot_aio_contexts.WorkflowContext, IMPORT_reboot_aio_external.ExternalContext])
        super().__init__(
            context=context,
            state_ref=state_ref,
            bearer_token=bearer_token,
        )

    # Node specific methods:

    async def Search(
        self,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeSearchRequest,
        *,
        metadata: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None,
        bearer_token: IMPORT_typing.Optional[str] = None,
        idempotency: IMPORT_typing.Optional[IMPORT_reboot_aio_idempotency.Idempotency] = None,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeSearchResponse:
        state_type_name = IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.Node')
        service_name = IMPORT_reboot_aio_types.ServiceName('rbt.std.collections.ordered_map.v1.NodeMethods')
        method = 'Search'

        async def call():
            async with self._call(
                state_type_name,
                service_name,
                method,
                self._rbt_std_collections_ordered_map_v1_nodemethods_stub.Search,
                request,
                unary=True,
                reader=True,
                response_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeSearchResponse,
                aborted_type=Node.SearchAborted,
                metadata=metadata,
                bearer_token=bearer_token,
            ) as call:
                assert isinstance(call, IMPORT_typing.Awaitable)
                return await call

        if isinstance(self._context, IMPORT_reboot_aio_contexts.WorkflowContext):
            # Use the idempotency manager to make sure that this
            # reader is being called following the rules.
            with self._context.idempotently(
                state_type_name=state_type_name,
                state_ref=self._headers.state_ref,
                service_name=service_name,
                method=method,
                mutation=False,
                request=request,
                metadata=metadata,
                idempotency=idempotency,
                # Only need to pass `aborted_type` for mutations.
                aborted_type=None,
            ) as idempotency_key:
                assert idempotency is not None
                # Check if this reader is from an `.always()` and if
                # so, don't memoize!
                if idempotency.always:
                    return await call()

                assert idempotency_key is not None
                return await IMPORT_reboot_aio_workflows.at_least_once(
                    (
                        # TODO: for easier debugging include the
                        # original alias (or generated alias in the
                        # case of `.per_iteration()` w/o an alias)
                        # instead of just `idempotency_key`.
                        f'{ service_name }.{ method } ({str(idempotency_key)})',
                        # NOTE: we want this to be `PER_WORKFLOW`
                        # because any per iteration concerns should
                        # have already been taken care of by caller
                        # using `.per_iteration()`.
                        IMPORT_reboot_aio_workflows.PER_WORKFLOW
                    ),
                    self._context,
                    call,
                    type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeSearchResponse,
                )
        return await call()



    async def Range(
        self,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRangeRequest,
        *,
        metadata: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None,
        bearer_token: IMPORT_typing.Optional[str] = None,
        idempotency: IMPORT_typing.Optional[IMPORT_reboot_aio_idempotency.Idempotency] = None,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRangeResponse:
        state_type_name = IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.Node')
        service_name = IMPORT_reboot_aio_types.ServiceName('rbt.std.collections.ordered_map.v1.NodeMethods')
        method = 'Range'

        async def call():
            async with self._call(
                state_type_name,
                service_name,
                method,
                self._rbt_std_collections_ordered_map_v1_nodemethods_stub.Range,
                request,
                unary=True,
                reader=True,
                response_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRangeResponse,
                aborted_type=Node.RangeAborted,
                metadata=metadata,
                bearer_token=bearer_token,
            ) as call:
                assert isinstance(call, IMPORT_typing.Awaitable)
                return await call

        if isinstance(self._context, IMPORT_reboot_aio_contexts.WorkflowContext):
            # Use the idempotency manager to make sure that this
            # reader is being called following the rules.
            with self._context.idempotently(
                state_type_name=state_type_name,
                state_ref=self._headers.state_ref,
                service_name=service_name,
                method=method,
                mutation=False,
                request=request,
                metadata=metadata,
                idempotency=idempotency,
                # Only need to pass `aborted_type` for mutations.
                aborted_type=None,
            ) as idempotency_key:
                assert idempotency is not None
                # Check if this reader is from an `.always()` and if
                # so, don't memoize!
                if idempotency.always:
                    return await call()

                assert idempotency_key is not None
                return await IMPORT_reboot_aio_workflows.at_least_once(
                    (
                        # TODO: for easier debugging include the
                        # original alias (or generated alias in the
                        # case of `.per_iteration()` w/o an alias)
                        # instead of just `idempotency_key`.
                        f'{ service_name }.{ method } ({str(idempotency_key)})',
                        # NOTE: we want this to be `PER_WORKFLOW`
                        # because any per iteration concerns should
                        # have already been taken care of by caller
                        # using `.per_iteration()`.
                        IMPORT_reboot_aio_workflows.PER_WORKFLOW
                    ),
                    self._context,
                    call,
                    type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRangeResponse,
                )
        return await call()

    async def ReverseRange(
        self,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeReverseRangeRequest,
        *,
        metadata: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None,
        bearer_token: IMPORT_typing.Optional[str] = None,
        idempotency: IMPORT_typing.Optional[IMPORT_reboot_aio_idempotency.Idempotency] = None,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeReverseRangeResponse:
        state_type_name = IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.Node')
        service_name = IMPORT_reboot_aio_types.ServiceName('rbt.std.collections.ordered_map.v1.NodeMethods')
        method = 'ReverseRange'

        async def call():
            async with self._call(
                state_type_name,
                service_name,
                method,
                self._rbt_std_collections_ordered_map_v1_nodemethods_stub.ReverseRange,
                request,
                unary=True,
                reader=True,
                response_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeReverseRangeResponse,
                aborted_type=Node.ReverseRangeAborted,
                metadata=metadata,
                bearer_token=bearer_token,
            ) as call:
                assert isinstance(call, IMPORT_typing.Awaitable)
                return await call

        if isinstance(self._context, IMPORT_reboot_aio_contexts.WorkflowContext):
            # Use the idempotency manager to make sure that this
            # reader is being called following the rules.
            with self._context.idempotently(
                state_type_name=state_type_name,
                state_ref=self._headers.state_ref,
                service_name=service_name,
                method=method,
                mutation=False,
                request=request,
                metadata=metadata,
                idempotency=idempotency,
                # Only need to pass `aborted_type` for mutations.
                aborted_type=None,
            ) as idempotency_key:
                assert idempotency is not None
                # Check if this reader is from an `.always()` and if
                # so, don't memoize!
                if idempotency.always:
                    return await call()

                assert idempotency_key is not None
                return await IMPORT_reboot_aio_workflows.at_least_once(
                    (
                        # TODO: for easier debugging include the
                        # original alias (or generated alias in the
                        # case of `.per_iteration()` w/o an alias)
                        # instead of just `idempotency_key`.
                        f'{ service_name }.{ method } ({str(idempotency_key)})',
                        # NOTE: we want this to be `PER_WORKFLOW`
                        # because any per iteration concerns should
                        # have already been taken care of by caller
                        # using `.per_iteration()`.
                        IMPORT_reboot_aio_workflows.PER_WORKFLOW
                    ),
                    self._context,
                    call,
                    type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeReverseRangeResponse,
                )
        return await call()

    async def Stringify(
        self,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeStringifyRequest,
        *,
        metadata: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None,
        bearer_token: IMPORT_typing.Optional[str] = None,
        idempotency: IMPORT_typing.Optional[IMPORT_reboot_aio_idempotency.Idempotency] = None,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeStringifyResponse:
        state_type_name = IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.Node')
        service_name = IMPORT_reboot_aio_types.ServiceName('rbt.std.collections.ordered_map.v1.NodeMethods')
        method = 'Stringify'

        async def call():
            async with self._call(
                state_type_name,
                service_name,
                method,
                self._rbt_std_collections_ordered_map_v1_nodemethods_stub.Stringify,
                request,
                unary=True,
                reader=True,
                response_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeStringifyResponse,
                aborted_type=Node.StringifyAborted,
                metadata=metadata,
                bearer_token=bearer_token,
            ) as call:
                assert isinstance(call, IMPORT_typing.Awaitable)
                return await call

        if isinstance(self._context, IMPORT_reboot_aio_contexts.WorkflowContext):
            # Use the idempotency manager to make sure that this
            # reader is being called following the rules.
            with self._context.idempotently(
                state_type_name=state_type_name,
                state_ref=self._headers.state_ref,
                service_name=service_name,
                method=method,
                mutation=False,
                request=request,
                metadata=metadata,
                idempotency=idempotency,
                # Only need to pass `aborted_type` for mutations.
                aborted_type=None,
            ) as idempotency_key:
                assert idempotency is not None
                # Check if this reader is from an `.always()` and if
                # so, don't memoize!
                if idempotency.always:
                    return await call()

                assert idempotency_key is not None
                return await IMPORT_reboot_aio_workflows.at_least_once(
                    (
                        # TODO: for easier debugging include the
                        # original alias (or generated alias in the
                        # case of `.per_iteration()` w/o an alias)
                        # instead of just `idempotency_key`.
                        f'{ service_name }.{ method } ({str(idempotency_key)})',
                        # NOTE: we want this to be `PER_WORKFLOW`
                        # because any per iteration concerns should
                        # have already been taken care of by caller
                        # using `.per_iteration()`.
                        IMPORT_reboot_aio_workflows.PER_WORKFLOW
                    ),
                    self._context,
                    call,
                    type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeStringifyResponse,
                )
        return await call()



class NodeWriterStub(_NodeStub):

    def __init__(
        self,
        context: IMPORT_reboot_aio_contexts.TransactionContext | IMPORT_reboot_aio_contexts.WorkflowContext | IMPORT_reboot_aio_external.ExternalContext,
        *,
        state_ref: IMPORT_reboot_aio_types.StateRef,
        bearer_token: IMPORT_typing.Optional[str] = None,
    ):
        IMPORT_reboot_aio_types.assert_type(context, [IMPORT_reboot_aio_contexts.TransactionContext, IMPORT_reboot_aio_contexts.WorkflowContext, IMPORT_reboot_aio_external.ExternalContext])
        super().__init__(
            context=context,
            state_ref=state_ref,
            bearer_token=bearer_token,
        )

    # Node specific methods:
    async def Create(
        self,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateRequest,
        idempotency: IMPORT_typing.Optional[IMPORT_reboot_aio_idempotency.Idempotency] = None,
        *,
        metadata: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None,
        bearer_token: IMPORT_typing.Optional[str] = None,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateResponse:
        with self._idempotency_manager.idempotently(
            state_type_name=IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.Node'),
            state_ref=self._headers.state_ref,
            service_name=IMPORT_reboot_aio_types.ServiceName('rbt.std.collections.ordered_map.v1.NodeMethods'),
            method='Create',
            mutation=True,
            request=request,
            metadata=metadata,
            idempotency=idempotency,
            aborted_type=Node.CreateAborted,
        ) as idempotency_key:
            async with self._call(
                IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.Node'),
                IMPORT_reboot_aio_types.ServiceName('rbt.std.collections.ordered_map.v1.NodeMethods'),
                'Create',
                self._rbt_std_collections_ordered_map_v1_nodemethods_stub.Create,
                request,
                unary=True,
                reader=False,
                response_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateResponse,
                aborted_type=Node.CreateAborted,
                metadata=metadata,
                idempotency_key=idempotency_key,
                bearer_token=bearer_token,
            ) as call:
                assert isinstance(call, IMPORT_typing.Awaitable), type(call)
                return await call

    async def Search(
        self,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeSearchRequest,
        *,
        metadata: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None,
        bearer_token: IMPORT_typing.Optional[str] = None,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeSearchResponse:
        async with self._call(
            IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.Node'),
            IMPORT_reboot_aio_types.ServiceName('rbt.std.collections.ordered_map.v1.NodeMethods'),
            'Search',
            self._rbt_std_collections_ordered_map_v1_nodemethods_stub.Search,
            request,
            unary=True,
            reader=True,
            response_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeSearchResponse,
            aborted_type=Node.SearchAborted,
            metadata=metadata,
            bearer_token=bearer_token,
        ) as call:
            assert isinstance(call, IMPORT_typing.Awaitable), type(call)
            return await call



    async def Range(
        self,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRangeRequest,
        *,
        metadata: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None,
        bearer_token: IMPORT_typing.Optional[str] = None,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRangeResponse:
        async with self._call(
            IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.Node'),
            IMPORT_reboot_aio_types.ServiceName('rbt.std.collections.ordered_map.v1.NodeMethods'),
            'Range',
            self._rbt_std_collections_ordered_map_v1_nodemethods_stub.Range,
            request,
            unary=True,
            reader=True,
            response_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRangeResponse,
            aborted_type=Node.RangeAborted,
            metadata=metadata,
            bearer_token=bearer_token,
        ) as call:
            assert isinstance(call, IMPORT_typing.Awaitable), type(call)
            return await call

    async def ReverseRange(
        self,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeReverseRangeRequest,
        *,
        metadata: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None,
        bearer_token: IMPORT_typing.Optional[str] = None,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeReverseRangeResponse:
        async with self._call(
            IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.Node'),
            IMPORT_reboot_aio_types.ServiceName('rbt.std.collections.ordered_map.v1.NodeMethods'),
            'ReverseRange',
            self._rbt_std_collections_ordered_map_v1_nodemethods_stub.ReverseRange,
            request,
            unary=True,
            reader=True,
            response_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeReverseRangeResponse,
            aborted_type=Node.ReverseRangeAborted,
            metadata=metadata,
            bearer_token=bearer_token,
        ) as call:
            assert isinstance(call, IMPORT_typing.Awaitable), type(call)
            return await call

    async def Stringify(
        self,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeStringifyRequest,
        *,
        metadata: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None,
        bearer_token: IMPORT_typing.Optional[str] = None,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeStringifyResponse:
        async with self._call(
            IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.Node'),
            IMPORT_reboot_aio_types.ServiceName('rbt.std.collections.ordered_map.v1.NodeMethods'),
            'Stringify',
            self._rbt_std_collections_ordered_map_v1_nodemethods_stub.Stringify,
            request,
            unary=True,
            reader=True,
            response_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeStringifyResponse,
            aborted_type=Node.StringifyAborted,
            metadata=metadata,
            bearer_token=bearer_token,
        ) as call:
            assert isinstance(call, IMPORT_typing.Awaitable), type(call)
            return await call


class NodeWorkflowStub(_NodeStub):

    def __init__(
        self,
        *,
        context: IMPORT_reboot_aio_contexts.TransactionContext | IMPORT_reboot_aio_contexts.WorkflowContext | IMPORT_reboot_aio_external.ExternalContext,
        state_ref: IMPORT_reboot_aio_types.StateRef,
        bearer_token: IMPORT_typing.Optional[str] = None,
    ):
        IMPORT_reboot_aio_types.assert_type(context, [IMPORT_reboot_aio_contexts.TransactionContext, IMPORT_reboot_aio_contexts.WorkflowContext, IMPORT_reboot_aio_external.ExternalContext])
        super().__init__(
            context=context,
            state_ref=state_ref,
            bearer_token=bearer_token,
        )

    # Node specific methods:
    async def Create(
        self,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateRequest,
        idempotency: IMPORT_typing.Optional[IMPORT_reboot_aio_idempotency.Idempotency] = None,
        *,
        metadata: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None,
        bearer_token: IMPORT_typing.Optional[str] = None,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateResponse:
        idempotency_key: IMPORT_typing.Optional[IMPORT_uuid.UUID]
        with self._idempotency_manager.idempotently(
            state_type_name=IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.Node'),
            state_ref=self._headers.state_ref,
            service_name=IMPORT_reboot_aio_types.ServiceName('rbt.std.collections.ordered_map.v1.NodeMethods'),
            method='Create',
            mutation=True,
            request=request,
            metadata=metadata,
            idempotency=idempotency,
            aborted_type=Node.CreateAborted,
        ) as idempotency_key:
            async with self._call(
                IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.Node'),
                IMPORT_reboot_aio_types.ServiceName('rbt.std.collections.ordered_map.v1.NodeMethods'),
                'Create',
                self._rbt_std_collections_ordered_map_v1_nodemethods_stub.Create,
                request,
                unary=True,
                reader=False,
                response_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateResponse,
                aborted_type=Node.CreateAborted,
                metadata=metadata,
                idempotency_key=idempotency_key,
                bearer_token=bearer_token,
            ) as call:
                assert isinstance(call, IMPORT_typing.Awaitable), type(call)
                return await call

    async def Search(
        self,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeSearchRequest,
        *,
        metadata: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None,
        bearer_token: IMPORT_typing.Optional[str] = None,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeSearchResponse:
        async with self._call(
            IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.Node'),
            IMPORT_reboot_aio_types.ServiceName('rbt.std.collections.ordered_map.v1.NodeMethods'),
            'Search',
            self._rbt_std_collections_ordered_map_v1_nodemethods_stub.Search,
            request,
            unary=True,
            reader=True,
            response_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeSearchResponse,
            aborted_type=Node.SearchAborted,
            metadata=metadata,
            bearer_token=bearer_token,
        ) as call:
            assert isinstance(call, IMPORT_typing.Awaitable), type(call)
            return await call

    async def Insert(
        self,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeInsertRequest,
        idempotency: IMPORT_typing.Optional[IMPORT_reboot_aio_idempotency.Idempotency] = None,
        *,
        metadata: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None,
        bearer_token: IMPORT_typing.Optional[str] = None,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeInsertResponse:
        idempotency_key: IMPORT_typing.Optional[IMPORT_uuid.UUID]
        with self._idempotency_manager.idempotently(
            state_type_name=IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.Node'),
            state_ref=self._headers.state_ref,
            service_name=IMPORT_reboot_aio_types.ServiceName('rbt.std.collections.ordered_map.v1.NodeMethods'),
            method='Insert',
            mutation=True,
            request=request,
            metadata=metadata,
            idempotency=idempotency,
            aborted_type=Node.InsertAborted,
        ) as idempotency_key:
            async with self._call(
                IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.Node'),
                IMPORT_reboot_aio_types.ServiceName('rbt.std.collections.ordered_map.v1.NodeMethods'),
                'Insert',
                self._rbt_std_collections_ordered_map_v1_nodemethods_stub.Insert,
                request,
                unary=True,
                reader=False,
                response_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeInsertResponse,
                aborted_type=Node.InsertAborted,
                metadata=metadata,
                idempotency_key=idempotency_key,
                bearer_token=bearer_token,
            ) as call:
                assert isinstance(call, IMPORT_typing.Awaitable), type(call)
                return await call

    async def Remove(
        self,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRemoveRequest,
        idempotency: IMPORT_typing.Optional[IMPORT_reboot_aio_idempotency.Idempotency] = None,
        *,
        metadata: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None,
        bearer_token: IMPORT_typing.Optional[str] = None,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRemoveResponse:
        idempotency_key: IMPORT_typing.Optional[IMPORT_uuid.UUID]
        with self._idempotency_manager.idempotently(
            state_type_name=IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.Node'),
            state_ref=self._headers.state_ref,
            service_name=IMPORT_reboot_aio_types.ServiceName('rbt.std.collections.ordered_map.v1.NodeMethods'),
            method='Remove',
            mutation=True,
            request=request,
            metadata=metadata,
            idempotency=idempotency,
            aborted_type=Node.RemoveAborted,
        ) as idempotency_key:
            async with self._call(
                IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.Node'),
                IMPORT_reboot_aio_types.ServiceName('rbt.std.collections.ordered_map.v1.NodeMethods'),
                'Remove',
                self._rbt_std_collections_ordered_map_v1_nodemethods_stub.Remove,
                request,
                unary=True,
                reader=False,
                response_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRemoveResponse,
                aborted_type=Node.RemoveAborted,
                metadata=metadata,
                idempotency_key=idempotency_key,
                bearer_token=bearer_token,
            ) as call:
                assert isinstance(call, IMPORT_typing.Awaitable), type(call)
                return await call

    async def Range(
        self,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRangeRequest,
        *,
        metadata: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None,
        bearer_token: IMPORT_typing.Optional[str] = None,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRangeResponse:
        async with self._call(
            IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.Node'),
            IMPORT_reboot_aio_types.ServiceName('rbt.std.collections.ordered_map.v1.NodeMethods'),
            'Range',
            self._rbt_std_collections_ordered_map_v1_nodemethods_stub.Range,
            request,
            unary=True,
            reader=True,
            response_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRangeResponse,
            aborted_type=Node.RangeAborted,
            metadata=metadata,
            bearer_token=bearer_token,
        ) as call:
            assert isinstance(call, IMPORT_typing.Awaitable), type(call)
            return await call

    async def ReverseRange(
        self,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeReverseRangeRequest,
        *,
        metadata: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None,
        bearer_token: IMPORT_typing.Optional[str] = None,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeReverseRangeResponse:
        async with self._call(
            IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.Node'),
            IMPORT_reboot_aio_types.ServiceName('rbt.std.collections.ordered_map.v1.NodeMethods'),
            'ReverseRange',
            self._rbt_std_collections_ordered_map_v1_nodemethods_stub.ReverseRange,
            request,
            unary=True,
            reader=True,
            response_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeReverseRangeResponse,
            aborted_type=Node.ReverseRangeAborted,
            metadata=metadata,
            bearer_token=bearer_token,
        ) as call:
            assert isinstance(call, IMPORT_typing.Awaitable), type(call)
            return await call

    async def Stringify(
        self,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeStringifyRequest,
        *,
        metadata: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None,
        bearer_token: IMPORT_typing.Optional[str] = None,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeStringifyResponse:
        async with self._call(
            IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.Node'),
            IMPORT_reboot_aio_types.ServiceName('rbt.std.collections.ordered_map.v1.NodeMethods'),
            'Stringify',
            self._rbt_std_collections_ordered_map_v1_nodemethods_stub.Stringify,
            request,
            unary=True,
            reader=True,
            response_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeStringifyResponse,
            aborted_type=Node.StringifyAborted,
            metadata=metadata,
            bearer_token=bearer_token,
        ) as call:
            assert isinstance(call, IMPORT_typing.Awaitable), type(call)
            return await call



class NodeTasksStub(_NodeStub):

    def __init__(
        self,
        context: IMPORT_reboot_aio_contexts.TransactionContext | IMPORT_reboot_aio_contexts.WorkflowContext | IMPORT_reboot_aio_external.ExternalContext,
        *,
        state_ref: IMPORT_reboot_aio_types.StateRef,
        bearer_token: IMPORT_typing.Optional[str] = None,
    ):
        IMPORT_reboot_aio_types.assert_type(context, [IMPORT_reboot_aio_contexts.TransactionContext, IMPORT_reboot_aio_contexts.WorkflowContext, IMPORT_reboot_aio_external.ExternalContext])
        super().__init__(
            context=context,
            state_ref=state_ref,
            bearer_token=bearer_token,
        )

    # Node specific methods:
    async def Create(
        self,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateRequest,
        idempotency: IMPORT_typing.Optional[IMPORT_reboot_aio_idempotency.Idempotency] = None,
        *,
        metadata: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None,
        bearer_token: IMPORT_typing.Optional[str] = None,
    ) -> IMPORT_rbt_v1alpha1.tasks_pb2.TaskId:
        idempotency_key: IMPORT_typing.Optional[IMPORT_uuid.UUID]
        with self._idempotency_manager.idempotently(
            state_type_name=IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.Node'),
            state_ref=self._headers.state_ref,
            service_name=IMPORT_reboot_aio_types.ServiceName('rbt.std.collections.ordered_map.v1.NodeMethods'),
            method='Create',
            mutation=True,
            request=request,
            metadata=metadata,
            idempotency=idempotency,
            aborted_type=Node.CreateAborted,
        ) as idempotency_key:
            async with self._call(
                IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.Node'),
                IMPORT_reboot_aio_types.ServiceName('rbt.std.collections.ordered_map.v1.NodeMethods'),
                'Create',
                self._rbt_std_collections_ordered_map_v1_nodemethods_stub.Create,
                request,
                unary=True,
                reader=False,
                response_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateResponse,
                aborted_type=Node.CreateAborted,
                metadata=metadata,
                idempotency_key=idempotency_key,
                bearer_token=bearer_token,
            ) as call:
                assert isinstance(call, IMPORT_typing.Awaitable), type(call)
                await call
                for (key, value) in await call.trailing_metadata():  # type: ignore[misc, attr-defined]
                    if key == IMPORT_reboot_aio_headers.TASK_ID_UUID:
                        return IMPORT_rbt_v1alpha1.tasks_pb2.TaskId(
                            state_type=IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.Node'),
                            state_ref=self._headers.state_ref.to_str(),
                            task_uuid=IMPORT_uuid.UUID(value).bytes,
                        )
                raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                    IMPORT_rbt_v1alpha1.errors_pb2.Internal(),
                    message='Trailing metadata missing for task schedule',
                )
    async def Search(
        self,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeSearchRequest,
        idempotency: IMPORT_typing.Optional[IMPORT_reboot_aio_idempotency.Idempotency] = None,
        *,
        metadata: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None,
        bearer_token: IMPORT_typing.Optional[str] = None,
    ) -> IMPORT_rbt_v1alpha1.tasks_pb2.TaskId:
        idempotency_key: IMPORT_typing.Optional[IMPORT_uuid.UUID]
        with self._idempotency_manager.idempotently(
            state_type_name=IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.Node'),
            state_ref=self._headers.state_ref,
            service_name=IMPORT_reboot_aio_types.ServiceName('rbt.std.collections.ordered_map.v1.NodeMethods'),
            method='Search',
            mutation=True,
            request=request,
            metadata=metadata,
            idempotency=idempotency,
            aborted_type=Node.SearchAborted,
        ) as idempotency_key:
            async with self._call(
                IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.Node'),
                IMPORT_reboot_aio_types.ServiceName('rbt.std.collections.ordered_map.v1.NodeMethods'),
                'Search',
                self._rbt_std_collections_ordered_map_v1_nodemethods_stub.Search,
                request,
                unary=True,
                reader=False,
                response_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeSearchResponse,
                aborted_type=Node.SearchAborted,
                metadata=metadata,
                idempotency_key=idempotency_key,
                bearer_token=bearer_token,
            ) as call:
                assert isinstance(call, IMPORT_typing.Awaitable), type(call)
                await call
                for (key, value) in await call.trailing_metadata():  # type: ignore[misc, attr-defined]
                    if key == IMPORT_reboot_aio_headers.TASK_ID_UUID:
                        return IMPORT_rbt_v1alpha1.tasks_pb2.TaskId(
                            state_type=IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.Node'),
                            state_ref=self._headers.state_ref.to_str(),
                            task_uuid=IMPORT_uuid.UUID(value).bytes,
                        )
                raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                    IMPORT_rbt_v1alpha1.errors_pb2.Internal(),
                    message='Trailing metadata missing for task schedule',
                )
    async def Insert(
        self,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeInsertRequest,
        idempotency: IMPORT_typing.Optional[IMPORT_reboot_aio_idempotency.Idempotency] = None,
        *,
        metadata: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None,
        bearer_token: IMPORT_typing.Optional[str] = None,
    ) -> IMPORT_rbt_v1alpha1.tasks_pb2.TaskId:
        idempotency_key: IMPORT_typing.Optional[IMPORT_uuid.UUID]
        with self._idempotency_manager.idempotently(
            state_type_name=IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.Node'),
            state_ref=self._headers.state_ref,
            service_name=IMPORT_reboot_aio_types.ServiceName('rbt.std.collections.ordered_map.v1.NodeMethods'),
            method='Insert',
            mutation=True,
            request=request,
            metadata=metadata,
            idempotency=idempotency,
            aborted_type=Node.InsertAborted,
        ) as idempotency_key:
            async with self._call(
                IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.Node'),
                IMPORT_reboot_aio_types.ServiceName('rbt.std.collections.ordered_map.v1.NodeMethods'),
                'Insert',
                self._rbt_std_collections_ordered_map_v1_nodemethods_stub.Insert,
                request,
                unary=True,
                reader=False,
                response_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeInsertResponse,
                aborted_type=Node.InsertAborted,
                metadata=metadata,
                idempotency_key=idempotency_key,
                bearer_token=bearer_token,
            ) as call:
                assert isinstance(call, IMPORT_typing.Awaitable), type(call)
                await call
                for (key, value) in await call.trailing_metadata():  # type: ignore[misc, attr-defined]
                    if key == IMPORT_reboot_aio_headers.TASK_ID_UUID:
                        return IMPORT_rbt_v1alpha1.tasks_pb2.TaskId(
                            state_type=IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.Node'),
                            state_ref=self._headers.state_ref.to_str(),
                            task_uuid=IMPORT_uuid.UUID(value).bytes,
                        )
                raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                    IMPORT_rbt_v1alpha1.errors_pb2.Internal(),
                    message='Trailing metadata missing for task schedule',
                )
    async def Remove(
        self,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRemoveRequest,
        idempotency: IMPORT_typing.Optional[IMPORT_reboot_aio_idempotency.Idempotency] = None,
        *,
        metadata: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None,
        bearer_token: IMPORT_typing.Optional[str] = None,
    ) -> IMPORT_rbt_v1alpha1.tasks_pb2.TaskId:
        idempotency_key: IMPORT_typing.Optional[IMPORT_uuid.UUID]
        with self._idempotency_manager.idempotently(
            state_type_name=IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.Node'),
            state_ref=self._headers.state_ref,
            service_name=IMPORT_reboot_aio_types.ServiceName('rbt.std.collections.ordered_map.v1.NodeMethods'),
            method='Remove',
            mutation=True,
            request=request,
            metadata=metadata,
            idempotency=idempotency,
            aborted_type=Node.RemoveAborted,
        ) as idempotency_key:
            async with self._call(
                IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.Node'),
                IMPORT_reboot_aio_types.ServiceName('rbt.std.collections.ordered_map.v1.NodeMethods'),
                'Remove',
                self._rbt_std_collections_ordered_map_v1_nodemethods_stub.Remove,
                request,
                unary=True,
                reader=False,
                response_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRemoveResponse,
                aborted_type=Node.RemoveAborted,
                metadata=metadata,
                idempotency_key=idempotency_key,
                bearer_token=bearer_token,
            ) as call:
                assert isinstance(call, IMPORT_typing.Awaitable), type(call)
                await call
                for (key, value) in await call.trailing_metadata():  # type: ignore[misc, attr-defined]
                    if key == IMPORT_reboot_aio_headers.TASK_ID_UUID:
                        return IMPORT_rbt_v1alpha1.tasks_pb2.TaskId(
                            state_type=IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.Node'),
                            state_ref=self._headers.state_ref.to_str(),
                            task_uuid=IMPORT_uuid.UUID(value).bytes,
                        )
                raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                    IMPORT_rbt_v1alpha1.errors_pb2.Internal(),
                    message='Trailing metadata missing for task schedule',
                )
    async def Range(
        self,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRangeRequest,
        idempotency: IMPORT_typing.Optional[IMPORT_reboot_aio_idempotency.Idempotency] = None,
        *,
        metadata: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None,
        bearer_token: IMPORT_typing.Optional[str] = None,
    ) -> IMPORT_rbt_v1alpha1.tasks_pb2.TaskId:
        idempotency_key: IMPORT_typing.Optional[IMPORT_uuid.UUID]
        with self._idempotency_manager.idempotently(
            state_type_name=IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.Node'),
            state_ref=self._headers.state_ref,
            service_name=IMPORT_reboot_aio_types.ServiceName('rbt.std.collections.ordered_map.v1.NodeMethods'),
            method='Range',
            mutation=True,
            request=request,
            metadata=metadata,
            idempotency=idempotency,
            aborted_type=Node.RangeAborted,
        ) as idempotency_key:
            async with self._call(
                IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.Node'),
                IMPORT_reboot_aio_types.ServiceName('rbt.std.collections.ordered_map.v1.NodeMethods'),
                'Range',
                self._rbt_std_collections_ordered_map_v1_nodemethods_stub.Range,
                request,
                unary=True,
                reader=False,
                response_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRangeResponse,
                aborted_type=Node.RangeAborted,
                metadata=metadata,
                idempotency_key=idempotency_key,
                bearer_token=bearer_token,
            ) as call:
                assert isinstance(call, IMPORT_typing.Awaitable), type(call)
                await call
                for (key, value) in await call.trailing_metadata():  # type: ignore[misc, attr-defined]
                    if key == IMPORT_reboot_aio_headers.TASK_ID_UUID:
                        return IMPORT_rbt_v1alpha1.tasks_pb2.TaskId(
                            state_type=IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.Node'),
                            state_ref=self._headers.state_ref.to_str(),
                            task_uuid=IMPORT_uuid.UUID(value).bytes,
                        )
                raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                    IMPORT_rbt_v1alpha1.errors_pb2.Internal(),
                    message='Trailing metadata missing for task schedule',
                )
    async def ReverseRange(
        self,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeReverseRangeRequest,
        idempotency: IMPORT_typing.Optional[IMPORT_reboot_aio_idempotency.Idempotency] = None,
        *,
        metadata: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None,
        bearer_token: IMPORT_typing.Optional[str] = None,
    ) -> IMPORT_rbt_v1alpha1.tasks_pb2.TaskId:
        idempotency_key: IMPORT_typing.Optional[IMPORT_uuid.UUID]
        with self._idempotency_manager.idempotently(
            state_type_name=IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.Node'),
            state_ref=self._headers.state_ref,
            service_name=IMPORT_reboot_aio_types.ServiceName('rbt.std.collections.ordered_map.v1.NodeMethods'),
            method='ReverseRange',
            mutation=True,
            request=request,
            metadata=metadata,
            idempotency=idempotency,
            aborted_type=Node.ReverseRangeAborted,
        ) as idempotency_key:
            async with self._call(
                IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.Node'),
                IMPORT_reboot_aio_types.ServiceName('rbt.std.collections.ordered_map.v1.NodeMethods'),
                'ReverseRange',
                self._rbt_std_collections_ordered_map_v1_nodemethods_stub.ReverseRange,
                request,
                unary=True,
                reader=False,
                response_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeReverseRangeResponse,
                aborted_type=Node.ReverseRangeAborted,
                metadata=metadata,
                idempotency_key=idempotency_key,
                bearer_token=bearer_token,
            ) as call:
                assert isinstance(call, IMPORT_typing.Awaitable), type(call)
                await call
                for (key, value) in await call.trailing_metadata():  # type: ignore[misc, attr-defined]
                    if key == IMPORT_reboot_aio_headers.TASK_ID_UUID:
                        return IMPORT_rbt_v1alpha1.tasks_pb2.TaskId(
                            state_type=IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.Node'),
                            state_ref=self._headers.state_ref.to_str(),
                            task_uuid=IMPORT_uuid.UUID(value).bytes,
                        )
                raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                    IMPORT_rbt_v1alpha1.errors_pb2.Internal(),
                    message='Trailing metadata missing for task schedule',
                )
    async def Stringify(
        self,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeStringifyRequest,
        idempotency: IMPORT_typing.Optional[IMPORT_reboot_aio_idempotency.Idempotency] = None,
        *,
        metadata: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None,
        bearer_token: IMPORT_typing.Optional[str] = None,
    ) -> IMPORT_rbt_v1alpha1.tasks_pb2.TaskId:
        idempotency_key: IMPORT_typing.Optional[IMPORT_uuid.UUID]
        with self._idempotency_manager.idempotently(
            state_type_name=IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.Node'),
            state_ref=self._headers.state_ref,
            service_name=IMPORT_reboot_aio_types.ServiceName('rbt.std.collections.ordered_map.v1.NodeMethods'),
            method='Stringify',
            mutation=True,
            request=request,
            metadata=metadata,
            idempotency=idempotency,
            aborted_type=Node.StringifyAborted,
        ) as idempotency_key:
            async with self._call(
                IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.Node'),
                IMPORT_reboot_aio_types.ServiceName('rbt.std.collections.ordered_map.v1.NodeMethods'),
                'Stringify',
                self._rbt_std_collections_ordered_map_v1_nodemethods_stub.Stringify,
                request,
                unary=True,
                reader=False,
                response_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeStringifyResponse,
                aborted_type=Node.StringifyAborted,
                metadata=metadata,
                idempotency_key=idempotency_key,
                bearer_token=bearer_token,
            ) as call:
                assert isinstance(call, IMPORT_typing.Awaitable), type(call)
                await call
                for (key, value) in await call.trailing_metadata():  # type: ignore[misc, attr-defined]
                    if key == IMPORT_reboot_aio_headers.TASK_ID_UUID:
                        return IMPORT_rbt_v1alpha1.tasks_pb2.TaskId(
                            state_type=IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.Node'),
                            state_ref=self._headers.state_ref.to_str(),
                            task_uuid=IMPORT_uuid.UUID(value).bytes,
                        )
                raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                    IMPORT_rbt_v1alpha1.errors_pb2.Internal(),
                    message='Trailing metadata missing for task schedule',
                )


class NodeServicerTasks:

    _context: IMPORT_reboot_aio_contexts.WriterContext

    def __init__(
        self,
        context: IMPORT_reboot_aio_contexts.WriterContext,
        *,
        state_ref: IMPORT_reboot_aio_types.StateRef,
        bearer_token: IMPORT_typing.Optional[str] = None,
    ):
        IMPORT_reboot_aio_types.assert_type(context, [IMPORT_reboot_aio_contexts.WriterContext])
        self._context = context
        self._state_ref = state_ref

    # Node specific methods:
    async def Create(
        self,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateRequest,
        *,
        schedule: IMPORT_typing.Optional[IMPORT_datetime_datetime | IMPORT_datetime_timedelta] = None,
    ) -> IMPORT_reboot_aio_tasks.TaskEffect:
        schedule = ensure_has_timezone(when=schedule)
        task = IMPORT_reboot_aio_tasks.TaskEffect(
            state_type=IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.Node'),
            state_ref=self._state_ref,
            method_name='Create',
            request=request,
            schedule=(IMPORT_reboot_time_DateTimeWithTimeZone.now() + schedule) if isinstance(
                schedule, IMPORT_datetime_timedelta
            ) else schedule,
        )

        self._context._tasks.append(task)

        return task

    async def Search(
        self,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeSearchRequest,
        *,
        schedule: IMPORT_typing.Optional[IMPORT_datetime_datetime | IMPORT_datetime_timedelta] = None,
    ) -> IMPORT_reboot_aio_tasks.TaskEffect:
        schedule = ensure_has_timezone(when=schedule)
        task = IMPORT_reboot_aio_tasks.TaskEffect(
            state_type=IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.Node'),
            state_ref=self._state_ref,
            method_name='Search',
            request=request,
            schedule=(IMPORT_reboot_time_DateTimeWithTimeZone.now() + schedule) if isinstance(
                schedule, IMPORT_datetime_timedelta
            ) else schedule,
        )

        self._context._tasks.append(task)

        return task

    async def Insert(
        self,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeInsertRequest,
        *,
        schedule: IMPORT_typing.Optional[IMPORT_datetime_datetime | IMPORT_datetime_timedelta] = None,
    ) -> IMPORT_reboot_aio_tasks.TaskEffect:
        schedule = ensure_has_timezone(when=schedule)
        task = IMPORT_reboot_aio_tasks.TaskEffect(
            state_type=IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.Node'),
            state_ref=self._state_ref,
            method_name='Insert',
            request=request,
            schedule=(IMPORT_reboot_time_DateTimeWithTimeZone.now() + schedule) if isinstance(
                schedule, IMPORT_datetime_timedelta
            ) else schedule,
        )

        self._context._tasks.append(task)

        return task

    async def Remove(
        self,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRemoveRequest,
        *,
        schedule: IMPORT_typing.Optional[IMPORT_datetime_datetime | IMPORT_datetime_timedelta] = None,
    ) -> IMPORT_reboot_aio_tasks.TaskEffect:
        schedule = ensure_has_timezone(when=schedule)
        task = IMPORT_reboot_aio_tasks.TaskEffect(
            state_type=IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.Node'),
            state_ref=self._state_ref,
            method_name='Remove',
            request=request,
            schedule=(IMPORT_reboot_time_DateTimeWithTimeZone.now() + schedule) if isinstance(
                schedule, IMPORT_datetime_timedelta
            ) else schedule,
        )

        self._context._tasks.append(task)

        return task

    async def Range(
        self,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRangeRequest,
        *,
        schedule: IMPORT_typing.Optional[IMPORT_datetime_datetime | IMPORT_datetime_timedelta] = None,
    ) -> IMPORT_reboot_aio_tasks.TaskEffect:
        schedule = ensure_has_timezone(when=schedule)
        task = IMPORT_reboot_aio_tasks.TaskEffect(
            state_type=IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.Node'),
            state_ref=self._state_ref,
            method_name='Range',
            request=request,
            schedule=(IMPORT_reboot_time_DateTimeWithTimeZone.now() + schedule) if isinstance(
                schedule, IMPORT_datetime_timedelta
            ) else schedule,
        )

        self._context._tasks.append(task)

        return task

    async def ReverseRange(
        self,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeReverseRangeRequest,
        *,
        schedule: IMPORT_typing.Optional[IMPORT_datetime_datetime | IMPORT_datetime_timedelta] = None,
    ) -> IMPORT_reboot_aio_tasks.TaskEffect:
        schedule = ensure_has_timezone(when=schedule)
        task = IMPORT_reboot_aio_tasks.TaskEffect(
            state_type=IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.Node'),
            state_ref=self._state_ref,
            method_name='ReverseRange',
            request=request,
            schedule=(IMPORT_reboot_time_DateTimeWithTimeZone.now() + schedule) if isinstance(
                schedule, IMPORT_datetime_timedelta
            ) else schedule,
        )

        self._context._tasks.append(task)

        return task

    async def Stringify(
        self,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeStringifyRequest,
        *,
        schedule: IMPORT_typing.Optional[IMPORT_datetime_datetime | IMPORT_datetime_timedelta] = None,
    ) -> IMPORT_reboot_aio_tasks.TaskEffect:
        schedule = ensure_has_timezone(when=schedule)
        task = IMPORT_reboot_aio_tasks.TaskEffect(
            state_type=IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.Node'),
            state_ref=self._state_ref,
            method_name='Stringify',
            request=request,
            schedule=(IMPORT_reboot_time_DateTimeWithTimeZone.now() + schedule) if isinstance(
                schedule, IMPORT_datetime_timedelta
            ) else schedule,
        )

        self._context._tasks.append(task)

        return task


class _OrderedMapStub(IMPORT_reboot_aio_stubs.Stub):

    __state_type_name__ = IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.OrderedMap')

    def __init__(
        self,
        *,
        context: IMPORT_reboot_aio_contexts.Context | IMPORT_reboot_aio_external.ExternalContext,
        state_ref: IMPORT_reboot_aio_types.StateRef,
        bearer_token: IMPORT_typing.Optional[str] = None,
    ):
        # Within a Reboot context we do not pass on the caller's bearer token, as that might
        # have security implications - we cannot simply trust any service we are calling with
        # the user's credentials. Instead, the developer can rely on the default app-internal
        # auth, or override that and set an explicit bearer token.
        #
        # In the case of `ExternalContext`, however, its `bearer_token` was set specifically
        # by the developer for the purpose of making these calls.
        app_internal_authorization: IMPORT_typing.Optional[str] = None
        if isinstance(context, IMPORT_reboot_aio_external.ExternalContext):
            # Note that only `ExternalContext` even has a `bearer_token` field.
            bearer_token = context.bearer_token
            app_internal_authorization = context.app_internal_authorization

        super().__init__(
            channel_manager=context.channel_manager,
            idempotency_manager=context,
            state_ref=state_ref,
            context=context if isinstance(context, IMPORT_reboot_aio_contexts.Context) else None,
            bearer_token=bearer_token,
            app_internal_authorization=app_internal_authorization,
        )

        # All the channels for all services of this state will go to the same
        # place, so we can just get a single channel and share it across all
        # stubs.
        channel = self._channel_manager.get_channel_to_state(
            self.__state_type_name__, state_ref
        )
        self._rbt_std_collections_ordered_map_v1_orderedmapmethods_stub = rbt.std.collections.ordered_map.v1.ordered_map_pb2_grpc.OrderedMapMethodsStub(channel)


class OrderedMapReaderStub(_OrderedMapStub):

    def __init__(
        self,
        context: IMPORT_reboot_aio_contexts.ReaderContext | IMPORT_reboot_aio_contexts.WriterContext | IMPORT_reboot_aio_contexts.TransactionContext | IMPORT_reboot_aio_contexts.WorkflowContext | IMPORT_reboot_aio_external.ExternalContext,
        *,
        state_ref: IMPORT_reboot_aio_types.StateRef,
        bearer_token: IMPORT_typing.Optional[str] = None,
    ):
        IMPORT_reboot_aio_types.assert_type(context, [IMPORT_reboot_aio_contexts.ReaderContext, IMPORT_reboot_aio_contexts.WriterContext, IMPORT_reboot_aio_contexts.TransactionContext, IMPORT_reboot_aio_contexts.WorkflowContext, IMPORT_reboot_aio_external.ExternalContext])
        super().__init__(
            context=context,
            state_ref=state_ref,
            bearer_token=bearer_token,
        )

    # OrderedMap specific methods:

    async def Search(
        self,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapSearchRequest,
        *,
        metadata: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None,
        bearer_token: IMPORT_typing.Optional[str] = None,
        idempotency: IMPORT_typing.Optional[IMPORT_reboot_aio_idempotency.Idempotency] = None,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapSearchResponse:
        state_type_name = IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.OrderedMap')
        service_name = IMPORT_reboot_aio_types.ServiceName('rbt.std.collections.ordered_map.v1.OrderedMapMethods')
        method = 'Search'

        async def call():
            async with self._call(
                state_type_name,
                service_name,
                method,
                self._rbt_std_collections_ordered_map_v1_orderedmapmethods_stub.Search,
                request,
                unary=True,
                reader=True,
                response_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapSearchResponse,
                aborted_type=OrderedMap.SearchAborted,
                metadata=metadata,
                bearer_token=bearer_token,
            ) as call:
                assert isinstance(call, IMPORT_typing.Awaitable)
                return await call

        if isinstance(self._context, IMPORT_reboot_aio_contexts.WorkflowContext):
            # Use the idempotency manager to make sure that this
            # reader is being called following the rules.
            with self._context.idempotently(
                state_type_name=state_type_name,
                state_ref=self._headers.state_ref,
                service_name=service_name,
                method=method,
                mutation=False,
                request=request,
                metadata=metadata,
                idempotency=idempotency,
                # Only need to pass `aborted_type` for mutations.
                aborted_type=None,
            ) as idempotency_key:
                assert idempotency is not None
                # Check if this reader is from an `.always()` and if
                # so, don't memoize!
                if idempotency.always:
                    return await call()

                assert idempotency_key is not None
                return await IMPORT_reboot_aio_workflows.at_least_once(
                    (
                        # TODO: for easier debugging include the
                        # original alias (or generated alias in the
                        # case of `.per_iteration()` w/o an alias)
                        # instead of just `idempotency_key`.
                        f'{ service_name }.{ method } ({str(idempotency_key)})',
                        # NOTE: we want this to be `PER_WORKFLOW`
                        # because any per iteration concerns should
                        # have already been taken care of by caller
                        # using `.per_iteration()`.
                        IMPORT_reboot_aio_workflows.PER_WORKFLOW
                    ),
                    self._context,
                    call,
                    type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapSearchResponse,
                )
        return await call()



    async def Range(
        self,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRangeRequest,
        *,
        metadata: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None,
        bearer_token: IMPORT_typing.Optional[str] = None,
        idempotency: IMPORT_typing.Optional[IMPORT_reboot_aio_idempotency.Idempotency] = None,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRangeResponse:
        state_type_name = IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.OrderedMap')
        service_name = IMPORT_reboot_aio_types.ServiceName('rbt.std.collections.ordered_map.v1.OrderedMapMethods')
        method = 'Range'

        async def call():
            async with self._call(
                state_type_name,
                service_name,
                method,
                self._rbt_std_collections_ordered_map_v1_orderedmapmethods_stub.Range,
                request,
                unary=True,
                reader=True,
                response_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRangeResponse,
                aborted_type=OrderedMap.RangeAborted,
                metadata=metadata,
                bearer_token=bearer_token,
            ) as call:
                assert isinstance(call, IMPORT_typing.Awaitable)
                return await call

        if isinstance(self._context, IMPORT_reboot_aio_contexts.WorkflowContext):
            # Use the idempotency manager to make sure that this
            # reader is being called following the rules.
            with self._context.idempotently(
                state_type_name=state_type_name,
                state_ref=self._headers.state_ref,
                service_name=service_name,
                method=method,
                mutation=False,
                request=request,
                metadata=metadata,
                idempotency=idempotency,
                # Only need to pass `aborted_type` for mutations.
                aborted_type=None,
            ) as idempotency_key:
                assert idempotency is not None
                # Check if this reader is from an `.always()` and if
                # so, don't memoize!
                if idempotency.always:
                    return await call()

                assert idempotency_key is not None
                return await IMPORT_reboot_aio_workflows.at_least_once(
                    (
                        # TODO: for easier debugging include the
                        # original alias (or generated alias in the
                        # case of `.per_iteration()` w/o an alias)
                        # instead of just `idempotency_key`.
                        f'{ service_name }.{ method } ({str(idempotency_key)})',
                        # NOTE: we want this to be `PER_WORKFLOW`
                        # because any per iteration concerns should
                        # have already been taken care of by caller
                        # using `.per_iteration()`.
                        IMPORT_reboot_aio_workflows.PER_WORKFLOW
                    ),
                    self._context,
                    call,
                    type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRangeResponse,
                )
        return await call()

    async def ReverseRange(
        self,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapReverseRangeRequest,
        *,
        metadata: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None,
        bearer_token: IMPORT_typing.Optional[str] = None,
        idempotency: IMPORT_typing.Optional[IMPORT_reboot_aio_idempotency.Idempotency] = None,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapReverseRangeResponse:
        state_type_name = IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.OrderedMap')
        service_name = IMPORT_reboot_aio_types.ServiceName('rbt.std.collections.ordered_map.v1.OrderedMapMethods')
        method = 'ReverseRange'

        async def call():
            async with self._call(
                state_type_name,
                service_name,
                method,
                self._rbt_std_collections_ordered_map_v1_orderedmapmethods_stub.ReverseRange,
                request,
                unary=True,
                reader=True,
                response_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapReverseRangeResponse,
                aborted_type=OrderedMap.ReverseRangeAborted,
                metadata=metadata,
                bearer_token=bearer_token,
            ) as call:
                assert isinstance(call, IMPORT_typing.Awaitable)
                return await call

        if isinstance(self._context, IMPORT_reboot_aio_contexts.WorkflowContext):
            # Use the idempotency manager to make sure that this
            # reader is being called following the rules.
            with self._context.idempotently(
                state_type_name=state_type_name,
                state_ref=self._headers.state_ref,
                service_name=service_name,
                method=method,
                mutation=False,
                request=request,
                metadata=metadata,
                idempotency=idempotency,
                # Only need to pass `aborted_type` for mutations.
                aborted_type=None,
            ) as idempotency_key:
                assert idempotency is not None
                # Check if this reader is from an `.always()` and if
                # so, don't memoize!
                if idempotency.always:
                    return await call()

                assert idempotency_key is not None
                return await IMPORT_reboot_aio_workflows.at_least_once(
                    (
                        # TODO: for easier debugging include the
                        # original alias (or generated alias in the
                        # case of `.per_iteration()` w/o an alias)
                        # instead of just `idempotency_key`.
                        f'{ service_name }.{ method } ({str(idempotency_key)})',
                        # NOTE: we want this to be `PER_WORKFLOW`
                        # because any per iteration concerns should
                        # have already been taken care of by caller
                        # using `.per_iteration()`.
                        IMPORT_reboot_aio_workflows.PER_WORKFLOW
                    ),
                    self._context,
                    call,
                    type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapReverseRangeResponse,
                )
        return await call()

    async def Stringify(
        self,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapStringifyRequest,
        *,
        metadata: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None,
        bearer_token: IMPORT_typing.Optional[str] = None,
        idempotency: IMPORT_typing.Optional[IMPORT_reboot_aio_idempotency.Idempotency] = None,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapStringifyResponse:
        state_type_name = IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.OrderedMap')
        service_name = IMPORT_reboot_aio_types.ServiceName('rbt.std.collections.ordered_map.v1.OrderedMapMethods')
        method = 'Stringify'

        async def call():
            async with self._call(
                state_type_name,
                service_name,
                method,
                self._rbt_std_collections_ordered_map_v1_orderedmapmethods_stub.Stringify,
                request,
                unary=True,
                reader=True,
                response_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapStringifyResponse,
                aborted_type=OrderedMap.StringifyAborted,
                metadata=metadata,
                bearer_token=bearer_token,
            ) as call:
                assert isinstance(call, IMPORT_typing.Awaitable)
                return await call

        if isinstance(self._context, IMPORT_reboot_aio_contexts.WorkflowContext):
            # Use the idempotency manager to make sure that this
            # reader is being called following the rules.
            with self._context.idempotently(
                state_type_name=state_type_name,
                state_ref=self._headers.state_ref,
                service_name=service_name,
                method=method,
                mutation=False,
                request=request,
                metadata=metadata,
                idempotency=idempotency,
                # Only need to pass `aborted_type` for mutations.
                aborted_type=None,
            ) as idempotency_key:
                assert idempotency is not None
                # Check if this reader is from an `.always()` and if
                # so, don't memoize!
                if idempotency.always:
                    return await call()

                assert idempotency_key is not None
                return await IMPORT_reboot_aio_workflows.at_least_once(
                    (
                        # TODO: for easier debugging include the
                        # original alias (or generated alias in the
                        # case of `.per_iteration()` w/o an alias)
                        # instead of just `idempotency_key`.
                        f'{ service_name }.{ method } ({str(idempotency_key)})',
                        # NOTE: we want this to be `PER_WORKFLOW`
                        # because any per iteration concerns should
                        # have already been taken care of by caller
                        # using `.per_iteration()`.
                        IMPORT_reboot_aio_workflows.PER_WORKFLOW
                    ),
                    self._context,
                    call,
                    type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapStringifyResponse,
                )
        return await call()



class OrderedMapWriterStub(_OrderedMapStub):

    def __init__(
        self,
        context: IMPORT_reboot_aio_contexts.TransactionContext | IMPORT_reboot_aio_contexts.WorkflowContext | IMPORT_reboot_aio_external.ExternalContext,
        *,
        state_ref: IMPORT_reboot_aio_types.StateRef,
        bearer_token: IMPORT_typing.Optional[str] = None,
    ):
        IMPORT_reboot_aio_types.assert_type(context, [IMPORT_reboot_aio_contexts.TransactionContext, IMPORT_reboot_aio_contexts.WorkflowContext, IMPORT_reboot_aio_external.ExternalContext])
        super().__init__(
            context=context,
            state_ref=state_ref,
            bearer_token=bearer_token,
        )

    # OrderedMap specific methods:

    async def Search(
        self,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapSearchRequest,
        *,
        metadata: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None,
        bearer_token: IMPORT_typing.Optional[str] = None,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapSearchResponse:
        async with self._call(
            IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.OrderedMap'),
            IMPORT_reboot_aio_types.ServiceName('rbt.std.collections.ordered_map.v1.OrderedMapMethods'),
            'Search',
            self._rbt_std_collections_ordered_map_v1_orderedmapmethods_stub.Search,
            request,
            unary=True,
            reader=True,
            response_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapSearchResponse,
            aborted_type=OrderedMap.SearchAborted,
            metadata=metadata,
            bearer_token=bearer_token,
        ) as call:
            assert isinstance(call, IMPORT_typing.Awaitable), type(call)
            return await call



    async def Range(
        self,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRangeRequest,
        *,
        metadata: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None,
        bearer_token: IMPORT_typing.Optional[str] = None,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRangeResponse:
        async with self._call(
            IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.OrderedMap'),
            IMPORT_reboot_aio_types.ServiceName('rbt.std.collections.ordered_map.v1.OrderedMapMethods'),
            'Range',
            self._rbt_std_collections_ordered_map_v1_orderedmapmethods_stub.Range,
            request,
            unary=True,
            reader=True,
            response_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRangeResponse,
            aborted_type=OrderedMap.RangeAborted,
            metadata=metadata,
            bearer_token=bearer_token,
        ) as call:
            assert isinstance(call, IMPORT_typing.Awaitable), type(call)
            return await call

    async def ReverseRange(
        self,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapReverseRangeRequest,
        *,
        metadata: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None,
        bearer_token: IMPORT_typing.Optional[str] = None,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapReverseRangeResponse:
        async with self._call(
            IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.OrderedMap'),
            IMPORT_reboot_aio_types.ServiceName('rbt.std.collections.ordered_map.v1.OrderedMapMethods'),
            'ReverseRange',
            self._rbt_std_collections_ordered_map_v1_orderedmapmethods_stub.ReverseRange,
            request,
            unary=True,
            reader=True,
            response_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapReverseRangeResponse,
            aborted_type=OrderedMap.ReverseRangeAborted,
            metadata=metadata,
            bearer_token=bearer_token,
        ) as call:
            assert isinstance(call, IMPORT_typing.Awaitable), type(call)
            return await call

    async def Stringify(
        self,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapStringifyRequest,
        *,
        metadata: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None,
        bearer_token: IMPORT_typing.Optional[str] = None,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapStringifyResponse:
        async with self._call(
            IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.OrderedMap'),
            IMPORT_reboot_aio_types.ServiceName('rbt.std.collections.ordered_map.v1.OrderedMapMethods'),
            'Stringify',
            self._rbt_std_collections_ordered_map_v1_orderedmapmethods_stub.Stringify,
            request,
            unary=True,
            reader=True,
            response_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapStringifyResponse,
            aborted_type=OrderedMap.StringifyAborted,
            metadata=metadata,
            bearer_token=bearer_token,
        ) as call:
            assert isinstance(call, IMPORT_typing.Awaitable), type(call)
            return await call


class OrderedMapWorkflowStub(_OrderedMapStub):

    def __init__(
        self,
        *,
        context: IMPORT_reboot_aio_contexts.TransactionContext | IMPORT_reboot_aio_contexts.WorkflowContext | IMPORT_reboot_aio_external.ExternalContext,
        state_ref: IMPORT_reboot_aio_types.StateRef,
        bearer_token: IMPORT_typing.Optional[str] = None,
    ):
        IMPORT_reboot_aio_types.assert_type(context, [IMPORT_reboot_aio_contexts.TransactionContext, IMPORT_reboot_aio_contexts.WorkflowContext, IMPORT_reboot_aio_external.ExternalContext])
        super().__init__(
            context=context,
            state_ref=state_ref,
            bearer_token=bearer_token,
        )

    # OrderedMap specific methods:
    async def Create(
        self,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapCreateRequest,
        idempotency: IMPORT_typing.Optional[IMPORT_reboot_aio_idempotency.Idempotency] = None,
        *,
        metadata: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None,
        bearer_token: IMPORT_typing.Optional[str] = None,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapCreateResponse:
        idempotency_key: IMPORT_typing.Optional[IMPORT_uuid.UUID]
        with self._idempotency_manager.idempotently(
            state_type_name=IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.OrderedMap'),
            state_ref=self._headers.state_ref,
            service_name=IMPORT_reboot_aio_types.ServiceName('rbt.std.collections.ordered_map.v1.OrderedMapMethods'),
            method='Create',
            mutation=True,
            request=request,
            metadata=metadata,
            idempotency=idempotency,
            aborted_type=OrderedMap.CreateAborted,
        ) as idempotency_key:
            async with self._call(
                IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.OrderedMap'),
                IMPORT_reboot_aio_types.ServiceName('rbt.std.collections.ordered_map.v1.OrderedMapMethods'),
                'Create',
                self._rbt_std_collections_ordered_map_v1_orderedmapmethods_stub.Create,
                request,
                unary=True,
                reader=False,
                response_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapCreateResponse,
                aborted_type=OrderedMap.CreateAborted,
                metadata=metadata,
                idempotency_key=idempotency_key,
                bearer_token=bearer_token,
            ) as call:
                assert isinstance(call, IMPORT_typing.Awaitable), type(call)
                return await call

    async def Search(
        self,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapSearchRequest,
        *,
        metadata: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None,
        bearer_token: IMPORT_typing.Optional[str] = None,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapSearchResponse:
        async with self._call(
            IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.OrderedMap'),
            IMPORT_reboot_aio_types.ServiceName('rbt.std.collections.ordered_map.v1.OrderedMapMethods'),
            'Search',
            self._rbt_std_collections_ordered_map_v1_orderedmapmethods_stub.Search,
            request,
            unary=True,
            reader=True,
            response_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapSearchResponse,
            aborted_type=OrderedMap.SearchAborted,
            metadata=metadata,
            bearer_token=bearer_token,
        ) as call:
            assert isinstance(call, IMPORT_typing.Awaitable), type(call)
            return await call

    async def Insert(
        self,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapInsertRequest,
        idempotency: IMPORT_typing.Optional[IMPORT_reboot_aio_idempotency.Idempotency] = None,
        *,
        metadata: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None,
        bearer_token: IMPORT_typing.Optional[str] = None,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapInsertResponse:
        idempotency_key: IMPORT_typing.Optional[IMPORT_uuid.UUID]
        with self._idempotency_manager.idempotently(
            state_type_name=IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.OrderedMap'),
            state_ref=self._headers.state_ref,
            service_name=IMPORT_reboot_aio_types.ServiceName('rbt.std.collections.ordered_map.v1.OrderedMapMethods'),
            method='Insert',
            mutation=True,
            request=request,
            metadata=metadata,
            idempotency=idempotency,
            aborted_type=OrderedMap.InsertAborted,
        ) as idempotency_key:
            async with self._call(
                IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.OrderedMap'),
                IMPORT_reboot_aio_types.ServiceName('rbt.std.collections.ordered_map.v1.OrderedMapMethods'),
                'Insert',
                self._rbt_std_collections_ordered_map_v1_orderedmapmethods_stub.Insert,
                request,
                unary=True,
                reader=False,
                response_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapInsertResponse,
                aborted_type=OrderedMap.InsertAborted,
                metadata=metadata,
                idempotency_key=idempotency_key,
                bearer_token=bearer_token,
            ) as call:
                assert isinstance(call, IMPORT_typing.Awaitable), type(call)
                return await call

    async def Remove(
        self,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRemoveRequest,
        idempotency: IMPORT_typing.Optional[IMPORT_reboot_aio_idempotency.Idempotency] = None,
        *,
        metadata: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None,
        bearer_token: IMPORT_typing.Optional[str] = None,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRemoveResponse:
        idempotency_key: IMPORT_typing.Optional[IMPORT_uuid.UUID]
        with self._idempotency_manager.idempotently(
            state_type_name=IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.OrderedMap'),
            state_ref=self._headers.state_ref,
            service_name=IMPORT_reboot_aio_types.ServiceName('rbt.std.collections.ordered_map.v1.OrderedMapMethods'),
            method='Remove',
            mutation=True,
            request=request,
            metadata=metadata,
            idempotency=idempotency,
            aborted_type=OrderedMap.RemoveAborted,
        ) as idempotency_key:
            async with self._call(
                IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.OrderedMap'),
                IMPORT_reboot_aio_types.ServiceName('rbt.std.collections.ordered_map.v1.OrderedMapMethods'),
                'Remove',
                self._rbt_std_collections_ordered_map_v1_orderedmapmethods_stub.Remove,
                request,
                unary=True,
                reader=False,
                response_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRemoveResponse,
                aborted_type=OrderedMap.RemoveAborted,
                metadata=metadata,
                idempotency_key=idempotency_key,
                bearer_token=bearer_token,
            ) as call:
                assert isinstance(call, IMPORT_typing.Awaitable), type(call)
                return await call

    async def Range(
        self,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRangeRequest,
        *,
        metadata: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None,
        bearer_token: IMPORT_typing.Optional[str] = None,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRangeResponse:
        async with self._call(
            IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.OrderedMap'),
            IMPORT_reboot_aio_types.ServiceName('rbt.std.collections.ordered_map.v1.OrderedMapMethods'),
            'Range',
            self._rbt_std_collections_ordered_map_v1_orderedmapmethods_stub.Range,
            request,
            unary=True,
            reader=True,
            response_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRangeResponse,
            aborted_type=OrderedMap.RangeAborted,
            metadata=metadata,
            bearer_token=bearer_token,
        ) as call:
            assert isinstance(call, IMPORT_typing.Awaitable), type(call)
            return await call

    async def ReverseRange(
        self,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapReverseRangeRequest,
        *,
        metadata: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None,
        bearer_token: IMPORT_typing.Optional[str] = None,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapReverseRangeResponse:
        async with self._call(
            IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.OrderedMap'),
            IMPORT_reboot_aio_types.ServiceName('rbt.std.collections.ordered_map.v1.OrderedMapMethods'),
            'ReverseRange',
            self._rbt_std_collections_ordered_map_v1_orderedmapmethods_stub.ReverseRange,
            request,
            unary=True,
            reader=True,
            response_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapReverseRangeResponse,
            aborted_type=OrderedMap.ReverseRangeAborted,
            metadata=metadata,
            bearer_token=bearer_token,
        ) as call:
            assert isinstance(call, IMPORT_typing.Awaitable), type(call)
            return await call

    async def Stringify(
        self,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapStringifyRequest,
        *,
        metadata: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None,
        bearer_token: IMPORT_typing.Optional[str] = None,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapStringifyResponse:
        async with self._call(
            IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.OrderedMap'),
            IMPORT_reboot_aio_types.ServiceName('rbt.std.collections.ordered_map.v1.OrderedMapMethods'),
            'Stringify',
            self._rbt_std_collections_ordered_map_v1_orderedmapmethods_stub.Stringify,
            request,
            unary=True,
            reader=True,
            response_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapStringifyResponse,
            aborted_type=OrderedMap.StringifyAborted,
            metadata=metadata,
            bearer_token=bearer_token,
        ) as call:
            assert isinstance(call, IMPORT_typing.Awaitable), type(call)
            return await call



class OrderedMapTasksStub(_OrderedMapStub):

    def __init__(
        self,
        context: IMPORT_reboot_aio_contexts.TransactionContext | IMPORT_reboot_aio_contexts.WorkflowContext | IMPORT_reboot_aio_external.ExternalContext,
        *,
        state_ref: IMPORT_reboot_aio_types.StateRef,
        bearer_token: IMPORT_typing.Optional[str] = None,
    ):
        IMPORT_reboot_aio_types.assert_type(context, [IMPORT_reboot_aio_contexts.TransactionContext, IMPORT_reboot_aio_contexts.WorkflowContext, IMPORT_reboot_aio_external.ExternalContext])
        super().__init__(
            context=context,
            state_ref=state_ref,
            bearer_token=bearer_token,
        )

    # OrderedMap specific methods:
    async def Create(
        self,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapCreateRequest,
        idempotency: IMPORT_typing.Optional[IMPORT_reboot_aio_idempotency.Idempotency] = None,
        *,
        metadata: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None,
        bearer_token: IMPORT_typing.Optional[str] = None,
    ) -> IMPORT_rbt_v1alpha1.tasks_pb2.TaskId:
        idempotency_key: IMPORT_typing.Optional[IMPORT_uuid.UUID]
        with self._idempotency_manager.idempotently(
            state_type_name=IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.OrderedMap'),
            state_ref=self._headers.state_ref,
            service_name=IMPORT_reboot_aio_types.ServiceName('rbt.std.collections.ordered_map.v1.OrderedMapMethods'),
            method='Create',
            mutation=True,
            request=request,
            metadata=metadata,
            idempotency=idempotency,
            aborted_type=OrderedMap.CreateAborted,
        ) as idempotency_key:
            async with self._call(
                IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.OrderedMap'),
                IMPORT_reboot_aio_types.ServiceName('rbt.std.collections.ordered_map.v1.OrderedMapMethods'),
                'Create',
                self._rbt_std_collections_ordered_map_v1_orderedmapmethods_stub.Create,
                request,
                unary=True,
                reader=False,
                response_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapCreateResponse,
                aborted_type=OrderedMap.CreateAborted,
                metadata=metadata,
                idempotency_key=idempotency_key,
                bearer_token=bearer_token,
            ) as call:
                assert isinstance(call, IMPORT_typing.Awaitable), type(call)
                await call
                for (key, value) in await call.trailing_metadata():  # type: ignore[misc, attr-defined]
                    if key == IMPORT_reboot_aio_headers.TASK_ID_UUID:
                        return IMPORT_rbt_v1alpha1.tasks_pb2.TaskId(
                            state_type=IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.OrderedMap'),
                            state_ref=self._headers.state_ref.to_str(),
                            task_uuid=IMPORT_uuid.UUID(value).bytes,
                        )
                raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                    IMPORT_rbt_v1alpha1.errors_pb2.Internal(),
                    message='Trailing metadata missing for task schedule',
                )
    async def Search(
        self,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapSearchRequest,
        idempotency: IMPORT_typing.Optional[IMPORT_reboot_aio_idempotency.Idempotency] = None,
        *,
        metadata: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None,
        bearer_token: IMPORT_typing.Optional[str] = None,
    ) -> IMPORT_rbt_v1alpha1.tasks_pb2.TaskId:
        idempotency_key: IMPORT_typing.Optional[IMPORT_uuid.UUID]
        with self._idempotency_manager.idempotently(
            state_type_name=IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.OrderedMap'),
            state_ref=self._headers.state_ref,
            service_name=IMPORT_reboot_aio_types.ServiceName('rbt.std.collections.ordered_map.v1.OrderedMapMethods'),
            method='Search',
            mutation=True,
            request=request,
            metadata=metadata,
            idempotency=idempotency,
            aborted_type=OrderedMap.SearchAborted,
        ) as idempotency_key:
            async with self._call(
                IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.OrderedMap'),
                IMPORT_reboot_aio_types.ServiceName('rbt.std.collections.ordered_map.v1.OrderedMapMethods'),
                'Search',
                self._rbt_std_collections_ordered_map_v1_orderedmapmethods_stub.Search,
                request,
                unary=True,
                reader=False,
                response_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapSearchResponse,
                aborted_type=OrderedMap.SearchAborted,
                metadata=metadata,
                idempotency_key=idempotency_key,
                bearer_token=bearer_token,
            ) as call:
                assert isinstance(call, IMPORT_typing.Awaitable), type(call)
                await call
                for (key, value) in await call.trailing_metadata():  # type: ignore[misc, attr-defined]
                    if key == IMPORT_reboot_aio_headers.TASK_ID_UUID:
                        return IMPORT_rbt_v1alpha1.tasks_pb2.TaskId(
                            state_type=IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.OrderedMap'),
                            state_ref=self._headers.state_ref.to_str(),
                            task_uuid=IMPORT_uuid.UUID(value).bytes,
                        )
                raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                    IMPORT_rbt_v1alpha1.errors_pb2.Internal(),
                    message='Trailing metadata missing for task schedule',
                )
    async def Insert(
        self,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapInsertRequest,
        idempotency: IMPORT_typing.Optional[IMPORT_reboot_aio_idempotency.Idempotency] = None,
        *,
        metadata: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None,
        bearer_token: IMPORT_typing.Optional[str] = None,
    ) -> IMPORT_rbt_v1alpha1.tasks_pb2.TaskId:
        idempotency_key: IMPORT_typing.Optional[IMPORT_uuid.UUID]
        with self._idempotency_manager.idempotently(
            state_type_name=IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.OrderedMap'),
            state_ref=self._headers.state_ref,
            service_name=IMPORT_reboot_aio_types.ServiceName('rbt.std.collections.ordered_map.v1.OrderedMapMethods'),
            method='Insert',
            mutation=True,
            request=request,
            metadata=metadata,
            idempotency=idempotency,
            aborted_type=OrderedMap.InsertAborted,
        ) as idempotency_key:
            async with self._call(
                IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.OrderedMap'),
                IMPORT_reboot_aio_types.ServiceName('rbt.std.collections.ordered_map.v1.OrderedMapMethods'),
                'Insert',
                self._rbt_std_collections_ordered_map_v1_orderedmapmethods_stub.Insert,
                request,
                unary=True,
                reader=False,
                response_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapInsertResponse,
                aborted_type=OrderedMap.InsertAborted,
                metadata=metadata,
                idempotency_key=idempotency_key,
                bearer_token=bearer_token,
            ) as call:
                assert isinstance(call, IMPORT_typing.Awaitable), type(call)
                await call
                for (key, value) in await call.trailing_metadata():  # type: ignore[misc, attr-defined]
                    if key == IMPORT_reboot_aio_headers.TASK_ID_UUID:
                        return IMPORT_rbt_v1alpha1.tasks_pb2.TaskId(
                            state_type=IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.OrderedMap'),
                            state_ref=self._headers.state_ref.to_str(),
                            task_uuid=IMPORT_uuid.UUID(value).bytes,
                        )
                raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                    IMPORT_rbt_v1alpha1.errors_pb2.Internal(),
                    message='Trailing metadata missing for task schedule',
                )
    async def Remove(
        self,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRemoveRequest,
        idempotency: IMPORT_typing.Optional[IMPORT_reboot_aio_idempotency.Idempotency] = None,
        *,
        metadata: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None,
        bearer_token: IMPORT_typing.Optional[str] = None,
    ) -> IMPORT_rbt_v1alpha1.tasks_pb2.TaskId:
        idempotency_key: IMPORT_typing.Optional[IMPORT_uuid.UUID]
        with self._idempotency_manager.idempotently(
            state_type_name=IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.OrderedMap'),
            state_ref=self._headers.state_ref,
            service_name=IMPORT_reboot_aio_types.ServiceName('rbt.std.collections.ordered_map.v1.OrderedMapMethods'),
            method='Remove',
            mutation=True,
            request=request,
            metadata=metadata,
            idempotency=idempotency,
            aborted_type=OrderedMap.RemoveAborted,
        ) as idempotency_key:
            async with self._call(
                IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.OrderedMap'),
                IMPORT_reboot_aio_types.ServiceName('rbt.std.collections.ordered_map.v1.OrderedMapMethods'),
                'Remove',
                self._rbt_std_collections_ordered_map_v1_orderedmapmethods_stub.Remove,
                request,
                unary=True,
                reader=False,
                response_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRemoveResponse,
                aborted_type=OrderedMap.RemoveAborted,
                metadata=metadata,
                idempotency_key=idempotency_key,
                bearer_token=bearer_token,
            ) as call:
                assert isinstance(call, IMPORT_typing.Awaitable), type(call)
                await call
                for (key, value) in await call.trailing_metadata():  # type: ignore[misc, attr-defined]
                    if key == IMPORT_reboot_aio_headers.TASK_ID_UUID:
                        return IMPORT_rbt_v1alpha1.tasks_pb2.TaskId(
                            state_type=IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.OrderedMap'),
                            state_ref=self._headers.state_ref.to_str(),
                            task_uuid=IMPORT_uuid.UUID(value).bytes,
                        )
                raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                    IMPORT_rbt_v1alpha1.errors_pb2.Internal(),
                    message='Trailing metadata missing for task schedule',
                )
    async def Range(
        self,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRangeRequest,
        idempotency: IMPORT_typing.Optional[IMPORT_reboot_aio_idempotency.Idempotency] = None,
        *,
        metadata: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None,
        bearer_token: IMPORT_typing.Optional[str] = None,
    ) -> IMPORT_rbt_v1alpha1.tasks_pb2.TaskId:
        idempotency_key: IMPORT_typing.Optional[IMPORT_uuid.UUID]
        with self._idempotency_manager.idempotently(
            state_type_name=IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.OrderedMap'),
            state_ref=self._headers.state_ref,
            service_name=IMPORT_reboot_aio_types.ServiceName('rbt.std.collections.ordered_map.v1.OrderedMapMethods'),
            method='Range',
            mutation=True,
            request=request,
            metadata=metadata,
            idempotency=idempotency,
            aborted_type=OrderedMap.RangeAborted,
        ) as idempotency_key:
            async with self._call(
                IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.OrderedMap'),
                IMPORT_reboot_aio_types.ServiceName('rbt.std.collections.ordered_map.v1.OrderedMapMethods'),
                'Range',
                self._rbt_std_collections_ordered_map_v1_orderedmapmethods_stub.Range,
                request,
                unary=True,
                reader=False,
                response_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRangeResponse,
                aborted_type=OrderedMap.RangeAborted,
                metadata=metadata,
                idempotency_key=idempotency_key,
                bearer_token=bearer_token,
            ) as call:
                assert isinstance(call, IMPORT_typing.Awaitable), type(call)
                await call
                for (key, value) in await call.trailing_metadata():  # type: ignore[misc, attr-defined]
                    if key == IMPORT_reboot_aio_headers.TASK_ID_UUID:
                        return IMPORT_rbt_v1alpha1.tasks_pb2.TaskId(
                            state_type=IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.OrderedMap'),
                            state_ref=self._headers.state_ref.to_str(),
                            task_uuid=IMPORT_uuid.UUID(value).bytes,
                        )
                raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                    IMPORT_rbt_v1alpha1.errors_pb2.Internal(),
                    message='Trailing metadata missing for task schedule',
                )
    async def ReverseRange(
        self,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapReverseRangeRequest,
        idempotency: IMPORT_typing.Optional[IMPORT_reboot_aio_idempotency.Idempotency] = None,
        *,
        metadata: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None,
        bearer_token: IMPORT_typing.Optional[str] = None,
    ) -> IMPORT_rbt_v1alpha1.tasks_pb2.TaskId:
        idempotency_key: IMPORT_typing.Optional[IMPORT_uuid.UUID]
        with self._idempotency_manager.idempotently(
            state_type_name=IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.OrderedMap'),
            state_ref=self._headers.state_ref,
            service_name=IMPORT_reboot_aio_types.ServiceName('rbt.std.collections.ordered_map.v1.OrderedMapMethods'),
            method='ReverseRange',
            mutation=True,
            request=request,
            metadata=metadata,
            idempotency=idempotency,
            aborted_type=OrderedMap.ReverseRangeAborted,
        ) as idempotency_key:
            async with self._call(
                IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.OrderedMap'),
                IMPORT_reboot_aio_types.ServiceName('rbt.std.collections.ordered_map.v1.OrderedMapMethods'),
                'ReverseRange',
                self._rbt_std_collections_ordered_map_v1_orderedmapmethods_stub.ReverseRange,
                request,
                unary=True,
                reader=False,
                response_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapReverseRangeResponse,
                aborted_type=OrderedMap.ReverseRangeAborted,
                metadata=metadata,
                idempotency_key=idempotency_key,
                bearer_token=bearer_token,
            ) as call:
                assert isinstance(call, IMPORT_typing.Awaitable), type(call)
                await call
                for (key, value) in await call.trailing_metadata():  # type: ignore[misc, attr-defined]
                    if key == IMPORT_reboot_aio_headers.TASK_ID_UUID:
                        return IMPORT_rbt_v1alpha1.tasks_pb2.TaskId(
                            state_type=IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.OrderedMap'),
                            state_ref=self._headers.state_ref.to_str(),
                            task_uuid=IMPORT_uuid.UUID(value).bytes,
                        )
                raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                    IMPORT_rbt_v1alpha1.errors_pb2.Internal(),
                    message='Trailing metadata missing for task schedule',
                )
    async def Stringify(
        self,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapStringifyRequest,
        idempotency: IMPORT_typing.Optional[IMPORT_reboot_aio_idempotency.Idempotency] = None,
        *,
        metadata: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None,
        bearer_token: IMPORT_typing.Optional[str] = None,
    ) -> IMPORT_rbt_v1alpha1.tasks_pb2.TaskId:
        idempotency_key: IMPORT_typing.Optional[IMPORT_uuid.UUID]
        with self._idempotency_manager.idempotently(
            state_type_name=IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.OrderedMap'),
            state_ref=self._headers.state_ref,
            service_name=IMPORT_reboot_aio_types.ServiceName('rbt.std.collections.ordered_map.v1.OrderedMapMethods'),
            method='Stringify',
            mutation=True,
            request=request,
            metadata=metadata,
            idempotency=idempotency,
            aborted_type=OrderedMap.StringifyAborted,
        ) as idempotency_key:
            async with self._call(
                IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.OrderedMap'),
                IMPORT_reboot_aio_types.ServiceName('rbt.std.collections.ordered_map.v1.OrderedMapMethods'),
                'Stringify',
                self._rbt_std_collections_ordered_map_v1_orderedmapmethods_stub.Stringify,
                request,
                unary=True,
                reader=False,
                response_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapStringifyResponse,
                aborted_type=OrderedMap.StringifyAborted,
                metadata=metadata,
                idempotency_key=idempotency_key,
                bearer_token=bearer_token,
            ) as call:
                assert isinstance(call, IMPORT_typing.Awaitable), type(call)
                await call
                for (key, value) in await call.trailing_metadata():  # type: ignore[misc, attr-defined]
                    if key == IMPORT_reboot_aio_headers.TASK_ID_UUID:
                        return IMPORT_rbt_v1alpha1.tasks_pb2.TaskId(
                            state_type=IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.OrderedMap'),
                            state_ref=self._headers.state_ref.to_str(),
                            task_uuid=IMPORT_uuid.UUID(value).bytes,
                        )
                raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                    IMPORT_rbt_v1alpha1.errors_pb2.Internal(),
                    message='Trailing metadata missing for task schedule',
                )


class OrderedMapServicerTasks:

    _context: IMPORT_reboot_aio_contexts.WriterContext

    def __init__(
        self,
        context: IMPORT_reboot_aio_contexts.WriterContext,
        *,
        state_ref: IMPORT_reboot_aio_types.StateRef,
        bearer_token: IMPORT_typing.Optional[str] = None,
    ):
        IMPORT_reboot_aio_types.assert_type(context, [IMPORT_reboot_aio_contexts.WriterContext])
        self._context = context
        self._state_ref = state_ref

    # OrderedMap specific methods:
    async def Create(
        self,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapCreateRequest,
        *,
        schedule: IMPORT_typing.Optional[IMPORT_datetime_datetime | IMPORT_datetime_timedelta] = None,
    ) -> IMPORT_reboot_aio_tasks.TaskEffect:
        schedule = ensure_has_timezone(when=schedule)
        task = IMPORT_reboot_aio_tasks.TaskEffect(
            state_type=IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.OrderedMap'),
            state_ref=self._state_ref,
            method_name='Create',
            request=request,
            schedule=(IMPORT_reboot_time_DateTimeWithTimeZone.now() + schedule) if isinstance(
                schedule, IMPORT_datetime_timedelta
            ) else schedule,
        )

        self._context._tasks.append(task)

        return task

    async def Search(
        self,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapSearchRequest,
        *,
        schedule: IMPORT_typing.Optional[IMPORT_datetime_datetime | IMPORT_datetime_timedelta] = None,
    ) -> IMPORT_reboot_aio_tasks.TaskEffect:
        schedule = ensure_has_timezone(when=schedule)
        task = IMPORT_reboot_aio_tasks.TaskEffect(
            state_type=IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.OrderedMap'),
            state_ref=self._state_ref,
            method_name='Search',
            request=request,
            schedule=(IMPORT_reboot_time_DateTimeWithTimeZone.now() + schedule) if isinstance(
                schedule, IMPORT_datetime_timedelta
            ) else schedule,
        )

        self._context._tasks.append(task)

        return task

    async def Insert(
        self,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapInsertRequest,
        *,
        schedule: IMPORT_typing.Optional[IMPORT_datetime_datetime | IMPORT_datetime_timedelta] = None,
    ) -> IMPORT_reboot_aio_tasks.TaskEffect:
        schedule = ensure_has_timezone(when=schedule)
        task = IMPORT_reboot_aio_tasks.TaskEffect(
            state_type=IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.OrderedMap'),
            state_ref=self._state_ref,
            method_name='Insert',
            request=request,
            schedule=(IMPORT_reboot_time_DateTimeWithTimeZone.now() + schedule) if isinstance(
                schedule, IMPORT_datetime_timedelta
            ) else schedule,
        )

        self._context._tasks.append(task)

        return task

    async def Remove(
        self,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRemoveRequest,
        *,
        schedule: IMPORT_typing.Optional[IMPORT_datetime_datetime | IMPORT_datetime_timedelta] = None,
    ) -> IMPORT_reboot_aio_tasks.TaskEffect:
        schedule = ensure_has_timezone(when=schedule)
        task = IMPORT_reboot_aio_tasks.TaskEffect(
            state_type=IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.OrderedMap'),
            state_ref=self._state_ref,
            method_name='Remove',
            request=request,
            schedule=(IMPORT_reboot_time_DateTimeWithTimeZone.now() + schedule) if isinstance(
                schedule, IMPORT_datetime_timedelta
            ) else schedule,
        )

        self._context._tasks.append(task)

        return task

    async def Range(
        self,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRangeRequest,
        *,
        schedule: IMPORT_typing.Optional[IMPORT_datetime_datetime | IMPORT_datetime_timedelta] = None,
    ) -> IMPORT_reboot_aio_tasks.TaskEffect:
        schedule = ensure_has_timezone(when=schedule)
        task = IMPORT_reboot_aio_tasks.TaskEffect(
            state_type=IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.OrderedMap'),
            state_ref=self._state_ref,
            method_name='Range',
            request=request,
            schedule=(IMPORT_reboot_time_DateTimeWithTimeZone.now() + schedule) if isinstance(
                schedule, IMPORT_datetime_timedelta
            ) else schedule,
        )

        self._context._tasks.append(task)

        return task

    async def ReverseRange(
        self,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapReverseRangeRequest,
        *,
        schedule: IMPORT_typing.Optional[IMPORT_datetime_datetime | IMPORT_datetime_timedelta] = None,
    ) -> IMPORT_reboot_aio_tasks.TaskEffect:
        schedule = ensure_has_timezone(when=schedule)
        task = IMPORT_reboot_aio_tasks.TaskEffect(
            state_type=IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.OrderedMap'),
            state_ref=self._state_ref,
            method_name='ReverseRange',
            request=request,
            schedule=(IMPORT_reboot_time_DateTimeWithTimeZone.now() + schedule) if isinstance(
                schedule, IMPORT_datetime_timedelta
            ) else schedule,
        )

        self._context._tasks.append(task)

        return task

    async def Stringify(
        self,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapStringifyRequest,
        *,
        schedule: IMPORT_typing.Optional[IMPORT_datetime_datetime | IMPORT_datetime_timedelta] = None,
    ) -> IMPORT_reboot_aio_tasks.TaskEffect:
        schedule = ensure_has_timezone(when=schedule)
        task = IMPORT_reboot_aio_tasks.TaskEffect(
            state_type=IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.OrderedMap'),
            state_ref=self._state_ref,
            method_name='Stringify',
            request=request,
            schedule=(IMPORT_reboot_time_DateTimeWithTimeZone.now() + schedule) if isinstance(
                schedule, IMPORT_datetime_timedelta
            ) else schedule,
        )

        self._context._tasks.append(task)

        return task



############################ Authorizers ############################
# Relevant to servicers; irrelevant to clients.

NodeStateType: IMPORT_typing.TypeAlias = rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node
NodeRequestTypes: IMPORT_typing.TypeAlias = \
        rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateRequest \
        | rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeSearchRequest \
        | rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeInsertRequest \
        | rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRemoveRequest \
        | rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRangeRequest \
        | rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeReverseRangeRequest \
        | rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeStringifyRequest

class NodeAuthorizer(
    IMPORT_rebootdev.aio.auth.authorizers.Authorizer[NodeStateType, NodeRequestTypes],
):
    StateType: IMPORT_typing.TypeAlias = NodeStateType
    RequestTypes: IMPORT_typing.TypeAlias = NodeRequestTypes
    Decision: IMPORT_typing.TypeAlias = IMPORT_rebootdev.aio.auth.authorizers.Authorizer.Decision

    def __init__(
        self,
        *,
        Create: IMPORT_typing.Optional[
            IMPORT_rebootdev.aio.auth.authorizers.AuthorizerRule[
              rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node,
              rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateRequest,
            ]
        ] = None,
        create: IMPORT_typing.Optional[
            IMPORT_rebootdev.aio.auth.authorizers.AuthorizerRule[
              rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node,
              rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateRequest,
            ]
        ] = None,
        Search: IMPORT_typing.Optional[
            IMPORT_rebootdev.aio.auth.authorizers.AuthorizerRule[
              rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node,
              rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeSearchRequest,
            ]
        ] = None,
        search: IMPORT_typing.Optional[
            IMPORT_rebootdev.aio.auth.authorizers.AuthorizerRule[
              rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node,
              rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeSearchRequest,
            ]
        ] = None,
        Insert: IMPORT_typing.Optional[
            IMPORT_rebootdev.aio.auth.authorizers.AuthorizerRule[
              rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node,
              rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeInsertRequest,
            ]
        ] = None,
        insert: IMPORT_typing.Optional[
            IMPORT_rebootdev.aio.auth.authorizers.AuthorizerRule[
              rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node,
              rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeInsertRequest,
            ]
        ] = None,
        Remove: IMPORT_typing.Optional[
            IMPORT_rebootdev.aio.auth.authorizers.AuthorizerRule[
              rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node,
              rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRemoveRequest,
            ]
        ] = None,
        remove: IMPORT_typing.Optional[
            IMPORT_rebootdev.aio.auth.authorizers.AuthorizerRule[
              rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node,
              rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRemoveRequest,
            ]
        ] = None,
        Range: IMPORT_typing.Optional[
            IMPORT_rebootdev.aio.auth.authorizers.AuthorizerRule[
              rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node,
              rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRangeRequest,
            ]
        ] = None,
        range: IMPORT_typing.Optional[
            IMPORT_rebootdev.aio.auth.authorizers.AuthorizerRule[
              rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node,
              rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRangeRequest,
            ]
        ] = None,
        ReverseRange: IMPORT_typing.Optional[
            IMPORT_rebootdev.aio.auth.authorizers.AuthorizerRule[
              rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node,
              rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeReverseRangeRequest,
            ]
        ] = None,
        reverse_range: IMPORT_typing.Optional[
            IMPORT_rebootdev.aio.auth.authorizers.AuthorizerRule[
              rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node,
              rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeReverseRangeRequest,
            ]
        ] = None,
        Stringify: IMPORT_typing.Optional[
            IMPORT_rebootdev.aio.auth.authorizers.AuthorizerRule[
              rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node,
              rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeStringifyRequest,
            ]
        ] = None,
        stringify: IMPORT_typing.Optional[
            IMPORT_rebootdev.aio.auth.authorizers.AuthorizerRule[
              rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node,
              rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeStringifyRequest,
            ]
        ] = None,
        # NOTE: using `_` prefix for `_default` so as not to collide
        # with any method names since a prefixed `_` is forbidden by
        # our protoc plugins.
        _default: IMPORT_rebootdev.aio.auth.authorizers.AuthorizerRule[
            rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node,
            IMPORT_google_protobuf_message.Message,
        ] = IMPORT_rebootdev.aio.auth.authorizers.allow_if(
            all=[IMPORT_rebootdev.aio.auth.authorizers.is_app_internal],
        ),
    ):
        if create is not None and Create is not None:
            raise ValueError(
                f"Cannot specify both 'Create' and 'create' authorizer rules"
            )
        self._create = create or Create
        if search is not None and Search is not None:
            raise ValueError(
                f"Cannot specify both 'Search' and 'search' authorizer rules"
            )
        self._search = search or Search
        if insert is not None and Insert is not None:
            raise ValueError(
                f"Cannot specify both 'Insert' and 'insert' authorizer rules"
            )
        self._insert = insert or Insert
        if remove is not None and Remove is not None:
            raise ValueError(
                f"Cannot specify both 'Remove' and 'remove' authorizer rules"
            )
        self._remove = remove or Remove
        if range is not None and Range is not None:
            raise ValueError(
                f"Cannot specify both 'Range' and 'range' authorizer rules"
            )
        self._range = range or Range
        if reverse_range is not None and ReverseRange is not None:
            raise ValueError(
                f"Cannot specify both 'ReverseRange' and 'reverse_range' authorizer rules"
            )
        self._reverse_range = reverse_range or ReverseRange
        if stringify is not None and Stringify is not None:
            raise ValueError(
                f"Cannot specify both 'Stringify' and 'stringify' authorizer rules"
            )
        self._stringify = stringify or Stringify
        self.__default = _default

    async def authorize(
        self,
        *,
        method_name: str,
        context: IMPORT_reboot_aio_contexts.ReaderContext,
        state: IMPORT_typing.Optional[NodeStateType],
        request: IMPORT_typing.Optional[NodeRequestTypes],
        **kwargs,
    ) -> IMPORT_rebootdev.aio.auth.authorizers.Authorizer.Decision:
        if method_name == 'rbt.std.collections.ordered_map.v1.NodeMethods.Create':
            return await self.Create(
                context=context,
                state=IMPORT_typing.cast(rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node, state),
                request=IMPORT_typing.cast(rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateRequest, request),
            )
        elif method_name == 'rbt.std.collections.ordered_map.v1.NodeMethods.Search':
            return await self.Search(
                context=context,
                state=IMPORT_typing.cast(rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node, state),
                request=IMPORT_typing.cast(rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeSearchRequest, request),
            )
        elif method_name == 'rbt.std.collections.ordered_map.v1.NodeMethods.Insert':
            return await self.Insert(
                context=context,
                state=IMPORT_typing.cast(rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node, state),
                request=IMPORT_typing.cast(rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeInsertRequest, request),
            )
        elif method_name == 'rbt.std.collections.ordered_map.v1.NodeMethods.Remove':
            return await self.Remove(
                context=context,
                state=IMPORT_typing.cast(rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node, state),
                request=IMPORT_typing.cast(rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRemoveRequest, request),
            )
        elif method_name == 'rbt.std.collections.ordered_map.v1.NodeMethods.Range':
            return await self.Range(
                context=context,
                state=IMPORT_typing.cast(rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node, state),
                request=IMPORT_typing.cast(rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRangeRequest, request),
            )
        elif method_name == 'rbt.std.collections.ordered_map.v1.NodeMethods.ReverseRange':
            return await self.ReverseRange(
                context=context,
                state=IMPORT_typing.cast(rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node, state),
                request=IMPORT_typing.cast(rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeReverseRangeRequest, request),
            )
        elif method_name == 'rbt.std.collections.ordered_map.v1.NodeMethods.Stringify':
            return await self.Stringify(
                context=context,
                state=IMPORT_typing.cast(rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node, state),
                request=IMPORT_typing.cast(rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeStringifyRequest, request),
            )
        else:
            return IMPORT_rbt_v1alpha1.errors_pb2.PermissionDenied()

    # For 'rbt.std.collections.ordered_map.v1.NodeMethods.Create'.
    async def Create(
        self,
        *,
        context: IMPORT_reboot_aio_contexts.ReaderContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateRequest,
    ) -> IMPORT_rebootdev.aio.auth.authorizers.Authorizer.Decision:
        return await (self._create or self.__default).execute(
            context=context,
            state=IMPORT_typing.cast(rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node, state),
            request=IMPORT_typing.cast(rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateRequest, request),
        )

    # For 'rbt.std.collections.ordered_map.v1.NodeMethods.Search'.
    async def Search(
        self,
        *,
        context: IMPORT_reboot_aio_contexts.ReaderContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeSearchRequest,
    ) -> IMPORT_rebootdev.aio.auth.authorizers.Authorizer.Decision:
        return await (self._search or self.__default).execute(
            context=context,
            state=IMPORT_typing.cast(rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node, state),
            request=IMPORT_typing.cast(rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeSearchRequest, request),
        )

    # For 'rbt.std.collections.ordered_map.v1.NodeMethods.Insert'.
    async def Insert(
        self,
        *,
        context: IMPORT_reboot_aio_contexts.ReaderContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeInsertRequest,
    ) -> IMPORT_rebootdev.aio.auth.authorizers.Authorizer.Decision:
        return await (self._insert or self.__default).execute(
            context=context,
            state=IMPORT_typing.cast(rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node, state),
            request=IMPORT_typing.cast(rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeInsertRequest, request),
        )

    # For 'rbt.std.collections.ordered_map.v1.NodeMethods.Remove'.
    async def Remove(
        self,
        *,
        context: IMPORT_reboot_aio_contexts.ReaderContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRemoveRequest,
    ) -> IMPORT_rebootdev.aio.auth.authorizers.Authorizer.Decision:
        return await (self._remove or self.__default).execute(
            context=context,
            state=IMPORT_typing.cast(rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node, state),
            request=IMPORT_typing.cast(rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRemoveRequest, request),
        )

    # For 'rbt.std.collections.ordered_map.v1.NodeMethods.Range'.
    async def Range(
        self,
        *,
        context: IMPORT_reboot_aio_contexts.ReaderContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRangeRequest,
    ) -> IMPORT_rebootdev.aio.auth.authorizers.Authorizer.Decision:
        return await (self._range or self.__default).execute(
            context=context,
            state=IMPORT_typing.cast(rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node, state),
            request=IMPORT_typing.cast(rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRangeRequest, request),
        )

    # For 'rbt.std.collections.ordered_map.v1.NodeMethods.ReverseRange'.
    async def ReverseRange(
        self,
        *,
        context: IMPORT_reboot_aio_contexts.ReaderContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeReverseRangeRequest,
    ) -> IMPORT_rebootdev.aio.auth.authorizers.Authorizer.Decision:
        return await (self._reverse_range or self.__default).execute(
            context=context,
            state=IMPORT_typing.cast(rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node, state),
            request=IMPORT_typing.cast(rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeReverseRangeRequest, request),
        )

    # For 'rbt.std.collections.ordered_map.v1.NodeMethods.Stringify'.
    async def Stringify(
        self,
        *,
        context: IMPORT_reboot_aio_contexts.ReaderContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeStringifyRequest,
    ) -> IMPORT_rebootdev.aio.auth.authorizers.Authorizer.Decision:
        return await (self._stringify or self.__default).execute(
            context=context,
            state=IMPORT_typing.cast(rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node, state),
            request=IMPORT_typing.cast(rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeStringifyRequest, request),
        )


OrderedMapStateType: IMPORT_typing.TypeAlias = rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap
OrderedMapRequestTypes: IMPORT_typing.TypeAlias = \
        rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapCreateRequest \
        | rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapSearchRequest \
        | rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapInsertRequest \
        | rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRemoveRequest \
        | rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRangeRequest \
        | rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapReverseRangeRequest \
        | rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapStringifyRequest

class OrderedMapAuthorizer(
    IMPORT_rebootdev.aio.auth.authorizers.Authorizer[OrderedMapStateType, OrderedMapRequestTypes],
):
    StateType: IMPORT_typing.TypeAlias = OrderedMapStateType
    RequestTypes: IMPORT_typing.TypeAlias = OrderedMapRequestTypes
    Decision: IMPORT_typing.TypeAlias = IMPORT_rebootdev.aio.auth.authorizers.Authorizer.Decision

    def __init__(
        self,
        *,
        Create: IMPORT_typing.Optional[
            IMPORT_rebootdev.aio.auth.authorizers.AuthorizerRule[
              rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap,
              rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapCreateRequest,
            ]
        ] = None,
        create: IMPORT_typing.Optional[
            IMPORT_rebootdev.aio.auth.authorizers.AuthorizerRule[
              rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap,
              rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapCreateRequest,
            ]
        ] = None,
        Search: IMPORT_typing.Optional[
            IMPORT_rebootdev.aio.auth.authorizers.AuthorizerRule[
              rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap,
              rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapSearchRequest,
            ]
        ] = None,
        search: IMPORT_typing.Optional[
            IMPORT_rebootdev.aio.auth.authorizers.AuthorizerRule[
              rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap,
              rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapSearchRequest,
            ]
        ] = None,
        Insert: IMPORT_typing.Optional[
            IMPORT_rebootdev.aio.auth.authorizers.AuthorizerRule[
              rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap,
              rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapInsertRequest,
            ]
        ] = None,
        insert: IMPORT_typing.Optional[
            IMPORT_rebootdev.aio.auth.authorizers.AuthorizerRule[
              rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap,
              rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapInsertRequest,
            ]
        ] = None,
        Remove: IMPORT_typing.Optional[
            IMPORT_rebootdev.aio.auth.authorizers.AuthorizerRule[
              rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap,
              rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRemoveRequest,
            ]
        ] = None,
        remove: IMPORT_typing.Optional[
            IMPORT_rebootdev.aio.auth.authorizers.AuthorizerRule[
              rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap,
              rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRemoveRequest,
            ]
        ] = None,
        Range: IMPORT_typing.Optional[
            IMPORT_rebootdev.aio.auth.authorizers.AuthorizerRule[
              rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap,
              rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRangeRequest,
            ]
        ] = None,
        range: IMPORT_typing.Optional[
            IMPORT_rebootdev.aio.auth.authorizers.AuthorizerRule[
              rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap,
              rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRangeRequest,
            ]
        ] = None,
        ReverseRange: IMPORT_typing.Optional[
            IMPORT_rebootdev.aio.auth.authorizers.AuthorizerRule[
              rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap,
              rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapReverseRangeRequest,
            ]
        ] = None,
        reverse_range: IMPORT_typing.Optional[
            IMPORT_rebootdev.aio.auth.authorizers.AuthorizerRule[
              rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap,
              rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapReverseRangeRequest,
            ]
        ] = None,
        Stringify: IMPORT_typing.Optional[
            IMPORT_rebootdev.aio.auth.authorizers.AuthorizerRule[
              rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap,
              rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapStringifyRequest,
            ]
        ] = None,
        stringify: IMPORT_typing.Optional[
            IMPORT_rebootdev.aio.auth.authorizers.AuthorizerRule[
              rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap,
              rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapStringifyRequest,
            ]
        ] = None,
        # NOTE: using `_` prefix for `_default` so as not to collide
        # with any method names since a prefixed `_` is forbidden by
        # our protoc plugins.
        _default: IMPORT_rebootdev.aio.auth.authorizers.AuthorizerRule[
            rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap,
            IMPORT_google_protobuf_message.Message,
        ] = IMPORT_rebootdev.aio.auth.authorizers.allow_if(
            all=[IMPORT_rebootdev.aio.auth.authorizers.is_app_internal],
        ),
    ):
        if create is not None and Create is not None:
            raise ValueError(
                f"Cannot specify both 'Create' and 'create' authorizer rules"
            )
        self._create = create or Create
        if search is not None and Search is not None:
            raise ValueError(
                f"Cannot specify both 'Search' and 'search' authorizer rules"
            )
        self._search = search or Search
        if insert is not None and Insert is not None:
            raise ValueError(
                f"Cannot specify both 'Insert' and 'insert' authorizer rules"
            )
        self._insert = insert or Insert
        if remove is not None and Remove is not None:
            raise ValueError(
                f"Cannot specify both 'Remove' and 'remove' authorizer rules"
            )
        self._remove = remove or Remove
        if range is not None and Range is not None:
            raise ValueError(
                f"Cannot specify both 'Range' and 'range' authorizer rules"
            )
        self._range = range or Range
        if reverse_range is not None and ReverseRange is not None:
            raise ValueError(
                f"Cannot specify both 'ReverseRange' and 'reverse_range' authorizer rules"
            )
        self._reverse_range = reverse_range or ReverseRange
        if stringify is not None and Stringify is not None:
            raise ValueError(
                f"Cannot specify both 'Stringify' and 'stringify' authorizer rules"
            )
        self._stringify = stringify or Stringify
        self.__default = _default

    async def authorize(
        self,
        *,
        method_name: str,
        context: IMPORT_reboot_aio_contexts.ReaderContext,
        state: IMPORT_typing.Optional[OrderedMapStateType],
        request: IMPORT_typing.Optional[OrderedMapRequestTypes],
        **kwargs,
    ) -> IMPORT_rebootdev.aio.auth.authorizers.Authorizer.Decision:
        if method_name == 'rbt.std.collections.ordered_map.v1.OrderedMapMethods.Create':
            return await self.Create(
                context=context,
                state=IMPORT_typing.cast(rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap, state),
                request=IMPORT_typing.cast(rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapCreateRequest, request),
            )
        elif method_name == 'rbt.std.collections.ordered_map.v1.OrderedMapMethods.Search':
            return await self.Search(
                context=context,
                state=IMPORT_typing.cast(rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap, state),
                request=IMPORT_typing.cast(rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapSearchRequest, request),
            )
        elif method_name == 'rbt.std.collections.ordered_map.v1.OrderedMapMethods.Insert':
            return await self.Insert(
                context=context,
                state=IMPORT_typing.cast(rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap, state),
                request=IMPORT_typing.cast(rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapInsertRequest, request),
            )
        elif method_name == 'rbt.std.collections.ordered_map.v1.OrderedMapMethods.Remove':
            return await self.Remove(
                context=context,
                state=IMPORT_typing.cast(rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap, state),
                request=IMPORT_typing.cast(rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRemoveRequest, request),
            )
        elif method_name == 'rbt.std.collections.ordered_map.v1.OrderedMapMethods.Range':
            return await self.Range(
                context=context,
                state=IMPORT_typing.cast(rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap, state),
                request=IMPORT_typing.cast(rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRangeRequest, request),
            )
        elif method_name == 'rbt.std.collections.ordered_map.v1.OrderedMapMethods.ReverseRange':
            return await self.ReverseRange(
                context=context,
                state=IMPORT_typing.cast(rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap, state),
                request=IMPORT_typing.cast(rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapReverseRangeRequest, request),
            )
        elif method_name == 'rbt.std.collections.ordered_map.v1.OrderedMapMethods.Stringify':
            return await self.Stringify(
                context=context,
                state=IMPORT_typing.cast(rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap, state),
                request=IMPORT_typing.cast(rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapStringifyRequest, request),
            )
        else:
            return IMPORT_rbt_v1alpha1.errors_pb2.PermissionDenied()

    # For 'rbt.std.collections.ordered_map.v1.OrderedMapMethods.Create'.
    async def Create(
        self,
        *,
        context: IMPORT_reboot_aio_contexts.ReaderContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapCreateRequest,
    ) -> IMPORT_rebootdev.aio.auth.authorizers.Authorizer.Decision:
        return await (self._create or self.__default).execute(
            context=context,
            state=IMPORT_typing.cast(rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap, state),
            request=IMPORT_typing.cast(rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapCreateRequest, request),
        )

    # For 'rbt.std.collections.ordered_map.v1.OrderedMapMethods.Search'.
    async def Search(
        self,
        *,
        context: IMPORT_reboot_aio_contexts.ReaderContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapSearchRequest,
    ) -> IMPORT_rebootdev.aio.auth.authorizers.Authorizer.Decision:
        return await (self._search or self.__default).execute(
            context=context,
            state=IMPORT_typing.cast(rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap, state),
            request=IMPORT_typing.cast(rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapSearchRequest, request),
        )

    # For 'rbt.std.collections.ordered_map.v1.OrderedMapMethods.Insert'.
    async def Insert(
        self,
        *,
        context: IMPORT_reboot_aio_contexts.ReaderContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapInsertRequest,
    ) -> IMPORT_rebootdev.aio.auth.authorizers.Authorizer.Decision:
        return await (self._insert or self.__default).execute(
            context=context,
            state=IMPORT_typing.cast(rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap, state),
            request=IMPORT_typing.cast(rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapInsertRequest, request),
        )

    # For 'rbt.std.collections.ordered_map.v1.OrderedMapMethods.Remove'.
    async def Remove(
        self,
        *,
        context: IMPORT_reboot_aio_contexts.ReaderContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRemoveRequest,
    ) -> IMPORT_rebootdev.aio.auth.authorizers.Authorizer.Decision:
        return await (self._remove or self.__default).execute(
            context=context,
            state=IMPORT_typing.cast(rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap, state),
            request=IMPORT_typing.cast(rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRemoveRequest, request),
        )

    # For 'rbt.std.collections.ordered_map.v1.OrderedMapMethods.Range'.
    async def Range(
        self,
        *,
        context: IMPORT_reboot_aio_contexts.ReaderContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRangeRequest,
    ) -> IMPORT_rebootdev.aio.auth.authorizers.Authorizer.Decision:
        return await (self._range or self.__default).execute(
            context=context,
            state=IMPORT_typing.cast(rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap, state),
            request=IMPORT_typing.cast(rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRangeRequest, request),
        )

    # For 'rbt.std.collections.ordered_map.v1.OrderedMapMethods.ReverseRange'.
    async def ReverseRange(
        self,
        *,
        context: IMPORT_reboot_aio_contexts.ReaderContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapReverseRangeRequest,
    ) -> IMPORT_rebootdev.aio.auth.authorizers.Authorizer.Decision:
        return await (self._reverse_range or self.__default).execute(
            context=context,
            state=IMPORT_typing.cast(rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap, state),
            request=IMPORT_typing.cast(rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapReverseRangeRequest, request),
        )

    # For 'rbt.std.collections.ordered_map.v1.OrderedMapMethods.Stringify'.
    async def Stringify(
        self,
        *,
        context: IMPORT_reboot_aio_contexts.ReaderContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapStringifyRequest,
    ) -> IMPORT_rebootdev.aio.auth.authorizers.Authorizer.Decision:
        return await (self._stringify or self.__default).execute(
            context=context,
            state=IMPORT_typing.cast(rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap, state),
            request=IMPORT_typing.cast(rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapStringifyRequest, request),
        )



############################ Reboot Servicers ############################
# Base classes for server-side implementations of Reboot servicers.
# Irrelevant to clients.

class NodeBaseServicer(IMPORT_reboot_aio_servicers.Servicer):
    Authorizer: IMPORT_typing.TypeAlias = NodeAuthorizer

    __service_names__ = [
        IMPORT_reboot_aio_types.ServiceName('rbt.std.collections.ordered_map.v1.NodeMethods'),
    ]
    __state_type_name__ = IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.Node')
    __state_type__ = rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node
    __file_descriptor__ = rbt.std.collections.ordered_map.v1.ordered_map_pb2.DESCRIPTOR

    def __init__(self):
        super().__init__()
        # NOTE: need to hold on to the middleware so we can do inline
        # writes (see 'self.write(...)').
        #
        # Because '_middleware' is not really private this does mean
        # users may do possibly dangerous things, but this is no more
        # likely given they could have already overridden
        # 'create_middleware()'.
        self._middleware: IMPORT_typing.Optional[NodeServicerMiddleware] = None

    def create_middleware(
        self,
        *,
        application_id: IMPORT_reboot_aio_types.ApplicationId,
        consensus_id: IMPORT_reboot_aio_types.ConsensusId,
        state_manager: IMPORT_reboot_aio_state_managers.StateManager,
        placement_client: IMPORT_reboot_aio_placement.PlacementClient,
        channel_manager: IMPORT_reboot_aio_internals_channel_manager._ChannelManager,
        tasks_cache: IMPORT_reboot_aio_internals_tasks_cache.TasksCache,
        token_verifier: IMPORT_typing.Optional[IMPORT_rebootdev.aio.auth.token_verifiers.TokenVerifier],
        effect_validation: IMPORT_reboot_aio_contexts.EffectValidation,
        app_internal_api_key_secret: str,
        ready: IMPORT_asyncio.Event,
    ) -> NodeServicerMiddleware:
        self._middleware = NodeServicerMiddleware(
            servicer=self,
            application_id=application_id,
            consensus_id=consensus_id,
            state_manager=state_manager,
            placement_client=placement_client,
            channel_manager=channel_manager,
            tasks_cache=tasks_cache,
            token_verifier=token_verifier,
            effect_validation=effect_validation,
            app_internal_api_key_secret=app_internal_api_key_secret,
            ready=ready,
        )
        return self._middleware

    def authorizer(self) -> IMPORT_typing.Optional[IMPORT_rebootdev.aio.auth.authorizers.Authorizer | IMPORT_rebootdev.aio.auth.authorizers.AuthorizerRule]:
        return None

    def token_verifier(self) -> IMPORT_typing.Optional[IMPORT_rebootdev.aio.auth.token_verifiers.TokenVerifier]:
        return None

    def ref(
        self,
        *,
        bearer_token: IMPORT_typing.Optional[str] = None,
    ) -> Node.WeakReference[Node.WeakReference._WriterSchedule]:
        context = IMPORT_reboot_aio_contexts.Context.get()

        if context is None:
            raise RuntimeError(
                'Missing asyncio context variable `context`; '
                'are you using this class without Reboot?'
            )

        return Node.WeakReference(
            # TODO(https://github.com/reboot-dev/mono/issues/3226): add support for calling other applications.
            # For now this always stays within the application that creates the context.
            application_id=None,
            state_id=context._state_ref.id,
            schedule_type=Node.WeakReference._WriterSchedule,
            # If the user didn't specify a bearer token we may still end up using the app-internal bearer token,
            # but that's decided at the time of the call.
            bearer_token=bearer_token,
            servicer=self,
        )

    class Effects(IMPORT_reboot_aio_state_managers.Effects):
        def __init__(
            self,
            *,
            state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node,
            response: IMPORT_typing.Optional[IMPORT_google_protobuf_message.Message] = None,
            tasks: IMPORT_typing.Optional[list[IMPORT_reboot_aio_tasks.TaskEffect]] = None,
            _colocated_upserts: IMPORT_typing.Optional[list[tuple[str, IMPORT_typing.Optional[bytes]]]] = None,
        ):
            IMPORT_reboot_aio_types.assert_type(state, [rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node])

            super().__init__(state=state, response=response, tasks=tasks, _colocated_upserts=_colocated_upserts)

    # For 'rbt.std.collections.ordered_map.v1.NodeMethods.Create'.
    class CreateEffects(Effects):
        def __init__(
            self,
            *,
            state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node,
            response: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateResponse,
            tasks: IMPORT_typing.Optional[list[IMPORT_reboot_aio_tasks.TaskEffect]] = None,
            _colocated_upserts: IMPORT_typing.Optional[list[tuple[str, IMPORT_typing.Optional[bytes]]]] = None,
        ):
            IMPORT_reboot_aio_types.assert_type(state, [rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node])
            IMPORT_reboot_aio_types.assert_type(response, [rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateResponse])

            super().__init__(state=state, response=response, tasks=tasks, _colocated_upserts=_colocated_upserts)









    InlineWriterCallableResult = IMPORT_typing.TypeVar('InlineWriterCallableResult', covariant=True)

    class InlineWriterCallable(IMPORT_typing.Protocol[InlineWriterCallableResult]):
        async def __call__(
            self,
            state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node
        ) -> NodeBaseServicer.InlineWriterCallableResult:
            ...

    class WorkflowState:

        def __init__(
            self,
            servicer,
        ):
            self._servicer = servicer

        async def read(
            self, context: IMPORT_reboot_aio_contexts.WorkflowContext
        ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node:
            """Read the current state within a workflow."""
            return await (
                self.always() if context.within_until()
                else (
                    self.per_iteration() if context.within_loop()
                    else self.per_workflow()
                )
            ).read(context)

        @IMPORT_typing.overload
        async def write(
            self,
            idempotency_alias: str,
            context: IMPORT_reboot_aio_contexts.WorkflowContext,
            writer: NodeBaseServicer.InlineWriterCallable[None],
            __options__: IMPORT_reboot_aio_call.Options = IMPORT_reboot_aio_call.Options(),
            *,
            type: type = type(None),
        ) -> None:
            ...

        @IMPORT_typing.overload
        async def write(
            self,
            idempotency_alias: str,
            context: IMPORT_reboot_aio_contexts.WorkflowContext,
            writer: NodeBaseServicer.InlineWriterCallable[NodeBaseServicer.InlineWriterCallableResult],
            __options__: IMPORT_reboot_aio_call.Options = IMPORT_reboot_aio_call.Options(),
            *,
            type: type[NodeBaseServicer.InlineWriterCallableResult],
        ) -> NodeBaseServicer.InlineWriterCallableResult:
            ...

        async def write(
            self,
            idempotency_alias: str,
            context: IMPORT_reboot_aio_contexts.WorkflowContext,
            writer: NodeBaseServicer.InlineWriterCallable[NodeBaseServicer.InlineWriterCallableResult],
            __options__: IMPORT_reboot_aio_call.Options = IMPORT_reboot_aio_call.Options(),
            *,
            type: type = type(None),
        ) -> NodeBaseServicer.InlineWriterCallableResult:
            """Perform an "inline write" within a workflow."""
            return await (
                self.per_iteration(idempotency_alias) if context.within_loop()
                else self.per_workflow(idempotency_alias)
            ).write(
                context, writer, __options__, type=type
            )

        class _Idempotently:

            def __init__(
                self,
                *,
                servicer: NodeBaseServicer,
                alias: IMPORT_typing.Optional[str],
                how: IMPORT_reboot_aio_workflows.How,
            ):
                self._servicer = servicer
                self._alias = alias
                self._how = how

            async def read(
                self, context: IMPORT_reboot_aio_contexts.WorkflowContext
            ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node:
                """Read the current state within a workflow."""
                return await self._read(
                    self._servicer,
                    context.idempotency(
                        key=IMPORT_uuid.uuid4(),
                        generated=True,
                    ) if self._how == IMPORT_reboot_aio_workflows.ALWAYS else context.idempotency(
                        alias=self._alias,
                        each_iteration=self._how == IMPORT_reboot_aio_workflows.PER_ITERATION
                    ),
                    context,
                )

            @staticmethod
            async def _read(
                servicer: NodeBaseServicer,
                idempotency: IMPORT_reboot_aio_idempotency.Idempotency,
                context: IMPORT_reboot_aio_contexts.WorkflowContext,
            ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node:
                """Read the current state within a workflow."""
                IMPORT_reboot_aio_types.assert_type(context, [IMPORT_reboot_aio_contexts.WorkflowContext])

                if servicer._middleware is None:
                    raise RuntimeError(
                        'Reboot middleware was not created; '
                        'are you using this class without Reboot?'
                    )

                async def read():
                    assert servicer._middleware is not None
                    return await servicer._middleware._state_manager.read(
                        context, servicer.__state_type__
                    )

                if idempotency.always:
                    return await read()

                state_type_name = IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.Node')

                # Use the idempotency manager to make sure that this
                # reader is being called following the rules.
                with context.idempotently(
                    state_type_name=state_type_name,
                    state_ref=context._state_ref,
                    # Not calling a method so `service_name`,
                    # `method`, `request`, etc are irrelevant.
                    service_name=None,
                    method=None,
                    mutation=False,
                    request=None,
                    metadata=None,
                    idempotency=idempotency,
                    # Only need to pass `aborted_type` for mutations.
                    aborted_type=None,
                ) as idempotency_key:
                    assert idempotency_key is not None
                    return await IMPORT_reboot_aio_workflows.at_least_once(
                        (
                            # TODO: for easier debugging include the
                            # original alias (or generated alias in
                            # the case of `.per_iteration()` w/o an
                            # alias) instead of just
                            # `idempotency_key`.
                            f"inline reader of '{ state_type_name }' ({str(idempotency_key)})",
                            # NOTE: we want this to be `PER_WORKFLOW`
                            # because any per iteration concerns
                            # should have already been taken care of
                            # by caller using `.per_iteration()`.
                            IMPORT_reboot_aio_workflows.PER_WORKFLOW
                        ),
                        context,
                        read,
                        type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node,
                    )

            @IMPORT_typing.overload
            async def write(
                self,
                context: IMPORT_reboot_aio_contexts.WorkflowContext,
                writer: NodeBaseServicer.InlineWriterCallable[None],
                __options__: IMPORT_reboot_aio_call.Options = IMPORT_reboot_aio_call.Options(),
                *,
                type: type = type(None),
                check_type: bool = True,
            ) -> None:
                ...

            @IMPORT_typing.overload
            async def write(
                self,
                context: IMPORT_reboot_aio_contexts.WorkflowContext,
                writer: NodeBaseServicer.InlineWriterCallable[NodeBaseServicer.InlineWriterCallableResult],
                __options__: IMPORT_reboot_aio_call.Options = IMPORT_reboot_aio_call.Options(),
                *,
                type: type[NodeBaseServicer.InlineWriterCallableResult],
                check_type: bool = True,
            ) -> NodeBaseServicer.InlineWriterCallableResult:
                ...

            async def write(
                self,
                context: IMPORT_reboot_aio_contexts.WorkflowContext,
                writer: NodeBaseServicer.InlineWriterCallable[NodeBaseServicer.InlineWriterCallableResult],
                __options__: IMPORT_reboot_aio_call.Options = IMPORT_reboot_aio_call.Options(),
                *,
                type: type = type(None),
                check_type: bool = True,
            ) -> NodeBaseServicer.InlineWriterCallableResult:
                return await self._write(
                    context,
                    writer,
                    __options__,
                    type_result=type,
                    check_type=check_type,
                )

            async def _write(
                self,
                context: IMPORT_reboot_aio_contexts.WorkflowContext,
                writer: NodeBaseServicer.InlineWriterCallable[NodeBaseServicer.InlineWriterCallableResult],
                __options__: IMPORT_reboot_aio_call.Options = IMPORT_reboot_aio_call.Options(),
                *,
                type_result: type,
                check_type: bool,
            ) -> NodeBaseServicer.InlineWriterCallableResult:
                unidempotently = self._how == IMPORT_reboot_aio_workflows.ALWAYS
                idempotency = (
                    context.idempotency(
                        key=IMPORT_uuid.uuid4(),
                        generated=True,
                    ) if unidempotently else context.idempotency(
                        alias=self._alias,
                        each_iteration=self._how == IMPORT_reboot_aio_workflows.PER_ITERATION
                    )
                )

                return await self._write_validating_effects(
                    self._servicer,
                    idempotency,
                    context,
                    writer,
                    __options__,
                    type_result=type_result,
                    check_type=check_type,
                    unidempotently=unidempotently,
                    checkpoint=context.checkpoint(),
                )

            @staticmethod
            @IMPORT_reboot_aio_internals_middleware.maybe_run_function_twice_to_validate_effects
            async def _write_validating_effects(
                validating_effects: bool,
                servicer: NodeBaseServicer,
                idempotency: IMPORT_reboot_aio_idempotency.Idempotency,
                context: IMPORT_reboot_aio_contexts.WorkflowContext,
                writer: NodeBaseServicer.InlineWriterCallable[NodeBaseServicer.InlineWriterCallableResult],
                __options__: IMPORT_reboot_aio_call.Options = IMPORT_reboot_aio_call.Options(),
                *,
                type_result: type,
                check_type: bool,
                unidempotently: bool,
                checkpoint: IMPORT_reboot_aio_idempotency.Checkpoint,
            ) -> NodeBaseServicer.InlineWriterCallableResult:
                IMPORT_reboot_aio_types.assert_type(__options__, [IMPORT_reboot_aio_call.Options])
                if __options__.idempotency is not None:
                    raise RuntimeError(
                        'Found redundant idempotency in `Options`'
                    )

                IMPORT_reboot_aio_types.assert_type(context, [IMPORT_reboot_aio_contexts.WorkflowContext])

                if servicer._middleware is None:
                    raise RuntimeError(
                        'Reboot middleware was not created; '
                        'are you using this class without Reboot?'
                    )

                metadata: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None

                if __options__ is not None:
                    if __options__.metadata is not None:
                        metadata = __options__.metadata

                if metadata is None:
                    metadata = ()

                headers = IMPORT_reboot_aio_headers.Headers(
                    application_id=context.application_id,
                    state_ref=context._state_ref,
                )

                metadata += headers.to_grpc_metadata()

                idempotency_key: IMPORT_typing.Optional[IMPORT_uuid.UUID]
                with context.idempotently(
                    state_type_name=IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.Node'),
                    state_ref=context._state_ref,
                    service_name=None,  # Indicates an inline writer.
                    method=None,  # Indicates an inline writer.
                    mutation=True,
                    request=None,  # Indicates an inline writer.
                    metadata=metadata,
                    idempotency=idempotency,
                    aborted_type=None,  # Indicates an inline writer.
                ) as idempotency_key:

                    if any(t[0] == IMPORT_reboot_aio_headers.IDEMPOTENCY_KEY_HEADER for t in metadata):
                        raise ValueError(
                            f"Do not set '{IMPORT_reboot_aio_headers.IDEMPOTENCY_KEY_HEADER}' metadata yourself"
                        )

                    if idempotency_key is not None:
                        metadata += (
                            (IMPORT_reboot_aio_headers.IDEMPOTENCY_KEY_HEADER, str(idempotency_key)),
                        )

                    with servicer._middleware.use_context(
                        headers=IMPORT_reboot_aio_headers.Headers.from_grpc_metadata(metadata),
                        state_type_name = IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.Node'),
                        method='inline writer',
                        context_type=IMPORT_reboot_aio_contexts.WriterContext,
                    ) as writer_context:
                        # Check if we already have performed this mutation!
                        #
                        # We do this _before_ calling 'transactionally()' because
                        # if this call is for a transaction method _and_ we've
                        # already performed the transaction then we don't want to
                        # become a transaction participant (again) we just want to
                        # return the transaction's response.
                        idempotent_mutation = (
                            servicer._middleware._state_manager.check_for_idempotent_mutation(
                                writer_context
                            )
                        )

                        if idempotent_mutation is not None:
                            assert len(idempotent_mutation.response) != 0
                            response = IMPORT_google_protobuf_wrappers_pb2.BytesValue()
                            response.ParseFromString(idempotent_mutation.response)
                            result: NodeBaseServicer.InlineWriterCallableResult = IMPORT_pickle.loads(response.value)

                            if check_type and type(result) is not type_result:
                                raise TypeError(
                                    f"Stored result of type '{type(result).__name__}' from 'writer' "
                                    f"is not of expected type '{type_result.__name__}'; have you changed "
                                    "the 'type' that you expect after having stored a result?"
                                )

                            return result

                        async with servicer._middleware._state_manager.transactionally(
                            writer_context,
                            servicer._middleware.tasks_dispatcher,
                            aborted_type=None,
                        ) as transaction:
                            async with servicer._middleware._state_manager.writer(
                                writer_context,
                                servicer.__state_type__,
                                servicer._middleware.tasks_dispatcher,
                                # TODO: Decide if we want to do any kind of authorization for inline
                                # writers otherwise passing `None` here is fine.
                                authorize=None,
                                transaction=transaction,
                            ) as (state, state_manager_writer):
                                # Serialize the state so we can see if it changed.
                                serialized_state = state.SerializeToString(
                                    deterministic=True,
                                )

                                result = await writer(state=state)

                                if check_type and type(result) is not type_result:
                                    raise TypeError(
                                        f"Result of type '{type(result).__name__}' from 'writer' is "
                                        f"not of expected type '{type_result.__name__}'; "
                                        "did you specify an incorrect 'type'?"
                                    )

                                task: IMPORT_typing.Optional[IMPORT_reboot_aio_tasks.TaskEffect] = context.task

                                assert task is not None, (
                                    "Should always have a task when running a `workflow`"
                                )

                                method_name = f"Node.{task.method_name} inline writer"

                                if idempotency.alias is not None:
                                    method_name += " with idempotency alias '" + idempotency.alias + "'"
                                elif idempotency.key is not None:
                                    method_name += " with idempotency key=" + str(idempotency.key)

                                servicer._middleware.maybe_raise_effect_validation_retry(
                                    logger=logger,
                                    idempotency_manager=context,
                                    method_name=method_name,
                                    validating_effects=validating_effects,
                                    context=context,
                                    checkpoint=checkpoint,
                                )

                                # We don't pass the context to the
                                # writer, so we don't expect there to
                                # be any scheduled tasks!
                                assert len(context._tasks) == 0

                                effects = IMPORT_reboot_aio_state_managers.Effects(
                                    state=(
                                        # Pass `None` if the state hasn't changed!
                                        state if serialized_state != state.SerializeToString(
                                            deterministic=True,
                                        )
                                        else None
                                    ),
                                    response=IMPORT_google_protobuf_wrappers_pb2.BytesValue(
                                        value=IMPORT_pickle.dumps(result)
                                    ),
                                )

                                await state_manager_writer.complete(effects)

                                return result

        def per_workflow(self, alias: IMPORT_typing.Optional[str] = None):
            return NodeBaseServicer.WorkflowState._Idempotently(
                servicer=self._servicer,
                alias=alias,
                how=IMPORT_reboot_aio_workflows.PER_WORKFLOW,
            )

        def per_iteration(self, alias: IMPORT_typing.Optional[str] = None):
            return NodeBaseServicer.WorkflowState._Idempotently(
                servicer=self._servicer,
                alias=alias,
                how=IMPORT_reboot_aio_workflows.PER_ITERATION,
            )

        class _Always:
            """Helper class for providing better types for `write` that don't
            require passing `type` or `check_type`."""

            def __init__(
                self,
                *,
                servicer: NodeBaseServicer,
            ):
                self._servicer = servicer

            async def read(
                self, context: IMPORT_reboot_aio_contexts.WorkflowContext
            ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node:
                return await NodeBaseServicer.WorkflowState._Idempotently(
                    servicer=self._servicer,
                    alias=None,
                    how=IMPORT_reboot_aio_workflows.ALWAYS,
                ).read(context)

            async def write(
                self,
                context: IMPORT_reboot_aio_contexts.WorkflowContext,
                writer: NodeBaseServicer.InlineWriterCallable[NodeBaseServicer.InlineWriterCallableResult],
                __options__: IMPORT_reboot_aio_call.Options = IMPORT_reboot_aio_call.Options(),
            ) -> NodeBaseServicer.InlineWriterCallableResult:
                return await NodeBaseServicer.WorkflowState._Idempotently(
                    servicer=self._servicer,
                    alias=None,
                    how=IMPORT_reboot_aio_workflows.ALWAYS,
                )._write(
                    context,
                    writer,
                    __options__,
                    type_result=type(None),
                    check_type=False,
                )

        def always(self):
            return NodeBaseServicer.WorkflowState._Always(
                servicer=self._servicer,
            )

    # For 'rbt.std.collections.ordered_map.v1.NodeMethods.Create'.
    @IMPORT_abc_abstractmethod
    async def _Create(
        self,
        context: IMPORT_reboot_aio_contexts.WriterContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateResponse:
        raise NotImplementedError

    # For 'rbt.std.collections.ordered_map.v1.NodeMethods.Search'.
    @IMPORT_abc_abstractmethod
    async def _Search(
        self,
        context: IMPORT_reboot_aio_contexts.ReaderContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeSearchRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeSearchResponse:
        raise NotImplementedError

    # For 'rbt.std.collections.ordered_map.v1.NodeMethods.Insert'.
    @IMPORT_abc_abstractmethod
    async def _Insert(
        self,
        context: IMPORT_reboot_aio_contexts.TransactionContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeInsertRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeInsertResponse:
        raise NotImplementedError

    # For 'rbt.std.collections.ordered_map.v1.NodeMethods.Remove'.
    @IMPORT_abc_abstractmethod
    async def _Remove(
        self,
        context: IMPORT_reboot_aio_contexts.TransactionContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRemoveRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRemoveResponse:
        raise NotImplementedError

    # For 'rbt.std.collections.ordered_map.v1.NodeMethods.Range'.
    @IMPORT_abc_abstractmethod
    async def _Range(
        self,
        context: IMPORT_reboot_aio_contexts.ReaderContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRangeRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRangeResponse:
        raise NotImplementedError

    # For 'rbt.std.collections.ordered_map.v1.NodeMethods.ReverseRange'.
    @IMPORT_abc_abstractmethod
    async def _ReverseRange(
        self,
        context: IMPORT_reboot_aio_contexts.ReaderContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeReverseRangeRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeReverseRangeResponse:
        raise NotImplementedError

    # For 'rbt.std.collections.ordered_map.v1.NodeMethods.Stringify'.
    @IMPORT_abc_abstractmethod
    async def _Stringify(
        self,
        context: IMPORT_reboot_aio_contexts.ReaderContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeStringifyRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeStringifyResponse:
        raise NotImplementedError



class NodeSingletonServicer(NodeBaseServicer):

    @property
    def state(self):
        return NodeBaseServicer.WorkflowState(
            servicer=self
        )

    # For 'rbt.std.collections.ordered_map.v1.NodeMethods.Create'.
    # To be backwards compatible during the renaming don't make this
    # method to be 'abstractmethod', so that new code that
    # doesn't implement it continues to work.
    # TODO: make it abstractmethod when renaming is done.
    async def Create(
        self,
        context: IMPORT_reboot_aio_contexts.WriterContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateResponse:
        # During the migration from 'PascalCase' to 'snake_case' method
        # naming in Python servicers, we call the 'snake_case' version
        # by default, so new names will do the correct thing making the
        # code to be backwards compatible for some time and if a servicer
        # overrides the 'PascalCase' version - it will override that
        # method and will just work.
        return await self.create(
            context,
            state,
            request,
        )

    # To be backwards compatible during the renaming don't make this
    # method to be 'abstractmethod', so that existing code that
    # doesn't implement it continues to work.
    async def create(
        self,
        context: IMPORT_reboot_aio_contexts.WriterContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateResponse:
        raise NotImplementedError

    # For 'rbt.std.collections.ordered_map.v1.NodeMethods.Search'.
    # To be backwards compatible during the renaming don't make this
    # method to be 'abstractmethod', so that new code that
    # doesn't implement it continues to work.
    # TODO: make it abstractmethod when renaming is done.
    async def Search(
        self,
        context: IMPORT_reboot_aio_contexts.ReaderContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeSearchRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeSearchResponse:
        # During the migration from 'PascalCase' to 'snake_case' method
        # naming in Python servicers, we call the 'snake_case' version
        # by default, so new names will do the correct thing making the
        # code to be backwards compatible for some time and if a servicer
        # overrides the 'PascalCase' version - it will override that
        # method and will just work.
        return await self.search(
            context,
            state,
            request,
        )

    # To be backwards compatible during the renaming don't make this
    # method to be 'abstractmethod', so that existing code that
    # doesn't implement it continues to work.
    # TODO: make it abstractmethod when renaming is done.
    async def search(
        self,
        context: IMPORT_reboot_aio_contexts.ReaderContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeSearchRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeSearchResponse:
        raise NotImplementedError

    # For 'rbt.std.collections.ordered_map.v1.NodeMethods.Insert'.
    # To be backwards compatible during the renaming don't make this
    # method to be 'abstractmethod', so that new code that
    # doesn't implement it continues to work.
    # TODO: make it abstractmethod when renaming is done.
    async def Insert(
        self,
        context: IMPORT_reboot_aio_contexts.TransactionContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeInsertRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeInsertResponse:
        # During the migration from 'PascalCase' to 'snake_case' method
        # naming in Python servicers, we call the 'snake_case' version
        # by default, so new names will do the correct thing making the
        # code to be backwards compatible for some time and if a servicer
        # overrides the 'PascalCase' version - it will override that
        # method and will just work.
        return await self.insert(
            context,
            state,
            request,
        )

    # To be backwards compatible during the renaming don't make this
    # method to be 'abstractmethod', so that existing code that
    # doesn't implement it continues to work.
    async def insert(
        self,
        context: IMPORT_reboot_aio_contexts.TransactionContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeInsertRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeInsertResponse:
        raise NotImplementedError

    # For 'rbt.std.collections.ordered_map.v1.NodeMethods.Remove'.
    # To be backwards compatible during the renaming don't make this
    # method to be 'abstractmethod', so that new code that
    # doesn't implement it continues to work.
    # TODO: make it abstractmethod when renaming is done.
    async def Remove(
        self,
        context: IMPORT_reboot_aio_contexts.TransactionContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRemoveRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRemoveResponse:
        # During the migration from 'PascalCase' to 'snake_case' method
        # naming in Python servicers, we call the 'snake_case' version
        # by default, so new names will do the correct thing making the
        # code to be backwards compatible for some time and if a servicer
        # overrides the 'PascalCase' version - it will override that
        # method and will just work.
        return await self.remove(
            context,
            state,
            request,
        )

    # To be backwards compatible during the renaming don't make this
    # method to be 'abstractmethod', so that existing code that
    # doesn't implement it continues to work.
    async def remove(
        self,
        context: IMPORT_reboot_aio_contexts.TransactionContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRemoveRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRemoveResponse:
        raise NotImplementedError

    # For 'rbt.std.collections.ordered_map.v1.NodeMethods.Range'.
    # To be backwards compatible during the renaming don't make this
    # method to be 'abstractmethod', so that new code that
    # doesn't implement it continues to work.
    # TODO: make it abstractmethod when renaming is done.
    async def Range(
        self,
        context: IMPORT_reboot_aio_contexts.ReaderContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRangeRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRangeResponse:
        # During the migration from 'PascalCase' to 'snake_case' method
        # naming in Python servicers, we call the 'snake_case' version
        # by default, so new names will do the correct thing making the
        # code to be backwards compatible for some time and if a servicer
        # overrides the 'PascalCase' version - it will override that
        # method and will just work.
        return await self.range(
            context,
            state,
            request,
        )

    # To be backwards compatible during the renaming don't make this
    # method to be 'abstractmethod', so that existing code that
    # doesn't implement it continues to work.
    # TODO: make it abstractmethod when renaming is done.
    async def range(
        self,
        context: IMPORT_reboot_aio_contexts.ReaderContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRangeRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRangeResponse:
        raise NotImplementedError

    # For 'rbt.std.collections.ordered_map.v1.NodeMethods.ReverseRange'.
    # To be backwards compatible during the renaming don't make this
    # method to be 'abstractmethod', so that new code that
    # doesn't implement it continues to work.
    # TODO: make it abstractmethod when renaming is done.
    async def ReverseRange(
        self,
        context: IMPORT_reboot_aio_contexts.ReaderContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeReverseRangeRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeReverseRangeResponse:
        # During the migration from 'PascalCase' to 'snake_case' method
        # naming in Python servicers, we call the 'snake_case' version
        # by default, so new names will do the correct thing making the
        # code to be backwards compatible for some time and if a servicer
        # overrides the 'PascalCase' version - it will override that
        # method and will just work.
        return await self.reverse_range(
            context,
            state,
            request,
        )

    # To be backwards compatible during the renaming don't make this
    # method to be 'abstractmethod', so that existing code that
    # doesn't implement it continues to work.
    # TODO: make it abstractmethod when renaming is done.
    async def reverse_range(
        self,
        context: IMPORT_reboot_aio_contexts.ReaderContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeReverseRangeRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeReverseRangeResponse:
        raise NotImplementedError

    # For 'rbt.std.collections.ordered_map.v1.NodeMethods.Stringify'.
    # To be backwards compatible during the renaming don't make this
    # method to be 'abstractmethod', so that new code that
    # doesn't implement it continues to work.
    # TODO: make it abstractmethod when renaming is done.
    async def Stringify(
        self,
        context: IMPORT_reboot_aio_contexts.ReaderContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeStringifyRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeStringifyResponse:
        # During the migration from 'PascalCase' to 'snake_case' method
        # naming in Python servicers, we call the 'snake_case' version
        # by default, so new names will do the correct thing making the
        # code to be backwards compatible for some time and if a servicer
        # overrides the 'PascalCase' version - it will override that
        # method and will just work.
        return await self.stringify(
            context,
            state,
            request,
        )

    # To be backwards compatible during the renaming don't make this
    # method to be 'abstractmethod', so that existing code that
    # doesn't implement it continues to work.
    # TODO: make it abstractmethod when renaming is done.
    async def stringify(
        self,
        context: IMPORT_reboot_aio_contexts.ReaderContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeStringifyRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeStringifyResponse:
        raise NotImplementedError


    # For 'rbt.std.collections.ordered_map.v1.NodeMethods.Create'.
    async def _Create(
        self,
        context: IMPORT_reboot_aio_contexts.WriterContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateResponse:
        # Wrap the call to the developer's method in a `span` so that it
        # is traced using its fully-qualified Python name.
        with IMPORT_reboot_aio_tracing.span(
                state_name=f"rbt.std.collections.ordered_map.v1.Node('{context.state_id}')",
                span_name=f"{IMPORT_reboot_aio_tracing.qualified_type_name(self)}.Create()",
                level=IMPORT_reboot_aio_tracing.TraceLevel.CUSTOMER,
                python_specific=True,
        ):
            return await self.Create(context, state, request)


    # For 'rbt.std.collections.ordered_map.v1.NodeMethods.Search'.
    async def _Search(
        self,
        context: IMPORT_reboot_aio_contexts.ReaderContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeSearchRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeSearchResponse:
        # Wrap the call to the developer's method in a `span` so that it
        # is traced using its fully-qualified Python name.
        with IMPORT_reboot_aio_tracing.span(
                state_name=f"rbt.std.collections.ordered_map.v1.Node('{context.state_id}')",
                span_name=f"{IMPORT_reboot_aio_tracing.qualified_type_name(self)}.Search()",
                level=IMPORT_reboot_aio_tracing.TraceLevel.CUSTOMER,
                python_specific=True,
        ):
            response = (
                self.Search(
                    context,
                    state,
                    request,
                )
            )
            return await response

    # For 'rbt.std.collections.ordered_map.v1.NodeMethods.Insert'.
    async def _Insert(
        self,
        context: IMPORT_reboot_aio_contexts.TransactionContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeInsertRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeInsertResponse:
        # Wrap the call to the developer's method in a `span` so that it
        # is traced using its fully-qualified Python name.
        with IMPORT_reboot_aio_tracing.span(
                state_name=f"rbt.std.collections.ordered_map.v1.Node('{context.state_id}')",
                span_name=f"{IMPORT_reboot_aio_tracing.qualified_type_name(self)}.Insert()",
                level=IMPORT_reboot_aio_tracing.TraceLevel.CUSTOMER,
                python_specific=True,
        ):
            return await self.Insert(context, state, request)


    # For 'rbt.std.collections.ordered_map.v1.NodeMethods.Remove'.
    async def _Remove(
        self,
        context: IMPORT_reboot_aio_contexts.TransactionContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRemoveRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRemoveResponse:
        # Wrap the call to the developer's method in a `span` so that it
        # is traced using its fully-qualified Python name.
        with IMPORT_reboot_aio_tracing.span(
                state_name=f"rbt.std.collections.ordered_map.v1.Node('{context.state_id}')",
                span_name=f"{IMPORT_reboot_aio_tracing.qualified_type_name(self)}.Remove()",
                level=IMPORT_reboot_aio_tracing.TraceLevel.CUSTOMER,
                python_specific=True,
        ):
            return await self.Remove(context, state, request)


    # For 'rbt.std.collections.ordered_map.v1.NodeMethods.Range'.
    async def _Range(
        self,
        context: IMPORT_reboot_aio_contexts.ReaderContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRangeRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRangeResponse:
        # Wrap the call to the developer's method in a `span` so that it
        # is traced using its fully-qualified Python name.
        with IMPORT_reboot_aio_tracing.span(
                state_name=f"rbt.std.collections.ordered_map.v1.Node('{context.state_id}')",
                span_name=f"{IMPORT_reboot_aio_tracing.qualified_type_name(self)}.Range()",
                level=IMPORT_reboot_aio_tracing.TraceLevel.CUSTOMER,
                python_specific=True,
        ):
            response = (
                self.Range(
                    context,
                    state,
                    request,
                )
            )
            return await response

    # For 'rbt.std.collections.ordered_map.v1.NodeMethods.ReverseRange'.
    async def _ReverseRange(
        self,
        context: IMPORT_reboot_aio_contexts.ReaderContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeReverseRangeRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeReverseRangeResponse:
        # Wrap the call to the developer's method in a `span` so that it
        # is traced using its fully-qualified Python name.
        with IMPORT_reboot_aio_tracing.span(
                state_name=f"rbt.std.collections.ordered_map.v1.Node('{context.state_id}')",
                span_name=f"{IMPORT_reboot_aio_tracing.qualified_type_name(self)}.ReverseRange()",
                level=IMPORT_reboot_aio_tracing.TraceLevel.CUSTOMER,
                python_specific=True,
        ):
            response = (
                self.ReverseRange(
                    context,
                    state,
                    request,
                )
            )
            return await response

    # For 'rbt.std.collections.ordered_map.v1.NodeMethods.Stringify'.
    async def _Stringify(
        self,
        context: IMPORT_reboot_aio_contexts.ReaderContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeStringifyRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeStringifyResponse:
        # Wrap the call to the developer's method in a `span` so that it
        # is traced using its fully-qualified Python name.
        with IMPORT_reboot_aio_tracing.span(
                state_name=f"rbt.std.collections.ordered_map.v1.Node('{context.state_id}')",
                span_name=f"{IMPORT_reboot_aio_tracing.qualified_type_name(self)}.Stringify()",
                level=IMPORT_reboot_aio_tracing.TraceLevel.CUSTOMER,
                python_specific=True,
        ):
            response = (
                self.Stringify(
                    context,
                    state,
                    request,
                )
            )
            return await response



class NodeServicer(NodeBaseServicer):

    _state: IMPORT_contextvars.ContextVar[
        IMPORT_typing.Optional[rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node]
    ] = IMPORT_contextvars.ContextVar(
        'Provides access to state for each call, i.e., there may be '
        'multiple readers executing concurrently but each might have '
        'a different `state`',
        default=None,
    )

    _workflow: IMPORT_contextvars.ContextVar[bool] = IMPORT_contextvars.ContextVar(
        'Whether or not current context is executing a workflow',
        default=False,
    )

    # An instance of the derived class for each state.
    _instances: dict[str, NodeServicer] = {}

    def _instance(self, state_id: str):
        instances = NodeServicer._instances
        instance = instances.get(state_id)
        if instance is None:
            instance = self.__class__()
            instance._middleware = self._middleware
        instances[state_id] = instance
        return instance

    @property
    def state(self) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node:
        state = NodeServicer._state.get()
        if state is None:
            raise RuntimeError(
                "`state` property is only relevant within a `Servicer` method"
            )
        workflow = NodeServicer._workflow.get()
        if workflow:
            raise RuntimeError(
                "`self.state` is not valid within a `workflow` because a "
                "`workflow ` is not _atomic_; use "
                "`await self.ref().read(context)` instead"
            )
        return state

    @state.setter
    def state(self, new_state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node):
        state = NodeServicer._state.get()
        if state is None:
            raise RuntimeError(
                "`state` property is only relevant within a `Servicer` method"
            )
        workflow = NodeServicer._workflow.get()
        if workflow:
            raise RuntimeError(
                "`self.state` is not valid within a `workflow` because a "
                "`workflow ` is not _atomic_; use "
                "`await self.ref().write(...)` instead"
            )
        state.CopyFrom(new_state)

    # For 'rbt.std.collections.ordered_map.v1.NodeMethods.Create'.
    # To be backwards compatible during the renaming don't make this
    # method to be 'abstractmethod', so that new code that
    # doesn't implement it continues to work.
    # TODO: make it abstractmethod when renaming is done.
    async def Create(
        self,
        context: IMPORT_reboot_aio_contexts.WriterContext,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateResponse:
        # During the migration from 'PascalCase' to 'snake_case' method
        # naming in Python servicers, we call the 'snake_case' version
        # by default, so new names will do the correct thing making the
        # code to be backwards compatible for some time and if a servicer
        # overrides the 'PascalCase' version - it will override that
        # method and will just work.
        return await self.create(
            context,
            request,
        )

    # To be backwards compatible during the renaming don't make this
    # method to be 'abstractmethod', so that existing code that
    # doesn't implement it continues to work.
    async def create(
        self,
        context: IMPORT_reboot_aio_contexts.WriterContext,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateResponse:
        raise NotImplementedError

    # For 'rbt.std.collections.ordered_map.v1.NodeMethods.Search'.
    # To be backwards compatible during the renaming don't make this
    # method to be 'abstractmethod', so that new code that
    # doesn't implement it continues to work.
    # TODO: make it abstractmethod when renaming is done.
    async def Search(
        self,
        context: IMPORT_reboot_aio_contexts.ReaderContext,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeSearchRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeSearchResponse:
        # During the migration from 'PascalCase' to 'snake_case' method
        # naming in Python servicers, we call the 'snake_case' version
        # by default, so new names will do the correct thing making the
        # code to be backwards compatible for some time and if a servicer
        # overrides the 'PascalCase' version - it will override that
        # method and will just work.
        return await self.search(
            context,
            request,
        )

    # To be backwards compatible during the renaming don't make this
    # method to be 'abstractmethod', so that existing code that
    # doesn't implement it continues to work.
    async def search(
        self,
        context: IMPORT_reboot_aio_contexts.ReaderContext,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeSearchRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeSearchResponse:
        raise NotImplementedError

    # For 'rbt.std.collections.ordered_map.v1.NodeMethods.Insert'.
    # To be backwards compatible during the renaming don't make this
    # method to be 'abstractmethod', so that new code that
    # doesn't implement it continues to work.
    # TODO: make it abstractmethod when renaming is done.
    async def Insert(
        self,
        context: IMPORT_reboot_aio_contexts.TransactionContext,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeInsertRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeInsertResponse:
        # During the migration from 'PascalCase' to 'snake_case' method
        # naming in Python servicers, we call the 'snake_case' version
        # by default, so new names will do the correct thing making the
        # code to be backwards compatible for some time and if a servicer
        # overrides the 'PascalCase' version - it will override that
        # method and will just work.
        return await self.insert(
            context,
            request,
        )

    # To be backwards compatible during the renaming don't make this
    # method to be 'abstractmethod', so that existing code that
    # doesn't implement it continues to work.
    async def insert(
        self,
        context: IMPORT_reboot_aio_contexts.TransactionContext,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeInsertRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeInsertResponse:
        raise NotImplementedError

    # For 'rbt.std.collections.ordered_map.v1.NodeMethods.Remove'.
    # To be backwards compatible during the renaming don't make this
    # method to be 'abstractmethod', so that new code that
    # doesn't implement it continues to work.
    # TODO: make it abstractmethod when renaming is done.
    async def Remove(
        self,
        context: IMPORT_reboot_aio_contexts.TransactionContext,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRemoveRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRemoveResponse:
        # During the migration from 'PascalCase' to 'snake_case' method
        # naming in Python servicers, we call the 'snake_case' version
        # by default, so new names will do the correct thing making the
        # code to be backwards compatible for some time and if a servicer
        # overrides the 'PascalCase' version - it will override that
        # method and will just work.
        return await self.remove(
            context,
            request,
        )

    # To be backwards compatible during the renaming don't make this
    # method to be 'abstractmethod', so that existing code that
    # doesn't implement it continues to work.
    async def remove(
        self,
        context: IMPORT_reboot_aio_contexts.TransactionContext,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRemoveRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRemoveResponse:
        raise NotImplementedError

    # For 'rbt.std.collections.ordered_map.v1.NodeMethods.Range'.
    # To be backwards compatible during the renaming don't make this
    # method to be 'abstractmethod', so that new code that
    # doesn't implement it continues to work.
    # TODO: make it abstractmethod when renaming is done.
    async def Range(
        self,
        context: IMPORT_reboot_aio_contexts.ReaderContext,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRangeRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRangeResponse:
        # During the migration from 'PascalCase' to 'snake_case' method
        # naming in Python servicers, we call the 'snake_case' version
        # by default, so new names will do the correct thing making the
        # code to be backwards compatible for some time and if a servicer
        # overrides the 'PascalCase' version - it will override that
        # method and will just work.
        return await self.range(
            context,
            request,
        )

    # To be backwards compatible during the renaming don't make this
    # method to be 'abstractmethod', so that existing code that
    # doesn't implement it continues to work.
    async def range(
        self,
        context: IMPORT_reboot_aio_contexts.ReaderContext,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRangeRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRangeResponse:
        raise NotImplementedError

    # For 'rbt.std.collections.ordered_map.v1.NodeMethods.ReverseRange'.
    # To be backwards compatible during the renaming don't make this
    # method to be 'abstractmethod', so that new code that
    # doesn't implement it continues to work.
    # TODO: make it abstractmethod when renaming is done.
    async def ReverseRange(
        self,
        context: IMPORT_reboot_aio_contexts.ReaderContext,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeReverseRangeRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeReverseRangeResponse:
        # During the migration from 'PascalCase' to 'snake_case' method
        # naming in Python servicers, we call the 'snake_case' version
        # by default, so new names will do the correct thing making the
        # code to be backwards compatible for some time and if a servicer
        # overrides the 'PascalCase' version - it will override that
        # method and will just work.
        return await self.reverse_range(
            context,
            request,
        )

    # To be backwards compatible during the renaming don't make this
    # method to be 'abstractmethod', so that existing code that
    # doesn't implement it continues to work.
    async def reverse_range(
        self,
        context: IMPORT_reboot_aio_contexts.ReaderContext,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeReverseRangeRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeReverseRangeResponse:
        raise NotImplementedError

    # For 'rbt.std.collections.ordered_map.v1.NodeMethods.Stringify'.
    # To be backwards compatible during the renaming don't make this
    # method to be 'abstractmethod', so that new code that
    # doesn't implement it continues to work.
    # TODO: make it abstractmethod when renaming is done.
    async def Stringify(
        self,
        context: IMPORT_reboot_aio_contexts.ReaderContext,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeStringifyRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeStringifyResponse:
        # During the migration from 'PascalCase' to 'snake_case' method
        # naming in Python servicers, we call the 'snake_case' version
        # by default, so new names will do the correct thing making the
        # code to be backwards compatible for some time and if a servicer
        # overrides the 'PascalCase' version - it will override that
        # method and will just work.
        return await self.stringify(
            context,
            request,
        )

    # To be backwards compatible during the renaming don't make this
    # method to be 'abstractmethod', so that existing code that
    # doesn't implement it continues to work.
    async def stringify(
        self,
        context: IMPORT_reboot_aio_contexts.ReaderContext,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeStringifyRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeStringifyResponse:
        raise NotImplementedError


    # For 'rbt.std.collections.ordered_map.v1.NodeMethods.Create'.
    async def _Create(
        self,
        context: IMPORT_reboot_aio_contexts.WriterContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateResponse:
        # We should have an asyncio task and thus context per request,
        # let's confirm this assumption by making sure that
        # `_state is None`.
        assert NodeServicer._state.get() is None
        NodeServicer._state.set(state)
        try:
            # Wrap the call to the developer's method in a `span` so that it
            # is traced using its fully-qualified Python name.
            instance = self._instance(context.state_id)
            with IMPORT_reboot_aio_tracing.span(
                    state_name=f"rbt.std.collections.ordered_map.v1.Node('{context.state_id}')",
                    span_name=f"{IMPORT_reboot_aio_tracing.qualified_type_name(instance)}.Create()",
                    level=IMPORT_reboot_aio_tracing.TraceLevel.CUSTOMER,
                    python_specific=True,
            ):
                return await instance.Create(context, request)
        finally:
            NodeServicer._state.set(None)

    # For 'rbt.std.collections.ordered_map.v1.NodeMethods.Search'.
    async def _Search(
        self,
        context: IMPORT_reboot_aio_contexts.ReaderContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeSearchRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeSearchResponse:
        # We should have an asyncio task and thus context per request,
        # let's confirm this assumption by making sure that
        # `_state is None`.
        assert NodeServicer._state.get() is None
        NodeServicer._state.set(state)
        try:
            # Wrap the call to the developer's method in a `span` so that it
            # is traced using its fully-qualified Python name.
            instance = self._instance(context.state_id)
            with IMPORT_reboot_aio_tracing.span(
                    state_name=f"rbt.std.collections.ordered_map.v1.Node('{context.state_id}')",
                    span_name=f"{IMPORT_reboot_aio_tracing.qualified_type_name(instance)}.Search()",
                    level=IMPORT_reboot_aio_tracing.TraceLevel.CUSTOMER,
                    python_specific=True,
            ):
                return await instance.Search(context, request)
        finally:
            NodeServicer._state.set(None)

    # For 'rbt.std.collections.ordered_map.v1.NodeMethods.Insert'.
    async def _Insert(
        self,
        context: IMPORT_reboot_aio_contexts.TransactionContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeInsertRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeInsertResponse:
        # We should have an asyncio task and thus context per request,
        # let's confirm this assumption by making sure that
        # `_state is None`.
        assert NodeServicer._state.get() is None
        NodeServicer._state.set(state)
        try:
            # Wrap the call to the developer's method in a `span` so that it
            # is traced using its fully-qualified Python name.
            instance = self._instance(context.state_id)
            with IMPORT_reboot_aio_tracing.span(
                    state_name=f"rbt.std.collections.ordered_map.v1.Node('{context.state_id}')",
                    span_name=f"{IMPORT_reboot_aio_tracing.qualified_type_name(instance)}.Insert()",
                    level=IMPORT_reboot_aio_tracing.TraceLevel.CUSTOMER,
                    python_specific=True,
            ):
                return await instance.Insert(context, request)
        finally:
            NodeServicer._state.set(None)

    # For 'rbt.std.collections.ordered_map.v1.NodeMethods.Remove'.
    async def _Remove(
        self,
        context: IMPORT_reboot_aio_contexts.TransactionContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRemoveRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRemoveResponse:
        # We should have an asyncio task and thus context per request,
        # let's confirm this assumption by making sure that
        # `_state is None`.
        assert NodeServicer._state.get() is None
        NodeServicer._state.set(state)
        try:
            # Wrap the call to the developer's method in a `span` so that it
            # is traced using its fully-qualified Python name.
            instance = self._instance(context.state_id)
            with IMPORT_reboot_aio_tracing.span(
                    state_name=f"rbt.std.collections.ordered_map.v1.Node('{context.state_id}')",
                    span_name=f"{IMPORT_reboot_aio_tracing.qualified_type_name(instance)}.Remove()",
                    level=IMPORT_reboot_aio_tracing.TraceLevel.CUSTOMER,
                    python_specific=True,
            ):
                return await instance.Remove(context, request)
        finally:
            NodeServicer._state.set(None)

    # For 'rbt.std.collections.ordered_map.v1.NodeMethods.Range'.
    async def _Range(
        self,
        context: IMPORT_reboot_aio_contexts.ReaderContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRangeRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRangeResponse:
        # We should have an asyncio task and thus context per request,
        # let's confirm this assumption by making sure that
        # `_state is None`.
        assert NodeServicer._state.get() is None
        NodeServicer._state.set(state)
        try:
            # Wrap the call to the developer's method in a `span` so that it
            # is traced using its fully-qualified Python name.
            instance = self._instance(context.state_id)
            with IMPORT_reboot_aio_tracing.span(
                    state_name=f"rbt.std.collections.ordered_map.v1.Node('{context.state_id}')",
                    span_name=f"{IMPORT_reboot_aio_tracing.qualified_type_name(instance)}.Range()",
                    level=IMPORT_reboot_aio_tracing.TraceLevel.CUSTOMER,
                    python_specific=True,
            ):
                return await instance.Range(context, request)
        finally:
            NodeServicer._state.set(None)

    # For 'rbt.std.collections.ordered_map.v1.NodeMethods.ReverseRange'.
    async def _ReverseRange(
        self,
        context: IMPORT_reboot_aio_contexts.ReaderContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeReverseRangeRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeReverseRangeResponse:
        # We should have an asyncio task and thus context per request,
        # let's confirm this assumption by making sure that
        # `_state is None`.
        assert NodeServicer._state.get() is None
        NodeServicer._state.set(state)
        try:
            # Wrap the call to the developer's method in a `span` so that it
            # is traced using its fully-qualified Python name.
            instance = self._instance(context.state_id)
            with IMPORT_reboot_aio_tracing.span(
                    state_name=f"rbt.std.collections.ordered_map.v1.Node('{context.state_id}')",
                    span_name=f"{IMPORT_reboot_aio_tracing.qualified_type_name(instance)}.ReverseRange()",
                    level=IMPORT_reboot_aio_tracing.TraceLevel.CUSTOMER,
                    python_specific=True,
            ):
                return await instance.ReverseRange(context, request)
        finally:
            NodeServicer._state.set(None)

    # For 'rbt.std.collections.ordered_map.v1.NodeMethods.Stringify'.
    async def _Stringify(
        self,
        context: IMPORT_reboot_aio_contexts.ReaderContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeStringifyRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeStringifyResponse:
        # We should have an asyncio task and thus context per request,
        # let's confirm this assumption by making sure that
        # `_state is None`.
        assert NodeServicer._state.get() is None
        NodeServicer._state.set(state)
        try:
            # Wrap the call to the developer's method in a `span` so that it
            # is traced using its fully-qualified Python name.
            instance = self._instance(context.state_id)
            with IMPORT_reboot_aio_tracing.span(
                    state_name=f"rbt.std.collections.ordered_map.v1.Node('{context.state_id}')",
                    span_name=f"{IMPORT_reboot_aio_tracing.qualified_type_name(instance)}.Stringify()",
                    level=IMPORT_reboot_aio_tracing.TraceLevel.CUSTOMER,
                    python_specific=True,
            ):
                return await instance.Stringify(context, request)
        finally:
            NodeServicer._state.set(None)


class OrderedMapBaseServicer(IMPORT_reboot_aio_servicers.Servicer):
    Authorizer: IMPORT_typing.TypeAlias = OrderedMapAuthorizer

    __service_names__ = [
        IMPORT_reboot_aio_types.ServiceName('rbt.std.collections.ordered_map.v1.OrderedMapMethods'),
    ]
    __state_type_name__ = IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.OrderedMap')
    __state_type__ = rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap
    __file_descriptor__ = rbt.std.collections.ordered_map.v1.ordered_map_pb2.DESCRIPTOR

    def __init__(self):
        super().__init__()
        # NOTE: need to hold on to the middleware so we can do inline
        # writes (see 'self.write(...)').
        #
        # Because '_middleware' is not really private this does mean
        # users may do possibly dangerous things, but this is no more
        # likely given they could have already overridden
        # 'create_middleware()'.
        self._middleware: IMPORT_typing.Optional[OrderedMapServicerMiddleware] = None

    def create_middleware(
        self,
        *,
        application_id: IMPORT_reboot_aio_types.ApplicationId,
        consensus_id: IMPORT_reboot_aio_types.ConsensusId,
        state_manager: IMPORT_reboot_aio_state_managers.StateManager,
        placement_client: IMPORT_reboot_aio_placement.PlacementClient,
        channel_manager: IMPORT_reboot_aio_internals_channel_manager._ChannelManager,
        tasks_cache: IMPORT_reboot_aio_internals_tasks_cache.TasksCache,
        token_verifier: IMPORT_typing.Optional[IMPORT_rebootdev.aio.auth.token_verifiers.TokenVerifier],
        effect_validation: IMPORT_reboot_aio_contexts.EffectValidation,
        app_internal_api_key_secret: str,
        ready: IMPORT_asyncio.Event,
    ) -> OrderedMapServicerMiddleware:
        self._middleware = OrderedMapServicerMiddleware(
            servicer=self,
            application_id=application_id,
            consensus_id=consensus_id,
            state_manager=state_manager,
            placement_client=placement_client,
            channel_manager=channel_manager,
            tasks_cache=tasks_cache,
            token_verifier=token_verifier,
            effect_validation=effect_validation,
            app_internal_api_key_secret=app_internal_api_key_secret,
            ready=ready,
        )
        return self._middleware

    def authorizer(self) -> IMPORT_typing.Optional[IMPORT_rebootdev.aio.auth.authorizers.Authorizer | IMPORT_rebootdev.aio.auth.authorizers.AuthorizerRule]:
        return None

    def token_verifier(self) -> IMPORT_typing.Optional[IMPORT_rebootdev.aio.auth.token_verifiers.TokenVerifier]:
        return None

    def ref(
        self,
        *,
        bearer_token: IMPORT_typing.Optional[str] = None,
    ) -> OrderedMap.WeakReference[OrderedMap.WeakReference._WriterSchedule]:
        context = IMPORT_reboot_aio_contexts.Context.get()

        if context is None:
            raise RuntimeError(
                'Missing asyncio context variable `context`; '
                'are you using this class without Reboot?'
            )

        return OrderedMap.WeakReference(
            # TODO(https://github.com/reboot-dev/mono/issues/3226): add support for calling other applications.
            # For now this always stays within the application that creates the context.
            application_id=None,
            state_id=context._state_ref.id,
            schedule_type=OrderedMap.WeakReference._WriterSchedule,
            # If the user didn't specify a bearer token we may still end up using the app-internal bearer token,
            # but that's decided at the time of the call.
            bearer_token=bearer_token,
            servicer=self,
        )

    class Effects(IMPORT_reboot_aio_state_managers.Effects):
        def __init__(
            self,
            *,
            state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap,
            response: IMPORT_typing.Optional[IMPORT_google_protobuf_message.Message] = None,
            tasks: IMPORT_typing.Optional[list[IMPORT_reboot_aio_tasks.TaskEffect]] = None,
            _colocated_upserts: IMPORT_typing.Optional[list[tuple[str, IMPORT_typing.Optional[bytes]]]] = None,
        ):
            IMPORT_reboot_aio_types.assert_type(state, [rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap])

            super().__init__(state=state, response=response, tasks=tasks, _colocated_upserts=_colocated_upserts)









    InlineWriterCallableResult = IMPORT_typing.TypeVar('InlineWriterCallableResult', covariant=True)

    class InlineWriterCallable(IMPORT_typing.Protocol[InlineWriterCallableResult]):
        async def __call__(
            self,
            state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap
        ) -> OrderedMapBaseServicer.InlineWriterCallableResult:
            ...

    class WorkflowState:

        def __init__(
            self,
            servicer,
        ):
            self._servicer = servicer

        async def read(
            self, context: IMPORT_reboot_aio_contexts.WorkflowContext
        ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap:
            """Read the current state within a workflow."""
            return await (
                self.always() if context.within_until()
                else (
                    self.per_iteration() if context.within_loop()
                    else self.per_workflow()
                )
            ).read(context)

        @IMPORT_typing.overload
        async def write(
            self,
            idempotency_alias: str,
            context: IMPORT_reboot_aio_contexts.WorkflowContext,
            writer: OrderedMapBaseServicer.InlineWriterCallable[None],
            __options__: IMPORT_reboot_aio_call.Options = IMPORT_reboot_aio_call.Options(),
            *,
            type: type = type(None),
        ) -> None:
            ...

        @IMPORT_typing.overload
        async def write(
            self,
            idempotency_alias: str,
            context: IMPORT_reboot_aio_contexts.WorkflowContext,
            writer: OrderedMapBaseServicer.InlineWriterCallable[OrderedMapBaseServicer.InlineWriterCallableResult],
            __options__: IMPORT_reboot_aio_call.Options = IMPORT_reboot_aio_call.Options(),
            *,
            type: type[OrderedMapBaseServicer.InlineWriterCallableResult],
        ) -> OrderedMapBaseServicer.InlineWriterCallableResult:
            ...

        async def write(
            self,
            idempotency_alias: str,
            context: IMPORT_reboot_aio_contexts.WorkflowContext,
            writer: OrderedMapBaseServicer.InlineWriterCallable[OrderedMapBaseServicer.InlineWriterCallableResult],
            __options__: IMPORT_reboot_aio_call.Options = IMPORT_reboot_aio_call.Options(),
            *,
            type: type = type(None),
        ) -> OrderedMapBaseServicer.InlineWriterCallableResult:
            """Perform an "inline write" within a workflow."""
            return await (
                self.per_iteration(idempotency_alias) if context.within_loop()
                else self.per_workflow(idempotency_alias)
            ).write(
                context, writer, __options__, type=type
            )

        class _Idempotently:

            def __init__(
                self,
                *,
                servicer: OrderedMapBaseServicer,
                alias: IMPORT_typing.Optional[str],
                how: IMPORT_reboot_aio_workflows.How,
            ):
                self._servicer = servicer
                self._alias = alias
                self._how = how

            async def read(
                self, context: IMPORT_reboot_aio_contexts.WorkflowContext
            ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap:
                """Read the current state within a workflow."""
                return await self._read(
                    self._servicer,
                    context.idempotency(
                        key=IMPORT_uuid.uuid4(),
                        generated=True,
                    ) if self._how == IMPORT_reboot_aio_workflows.ALWAYS else context.idempotency(
                        alias=self._alias,
                        each_iteration=self._how == IMPORT_reboot_aio_workflows.PER_ITERATION
                    ),
                    context,
                )

            @staticmethod
            async def _read(
                servicer: OrderedMapBaseServicer,
                idempotency: IMPORT_reboot_aio_idempotency.Idempotency,
                context: IMPORT_reboot_aio_contexts.WorkflowContext,
            ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap:
                """Read the current state within a workflow."""
                IMPORT_reboot_aio_types.assert_type(context, [IMPORT_reboot_aio_contexts.WorkflowContext])

                if servicer._middleware is None:
                    raise RuntimeError(
                        'Reboot middleware was not created; '
                        'are you using this class without Reboot?'
                    )

                async def read():
                    assert servicer._middleware is not None
                    return await servicer._middleware._state_manager.read(
                        context, servicer.__state_type__
                    )

                if idempotency.always:
                    return await read()

                state_type_name = IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.OrderedMap')

                # Use the idempotency manager to make sure that this
                # reader is being called following the rules.
                with context.idempotently(
                    state_type_name=state_type_name,
                    state_ref=context._state_ref,
                    # Not calling a method so `service_name`,
                    # `method`, `request`, etc are irrelevant.
                    service_name=None,
                    method=None,
                    mutation=False,
                    request=None,
                    metadata=None,
                    idempotency=idempotency,
                    # Only need to pass `aborted_type` for mutations.
                    aborted_type=None,
                ) as idempotency_key:
                    assert idempotency_key is not None
                    return await IMPORT_reboot_aio_workflows.at_least_once(
                        (
                            # TODO: for easier debugging include the
                            # original alias (or generated alias in
                            # the case of `.per_iteration()` w/o an
                            # alias) instead of just
                            # `idempotency_key`.
                            f"inline reader of '{ state_type_name }' ({str(idempotency_key)})",
                            # NOTE: we want this to be `PER_WORKFLOW`
                            # because any per iteration concerns
                            # should have already been taken care of
                            # by caller using `.per_iteration()`.
                            IMPORT_reboot_aio_workflows.PER_WORKFLOW
                        ),
                        context,
                        read,
                        type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap,
                    )

            @IMPORT_typing.overload
            async def write(
                self,
                context: IMPORT_reboot_aio_contexts.WorkflowContext,
                writer: OrderedMapBaseServicer.InlineWriterCallable[None],
                __options__: IMPORT_reboot_aio_call.Options = IMPORT_reboot_aio_call.Options(),
                *,
                type: type = type(None),
                check_type: bool = True,
            ) -> None:
                ...

            @IMPORT_typing.overload
            async def write(
                self,
                context: IMPORT_reboot_aio_contexts.WorkflowContext,
                writer: OrderedMapBaseServicer.InlineWriterCallable[OrderedMapBaseServicer.InlineWriterCallableResult],
                __options__: IMPORT_reboot_aio_call.Options = IMPORT_reboot_aio_call.Options(),
                *,
                type: type[OrderedMapBaseServicer.InlineWriterCallableResult],
                check_type: bool = True,
            ) -> OrderedMapBaseServicer.InlineWriterCallableResult:
                ...

            async def write(
                self,
                context: IMPORT_reboot_aio_contexts.WorkflowContext,
                writer: OrderedMapBaseServicer.InlineWriterCallable[OrderedMapBaseServicer.InlineWriterCallableResult],
                __options__: IMPORT_reboot_aio_call.Options = IMPORT_reboot_aio_call.Options(),
                *,
                type: type = type(None),
                check_type: bool = True,
            ) -> OrderedMapBaseServicer.InlineWriterCallableResult:
                return await self._write(
                    context,
                    writer,
                    __options__,
                    type_result=type,
                    check_type=check_type,
                )

            async def _write(
                self,
                context: IMPORT_reboot_aio_contexts.WorkflowContext,
                writer: OrderedMapBaseServicer.InlineWriterCallable[OrderedMapBaseServicer.InlineWriterCallableResult],
                __options__: IMPORT_reboot_aio_call.Options = IMPORT_reboot_aio_call.Options(),
                *,
                type_result: type,
                check_type: bool,
            ) -> OrderedMapBaseServicer.InlineWriterCallableResult:
                unidempotently = self._how == IMPORT_reboot_aio_workflows.ALWAYS
                idempotency = (
                    context.idempotency(
                        key=IMPORT_uuid.uuid4(),
                        generated=True,
                    ) if unidempotently else context.idempotency(
                        alias=self._alias,
                        each_iteration=self._how == IMPORT_reboot_aio_workflows.PER_ITERATION
                    )
                )

                return await self._write_validating_effects(
                    self._servicer,
                    idempotency,
                    context,
                    writer,
                    __options__,
                    type_result=type_result,
                    check_type=check_type,
                    unidempotently=unidempotently,
                    checkpoint=context.checkpoint(),
                )

            @staticmethod
            @IMPORT_reboot_aio_internals_middleware.maybe_run_function_twice_to_validate_effects
            async def _write_validating_effects(
                validating_effects: bool,
                servicer: OrderedMapBaseServicer,
                idempotency: IMPORT_reboot_aio_idempotency.Idempotency,
                context: IMPORT_reboot_aio_contexts.WorkflowContext,
                writer: OrderedMapBaseServicer.InlineWriterCallable[OrderedMapBaseServicer.InlineWriterCallableResult],
                __options__: IMPORT_reboot_aio_call.Options = IMPORT_reboot_aio_call.Options(),
                *,
                type_result: type,
                check_type: bool,
                unidempotently: bool,
                checkpoint: IMPORT_reboot_aio_idempotency.Checkpoint,
            ) -> OrderedMapBaseServicer.InlineWriterCallableResult:
                IMPORT_reboot_aio_types.assert_type(__options__, [IMPORT_reboot_aio_call.Options])
                if __options__.idempotency is not None:
                    raise RuntimeError(
                        'Found redundant idempotency in `Options`'
                    )

                IMPORT_reboot_aio_types.assert_type(context, [IMPORT_reboot_aio_contexts.WorkflowContext])

                if servicer._middleware is None:
                    raise RuntimeError(
                        'Reboot middleware was not created; '
                        'are you using this class without Reboot?'
                    )

                metadata: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None

                if __options__ is not None:
                    if __options__.metadata is not None:
                        metadata = __options__.metadata

                if metadata is None:
                    metadata = ()

                headers = IMPORT_reboot_aio_headers.Headers(
                    application_id=context.application_id,
                    state_ref=context._state_ref,
                )

                metadata += headers.to_grpc_metadata()

                idempotency_key: IMPORT_typing.Optional[IMPORT_uuid.UUID]
                with context.idempotently(
                    state_type_name=IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.OrderedMap'),
                    state_ref=context._state_ref,
                    service_name=None,  # Indicates an inline writer.
                    method=None,  # Indicates an inline writer.
                    mutation=True,
                    request=None,  # Indicates an inline writer.
                    metadata=metadata,
                    idempotency=idempotency,
                    aborted_type=None,  # Indicates an inline writer.
                ) as idempotency_key:

                    if any(t[0] == IMPORT_reboot_aio_headers.IDEMPOTENCY_KEY_HEADER for t in metadata):
                        raise ValueError(
                            f"Do not set '{IMPORT_reboot_aio_headers.IDEMPOTENCY_KEY_HEADER}' metadata yourself"
                        )

                    if idempotency_key is not None:
                        metadata += (
                            (IMPORT_reboot_aio_headers.IDEMPOTENCY_KEY_HEADER, str(idempotency_key)),
                        )

                    with servicer._middleware.use_context(
                        headers=IMPORT_reboot_aio_headers.Headers.from_grpc_metadata(metadata),
                        state_type_name = IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.OrderedMap'),
                        method='inline writer',
                        context_type=IMPORT_reboot_aio_contexts.WriterContext,
                    ) as writer_context:
                        # Check if we already have performed this mutation!
                        #
                        # We do this _before_ calling 'transactionally()' because
                        # if this call is for a transaction method _and_ we've
                        # already performed the transaction then we don't want to
                        # become a transaction participant (again) we just want to
                        # return the transaction's response.
                        idempotent_mutation = (
                            servicer._middleware._state_manager.check_for_idempotent_mutation(
                                writer_context
                            )
                        )

                        if idempotent_mutation is not None:
                            assert len(idempotent_mutation.response) != 0
                            response = IMPORT_google_protobuf_wrappers_pb2.BytesValue()
                            response.ParseFromString(idempotent_mutation.response)
                            result: OrderedMapBaseServicer.InlineWriterCallableResult = IMPORT_pickle.loads(response.value)

                            if check_type and type(result) is not type_result:
                                raise TypeError(
                                    f"Stored result of type '{type(result).__name__}' from 'writer' "
                                    f"is not of expected type '{type_result.__name__}'; have you changed "
                                    "the 'type' that you expect after having stored a result?"
                                )

                            return result

                        async with servicer._middleware._state_manager.transactionally(
                            writer_context,
                            servicer._middleware.tasks_dispatcher,
                            aborted_type=None,
                        ) as transaction:
                            async with servicer._middleware._state_manager.writer(
                                writer_context,
                                servicer.__state_type__,
                                servicer._middleware.tasks_dispatcher,
                                # TODO: Decide if we want to do any kind of authorization for inline
                                # writers otherwise passing `None` here is fine.
                                authorize=None,
                                transaction=transaction,
                            ) as (state, state_manager_writer):
                                # Serialize the state so we can see if it changed.
                                serialized_state = state.SerializeToString(
                                    deterministic=True,
                                )

                                result = await writer(state=state)

                                if check_type and type(result) is not type_result:
                                    raise TypeError(
                                        f"Result of type '{type(result).__name__}' from 'writer' is "
                                        f"not of expected type '{type_result.__name__}'; "
                                        "did you specify an incorrect 'type'?"
                                    )

                                task: IMPORT_typing.Optional[IMPORT_reboot_aio_tasks.TaskEffect] = context.task

                                assert task is not None, (
                                    "Should always have a task when running a `workflow`"
                                )

                                method_name = f"OrderedMap.{task.method_name} inline writer"

                                if idempotency.alias is not None:
                                    method_name += " with idempotency alias '" + idempotency.alias + "'"
                                elif idempotency.key is not None:
                                    method_name += " with idempotency key=" + str(idempotency.key)

                                servicer._middleware.maybe_raise_effect_validation_retry(
                                    logger=logger,
                                    idempotency_manager=context,
                                    method_name=method_name,
                                    validating_effects=validating_effects,
                                    context=context,
                                    checkpoint=checkpoint,
                                )

                                # We don't pass the context to the
                                # writer, so we don't expect there to
                                # be any scheduled tasks!
                                assert len(context._tasks) == 0

                                effects = IMPORT_reboot_aio_state_managers.Effects(
                                    state=(
                                        # Pass `None` if the state hasn't changed!
                                        state if serialized_state != state.SerializeToString(
                                            deterministic=True,
                                        )
                                        else None
                                    ),
                                    response=IMPORT_google_protobuf_wrappers_pb2.BytesValue(
                                        value=IMPORT_pickle.dumps(result)
                                    ),
                                )

                                await state_manager_writer.complete(effects)

                                return result

        def per_workflow(self, alias: IMPORT_typing.Optional[str] = None):
            return OrderedMapBaseServicer.WorkflowState._Idempotently(
                servicer=self._servicer,
                alias=alias,
                how=IMPORT_reboot_aio_workflows.PER_WORKFLOW,
            )

        def per_iteration(self, alias: IMPORT_typing.Optional[str] = None):
            return OrderedMapBaseServicer.WorkflowState._Idempotently(
                servicer=self._servicer,
                alias=alias,
                how=IMPORT_reboot_aio_workflows.PER_ITERATION,
            )

        class _Always:
            """Helper class for providing better types for `write` that don't
            require passing `type` or `check_type`."""

            def __init__(
                self,
                *,
                servicer: OrderedMapBaseServicer,
            ):
                self._servicer = servicer

            async def read(
                self, context: IMPORT_reboot_aio_contexts.WorkflowContext
            ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap:
                return await OrderedMapBaseServicer.WorkflowState._Idempotently(
                    servicer=self._servicer,
                    alias=None,
                    how=IMPORT_reboot_aio_workflows.ALWAYS,
                ).read(context)

            async def write(
                self,
                context: IMPORT_reboot_aio_contexts.WorkflowContext,
                writer: OrderedMapBaseServicer.InlineWriterCallable[OrderedMapBaseServicer.InlineWriterCallableResult],
                __options__: IMPORT_reboot_aio_call.Options = IMPORT_reboot_aio_call.Options(),
            ) -> OrderedMapBaseServicer.InlineWriterCallableResult:
                return await OrderedMapBaseServicer.WorkflowState._Idempotently(
                    servicer=self._servicer,
                    alias=None,
                    how=IMPORT_reboot_aio_workflows.ALWAYS,
                )._write(
                    context,
                    writer,
                    __options__,
                    type_result=type(None),
                    check_type=False,
                )

        def always(self):
            return OrderedMapBaseServicer.WorkflowState._Always(
                servicer=self._servicer,
            )

    # For 'rbt.std.collections.ordered_map.v1.OrderedMapMethods.Create'.
    @IMPORT_abc_abstractmethod
    async def _Create(
        self,
        context: IMPORT_reboot_aio_contexts.TransactionContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapCreateRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapCreateResponse:
        raise NotImplementedError

    # For 'rbt.std.collections.ordered_map.v1.OrderedMapMethods.Search'.
    @IMPORT_abc_abstractmethod
    async def _Search(
        self,
        context: IMPORT_reboot_aio_contexts.ReaderContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapSearchRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapSearchResponse:
        raise NotImplementedError

    # For 'rbt.std.collections.ordered_map.v1.OrderedMapMethods.Insert'.
    @IMPORT_abc_abstractmethod
    async def _Insert(
        self,
        context: IMPORT_reboot_aio_contexts.TransactionContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapInsertRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapInsertResponse:
        raise NotImplementedError

    # For 'rbt.std.collections.ordered_map.v1.OrderedMapMethods.Remove'.
    @IMPORT_abc_abstractmethod
    async def _Remove(
        self,
        context: IMPORT_reboot_aio_contexts.TransactionContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRemoveRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRemoveResponse:
        raise NotImplementedError

    # For 'rbt.std.collections.ordered_map.v1.OrderedMapMethods.Range'.
    @IMPORT_abc_abstractmethod
    async def _Range(
        self,
        context: IMPORT_reboot_aio_contexts.ReaderContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRangeRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRangeResponse:
        raise NotImplementedError

    # For 'rbt.std.collections.ordered_map.v1.OrderedMapMethods.ReverseRange'.
    @IMPORT_abc_abstractmethod
    async def _ReverseRange(
        self,
        context: IMPORT_reboot_aio_contexts.ReaderContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapReverseRangeRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapReverseRangeResponse:
        raise NotImplementedError

    # For 'rbt.std.collections.ordered_map.v1.OrderedMapMethods.Stringify'.
    @IMPORT_abc_abstractmethod
    async def _Stringify(
        self,
        context: IMPORT_reboot_aio_contexts.ReaderContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapStringifyRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapStringifyResponse:
        raise NotImplementedError



class OrderedMapSingletonServicer(OrderedMapBaseServicer):

    @property
    def state(self):
        return OrderedMapBaseServicer.WorkflowState(
            servicer=self
        )

    # For 'rbt.std.collections.ordered_map.v1.OrderedMapMethods.Create'.
    # To be backwards compatible during the renaming don't make this
    # method to be 'abstractmethod', so that new code that
    # doesn't implement it continues to work.
    # TODO: make it abstractmethod when renaming is done.
    async def Create(
        self,
        context: IMPORT_reboot_aio_contexts.TransactionContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapCreateRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapCreateResponse:
        # During the migration from 'PascalCase' to 'snake_case' method
        # naming in Python servicers, we call the 'snake_case' version
        # by default, so new names will do the correct thing making the
        # code to be backwards compatible for some time and if a servicer
        # overrides the 'PascalCase' version - it will override that
        # method and will just work.
        return await self.create(
            context,
            state,
            request,
        )

    # To be backwards compatible during the renaming don't make this
    # method to be 'abstractmethod', so that existing code that
    # doesn't implement it continues to work.
    async def create(
        self,
        context: IMPORT_reboot_aio_contexts.TransactionContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapCreateRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapCreateResponse:
        raise NotImplementedError

    # For 'rbt.std.collections.ordered_map.v1.OrderedMapMethods.Search'.
    # To be backwards compatible during the renaming don't make this
    # method to be 'abstractmethod', so that new code that
    # doesn't implement it continues to work.
    # TODO: make it abstractmethod when renaming is done.
    async def Search(
        self,
        context: IMPORT_reboot_aio_contexts.ReaderContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapSearchRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapSearchResponse:
        # During the migration from 'PascalCase' to 'snake_case' method
        # naming in Python servicers, we call the 'snake_case' version
        # by default, so new names will do the correct thing making the
        # code to be backwards compatible for some time and if a servicer
        # overrides the 'PascalCase' version - it will override that
        # method and will just work.
        return await self.search(
            context,
            state,
            request,
        )

    # To be backwards compatible during the renaming don't make this
    # method to be 'abstractmethod', so that existing code that
    # doesn't implement it continues to work.
    # TODO: make it abstractmethod when renaming is done.
    async def search(
        self,
        context: IMPORT_reboot_aio_contexts.ReaderContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapSearchRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapSearchResponse:
        raise NotImplementedError

    # For 'rbt.std.collections.ordered_map.v1.OrderedMapMethods.Insert'.
    # To be backwards compatible during the renaming don't make this
    # method to be 'abstractmethod', so that new code that
    # doesn't implement it continues to work.
    # TODO: make it abstractmethod when renaming is done.
    async def Insert(
        self,
        context: IMPORT_reboot_aio_contexts.TransactionContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapInsertRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapInsertResponse:
        # During the migration from 'PascalCase' to 'snake_case' method
        # naming in Python servicers, we call the 'snake_case' version
        # by default, so new names will do the correct thing making the
        # code to be backwards compatible for some time and if a servicer
        # overrides the 'PascalCase' version - it will override that
        # method and will just work.
        return await self.insert(
            context,
            state,
            request,
        )

    # To be backwards compatible during the renaming don't make this
    # method to be 'abstractmethod', so that existing code that
    # doesn't implement it continues to work.
    async def insert(
        self,
        context: IMPORT_reboot_aio_contexts.TransactionContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapInsertRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapInsertResponse:
        raise NotImplementedError

    # For 'rbt.std.collections.ordered_map.v1.OrderedMapMethods.Remove'.
    # To be backwards compatible during the renaming don't make this
    # method to be 'abstractmethod', so that new code that
    # doesn't implement it continues to work.
    # TODO: make it abstractmethod when renaming is done.
    async def Remove(
        self,
        context: IMPORT_reboot_aio_contexts.TransactionContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRemoveRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRemoveResponse:
        # During the migration from 'PascalCase' to 'snake_case' method
        # naming in Python servicers, we call the 'snake_case' version
        # by default, so new names will do the correct thing making the
        # code to be backwards compatible for some time and if a servicer
        # overrides the 'PascalCase' version - it will override that
        # method and will just work.
        return await self.remove(
            context,
            state,
            request,
        )

    # To be backwards compatible during the renaming don't make this
    # method to be 'abstractmethod', so that existing code that
    # doesn't implement it continues to work.
    async def remove(
        self,
        context: IMPORT_reboot_aio_contexts.TransactionContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRemoveRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRemoveResponse:
        raise NotImplementedError

    # For 'rbt.std.collections.ordered_map.v1.OrderedMapMethods.Range'.
    # To be backwards compatible during the renaming don't make this
    # method to be 'abstractmethod', so that new code that
    # doesn't implement it continues to work.
    # TODO: make it abstractmethod when renaming is done.
    async def Range(
        self,
        context: IMPORT_reboot_aio_contexts.ReaderContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRangeRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRangeResponse:
        # During the migration from 'PascalCase' to 'snake_case' method
        # naming in Python servicers, we call the 'snake_case' version
        # by default, so new names will do the correct thing making the
        # code to be backwards compatible for some time and if a servicer
        # overrides the 'PascalCase' version - it will override that
        # method and will just work.
        return await self.range(
            context,
            state,
            request,
        )

    # To be backwards compatible during the renaming don't make this
    # method to be 'abstractmethod', so that existing code that
    # doesn't implement it continues to work.
    # TODO: make it abstractmethod when renaming is done.
    async def range(
        self,
        context: IMPORT_reboot_aio_contexts.ReaderContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRangeRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRangeResponse:
        raise NotImplementedError

    # For 'rbt.std.collections.ordered_map.v1.OrderedMapMethods.ReverseRange'.
    # To be backwards compatible during the renaming don't make this
    # method to be 'abstractmethod', so that new code that
    # doesn't implement it continues to work.
    # TODO: make it abstractmethod when renaming is done.
    async def ReverseRange(
        self,
        context: IMPORT_reboot_aio_contexts.ReaderContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapReverseRangeRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapReverseRangeResponse:
        # During the migration from 'PascalCase' to 'snake_case' method
        # naming in Python servicers, we call the 'snake_case' version
        # by default, so new names will do the correct thing making the
        # code to be backwards compatible for some time and if a servicer
        # overrides the 'PascalCase' version - it will override that
        # method and will just work.
        return await self.reverse_range(
            context,
            state,
            request,
        )

    # To be backwards compatible during the renaming don't make this
    # method to be 'abstractmethod', so that existing code that
    # doesn't implement it continues to work.
    # TODO: make it abstractmethod when renaming is done.
    async def reverse_range(
        self,
        context: IMPORT_reboot_aio_contexts.ReaderContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapReverseRangeRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapReverseRangeResponse:
        raise NotImplementedError

    # For 'rbt.std.collections.ordered_map.v1.OrderedMapMethods.Stringify'.
    # To be backwards compatible during the renaming don't make this
    # method to be 'abstractmethod', so that new code that
    # doesn't implement it continues to work.
    # TODO: make it abstractmethod when renaming is done.
    async def Stringify(
        self,
        context: IMPORT_reboot_aio_contexts.ReaderContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapStringifyRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapStringifyResponse:
        # During the migration from 'PascalCase' to 'snake_case' method
        # naming in Python servicers, we call the 'snake_case' version
        # by default, so new names will do the correct thing making the
        # code to be backwards compatible for some time and if a servicer
        # overrides the 'PascalCase' version - it will override that
        # method and will just work.
        return await self.stringify(
            context,
            state,
            request,
        )

    # To be backwards compatible during the renaming don't make this
    # method to be 'abstractmethod', so that existing code that
    # doesn't implement it continues to work.
    # TODO: make it abstractmethod when renaming is done.
    async def stringify(
        self,
        context: IMPORT_reboot_aio_contexts.ReaderContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapStringifyRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapStringifyResponse:
        raise NotImplementedError


    # For 'rbt.std.collections.ordered_map.v1.OrderedMapMethods.Create'.
    async def _Create(
        self,
        context: IMPORT_reboot_aio_contexts.TransactionContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapCreateRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapCreateResponse:
        # Wrap the call to the developer's method in a `span` so that it
        # is traced using its fully-qualified Python name.
        with IMPORT_reboot_aio_tracing.span(
                state_name=f"rbt.std.collections.ordered_map.v1.OrderedMap('{context.state_id}')",
                span_name=f"{IMPORT_reboot_aio_tracing.qualified_type_name(self)}.Create()",
                level=IMPORT_reboot_aio_tracing.TraceLevel.CUSTOMER,
                python_specific=True,
        ):
            return await self.Create(context, state, request)


    # For 'rbt.std.collections.ordered_map.v1.OrderedMapMethods.Search'.
    async def _Search(
        self,
        context: IMPORT_reboot_aio_contexts.ReaderContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapSearchRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapSearchResponse:
        # Wrap the call to the developer's method in a `span` so that it
        # is traced using its fully-qualified Python name.
        with IMPORT_reboot_aio_tracing.span(
                state_name=f"rbt.std.collections.ordered_map.v1.OrderedMap('{context.state_id}')",
                span_name=f"{IMPORT_reboot_aio_tracing.qualified_type_name(self)}.Search()",
                level=IMPORT_reboot_aio_tracing.TraceLevel.CUSTOMER,
                python_specific=True,
        ):
            response = (
                self.Search(
                    context,
                    state,
                    request,
                )
            )
            return await response

    # For 'rbt.std.collections.ordered_map.v1.OrderedMapMethods.Insert'.
    async def _Insert(
        self,
        context: IMPORT_reboot_aio_contexts.TransactionContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapInsertRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapInsertResponse:
        # Wrap the call to the developer's method in a `span` so that it
        # is traced using its fully-qualified Python name.
        with IMPORT_reboot_aio_tracing.span(
                state_name=f"rbt.std.collections.ordered_map.v1.OrderedMap('{context.state_id}')",
                span_name=f"{IMPORT_reboot_aio_tracing.qualified_type_name(self)}.Insert()",
                level=IMPORT_reboot_aio_tracing.TraceLevel.CUSTOMER,
                python_specific=True,
        ):
            return await self.Insert(context, state, request)


    # For 'rbt.std.collections.ordered_map.v1.OrderedMapMethods.Remove'.
    async def _Remove(
        self,
        context: IMPORT_reboot_aio_contexts.TransactionContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRemoveRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRemoveResponse:
        # Wrap the call to the developer's method in a `span` so that it
        # is traced using its fully-qualified Python name.
        with IMPORT_reboot_aio_tracing.span(
                state_name=f"rbt.std.collections.ordered_map.v1.OrderedMap('{context.state_id}')",
                span_name=f"{IMPORT_reboot_aio_tracing.qualified_type_name(self)}.Remove()",
                level=IMPORT_reboot_aio_tracing.TraceLevel.CUSTOMER,
                python_specific=True,
        ):
            return await self.Remove(context, state, request)


    # For 'rbt.std.collections.ordered_map.v1.OrderedMapMethods.Range'.
    async def _Range(
        self,
        context: IMPORT_reboot_aio_contexts.ReaderContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRangeRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRangeResponse:
        # Wrap the call to the developer's method in a `span` so that it
        # is traced using its fully-qualified Python name.
        with IMPORT_reboot_aio_tracing.span(
                state_name=f"rbt.std.collections.ordered_map.v1.OrderedMap('{context.state_id}')",
                span_name=f"{IMPORT_reboot_aio_tracing.qualified_type_name(self)}.Range()",
                level=IMPORT_reboot_aio_tracing.TraceLevel.CUSTOMER,
                python_specific=True,
        ):
            response = (
                self.Range(
                    context,
                    state,
                    request,
                )
            )
            return await response

    # For 'rbt.std.collections.ordered_map.v1.OrderedMapMethods.ReverseRange'.
    async def _ReverseRange(
        self,
        context: IMPORT_reboot_aio_contexts.ReaderContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapReverseRangeRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapReverseRangeResponse:
        # Wrap the call to the developer's method in a `span` so that it
        # is traced using its fully-qualified Python name.
        with IMPORT_reboot_aio_tracing.span(
                state_name=f"rbt.std.collections.ordered_map.v1.OrderedMap('{context.state_id}')",
                span_name=f"{IMPORT_reboot_aio_tracing.qualified_type_name(self)}.ReverseRange()",
                level=IMPORT_reboot_aio_tracing.TraceLevel.CUSTOMER,
                python_specific=True,
        ):
            response = (
                self.ReverseRange(
                    context,
                    state,
                    request,
                )
            )
            return await response

    # For 'rbt.std.collections.ordered_map.v1.OrderedMapMethods.Stringify'.
    async def _Stringify(
        self,
        context: IMPORT_reboot_aio_contexts.ReaderContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapStringifyRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapStringifyResponse:
        # Wrap the call to the developer's method in a `span` so that it
        # is traced using its fully-qualified Python name.
        with IMPORT_reboot_aio_tracing.span(
                state_name=f"rbt.std.collections.ordered_map.v1.OrderedMap('{context.state_id}')",
                span_name=f"{IMPORT_reboot_aio_tracing.qualified_type_name(self)}.Stringify()",
                level=IMPORT_reboot_aio_tracing.TraceLevel.CUSTOMER,
                python_specific=True,
        ):
            response = (
                self.Stringify(
                    context,
                    state,
                    request,
                )
            )
            return await response



class OrderedMapServicer(OrderedMapBaseServicer):

    _state: IMPORT_contextvars.ContextVar[
        IMPORT_typing.Optional[rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap]
    ] = IMPORT_contextvars.ContextVar(
        'Provides access to state for each call, i.e., there may be '
        'multiple readers executing concurrently but each might have '
        'a different `state`',
        default=None,
    )

    _workflow: IMPORT_contextvars.ContextVar[bool] = IMPORT_contextvars.ContextVar(
        'Whether or not current context is executing a workflow',
        default=False,
    )

    # An instance of the derived class for each state.
    _instances: dict[str, OrderedMapServicer] = {}

    def _instance(self, state_id: str):
        instances = OrderedMapServicer._instances
        instance = instances.get(state_id)
        if instance is None:
            instance = self.__class__()
            instance._middleware = self._middleware
        instances[state_id] = instance
        return instance

    @property
    def state(self) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap:
        state = OrderedMapServicer._state.get()
        if state is None:
            raise RuntimeError(
                "`state` property is only relevant within a `Servicer` method"
            )
        workflow = OrderedMapServicer._workflow.get()
        if workflow:
            raise RuntimeError(
                "`self.state` is not valid within a `workflow` because a "
                "`workflow ` is not _atomic_; use "
                "`await self.ref().read(context)` instead"
            )
        return state

    @state.setter
    def state(self, new_state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap):
        state = OrderedMapServicer._state.get()
        if state is None:
            raise RuntimeError(
                "`state` property is only relevant within a `Servicer` method"
            )
        workflow = OrderedMapServicer._workflow.get()
        if workflow:
            raise RuntimeError(
                "`self.state` is not valid within a `workflow` because a "
                "`workflow ` is not _atomic_; use "
                "`await self.ref().write(...)` instead"
            )
        state.CopyFrom(new_state)

    # For 'rbt.std.collections.ordered_map.v1.OrderedMapMethods.Create'.
    # To be backwards compatible during the renaming don't make this
    # method to be 'abstractmethod', so that new code that
    # doesn't implement it continues to work.
    # TODO: make it abstractmethod when renaming is done.
    async def Create(
        self,
        context: IMPORT_reboot_aio_contexts.TransactionContext,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapCreateRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapCreateResponse:
        # During the migration from 'PascalCase' to 'snake_case' method
        # naming in Python servicers, we call the 'snake_case' version
        # by default, so new names will do the correct thing making the
        # code to be backwards compatible for some time and if a servicer
        # overrides the 'PascalCase' version - it will override that
        # method and will just work.
        return await self.create(
            context,
            request,
        )

    # To be backwards compatible during the renaming don't make this
    # method to be 'abstractmethod', so that existing code that
    # doesn't implement it continues to work.
    async def create(
        self,
        context: IMPORT_reboot_aio_contexts.TransactionContext,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapCreateRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapCreateResponse:
        raise NotImplementedError

    # For 'rbt.std.collections.ordered_map.v1.OrderedMapMethods.Search'.
    # To be backwards compatible during the renaming don't make this
    # method to be 'abstractmethod', so that new code that
    # doesn't implement it continues to work.
    # TODO: make it abstractmethod when renaming is done.
    async def Search(
        self,
        context: IMPORT_reboot_aio_contexts.ReaderContext,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapSearchRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapSearchResponse:
        # During the migration from 'PascalCase' to 'snake_case' method
        # naming in Python servicers, we call the 'snake_case' version
        # by default, so new names will do the correct thing making the
        # code to be backwards compatible for some time and if a servicer
        # overrides the 'PascalCase' version - it will override that
        # method and will just work.
        return await self.search(
            context,
            request,
        )

    # To be backwards compatible during the renaming don't make this
    # method to be 'abstractmethod', so that existing code that
    # doesn't implement it continues to work.
    async def search(
        self,
        context: IMPORT_reboot_aio_contexts.ReaderContext,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapSearchRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapSearchResponse:
        raise NotImplementedError

    # For 'rbt.std.collections.ordered_map.v1.OrderedMapMethods.Insert'.
    # To be backwards compatible during the renaming don't make this
    # method to be 'abstractmethod', so that new code that
    # doesn't implement it continues to work.
    # TODO: make it abstractmethod when renaming is done.
    async def Insert(
        self,
        context: IMPORT_reboot_aio_contexts.TransactionContext,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapInsertRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapInsertResponse:
        # During the migration from 'PascalCase' to 'snake_case' method
        # naming in Python servicers, we call the 'snake_case' version
        # by default, so new names will do the correct thing making the
        # code to be backwards compatible for some time and if a servicer
        # overrides the 'PascalCase' version - it will override that
        # method and will just work.
        return await self.insert(
            context,
            request,
        )

    # To be backwards compatible during the renaming don't make this
    # method to be 'abstractmethod', so that existing code that
    # doesn't implement it continues to work.
    async def insert(
        self,
        context: IMPORT_reboot_aio_contexts.TransactionContext,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapInsertRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapInsertResponse:
        raise NotImplementedError

    # For 'rbt.std.collections.ordered_map.v1.OrderedMapMethods.Remove'.
    # To be backwards compatible during the renaming don't make this
    # method to be 'abstractmethod', so that new code that
    # doesn't implement it continues to work.
    # TODO: make it abstractmethod when renaming is done.
    async def Remove(
        self,
        context: IMPORT_reboot_aio_contexts.TransactionContext,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRemoveRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRemoveResponse:
        # During the migration from 'PascalCase' to 'snake_case' method
        # naming in Python servicers, we call the 'snake_case' version
        # by default, so new names will do the correct thing making the
        # code to be backwards compatible for some time and if a servicer
        # overrides the 'PascalCase' version - it will override that
        # method and will just work.
        return await self.remove(
            context,
            request,
        )

    # To be backwards compatible during the renaming don't make this
    # method to be 'abstractmethod', so that existing code that
    # doesn't implement it continues to work.
    async def remove(
        self,
        context: IMPORT_reboot_aio_contexts.TransactionContext,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRemoveRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRemoveResponse:
        raise NotImplementedError

    # For 'rbt.std.collections.ordered_map.v1.OrderedMapMethods.Range'.
    # To be backwards compatible during the renaming don't make this
    # method to be 'abstractmethod', so that new code that
    # doesn't implement it continues to work.
    # TODO: make it abstractmethod when renaming is done.
    async def Range(
        self,
        context: IMPORT_reboot_aio_contexts.ReaderContext,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRangeRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRangeResponse:
        # During the migration from 'PascalCase' to 'snake_case' method
        # naming in Python servicers, we call the 'snake_case' version
        # by default, so new names will do the correct thing making the
        # code to be backwards compatible for some time and if a servicer
        # overrides the 'PascalCase' version - it will override that
        # method and will just work.
        return await self.range(
            context,
            request,
        )

    # To be backwards compatible during the renaming don't make this
    # method to be 'abstractmethod', so that existing code that
    # doesn't implement it continues to work.
    async def range(
        self,
        context: IMPORT_reboot_aio_contexts.ReaderContext,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRangeRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRangeResponse:
        raise NotImplementedError

    # For 'rbt.std.collections.ordered_map.v1.OrderedMapMethods.ReverseRange'.
    # To be backwards compatible during the renaming don't make this
    # method to be 'abstractmethod', so that new code that
    # doesn't implement it continues to work.
    # TODO: make it abstractmethod when renaming is done.
    async def ReverseRange(
        self,
        context: IMPORT_reboot_aio_contexts.ReaderContext,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapReverseRangeRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapReverseRangeResponse:
        # During the migration from 'PascalCase' to 'snake_case' method
        # naming in Python servicers, we call the 'snake_case' version
        # by default, so new names will do the correct thing making the
        # code to be backwards compatible for some time and if a servicer
        # overrides the 'PascalCase' version - it will override that
        # method and will just work.
        return await self.reverse_range(
            context,
            request,
        )

    # To be backwards compatible during the renaming don't make this
    # method to be 'abstractmethod', so that existing code that
    # doesn't implement it continues to work.
    async def reverse_range(
        self,
        context: IMPORT_reboot_aio_contexts.ReaderContext,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapReverseRangeRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapReverseRangeResponse:
        raise NotImplementedError

    # For 'rbt.std.collections.ordered_map.v1.OrderedMapMethods.Stringify'.
    # To be backwards compatible during the renaming don't make this
    # method to be 'abstractmethod', so that new code that
    # doesn't implement it continues to work.
    # TODO: make it abstractmethod when renaming is done.
    async def Stringify(
        self,
        context: IMPORT_reboot_aio_contexts.ReaderContext,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapStringifyRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapStringifyResponse:
        # During the migration from 'PascalCase' to 'snake_case' method
        # naming in Python servicers, we call the 'snake_case' version
        # by default, so new names will do the correct thing making the
        # code to be backwards compatible for some time and if a servicer
        # overrides the 'PascalCase' version - it will override that
        # method and will just work.
        return await self.stringify(
            context,
            request,
        )

    # To be backwards compatible during the renaming don't make this
    # method to be 'abstractmethod', so that existing code that
    # doesn't implement it continues to work.
    async def stringify(
        self,
        context: IMPORT_reboot_aio_contexts.ReaderContext,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapStringifyRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapStringifyResponse:
        raise NotImplementedError


    # For 'rbt.std.collections.ordered_map.v1.OrderedMapMethods.Create'.
    async def _Create(
        self,
        context: IMPORT_reboot_aio_contexts.TransactionContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapCreateRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapCreateResponse:
        # We should have an asyncio task and thus context per request,
        # let's confirm this assumption by making sure that
        # `_state is None`.
        assert OrderedMapServicer._state.get() is None
        OrderedMapServicer._state.set(state)
        try:
            # Wrap the call to the developer's method in a `span` so that it
            # is traced using its fully-qualified Python name.
            instance = self._instance(context.state_id)
            with IMPORT_reboot_aio_tracing.span(
                    state_name=f"rbt.std.collections.ordered_map.v1.OrderedMap('{context.state_id}')",
                    span_name=f"{IMPORT_reboot_aio_tracing.qualified_type_name(instance)}.Create()",
                    level=IMPORT_reboot_aio_tracing.TraceLevel.CUSTOMER,
                    python_specific=True,
            ):
                return await instance.Create(context, request)
        finally:
            OrderedMapServicer._state.set(None)

    # For 'rbt.std.collections.ordered_map.v1.OrderedMapMethods.Search'.
    async def _Search(
        self,
        context: IMPORT_reboot_aio_contexts.ReaderContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapSearchRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapSearchResponse:
        # We should have an asyncio task and thus context per request,
        # let's confirm this assumption by making sure that
        # `_state is None`.
        assert OrderedMapServicer._state.get() is None
        OrderedMapServicer._state.set(state)
        try:
            # Wrap the call to the developer's method in a `span` so that it
            # is traced using its fully-qualified Python name.
            instance = self._instance(context.state_id)
            with IMPORT_reboot_aio_tracing.span(
                    state_name=f"rbt.std.collections.ordered_map.v1.OrderedMap('{context.state_id}')",
                    span_name=f"{IMPORT_reboot_aio_tracing.qualified_type_name(instance)}.Search()",
                    level=IMPORT_reboot_aio_tracing.TraceLevel.CUSTOMER,
                    python_specific=True,
            ):
                return await instance.Search(context, request)
        finally:
            OrderedMapServicer._state.set(None)

    # For 'rbt.std.collections.ordered_map.v1.OrderedMapMethods.Insert'.
    async def _Insert(
        self,
        context: IMPORT_reboot_aio_contexts.TransactionContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapInsertRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapInsertResponse:
        # We should have an asyncio task and thus context per request,
        # let's confirm this assumption by making sure that
        # `_state is None`.
        assert OrderedMapServicer._state.get() is None
        OrderedMapServicer._state.set(state)
        try:
            # Wrap the call to the developer's method in a `span` so that it
            # is traced using its fully-qualified Python name.
            instance = self._instance(context.state_id)
            with IMPORT_reboot_aio_tracing.span(
                    state_name=f"rbt.std.collections.ordered_map.v1.OrderedMap('{context.state_id}')",
                    span_name=f"{IMPORT_reboot_aio_tracing.qualified_type_name(instance)}.Insert()",
                    level=IMPORT_reboot_aio_tracing.TraceLevel.CUSTOMER,
                    python_specific=True,
            ):
                return await instance.Insert(context, request)
        finally:
            OrderedMapServicer._state.set(None)

    # For 'rbt.std.collections.ordered_map.v1.OrderedMapMethods.Remove'.
    async def _Remove(
        self,
        context: IMPORT_reboot_aio_contexts.TransactionContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRemoveRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRemoveResponse:
        # We should have an asyncio task and thus context per request,
        # let's confirm this assumption by making sure that
        # `_state is None`.
        assert OrderedMapServicer._state.get() is None
        OrderedMapServicer._state.set(state)
        try:
            # Wrap the call to the developer's method in a `span` so that it
            # is traced using its fully-qualified Python name.
            instance = self._instance(context.state_id)
            with IMPORT_reboot_aio_tracing.span(
                    state_name=f"rbt.std.collections.ordered_map.v1.OrderedMap('{context.state_id}')",
                    span_name=f"{IMPORT_reboot_aio_tracing.qualified_type_name(instance)}.Remove()",
                    level=IMPORT_reboot_aio_tracing.TraceLevel.CUSTOMER,
                    python_specific=True,
            ):
                return await instance.Remove(context, request)
        finally:
            OrderedMapServicer._state.set(None)

    # For 'rbt.std.collections.ordered_map.v1.OrderedMapMethods.Range'.
    async def _Range(
        self,
        context: IMPORT_reboot_aio_contexts.ReaderContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRangeRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRangeResponse:
        # We should have an asyncio task and thus context per request,
        # let's confirm this assumption by making sure that
        # `_state is None`.
        assert OrderedMapServicer._state.get() is None
        OrderedMapServicer._state.set(state)
        try:
            # Wrap the call to the developer's method in a `span` so that it
            # is traced using its fully-qualified Python name.
            instance = self._instance(context.state_id)
            with IMPORT_reboot_aio_tracing.span(
                    state_name=f"rbt.std.collections.ordered_map.v1.OrderedMap('{context.state_id}')",
                    span_name=f"{IMPORT_reboot_aio_tracing.qualified_type_name(instance)}.Range()",
                    level=IMPORT_reboot_aio_tracing.TraceLevel.CUSTOMER,
                    python_specific=True,
            ):
                return await instance.Range(context, request)
        finally:
            OrderedMapServicer._state.set(None)

    # For 'rbt.std.collections.ordered_map.v1.OrderedMapMethods.ReverseRange'.
    async def _ReverseRange(
        self,
        context: IMPORT_reboot_aio_contexts.ReaderContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapReverseRangeRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapReverseRangeResponse:
        # We should have an asyncio task and thus context per request,
        # let's confirm this assumption by making sure that
        # `_state is None`.
        assert OrderedMapServicer._state.get() is None
        OrderedMapServicer._state.set(state)
        try:
            # Wrap the call to the developer's method in a `span` so that it
            # is traced using its fully-qualified Python name.
            instance = self._instance(context.state_id)
            with IMPORT_reboot_aio_tracing.span(
                    state_name=f"rbt.std.collections.ordered_map.v1.OrderedMap('{context.state_id}')",
                    span_name=f"{IMPORT_reboot_aio_tracing.qualified_type_name(instance)}.ReverseRange()",
                    level=IMPORT_reboot_aio_tracing.TraceLevel.CUSTOMER,
                    python_specific=True,
            ):
                return await instance.ReverseRange(context, request)
        finally:
            OrderedMapServicer._state.set(None)

    # For 'rbt.std.collections.ordered_map.v1.OrderedMapMethods.Stringify'.
    async def _Stringify(
        self,
        context: IMPORT_reboot_aio_contexts.ReaderContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapStringifyRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapStringifyResponse:
        # We should have an asyncio task and thus context per request,
        # let's confirm this assumption by making sure that
        # `_state is None`.
        assert OrderedMapServicer._state.get() is None
        OrderedMapServicer._state.set(state)
        try:
            # Wrap the call to the developer's method in a `span` so that it
            # is traced using its fully-qualified Python name.
            instance = self._instance(context.state_id)
            with IMPORT_reboot_aio_tracing.span(
                    state_name=f"rbt.std.collections.ordered_map.v1.OrderedMap('{context.state_id}')",
                    span_name=f"{IMPORT_reboot_aio_tracing.qualified_type_name(instance)}.Stringify()",
                    level=IMPORT_reboot_aio_tracing.TraceLevel.CUSTOMER,
                    python_specific=True,
            ):
                return await instance.Stringify(context, request)
        finally:
            OrderedMapServicer._state.set(None)



############################ Clients ############################
# The main developer-facing entrypoints for any Reboot type. Relevant to both
# clients and servicers (who use it to find the right servicer base types, as well
# as often being clients themselves).

# Attach an explicit time time zone to "naive" `datetime` objects. A "naive" `datetime` doesn't have a
# time zone. Such objects are typically interpreted as representing local time, but could be confused
# for objects representing UTC. This helper function disambiguates by explicitly attaching the local
# time zone to `datetime` objects that don't already have an explicit time zone. If the `datetime` object
# is already timezone-aware, we still convert it to our custom `DateTimeWithTimeZone` type.
def ensure_has_timezone(
    *,
    when: IMPORT_typing.Optional[IMPORT_datetime_datetime | IMPORT_datetime_timedelta] = None,
) -> IMPORT_typing.Optional[IMPORT_reboot_time_DateTimeWithTimeZone | IMPORT_datetime_timedelta]:
    if isinstance(when, IMPORT_datetime_datetime):
        return IMPORT_reboot_time_DateTimeWithTimeZone.from_datetime(when)
    return when

Node_ScheduleTypeVar = IMPORT_typing.TypeVar('Node_ScheduleTypeVar', 'Node.WeakReference._Schedule', 'Node.WeakReference._WriterSchedule')
Node_IdempotentlyScheduleTypeVar = IMPORT_typing.TypeVar('Node_IdempotentlyScheduleTypeVar', 'Node.WeakReference._Schedule', 'Node.WeakReference._WriterSchedule')

Node_UntilCallableType = IMPORT_typing.TypeVar('Node_UntilCallableType')

class NodeSingleton:
    Servicer: IMPORT_typing.TypeAlias = NodeSingletonServicer


class Node:


    Servicer: IMPORT_typing.TypeAlias = NodeServicer

    singleton: IMPORT_typing.TypeAlias = NodeSingleton

    Effects: IMPORT_typing.TypeAlias = NodeBaseServicer.Effects

    Authorizer: IMPORT_typing.TypeAlias = NodeAuthorizer

    State: IMPORT_typing.TypeAlias = rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node

    __state_type_name__ = IMPORT_reboot_aio_types.StateTypeName("rbt.std.collections.ordered_map.v1.Node")

    class CreateTask:
        """Represents a scheduled task running for the
        state. Note that this is not a coroutine because we are trying
        to convey the semantics that the task is already running (or
        will soon be).
        """

        @classmethod
        def retrieve(
            cls,
            context: IMPORT_reboot_aio_contexts.Context | IMPORT_reboot_aio_external.ExternalContext,
            *,
            task_id: IMPORT_rbt_v1alpha1.tasks_pb2.TaskId,
        ):
            return cls(context, task_id=task_id)

        def __init__(
            self,
            context: IMPORT_reboot_aio_contexts.Context | IMPORT_reboot_aio_external.ExternalContext,
            *,
            task_id: IMPORT_rbt_v1alpha1.tasks_pb2.TaskId,
        ) -> None:
            # Depending on the context type (inside or outside a Reboot application)
            # we may or may not know the application ID. If we don't know it, then
            # the `ExternalContext.gateway` will determine it.
            #
            # TODO: in the future we expect to support cross-application calls, in
            #       which case the developer may explicitly pass in an application ID
            #       here.
            self._application_id: IMPORT_typing.Optional[IMPORT_reboot_aio_types.ApplicationId] = None
            if isinstance(context, IMPORT_reboot_aio_contexts.Context):
                self._application_id = context.application_id
            self._channel_manager = context.channel_manager
            self._task_id = task_id

        @property
        def task_id(self) -> IMPORT_rbt_v1alpha1.tasks_pb2.TaskId:
            return self._task_id

        def __await__(self) -> IMPORT_typing.Generator[None, None, rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateResponse]:
            """Awaits for task to finish and returns its response."""
            async def wait_for_task() -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateResponse:
                channel = self._channel_manager.get_channel_to_state(
                    IMPORT_reboot_aio_types.StateTypeName(self._task_id.state_type),
                    IMPORT_reboot_aio_types.StateRef(self._task_id.state_ref),
                )

                stub = IMPORT_rbt_v1alpha1.tasks_pb2_grpc.TasksStub(channel)

                try:
                    call = IMPORT_reboot_aio_stubs.UnaryRetriedCall(
                        call=None,  # `RetriedCall` can create the call itself.
                        stub_method=stub.Wait,
                        method_name="Wait",
                        request=IMPORT_rbt_v1alpha1.tasks_pb2.WaitRequest(task_id=self._task_id),
                        metadata=IMPORT_reboot_aio_headers.Headers(
                            state_ref=IMPORT_reboot_aio_types.StateRef(self._task_id.state_ref),
                            application_id=self._application_id,
                        ).to_grpc_metadata(),
                        aborted_type=IMPORT_rebootdev.aio.aborted.SystemAborted,
                    )

                    wait_for_task_response = await call
                except IMPORT_rebootdev.aio.aborted.SystemAborted as error:
                    if error.code == IMPORT_grpc.StatusCode.NOT_FOUND:
                        raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                            IMPORT_rbt_v1alpha1.errors_pb2.UnknownTask()
                        ) from None

                    raise
                else:
                    response_or_error: IMPORT_typing.Optional[IMPORT_google_protobuf_any_pb2.Any] = None
                    is_error = False

                    if wait_for_task_response.response_or_error.WhichOneof("response_or_error") == "response":
                        response_or_error = wait_for_task_response.response_or_error.response
                    else:
                        is_error = True
                        response_or_error = wait_for_task_response.response_or_error.error

                    assert response_or_error is not None
                    assert response_or_error.TypeName() != ""

                    response = rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateResponse()

                    if (
                        not is_error and response_or_error.TypeName() != response.DESCRIPTOR.full_name
                    ):
                        raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                            IMPORT_rbt_v1alpha1.errors_pb2.InvalidArgument(),
                            message=
                            f"task with UUID {IMPORT_uuid.UUID(bytes=self._task_id.task_uuid)} "
                            f"has a response of type '{response_or_error.TypeName()}' "
                            "but expecting type 'rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateResponse'; "
                            "are you waiting on a task of the correct method?",
                        ) from None

                    if is_error:
                        # Currently only cancelled error is supported.
                        raise IMPORT_asyncio.CancelledError("Task was cancelled by a TasksServicer")
                    else:
                        response_or_error.Unpack(response)
                        return response

            return wait_for_task().__await__()

    CreateEffects: IMPORT_typing.TypeAlias = NodeBaseServicer.CreateEffects

    class CreateAborted(IMPORT_rebootdev.aio.aborted.Aborted):

        MethodError = IMPORT_typing.Union[
            rbt.v1alpha1.errors_pb2.StateAlreadyConstructed        ]

        Error = IMPORT_typing.Union[
            MethodError,
            IMPORT_rebootdev.aio.aborted.GrpcError,
            IMPORT_rebootdev.aio.aborted.RebootError,
        ]

        METHOD_ERROR_TYPES: list[type[IMPORT_google_protobuf_message.Message]] = [
            rbt.v1alpha1.errors_pb2.StateAlreadyConstructed        ]

        ERROR_TYPES: list[type[IMPORT_google_protobuf_message.Message]] = (
            METHOD_ERROR_TYPES +
            IMPORT_rebootdev.aio.aborted.GRPC_ERROR_TYPES +
            IMPORT_rebootdev.aio.aborted.REBOOT_ERROR_TYPES
        )

        _error: Error
        _code: IMPORT_grpc.StatusCode
        _message: IMPORT_typing.Optional[str]

        def __init__(
            self,
            error: MethodError |  IMPORT_rebootdev.aio.aborted.GrpcError,
            *,
            message: IMPORT_typing.Optional[str] = None,
            # Do not set this value when constructing in order to
            # raise. This is only used internally when constructing
            # from aborted calls.
            error_types: IMPORT_typing.Sequence[type[Error]] = (
                METHOD_ERROR_TYPES + IMPORT_rebootdev.aio.aborted.GRPC_ERROR_TYPES
            ),
        ):
            super().__init__()

            IMPORT_reboot_aio_types.assert_type(error, error_types)

            self._error = error

            code = self.grpc_status_code_from_error(self._error)

            if code is None:
                # Must be a Reboot specific or declared method error.
                code = IMPORT_grpc.StatusCode.ABORTED

            self._code = code

            self._message = message

        @property
        def error(self) -> Error:
            return self._error

        @property
        def code(self) -> IMPORT_grpc.StatusCode:
            return self._code

        @property
        def message(self) -> IMPORT_typing.Optional[str]:
            return self._message

        @classmethod
        def from_status(cls, status: IMPORT_google_rpc_status_pb2.Status):
            error = cls.error_from_google_rpc_status_details(
                status,
                cls.ERROR_TYPES,
            )

            message = status.message if len(status.message) > 0 else None

            if error is not None:
                return cls(error, message=message, error_types=cls.ERROR_TYPES)

            error = cls.error_from_google_rpc_status_code(status)

            assert error is not None

            # TODO(benh): also consider getting the type names from
            # `status.details` and including that in `message` to make
            # debugging easier.

            return cls(error, message=message)

        @classmethod
        def from_grpc_aio_rpc_error(cls, aio_rpc_error: IMPORT_grpc.aio.AioRpcError):
            return cls(
                cls.error_from_grpc_aio_rpc_error(aio_rpc_error),
                message=aio_rpc_error.details(),
            )

        @classmethod
        def is_declared_error(cls, message: IMPORT_google_protobuf_message.Message) -> bool:
            if message.DESCRIPTOR.full_name == 'rbt.v1alpha1.StateAlreadyConstructed':
                return True
            return False

    class SearchTask:
        """Represents a scheduled task running for the
        state. Note that this is not a coroutine because we are trying
        to convey the semantics that the task is already running (or
        will soon be).
        """

        @classmethod
        def retrieve(
            cls,
            context: IMPORT_reboot_aio_contexts.Context | IMPORT_reboot_aio_external.ExternalContext,
            *,
            task_id: IMPORT_rbt_v1alpha1.tasks_pb2.TaskId,
        ):
            return cls(context, task_id=task_id)

        def __init__(
            self,
            context: IMPORT_reboot_aio_contexts.Context | IMPORT_reboot_aio_external.ExternalContext,
            *,
            task_id: IMPORT_rbt_v1alpha1.tasks_pb2.TaskId,
        ) -> None:
            # Depending on the context type (inside or outside a Reboot application)
            # we may or may not know the application ID. If we don't know it, then
            # the `ExternalContext.gateway` will determine it.
            #
            # TODO: in the future we expect to support cross-application calls, in
            #       which case the developer may explicitly pass in an application ID
            #       here.
            self._application_id: IMPORT_typing.Optional[IMPORT_reboot_aio_types.ApplicationId] = None
            if isinstance(context, IMPORT_reboot_aio_contexts.Context):
                self._application_id = context.application_id
            self._channel_manager = context.channel_manager
            self._task_id = task_id

        @property
        def task_id(self) -> IMPORT_rbt_v1alpha1.tasks_pb2.TaskId:
            return self._task_id

        def __await__(self) -> IMPORT_typing.Generator[None, None, rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeSearchResponse]:
            """Awaits for task to finish and returns its response."""
            async def wait_for_task() -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeSearchResponse:
                channel = self._channel_manager.get_channel_to_state(
                    IMPORT_reboot_aio_types.StateTypeName(self._task_id.state_type),
                    IMPORT_reboot_aio_types.StateRef(self._task_id.state_ref),
                )

                stub = IMPORT_rbt_v1alpha1.tasks_pb2_grpc.TasksStub(channel)

                try:
                    call = IMPORT_reboot_aio_stubs.UnaryRetriedCall(
                        call=None,  # `RetriedCall` can create the call itself.
                        stub_method=stub.Wait,
                        method_name="Wait",
                        request=IMPORT_rbt_v1alpha1.tasks_pb2.WaitRequest(task_id=self._task_id),
                        metadata=IMPORT_reboot_aio_headers.Headers(
                            state_ref=IMPORT_reboot_aio_types.StateRef(self._task_id.state_ref),
                            application_id=self._application_id,
                        ).to_grpc_metadata(),
                        aborted_type=IMPORT_rebootdev.aio.aborted.SystemAborted,
                    )

                    wait_for_task_response = await call
                except IMPORT_rebootdev.aio.aborted.SystemAborted as error:
                    if error.code == IMPORT_grpc.StatusCode.NOT_FOUND:
                        raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                            IMPORT_rbt_v1alpha1.errors_pb2.UnknownTask()
                        ) from None

                    raise
                else:
                    response_or_error: IMPORT_typing.Optional[IMPORT_google_protobuf_any_pb2.Any] = None
                    is_error = False

                    if wait_for_task_response.response_or_error.WhichOneof("response_or_error") == "response":
                        response_or_error = wait_for_task_response.response_or_error.response
                    else:
                        is_error = True
                        response_or_error = wait_for_task_response.response_or_error.error

                    assert response_or_error is not None
                    assert response_or_error.TypeName() != ""

                    response = rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeSearchResponse()

                    if (
                        not is_error and response_or_error.TypeName() != response.DESCRIPTOR.full_name
                    ):
                        raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                            IMPORT_rbt_v1alpha1.errors_pb2.InvalidArgument(),
                            message=
                            f"task with UUID {IMPORT_uuid.UUID(bytes=self._task_id.task_uuid)} "
                            f"has a response of type '{response_or_error.TypeName()}' "
                            "but expecting type 'rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeSearchResponse'; "
                            "are you waiting on a task of the correct method?",
                        ) from None

                    if is_error:
                        # Currently only cancelled error is supported.
                        raise IMPORT_asyncio.CancelledError("Task was cancelled by a TasksServicer")
                    else:
                        response_or_error.Unpack(response)
                        return response

            return wait_for_task().__await__()


    class SearchAborted(IMPORT_rebootdev.aio.aborted.Aborted):


        Error = IMPORT_typing.Union[
            IMPORT_rebootdev.aio.aborted.GrpcError,
            IMPORT_rebootdev.aio.aborted.RebootError,
        ]

        METHOD_ERROR_TYPES: list[type[IMPORT_google_protobuf_message.Message]] = [
        ]

        ERROR_TYPES: list[type[IMPORT_google_protobuf_message.Message]] = (
            METHOD_ERROR_TYPES +
            IMPORT_rebootdev.aio.aborted.GRPC_ERROR_TYPES +
            IMPORT_rebootdev.aio.aborted.REBOOT_ERROR_TYPES
        )

        _error: Error
        _code: IMPORT_grpc.StatusCode
        _message: IMPORT_typing.Optional[str]

        def __init__(
            self,
            error:  IMPORT_rebootdev.aio.aborted.GrpcError,
            *,
            message: IMPORT_typing.Optional[str] = None,
            # Do not set this value when constructing in order to
            # raise. This is only used internally when constructing
            # from aborted calls.
            error_types: IMPORT_typing.Sequence[type[Error]] = (
                METHOD_ERROR_TYPES + IMPORT_rebootdev.aio.aborted.GRPC_ERROR_TYPES
            ),
        ):
            super().__init__()

            IMPORT_reboot_aio_types.assert_type(error, error_types)

            self._error = error

            code = self.grpc_status_code_from_error(self._error)

            if code is None:
                # Must be a Reboot specific or declared method error.
                code = IMPORT_grpc.StatusCode.ABORTED

            self._code = code

            self._message = message

        @property
        def error(self) -> Error:
            return self._error

        @property
        def code(self) -> IMPORT_grpc.StatusCode:
            return self._code

        @property
        def message(self) -> IMPORT_typing.Optional[str]:
            return self._message

        @classmethod
        def from_status(cls, status: IMPORT_google_rpc_status_pb2.Status):
            error = cls.error_from_google_rpc_status_details(
                status,
                cls.ERROR_TYPES,
            )

            message = status.message if len(status.message) > 0 else None

            if error is not None:
                return cls(error, message=message, error_types=cls.ERROR_TYPES)

            error = cls.error_from_google_rpc_status_code(status)

            assert error is not None

            # TODO(benh): also consider getting the type names from
            # `status.details` and including that in `message` to make
            # debugging easier.

            return cls(error, message=message)

        @classmethod
        def from_grpc_aio_rpc_error(cls, aio_rpc_error: IMPORT_grpc.aio.AioRpcError):
            return cls(
                cls.error_from_grpc_aio_rpc_error(aio_rpc_error),
                message=aio_rpc_error.details(),
            )

        @classmethod
        def is_declared_error(cls, message: IMPORT_google_protobuf_message.Message) -> bool:
            return False

    class InsertTask:
        """Represents a scheduled task running for the
        state. Note that this is not a coroutine because we are trying
        to convey the semantics that the task is already running (or
        will soon be).
        """

        @classmethod
        def retrieve(
            cls,
            context: IMPORT_reboot_aio_contexts.Context | IMPORT_reboot_aio_external.ExternalContext,
            *,
            task_id: IMPORT_rbt_v1alpha1.tasks_pb2.TaskId,
        ):
            return cls(context, task_id=task_id)

        def __init__(
            self,
            context: IMPORT_reboot_aio_contexts.Context | IMPORT_reboot_aio_external.ExternalContext,
            *,
            task_id: IMPORT_rbt_v1alpha1.tasks_pb2.TaskId,
        ) -> None:
            # Depending on the context type (inside or outside a Reboot application)
            # we may or may not know the application ID. If we don't know it, then
            # the `ExternalContext.gateway` will determine it.
            #
            # TODO: in the future we expect to support cross-application calls, in
            #       which case the developer may explicitly pass in an application ID
            #       here.
            self._application_id: IMPORT_typing.Optional[IMPORT_reboot_aio_types.ApplicationId] = None
            if isinstance(context, IMPORT_reboot_aio_contexts.Context):
                self._application_id = context.application_id
            self._channel_manager = context.channel_manager
            self._task_id = task_id

        @property
        def task_id(self) -> IMPORT_rbt_v1alpha1.tasks_pb2.TaskId:
            return self._task_id

        def __await__(self) -> IMPORT_typing.Generator[None, None, rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeInsertResponse]:
            """Awaits for task to finish and returns its response."""
            async def wait_for_task() -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeInsertResponse:
                channel = self._channel_manager.get_channel_to_state(
                    IMPORT_reboot_aio_types.StateTypeName(self._task_id.state_type),
                    IMPORT_reboot_aio_types.StateRef(self._task_id.state_ref),
                )

                stub = IMPORT_rbt_v1alpha1.tasks_pb2_grpc.TasksStub(channel)

                try:
                    call = IMPORT_reboot_aio_stubs.UnaryRetriedCall(
                        call=None,  # `RetriedCall` can create the call itself.
                        stub_method=stub.Wait,
                        method_name="Wait",
                        request=IMPORT_rbt_v1alpha1.tasks_pb2.WaitRequest(task_id=self._task_id),
                        metadata=IMPORT_reboot_aio_headers.Headers(
                            state_ref=IMPORT_reboot_aio_types.StateRef(self._task_id.state_ref),
                            application_id=self._application_id,
                        ).to_grpc_metadata(),
                        aborted_type=IMPORT_rebootdev.aio.aborted.SystemAborted,
                    )

                    wait_for_task_response = await call
                except IMPORT_rebootdev.aio.aborted.SystemAborted as error:
                    if error.code == IMPORT_grpc.StatusCode.NOT_FOUND:
                        raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                            IMPORT_rbt_v1alpha1.errors_pb2.UnknownTask()
                        ) from None

                    raise
                else:
                    response_or_error: IMPORT_typing.Optional[IMPORT_google_protobuf_any_pb2.Any] = None
                    is_error = False

                    if wait_for_task_response.response_or_error.WhichOneof("response_or_error") == "response":
                        response_or_error = wait_for_task_response.response_or_error.response
                    else:
                        is_error = True
                        response_or_error = wait_for_task_response.response_or_error.error

                    assert response_or_error is not None
                    assert response_or_error.TypeName() != ""

                    response = rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeInsertResponse()

                    if (
                        not is_error and response_or_error.TypeName() != response.DESCRIPTOR.full_name
                    ):
                        raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                            IMPORT_rbt_v1alpha1.errors_pb2.InvalidArgument(),
                            message=
                            f"task with UUID {IMPORT_uuid.UUID(bytes=self._task_id.task_uuid)} "
                            f"has a response of type '{response_or_error.TypeName()}' "
                            "but expecting type 'rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeInsertResponse'; "
                            "are you waiting on a task of the correct method?",
                        ) from None

                    if is_error:
                        # Currently only cancelled error is supported.
                        raise IMPORT_asyncio.CancelledError("Task was cancelled by a TasksServicer")
                    else:
                        response_or_error.Unpack(response)
                        return response

            return wait_for_task().__await__()


    class InsertAborted(IMPORT_rebootdev.aio.aborted.Aborted):


        Error = IMPORT_typing.Union[
            IMPORT_rebootdev.aio.aborted.GrpcError,
            IMPORT_rebootdev.aio.aborted.RebootError,
        ]

        METHOD_ERROR_TYPES: list[type[IMPORT_google_protobuf_message.Message]] = [
        ]

        ERROR_TYPES: list[type[IMPORT_google_protobuf_message.Message]] = (
            METHOD_ERROR_TYPES +
            IMPORT_rebootdev.aio.aborted.GRPC_ERROR_TYPES +
            IMPORT_rebootdev.aio.aborted.REBOOT_ERROR_TYPES
        )

        _error: Error
        _code: IMPORT_grpc.StatusCode
        _message: IMPORT_typing.Optional[str]

        def __init__(
            self,
            error:  IMPORT_rebootdev.aio.aborted.GrpcError,
            *,
            message: IMPORT_typing.Optional[str] = None,
            # Do not set this value when constructing in order to
            # raise. This is only used internally when constructing
            # from aborted calls.
            error_types: IMPORT_typing.Sequence[type[Error]] = (
                METHOD_ERROR_TYPES + IMPORT_rebootdev.aio.aborted.GRPC_ERROR_TYPES
            ),
        ):
            super().__init__()

            IMPORT_reboot_aio_types.assert_type(error, error_types)

            self._error = error

            code = self.grpc_status_code_from_error(self._error)

            if code is None:
                # Must be a Reboot specific or declared method error.
                code = IMPORT_grpc.StatusCode.ABORTED

            self._code = code

            self._message = message

        @property
        def error(self) -> Error:
            return self._error

        @property
        def code(self) -> IMPORT_grpc.StatusCode:
            return self._code

        @property
        def message(self) -> IMPORT_typing.Optional[str]:
            return self._message

        @classmethod
        def from_status(cls, status: IMPORT_google_rpc_status_pb2.Status):
            error = cls.error_from_google_rpc_status_details(
                status,
                cls.ERROR_TYPES,
            )

            message = status.message if len(status.message) > 0 else None

            if error is not None:
                return cls(error, message=message, error_types=cls.ERROR_TYPES)

            error = cls.error_from_google_rpc_status_code(status)

            assert error is not None

            # TODO(benh): also consider getting the type names from
            # `status.details` and including that in `message` to make
            # debugging easier.

            return cls(error, message=message)

        @classmethod
        def from_grpc_aio_rpc_error(cls, aio_rpc_error: IMPORT_grpc.aio.AioRpcError):
            return cls(
                cls.error_from_grpc_aio_rpc_error(aio_rpc_error),
                message=aio_rpc_error.details(),
            )

        @classmethod
        def is_declared_error(cls, message: IMPORT_google_protobuf_message.Message) -> bool:
            return False

    class RemoveTask:
        """Represents a scheduled task running for the
        state. Note that this is not a coroutine because we are trying
        to convey the semantics that the task is already running (or
        will soon be).
        """

        @classmethod
        def retrieve(
            cls,
            context: IMPORT_reboot_aio_contexts.Context | IMPORT_reboot_aio_external.ExternalContext,
            *,
            task_id: IMPORT_rbt_v1alpha1.tasks_pb2.TaskId,
        ):
            return cls(context, task_id=task_id)

        def __init__(
            self,
            context: IMPORT_reboot_aio_contexts.Context | IMPORT_reboot_aio_external.ExternalContext,
            *,
            task_id: IMPORT_rbt_v1alpha1.tasks_pb2.TaskId,
        ) -> None:
            # Depending on the context type (inside or outside a Reboot application)
            # we may or may not know the application ID. If we don't know it, then
            # the `ExternalContext.gateway` will determine it.
            #
            # TODO: in the future we expect to support cross-application calls, in
            #       which case the developer may explicitly pass in an application ID
            #       here.
            self._application_id: IMPORT_typing.Optional[IMPORT_reboot_aio_types.ApplicationId] = None
            if isinstance(context, IMPORT_reboot_aio_contexts.Context):
                self._application_id = context.application_id
            self._channel_manager = context.channel_manager
            self._task_id = task_id

        @property
        def task_id(self) -> IMPORT_rbt_v1alpha1.tasks_pb2.TaskId:
            return self._task_id

        def __await__(self) -> IMPORT_typing.Generator[None, None, rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRemoveResponse]:
            """Awaits for task to finish and returns its response."""
            async def wait_for_task() -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRemoveResponse:
                channel = self._channel_manager.get_channel_to_state(
                    IMPORT_reboot_aio_types.StateTypeName(self._task_id.state_type),
                    IMPORT_reboot_aio_types.StateRef(self._task_id.state_ref),
                )

                stub = IMPORT_rbt_v1alpha1.tasks_pb2_grpc.TasksStub(channel)

                try:
                    call = IMPORT_reboot_aio_stubs.UnaryRetriedCall(
                        call=None,  # `RetriedCall` can create the call itself.
                        stub_method=stub.Wait,
                        method_name="Wait",
                        request=IMPORT_rbt_v1alpha1.tasks_pb2.WaitRequest(task_id=self._task_id),
                        metadata=IMPORT_reboot_aio_headers.Headers(
                            state_ref=IMPORT_reboot_aio_types.StateRef(self._task_id.state_ref),
                            application_id=self._application_id,
                        ).to_grpc_metadata(),
                        aborted_type=IMPORT_rebootdev.aio.aborted.SystemAborted,
                    )

                    wait_for_task_response = await call
                except IMPORT_rebootdev.aio.aborted.SystemAborted as error:
                    if error.code == IMPORT_grpc.StatusCode.NOT_FOUND:
                        raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                            IMPORT_rbt_v1alpha1.errors_pb2.UnknownTask()
                        ) from None

                    raise
                else:
                    response_or_error: IMPORT_typing.Optional[IMPORT_google_protobuf_any_pb2.Any] = None
                    is_error = False

                    if wait_for_task_response.response_or_error.WhichOneof("response_or_error") == "response":
                        response_or_error = wait_for_task_response.response_or_error.response
                    else:
                        is_error = True
                        response_or_error = wait_for_task_response.response_or_error.error

                    assert response_or_error is not None
                    assert response_or_error.TypeName() != ""

                    response = rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRemoveResponse()

                    if (
                        not is_error and response_or_error.TypeName() != response.DESCRIPTOR.full_name
                    ):
                        raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                            IMPORT_rbt_v1alpha1.errors_pb2.InvalidArgument(),
                            message=
                            f"task with UUID {IMPORT_uuid.UUID(bytes=self._task_id.task_uuid)} "
                            f"has a response of type '{response_or_error.TypeName()}' "
                            "but expecting type 'rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRemoveResponse'; "
                            "are you waiting on a task of the correct method?",
                        ) from None

                    if is_error:
                        # Currently only cancelled error is supported.
                        raise IMPORT_asyncio.CancelledError("Task was cancelled by a TasksServicer")
                    else:
                        response_or_error.Unpack(response)
                        return response

            return wait_for_task().__await__()


    class RemoveAborted(IMPORT_rebootdev.aio.aborted.Aborted):


        Error = IMPORT_typing.Union[
            IMPORT_rebootdev.aio.aborted.GrpcError,
            IMPORT_rebootdev.aio.aborted.RebootError,
        ]

        METHOD_ERROR_TYPES: list[type[IMPORT_google_protobuf_message.Message]] = [
        ]

        ERROR_TYPES: list[type[IMPORT_google_protobuf_message.Message]] = (
            METHOD_ERROR_TYPES +
            IMPORT_rebootdev.aio.aborted.GRPC_ERROR_TYPES +
            IMPORT_rebootdev.aio.aborted.REBOOT_ERROR_TYPES
        )

        _error: Error
        _code: IMPORT_grpc.StatusCode
        _message: IMPORT_typing.Optional[str]

        def __init__(
            self,
            error:  IMPORT_rebootdev.aio.aborted.GrpcError,
            *,
            message: IMPORT_typing.Optional[str] = None,
            # Do not set this value when constructing in order to
            # raise. This is only used internally when constructing
            # from aborted calls.
            error_types: IMPORT_typing.Sequence[type[Error]] = (
                METHOD_ERROR_TYPES + IMPORT_rebootdev.aio.aborted.GRPC_ERROR_TYPES
            ),
        ):
            super().__init__()

            IMPORT_reboot_aio_types.assert_type(error, error_types)

            self._error = error

            code = self.grpc_status_code_from_error(self._error)

            if code is None:
                # Must be a Reboot specific or declared method error.
                code = IMPORT_grpc.StatusCode.ABORTED

            self._code = code

            self._message = message

        @property
        def error(self) -> Error:
            return self._error

        @property
        def code(self) -> IMPORT_grpc.StatusCode:
            return self._code

        @property
        def message(self) -> IMPORT_typing.Optional[str]:
            return self._message

        @classmethod
        def from_status(cls, status: IMPORT_google_rpc_status_pb2.Status):
            error = cls.error_from_google_rpc_status_details(
                status,
                cls.ERROR_TYPES,
            )

            message = status.message if len(status.message) > 0 else None

            if error is not None:
                return cls(error, message=message, error_types=cls.ERROR_TYPES)

            error = cls.error_from_google_rpc_status_code(status)

            assert error is not None

            # TODO(benh): also consider getting the type names from
            # `status.details` and including that in `message` to make
            # debugging easier.

            return cls(error, message=message)

        @classmethod
        def from_grpc_aio_rpc_error(cls, aio_rpc_error: IMPORT_grpc.aio.AioRpcError):
            return cls(
                cls.error_from_grpc_aio_rpc_error(aio_rpc_error),
                message=aio_rpc_error.details(),
            )

        @classmethod
        def is_declared_error(cls, message: IMPORT_google_protobuf_message.Message) -> bool:
            return False

    class RangeTask:
        """Represents a scheduled task running for the
        state. Note that this is not a coroutine because we are trying
        to convey the semantics that the task is already running (or
        will soon be).
        """

        @classmethod
        def retrieve(
            cls,
            context: IMPORT_reboot_aio_contexts.Context | IMPORT_reboot_aio_external.ExternalContext,
            *,
            task_id: IMPORT_rbt_v1alpha1.tasks_pb2.TaskId,
        ):
            return cls(context, task_id=task_id)

        def __init__(
            self,
            context: IMPORT_reboot_aio_contexts.Context | IMPORT_reboot_aio_external.ExternalContext,
            *,
            task_id: IMPORT_rbt_v1alpha1.tasks_pb2.TaskId,
        ) -> None:
            # Depending on the context type (inside or outside a Reboot application)
            # we may or may not know the application ID. If we don't know it, then
            # the `ExternalContext.gateway` will determine it.
            #
            # TODO: in the future we expect to support cross-application calls, in
            #       which case the developer may explicitly pass in an application ID
            #       here.
            self._application_id: IMPORT_typing.Optional[IMPORT_reboot_aio_types.ApplicationId] = None
            if isinstance(context, IMPORT_reboot_aio_contexts.Context):
                self._application_id = context.application_id
            self._channel_manager = context.channel_manager
            self._task_id = task_id

        @property
        def task_id(self) -> IMPORT_rbt_v1alpha1.tasks_pb2.TaskId:
            return self._task_id

        def __await__(self) -> IMPORT_typing.Generator[None, None, rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRangeResponse]:
            """Awaits for task to finish and returns its response."""
            async def wait_for_task() -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRangeResponse:
                channel = self._channel_manager.get_channel_to_state(
                    IMPORT_reboot_aio_types.StateTypeName(self._task_id.state_type),
                    IMPORT_reboot_aio_types.StateRef(self._task_id.state_ref),
                )

                stub = IMPORT_rbt_v1alpha1.tasks_pb2_grpc.TasksStub(channel)

                try:
                    call = IMPORT_reboot_aio_stubs.UnaryRetriedCall(
                        call=None,  # `RetriedCall` can create the call itself.
                        stub_method=stub.Wait,
                        method_name="Wait",
                        request=IMPORT_rbt_v1alpha1.tasks_pb2.WaitRequest(task_id=self._task_id),
                        metadata=IMPORT_reboot_aio_headers.Headers(
                            state_ref=IMPORT_reboot_aio_types.StateRef(self._task_id.state_ref),
                            application_id=self._application_id,
                        ).to_grpc_metadata(),
                        aborted_type=IMPORT_rebootdev.aio.aborted.SystemAborted,
                    )

                    wait_for_task_response = await call
                except IMPORT_rebootdev.aio.aborted.SystemAborted as error:
                    if error.code == IMPORT_grpc.StatusCode.NOT_FOUND:
                        raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                            IMPORT_rbt_v1alpha1.errors_pb2.UnknownTask()
                        ) from None

                    raise
                else:
                    response_or_error: IMPORT_typing.Optional[IMPORT_google_protobuf_any_pb2.Any] = None
                    is_error = False

                    if wait_for_task_response.response_or_error.WhichOneof("response_or_error") == "response":
                        response_or_error = wait_for_task_response.response_or_error.response
                    else:
                        is_error = True
                        response_or_error = wait_for_task_response.response_or_error.error

                    assert response_or_error is not None
                    assert response_or_error.TypeName() != ""

                    response = rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRangeResponse()

                    if (
                        not is_error and response_or_error.TypeName() != response.DESCRIPTOR.full_name
                    ):
                        raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                            IMPORT_rbt_v1alpha1.errors_pb2.InvalidArgument(),
                            message=
                            f"task with UUID {IMPORT_uuid.UUID(bytes=self._task_id.task_uuid)} "
                            f"has a response of type '{response_or_error.TypeName()}' "
                            "but expecting type 'rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRangeResponse'; "
                            "are you waiting on a task of the correct method?",
                        ) from None

                    if is_error:
                        # Currently only cancelled error is supported.
                        raise IMPORT_asyncio.CancelledError("Task was cancelled by a TasksServicer")
                    else:
                        response_or_error.Unpack(response)
                        return response

            return wait_for_task().__await__()


    class RangeAborted(IMPORT_rebootdev.aio.aborted.Aborted):

        MethodError = IMPORT_typing.Union[
            rbt.std.collections.ordered_map.v1.ordered_map_pb2.InvalidRangeError        ]

        Error = IMPORT_typing.Union[
            MethodError,
            IMPORT_rebootdev.aio.aborted.GrpcError,
            IMPORT_rebootdev.aio.aborted.RebootError,
        ]

        METHOD_ERROR_TYPES: list[type[IMPORT_google_protobuf_message.Message]] = [
            rbt.std.collections.ordered_map.v1.ordered_map_pb2.InvalidRangeError        ]

        ERROR_TYPES: list[type[IMPORT_google_protobuf_message.Message]] = (
            METHOD_ERROR_TYPES +
            IMPORT_rebootdev.aio.aborted.GRPC_ERROR_TYPES +
            IMPORT_rebootdev.aio.aborted.REBOOT_ERROR_TYPES
        )

        _error: Error
        _code: IMPORT_grpc.StatusCode
        _message: IMPORT_typing.Optional[str]

        def __init__(
            self,
            error: MethodError |  IMPORT_rebootdev.aio.aborted.GrpcError,
            *,
            message: IMPORT_typing.Optional[str] = None,
            # Do not set this value when constructing in order to
            # raise. This is only used internally when constructing
            # from aborted calls.
            error_types: IMPORT_typing.Sequence[type[Error]] = (
                METHOD_ERROR_TYPES + IMPORT_rebootdev.aio.aborted.GRPC_ERROR_TYPES
            ),
        ):
            super().__init__()

            IMPORT_reboot_aio_types.assert_type(error, error_types)

            self._error = error

            code = self.grpc_status_code_from_error(self._error)

            if code is None:
                # Must be a Reboot specific or declared method error.
                code = IMPORT_grpc.StatusCode.ABORTED

            self._code = code

            self._message = message

        @property
        def error(self) -> Error:
            return self._error

        @property
        def code(self) -> IMPORT_grpc.StatusCode:
            return self._code

        @property
        def message(self) -> IMPORT_typing.Optional[str]:
            return self._message

        @classmethod
        def from_status(cls, status: IMPORT_google_rpc_status_pb2.Status):
            error = cls.error_from_google_rpc_status_details(
                status,
                cls.ERROR_TYPES,
            )

            message = status.message if len(status.message) > 0 else None

            if error is not None:
                return cls(error, message=message, error_types=cls.ERROR_TYPES)

            error = cls.error_from_google_rpc_status_code(status)

            assert error is not None

            # TODO(benh): also consider getting the type names from
            # `status.details` and including that in `message` to make
            # debugging easier.

            return cls(error, message=message)

        @classmethod
        def from_grpc_aio_rpc_error(cls, aio_rpc_error: IMPORT_grpc.aio.AioRpcError):
            return cls(
                cls.error_from_grpc_aio_rpc_error(aio_rpc_error),
                message=aio_rpc_error.details(),
            )

        @classmethod
        def is_declared_error(cls, message: IMPORT_google_protobuf_message.Message) -> bool:
            if message.DESCRIPTOR.full_name == 'rbt.std.collections.ordered_map.v1.InvalidRangeError':
                return True
            return False

    class ReverseRangeTask:
        """Represents a scheduled task running for the
        state. Note that this is not a coroutine because we are trying
        to convey the semantics that the task is already running (or
        will soon be).
        """

        @classmethod
        def retrieve(
            cls,
            context: IMPORT_reboot_aio_contexts.Context | IMPORT_reboot_aio_external.ExternalContext,
            *,
            task_id: IMPORT_rbt_v1alpha1.tasks_pb2.TaskId,
        ):
            return cls(context, task_id=task_id)

        def __init__(
            self,
            context: IMPORT_reboot_aio_contexts.Context | IMPORT_reboot_aio_external.ExternalContext,
            *,
            task_id: IMPORT_rbt_v1alpha1.tasks_pb2.TaskId,
        ) -> None:
            # Depending on the context type (inside or outside a Reboot application)
            # we may or may not know the application ID. If we don't know it, then
            # the `ExternalContext.gateway` will determine it.
            #
            # TODO: in the future we expect to support cross-application calls, in
            #       which case the developer may explicitly pass in an application ID
            #       here.
            self._application_id: IMPORT_typing.Optional[IMPORT_reboot_aio_types.ApplicationId] = None
            if isinstance(context, IMPORT_reboot_aio_contexts.Context):
                self._application_id = context.application_id
            self._channel_manager = context.channel_manager
            self._task_id = task_id

        @property
        def task_id(self) -> IMPORT_rbt_v1alpha1.tasks_pb2.TaskId:
            return self._task_id

        def __await__(self) -> IMPORT_typing.Generator[None, None, rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeReverseRangeResponse]:
            """Awaits for task to finish and returns its response."""
            async def wait_for_task() -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeReverseRangeResponse:
                channel = self._channel_manager.get_channel_to_state(
                    IMPORT_reboot_aio_types.StateTypeName(self._task_id.state_type),
                    IMPORT_reboot_aio_types.StateRef(self._task_id.state_ref),
                )

                stub = IMPORT_rbt_v1alpha1.tasks_pb2_grpc.TasksStub(channel)

                try:
                    call = IMPORT_reboot_aio_stubs.UnaryRetriedCall(
                        call=None,  # `RetriedCall` can create the call itself.
                        stub_method=stub.Wait,
                        method_name="Wait",
                        request=IMPORT_rbt_v1alpha1.tasks_pb2.WaitRequest(task_id=self._task_id),
                        metadata=IMPORT_reboot_aio_headers.Headers(
                            state_ref=IMPORT_reboot_aio_types.StateRef(self._task_id.state_ref),
                            application_id=self._application_id,
                        ).to_grpc_metadata(),
                        aborted_type=IMPORT_rebootdev.aio.aborted.SystemAborted,
                    )

                    wait_for_task_response = await call
                except IMPORT_rebootdev.aio.aborted.SystemAborted as error:
                    if error.code == IMPORT_grpc.StatusCode.NOT_FOUND:
                        raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                            IMPORT_rbt_v1alpha1.errors_pb2.UnknownTask()
                        ) from None

                    raise
                else:
                    response_or_error: IMPORT_typing.Optional[IMPORT_google_protobuf_any_pb2.Any] = None
                    is_error = False

                    if wait_for_task_response.response_or_error.WhichOneof("response_or_error") == "response":
                        response_or_error = wait_for_task_response.response_or_error.response
                    else:
                        is_error = True
                        response_or_error = wait_for_task_response.response_or_error.error

                    assert response_or_error is not None
                    assert response_or_error.TypeName() != ""

                    response = rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeReverseRangeResponse()

                    if (
                        not is_error and response_or_error.TypeName() != response.DESCRIPTOR.full_name
                    ):
                        raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                            IMPORT_rbt_v1alpha1.errors_pb2.InvalidArgument(),
                            message=
                            f"task with UUID {IMPORT_uuid.UUID(bytes=self._task_id.task_uuid)} "
                            f"has a response of type '{response_or_error.TypeName()}' "
                            "but expecting type 'rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeReverseRangeResponse'; "
                            "are you waiting on a task of the correct method?",
                        ) from None

                    if is_error:
                        # Currently only cancelled error is supported.
                        raise IMPORT_asyncio.CancelledError("Task was cancelled by a TasksServicer")
                    else:
                        response_or_error.Unpack(response)
                        return response

            return wait_for_task().__await__()


    class ReverseRangeAborted(IMPORT_rebootdev.aio.aborted.Aborted):

        MethodError = IMPORT_typing.Union[
            rbt.std.collections.ordered_map.v1.ordered_map_pb2.InvalidRangeError        ]

        Error = IMPORT_typing.Union[
            MethodError,
            IMPORT_rebootdev.aio.aborted.GrpcError,
            IMPORT_rebootdev.aio.aborted.RebootError,
        ]

        METHOD_ERROR_TYPES: list[type[IMPORT_google_protobuf_message.Message]] = [
            rbt.std.collections.ordered_map.v1.ordered_map_pb2.InvalidRangeError        ]

        ERROR_TYPES: list[type[IMPORT_google_protobuf_message.Message]] = (
            METHOD_ERROR_TYPES +
            IMPORT_rebootdev.aio.aborted.GRPC_ERROR_TYPES +
            IMPORT_rebootdev.aio.aborted.REBOOT_ERROR_TYPES
        )

        _error: Error
        _code: IMPORT_grpc.StatusCode
        _message: IMPORT_typing.Optional[str]

        def __init__(
            self,
            error: MethodError |  IMPORT_rebootdev.aio.aborted.GrpcError,
            *,
            message: IMPORT_typing.Optional[str] = None,
            # Do not set this value when constructing in order to
            # raise. This is only used internally when constructing
            # from aborted calls.
            error_types: IMPORT_typing.Sequence[type[Error]] = (
                METHOD_ERROR_TYPES + IMPORT_rebootdev.aio.aborted.GRPC_ERROR_TYPES
            ),
        ):
            super().__init__()

            IMPORT_reboot_aio_types.assert_type(error, error_types)

            self._error = error

            code = self.grpc_status_code_from_error(self._error)

            if code is None:
                # Must be a Reboot specific or declared method error.
                code = IMPORT_grpc.StatusCode.ABORTED

            self._code = code

            self._message = message

        @property
        def error(self) -> Error:
            return self._error

        @property
        def code(self) -> IMPORT_grpc.StatusCode:
            return self._code

        @property
        def message(self) -> IMPORT_typing.Optional[str]:
            return self._message

        @classmethod
        def from_status(cls, status: IMPORT_google_rpc_status_pb2.Status):
            error = cls.error_from_google_rpc_status_details(
                status,
                cls.ERROR_TYPES,
            )

            message = status.message if len(status.message) > 0 else None

            if error is not None:
                return cls(error, message=message, error_types=cls.ERROR_TYPES)

            error = cls.error_from_google_rpc_status_code(status)

            assert error is not None

            # TODO(benh): also consider getting the type names from
            # `status.details` and including that in `message` to make
            # debugging easier.

            return cls(error, message=message)

        @classmethod
        def from_grpc_aio_rpc_error(cls, aio_rpc_error: IMPORT_grpc.aio.AioRpcError):
            return cls(
                cls.error_from_grpc_aio_rpc_error(aio_rpc_error),
                message=aio_rpc_error.details(),
            )

        @classmethod
        def is_declared_error(cls, message: IMPORT_google_protobuf_message.Message) -> bool:
            if message.DESCRIPTOR.full_name == 'rbt.std.collections.ordered_map.v1.InvalidRangeError':
                return True
            return False

    class StringifyTask:
        """Represents a scheduled task running for the
        state. Note that this is not a coroutine because we are trying
        to convey the semantics that the task is already running (or
        will soon be).
        """

        @classmethod
        def retrieve(
            cls,
            context: IMPORT_reboot_aio_contexts.Context | IMPORT_reboot_aio_external.ExternalContext,
            *,
            task_id: IMPORT_rbt_v1alpha1.tasks_pb2.TaskId,
        ):
            return cls(context, task_id=task_id)

        def __init__(
            self,
            context: IMPORT_reboot_aio_contexts.Context | IMPORT_reboot_aio_external.ExternalContext,
            *,
            task_id: IMPORT_rbt_v1alpha1.tasks_pb2.TaskId,
        ) -> None:
            # Depending on the context type (inside or outside a Reboot application)
            # we may or may not know the application ID. If we don't know it, then
            # the `ExternalContext.gateway` will determine it.
            #
            # TODO: in the future we expect to support cross-application calls, in
            #       which case the developer may explicitly pass in an application ID
            #       here.
            self._application_id: IMPORT_typing.Optional[IMPORT_reboot_aio_types.ApplicationId] = None
            if isinstance(context, IMPORT_reboot_aio_contexts.Context):
                self._application_id = context.application_id
            self._channel_manager = context.channel_manager
            self._task_id = task_id

        @property
        def task_id(self) -> IMPORT_rbt_v1alpha1.tasks_pb2.TaskId:
            return self._task_id

        def __await__(self) -> IMPORT_typing.Generator[None, None, rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeStringifyResponse]:
            """Awaits for task to finish and returns its response."""
            async def wait_for_task() -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeStringifyResponse:
                channel = self._channel_manager.get_channel_to_state(
                    IMPORT_reboot_aio_types.StateTypeName(self._task_id.state_type),
                    IMPORT_reboot_aio_types.StateRef(self._task_id.state_ref),
                )

                stub = IMPORT_rbt_v1alpha1.tasks_pb2_grpc.TasksStub(channel)

                try:
                    call = IMPORT_reboot_aio_stubs.UnaryRetriedCall(
                        call=None,  # `RetriedCall` can create the call itself.
                        stub_method=stub.Wait,
                        method_name="Wait",
                        request=IMPORT_rbt_v1alpha1.tasks_pb2.WaitRequest(task_id=self._task_id),
                        metadata=IMPORT_reboot_aio_headers.Headers(
                            state_ref=IMPORT_reboot_aio_types.StateRef(self._task_id.state_ref),
                            application_id=self._application_id,
                        ).to_grpc_metadata(),
                        aborted_type=IMPORT_rebootdev.aio.aborted.SystemAborted,
                    )

                    wait_for_task_response = await call
                except IMPORT_rebootdev.aio.aborted.SystemAborted as error:
                    if error.code == IMPORT_grpc.StatusCode.NOT_FOUND:
                        raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                            IMPORT_rbt_v1alpha1.errors_pb2.UnknownTask()
                        ) from None

                    raise
                else:
                    response_or_error: IMPORT_typing.Optional[IMPORT_google_protobuf_any_pb2.Any] = None
                    is_error = False

                    if wait_for_task_response.response_or_error.WhichOneof("response_or_error") == "response":
                        response_or_error = wait_for_task_response.response_or_error.response
                    else:
                        is_error = True
                        response_or_error = wait_for_task_response.response_or_error.error

                    assert response_or_error is not None
                    assert response_or_error.TypeName() != ""

                    response = rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeStringifyResponse()

                    if (
                        not is_error and response_or_error.TypeName() != response.DESCRIPTOR.full_name
                    ):
                        raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                            IMPORT_rbt_v1alpha1.errors_pb2.InvalidArgument(),
                            message=
                            f"task with UUID {IMPORT_uuid.UUID(bytes=self._task_id.task_uuid)} "
                            f"has a response of type '{response_or_error.TypeName()}' "
                            "but expecting type 'rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeStringifyResponse'; "
                            "are you waiting on a task of the correct method?",
                        ) from None

                    if is_error:
                        # Currently only cancelled error is supported.
                        raise IMPORT_asyncio.CancelledError("Task was cancelled by a TasksServicer")
                    else:
                        response_or_error.Unpack(response)
                        return response

            return wait_for_task().__await__()


    class StringifyAborted(IMPORT_rebootdev.aio.aborted.Aborted):


        Error = IMPORT_typing.Union[
            IMPORT_rebootdev.aio.aborted.GrpcError,
            IMPORT_rebootdev.aio.aborted.RebootError,
        ]

        METHOD_ERROR_TYPES: list[type[IMPORT_google_protobuf_message.Message]] = [
        ]

        ERROR_TYPES: list[type[IMPORT_google_protobuf_message.Message]] = (
            METHOD_ERROR_TYPES +
            IMPORT_rebootdev.aio.aborted.GRPC_ERROR_TYPES +
            IMPORT_rebootdev.aio.aborted.REBOOT_ERROR_TYPES
        )

        _error: Error
        _code: IMPORT_grpc.StatusCode
        _message: IMPORT_typing.Optional[str]

        def __init__(
            self,
            error:  IMPORT_rebootdev.aio.aborted.GrpcError,
            *,
            message: IMPORT_typing.Optional[str] = None,
            # Do not set this value when constructing in order to
            # raise. This is only used internally when constructing
            # from aborted calls.
            error_types: IMPORT_typing.Sequence[type[Error]] = (
                METHOD_ERROR_TYPES + IMPORT_rebootdev.aio.aborted.GRPC_ERROR_TYPES
            ),
        ):
            super().__init__()

            IMPORT_reboot_aio_types.assert_type(error, error_types)

            self._error = error

            code = self.grpc_status_code_from_error(self._error)

            if code is None:
                # Must be a Reboot specific or declared method error.
                code = IMPORT_grpc.StatusCode.ABORTED

            self._code = code

            self._message = message

        @property
        def error(self) -> Error:
            return self._error

        @property
        def code(self) -> IMPORT_grpc.StatusCode:
            return self._code

        @property
        def message(self) -> IMPORT_typing.Optional[str]:
            return self._message

        @classmethod
        def from_status(cls, status: IMPORT_google_rpc_status_pb2.Status):
            error = cls.error_from_google_rpc_status_details(
                status,
                cls.ERROR_TYPES,
            )

            message = status.message if len(status.message) > 0 else None

            if error is not None:
                return cls(error, message=message, error_types=cls.ERROR_TYPES)

            error = cls.error_from_google_rpc_status_code(status)

            assert error is not None

            # TODO(benh): also consider getting the type names from
            # `status.details` and including that in `message` to make
            # debugging easier.

            return cls(error, message=message)

        @classmethod
        def from_grpc_aio_rpc_error(cls, aio_rpc_error: IMPORT_grpc.aio.AioRpcError):
            return cls(
                cls.error_from_grpc_aio_rpc_error(aio_rpc_error),
                message=aio_rpc_error.details(),
            )

        @classmethod
        def is_declared_error(cls, message: IMPORT_google_protobuf_message.Message) -> bool:
            return False


    class WeakReference(IMPORT_typing.Generic[Node_ScheduleTypeVar]):

        _schedule_type: type[Node_ScheduleTypeVar]

        def __init__(
            self,
            # When application ID is None, refers to a state within the application given by the context.
            application_id: IMPORT_typing.Optional[IMPORT_reboot_aio_types.ApplicationId],
            state_id: IMPORT_reboot_aio_types.StateId,
            *,
            schedule_type: type[Node_ScheduleTypeVar],
            bearer_token: IMPORT_typing.Optional[str] = None,
            servicer: IMPORT_typing.Optional[NodeBaseServicer] = None,
        ):
            self._application_id = application_id
            self._state_ref = IMPORT_reboot_aio_types.StateRef.from_id(
              Node.__state_type_name__,
              state_id,
            )
            self._schedule_type = schedule_type
            self._idempotency_manager: IMPORT_typing.Optional[IMPORT_reboot_aio_idempotency.IdempotencyManager] = None
            self._reader_stub: IMPORT_typing.Optional[NodeReaderStub] = None
            self._writer_stub: IMPORT_typing.Optional[NodeWriterStub] = None
            self._workflow_stub: IMPORT_typing.Optional[NodeWorkflowStub] = None
            self._tasks_stub: IMPORT_typing.Optional[NodeTasksStub] = None
            self._bearer_token = bearer_token
            self._servicer = servicer

        @property
        def state_id(self) -> IMPORT_reboot_aio_types.StateId:
            return self._state_ref.id

        def _reader(
            self,
            context: IMPORT_reboot_aio_contexts.ReaderContext | IMPORT_reboot_aio_contexts.WriterContext | IMPORT_reboot_aio_contexts.TransactionContext | IMPORT_reboot_aio_contexts.WorkflowContext | IMPORT_reboot_aio_external.ExternalContext,
        ) -> NodeReaderStub:
            if self._reader_stub is None:
                self._reader_stub = NodeReaderStub(
                    context=context,
                    state_ref=self._state_ref,
                    bearer_token=self._bearer_token,
                )
            assert self._reader_stub is not None
            if self._idempotency_manager is None:
                self._idempotency_manager = context
            elif self._idempotency_manager != context:
                raise IMPORT_reboot_aio_call.MixedContextsError(
                    "This `WeakReference` for `Node` with ID "
                    f"'{self.state_id}' has previously been used by a "
                    "different `Context`. That is not allowed. "
                    "Instead create a new `WeakReference` for every `Context` by calling "
                    f"`Node.ref('{self.state_id}')`."
                )
            return self._reader_stub

        def _writer(
            self,
            context: IMPORT_reboot_aio_contexts.TransactionContext | IMPORT_reboot_aio_contexts.WorkflowContext | IMPORT_reboot_aio_external.ExternalContext,
        ) -> NodeWriterStub:
            if self._writer_stub is None:
                self._writer_stub = NodeWriterStub(
                    context=context,
                    state_ref=self._state_ref,
                    bearer_token=self._bearer_token,
                )
            assert self._writer_stub is not None
            if self._idempotency_manager is None:
                self._idempotency_manager = context
            elif self._idempotency_manager != context:
                raise IMPORT_reboot_aio_call.MixedContextsError(
                    "This `WeakReference` for `Node` with ID "
                    f"'{self.state_id}' has previously been used by a "
                    "different `Context`. That is not allowed. "
                    "Instead create a new `WeakReference` for every `Context` by calling "
                    f"`Node.ref('{self.state_id}')`."
                )
            return self._writer_stub

        def _workflow(
            self,
            context: IMPORT_reboot_aio_contexts.TransactionContext | IMPORT_reboot_aio_contexts.WorkflowContext | IMPORT_reboot_aio_external.ExternalContext,
        ) -> NodeWorkflowStub:
            if self._workflow_stub is None:
                self._workflow_stub = NodeWorkflowStub(
                    context=context,
                    state_ref=self._state_ref,
                    bearer_token=self._bearer_token,
                )
            assert self._workflow_stub is not None
            if self._idempotency_manager is None:
                self._idempotency_manager = context
            elif self._idempotency_manager != context:
                raise IMPORT_reboot_aio_call.MixedContextsError(
                    "This `WeakReference` for `Node` with ID "
                    f"'{self.state_id}' has previously been used by a "
                    "different `Context`. That is not allowed. "
                    "Instead create a new `WeakReference` for every `Context` by calling "
                    f"`Node.ref('{self.state_id}')`."
                )
            return self._workflow_stub

        def _tasks(
            self,
            context: IMPORT_reboot_aio_contexts.TransactionContext | IMPORT_reboot_aio_contexts.WorkflowContext | IMPORT_reboot_aio_external.ExternalContext,
        ) -> NodeTasksStub:
            if self._tasks_stub is None:
                self._tasks_stub = NodeTasksStub(
                    context=context,
                    state_ref=self._state_ref,
                    bearer_token=self._bearer_token,
                )
            assert self._tasks_stub is not None
            if self._idempotency_manager is None:
                self._idempotency_manager = context
            elif self._idempotency_manager != context:
                raise IMPORT_reboot_aio_call.MixedContextsError(
                    "This `WeakReference` for `Node` with ID "
                    f"'{self.state_id}' has previously been used by a "
                    "different `Context`. That is not allowed. "
                    "Instead create a new `WeakReference` for every `Context` by calling "
                    f"`Node.ref('{self.state_id}')`."
                )
            return self._tasks_stub

        class _Reactively:

            def __init__(
                self,
                *,
                application_id: IMPORT_typing.Optional[IMPORT_reboot_aio_types.ApplicationId],
                state_ref: IMPORT_reboot_aio_types.StateRef,
                bearer_token: IMPORT_typing.Optional[str] = None,
            ):
                self._application_id = application_id
                self._state_ref = state_ref
                self._bearer_token = bearer_token

            async def Search(
                # In methods which are dealing with user input, (i.e.,
                # proto message field names), we should use '__double_underscored__'
                # variables to avoid any potential name conflicts with the method's
                # parameters.
                # The '__self__' parameter is a convention in Python to
                # indicate that this method is a bound method, so we use
                # '__this__' instead.
                __this__,
                # Explicitly-reactive calls only make sense in the context of either...
                # (A) an external client, or...
                # (B) methods that may reasonably run for a long time, which in Reboot means: readers or workflows.
                __context__: IMPORT_reboot_aio_external.ExternalContext | IMPORT_reboot_aio_contexts.ReaderContext | IMPORT_reboot_aio_contexts.WorkflowContext,
                __options__: IMPORT_typing.Optional[IMPORT_reboot_aio_call.Options] = None,
                *,
                key: IMPORT_typing.Optional[str] = None,
            ) -> IMPORT_typing.AsyncIterator[rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeSearchResponse]:
                # UX improvement: check that neither positional argument was accidentally
                # given a gRPC request type.
                IMPORT_reboot_aio_types.assert_not_request_type(__context__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeSearchRequest)
                IMPORT_reboot_aio_types.assert_not_request_type(__options__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeSearchRequest)
                IMPORT_reboot_aio_types.assert_type(__context__, [IMPORT_reboot_aio_external.ExternalContext, IMPORT_reboot_aio_contexts.ReaderContext, IMPORT_reboot_aio_contexts.WorkflowContext])

                # TODO: mypy-protobuf declares that
                # `google.protobuf.message.Message` constructor arguments are
                # always non-None, when in reality they are optional.
                __request__ = rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeSearchRequest(
                    key=key,  # type: ignore[arg-type]
                )

                __metadata__: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None
                __bearer_token__: IMPORT_typing.Optional[str] = None
                __app_internal_authorization__: IMPORT_typing.Optional[str] = None

                if __options__ is not None:
                    IMPORT_reboot_aio_types.assert_type(__options__, [IMPORT_reboot_aio_call.Options])
                    if __options__.metadata is not None:
                        __metadata__ = __options__.metadata
                    if __options__.bearer_token is not None:
                        __bearer_token__ = __options__.bearer_token

                if __bearer_token__ is None:
                    __bearer_token__ = __this__._bearer_token
                if __bearer_token__ is None and isinstance(__context__, IMPORT_reboot_aio_external.ExternalContext):
                    # Within a Reboot context we do not pass on the caller's bearer token, as that might
                    # have security implications - we cannot simply trust any service we are calling with
                    # the user's credentials. Instead, the developer can rely on the default app-internal
                    # auth, or override that and set an explicit bearer token.
                    #
                    # In the case of `ExternalContext`, however, its `bearer_token` was set specifically
                    # by the developer for the purpose of making these calls. Note that only
                    # `ExternalContext` even has a `bearer_token` field.
                    __bearer_token__ = __context__.bearer_token

                if __metadata__ is None:
                    __metadata__ = ()

                if isinstance(__context__, IMPORT_reboot_aio_contexts.Context):
                    if __this__._application_id is None:
                        # Given our context type (inside a Reboot application) we can default to
                        # making the application send traffic to itself.
                        __this__._application_id = __context__.application_id
                        # It is safe to use app-internal auth given we know we're talking to
                        # the same application.
                        __app_internal_authorization__ = __context__._app_internal_api_key_secret

                __headers__ = IMPORT_reboot_aio_headers.Headers(
                    bearer_token=__bearer_token__,
                    state_ref=__this__._state_ref,
                    application_id=__this__._application_id,
                    app_internal_authorization=__app_internal_authorization__
                )

                __metadata__ += __headers__.to_grpc_metadata()

                __query_backoff__ = IMPORT_reboot_aio_backoff.Backoff()
                while True:
                    try:
                        async with __context__.channel_manager.get_channel_to_state(
                            IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.Node'),
                            __this__._state_ref,
                        ) as __channel__:

                            __call__ = IMPORT_rbt_v1alpha1.react_pb2_grpc.ReactStub(
                                __channel__
                            ).Query(
                                IMPORT_rbt_v1alpha1.react_pb2.QueryRequest(
                                    method='Search',
                                    request=__request__.SerializeToString(),
                                ),
                                metadata=__metadata__,
                            )

                            async for __query_response__ in __call__:
                                # Clear the backoff so we don't wait
                                # as long the next time we get
                                # disconnected.
                                __query_backoff__.clear()

                                # The backend may have sent us this query
                                # response only to let us know that a new
                                # idempotency key has been recorded; there may
                                # not be a new response. Python callers don't
                                # (currently) care about such an event, so we
                                # simply ignore it.
                                if not __query_response__.HasField("response"):
                                    continue

                                __response__ = rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeSearchResponse()
                                __response__.ParseFromString(__query_response__.response)
                                yield __response__

                    except BaseException as exception:
                        # We expect to get disconnected from the server
                        # from time to time, e.g., when it is being
                        # updated, but we don't want that error to
                        # propagate, we just want to retry.
                        if IMPORT_rebootdev.aio.aborted.is_grpc_retryable_exception(exception):
                            logger.debug(
                                "Reactive read to 'rbt.std.collections.ordered_map.v1.NodeMethods.Search' "
                                f"failed with a retryable error: '{exception}'; "
                                "will retry..."
                            )
                            await __query_backoff__()
                            continue
                        raise

            # Keep the original functions on the client, so old code will
            # continue to work, but use the new 'snake_case' method in
            # the new code.
            search = Search
            async def Range(
                # In methods which are dealing with user input, (i.e.,
                # proto message field names), we should use '__double_underscored__'
                # variables to avoid any potential name conflicts with the method's
                # parameters.
                # The '__self__' parameter is a convention in Python to
                # indicate that this method is a bound method, so we use
                # '__this__' instead.
                __this__,
                # Explicitly-reactive calls only make sense in the context of either...
                # (A) an external client, or...
                # (B) methods that may reasonably run for a long time, which in Reboot means: readers or workflows.
                __context__: IMPORT_reboot_aio_external.ExternalContext | IMPORT_reboot_aio_contexts.ReaderContext | IMPORT_reboot_aio_contexts.WorkflowContext,
                __options__: IMPORT_typing.Optional[IMPORT_reboot_aio_call.Options] = None,
                *,
                start_key: IMPORT_typing.Optional[str] = None,
                limit: IMPORT_typing.Optional[int] = None,
            ) -> IMPORT_typing.AsyncIterator[rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRangeResponse]:
                # UX improvement: check that neither positional argument was accidentally
                # given a gRPC request type.
                IMPORT_reboot_aio_types.assert_not_request_type(__context__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRangeRequest)
                IMPORT_reboot_aio_types.assert_not_request_type(__options__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRangeRequest)
                IMPORT_reboot_aio_types.assert_type(__context__, [IMPORT_reboot_aio_external.ExternalContext, IMPORT_reboot_aio_contexts.ReaderContext, IMPORT_reboot_aio_contexts.WorkflowContext])

                # TODO: mypy-protobuf declares that
                # `google.protobuf.message.Message` constructor arguments are
                # always non-None, when in reality they are optional.
                __request__ = rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRangeRequest(
                    start_key=start_key,  # type: ignore[arg-type]
                    limit=limit,  # type: ignore[arg-type]
                )

                __metadata__: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None
                __bearer_token__: IMPORT_typing.Optional[str] = None
                __app_internal_authorization__: IMPORT_typing.Optional[str] = None

                if __options__ is not None:
                    IMPORT_reboot_aio_types.assert_type(__options__, [IMPORT_reboot_aio_call.Options])
                    if __options__.metadata is not None:
                        __metadata__ = __options__.metadata
                    if __options__.bearer_token is not None:
                        __bearer_token__ = __options__.bearer_token

                if __bearer_token__ is None:
                    __bearer_token__ = __this__._bearer_token
                if __bearer_token__ is None and isinstance(__context__, IMPORT_reboot_aio_external.ExternalContext):
                    # Within a Reboot context we do not pass on the caller's bearer token, as that might
                    # have security implications - we cannot simply trust any service we are calling with
                    # the user's credentials. Instead, the developer can rely on the default app-internal
                    # auth, or override that and set an explicit bearer token.
                    #
                    # In the case of `ExternalContext`, however, its `bearer_token` was set specifically
                    # by the developer for the purpose of making these calls. Note that only
                    # `ExternalContext` even has a `bearer_token` field.
                    __bearer_token__ = __context__.bearer_token

                if __metadata__ is None:
                    __metadata__ = ()

                if isinstance(__context__, IMPORT_reboot_aio_contexts.Context):
                    if __this__._application_id is None:
                        # Given our context type (inside a Reboot application) we can default to
                        # making the application send traffic to itself.
                        __this__._application_id = __context__.application_id
                        # It is safe to use app-internal auth given we know we're talking to
                        # the same application.
                        __app_internal_authorization__ = __context__._app_internal_api_key_secret

                __headers__ = IMPORT_reboot_aio_headers.Headers(
                    bearer_token=__bearer_token__,
                    state_ref=__this__._state_ref,
                    application_id=__this__._application_id,
                    app_internal_authorization=__app_internal_authorization__
                )

                __metadata__ += __headers__.to_grpc_metadata()

                __query_backoff__ = IMPORT_reboot_aio_backoff.Backoff()
                while True:
                    try:
                        async with __context__.channel_manager.get_channel_to_state(
                            IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.Node'),
                            __this__._state_ref,
                        ) as __channel__:

                            __call__ = IMPORT_rbt_v1alpha1.react_pb2_grpc.ReactStub(
                                __channel__
                            ).Query(
                                IMPORT_rbt_v1alpha1.react_pb2.QueryRequest(
                                    method='Range',
                                    request=__request__.SerializeToString(),
                                ),
                                metadata=__metadata__,
                            )

                            async for __query_response__ in __call__:
                                # Clear the backoff so we don't wait
                                # as long the next time we get
                                # disconnected.
                                __query_backoff__.clear()

                                # The backend may have sent us this query
                                # response only to let us know that a new
                                # idempotency key has been recorded; there may
                                # not be a new response. Python callers don't
                                # (currently) care about such an event, so we
                                # simply ignore it.
                                if not __query_response__.HasField("response"):
                                    continue

                                __response__ = rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRangeResponse()
                                __response__.ParseFromString(__query_response__.response)
                                yield __response__

                    except BaseException as exception:
                        # We expect to get disconnected from the server
                        # from time to time, e.g., when it is being
                        # updated, but we don't want that error to
                        # propagate, we just want to retry.
                        if IMPORT_rebootdev.aio.aborted.is_grpc_retryable_exception(exception):
                            logger.debug(
                                "Reactive read to 'rbt.std.collections.ordered_map.v1.NodeMethods.Range' "
                                f"failed with a retryable error: '{exception}'; "
                                "will retry..."
                            )
                            await __query_backoff__()
                            continue
                        raise

            # Keep the original functions on the client, so old code will
            # continue to work, but use the new 'snake_case' method in
            # the new code.
            range = Range
            async def ReverseRange(
                # In methods which are dealing with user input, (i.e.,
                # proto message field names), we should use '__double_underscored__'
                # variables to avoid any potential name conflicts with the method's
                # parameters.
                # The '__self__' parameter is a convention in Python to
                # indicate that this method is a bound method, so we use
                # '__this__' instead.
                __this__,
                # Explicitly-reactive calls only make sense in the context of either...
                # (A) an external client, or...
                # (B) methods that may reasonably run for a long time, which in Reboot means: readers or workflows.
                __context__: IMPORT_reboot_aio_external.ExternalContext | IMPORT_reboot_aio_contexts.ReaderContext | IMPORT_reboot_aio_contexts.WorkflowContext,
                __options__: IMPORT_typing.Optional[IMPORT_reboot_aio_call.Options] = None,
                *,
                start_key: IMPORT_typing.Optional[str] = None,
                limit: IMPORT_typing.Optional[int] = None,
            ) -> IMPORT_typing.AsyncIterator[rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeReverseRangeResponse]:
                # UX improvement: check that neither positional argument was accidentally
                # given a gRPC request type.
                IMPORT_reboot_aio_types.assert_not_request_type(__context__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeReverseRangeRequest)
                IMPORT_reboot_aio_types.assert_not_request_type(__options__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeReverseRangeRequest)
                IMPORT_reboot_aio_types.assert_type(__context__, [IMPORT_reboot_aio_external.ExternalContext, IMPORT_reboot_aio_contexts.ReaderContext, IMPORT_reboot_aio_contexts.WorkflowContext])

                # TODO: mypy-protobuf declares that
                # `google.protobuf.message.Message` constructor arguments are
                # always non-None, when in reality they are optional.
                __request__ = rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeReverseRangeRequest(
                    start_key=start_key,  # type: ignore[arg-type]
                    limit=limit,  # type: ignore[arg-type]
                )

                __metadata__: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None
                __bearer_token__: IMPORT_typing.Optional[str] = None
                __app_internal_authorization__: IMPORT_typing.Optional[str] = None

                if __options__ is not None:
                    IMPORT_reboot_aio_types.assert_type(__options__, [IMPORT_reboot_aio_call.Options])
                    if __options__.metadata is not None:
                        __metadata__ = __options__.metadata
                    if __options__.bearer_token is not None:
                        __bearer_token__ = __options__.bearer_token

                if __bearer_token__ is None:
                    __bearer_token__ = __this__._bearer_token
                if __bearer_token__ is None and isinstance(__context__, IMPORT_reboot_aio_external.ExternalContext):
                    # Within a Reboot context we do not pass on the caller's bearer token, as that might
                    # have security implications - we cannot simply trust any service we are calling with
                    # the user's credentials. Instead, the developer can rely on the default app-internal
                    # auth, or override that and set an explicit bearer token.
                    #
                    # In the case of `ExternalContext`, however, its `bearer_token` was set specifically
                    # by the developer for the purpose of making these calls. Note that only
                    # `ExternalContext` even has a `bearer_token` field.
                    __bearer_token__ = __context__.bearer_token

                if __metadata__ is None:
                    __metadata__ = ()

                if isinstance(__context__, IMPORT_reboot_aio_contexts.Context):
                    if __this__._application_id is None:
                        # Given our context type (inside a Reboot application) we can default to
                        # making the application send traffic to itself.
                        __this__._application_id = __context__.application_id
                        # It is safe to use app-internal auth given we know we're talking to
                        # the same application.
                        __app_internal_authorization__ = __context__._app_internal_api_key_secret

                __headers__ = IMPORT_reboot_aio_headers.Headers(
                    bearer_token=__bearer_token__,
                    state_ref=__this__._state_ref,
                    application_id=__this__._application_id,
                    app_internal_authorization=__app_internal_authorization__
                )

                __metadata__ += __headers__.to_grpc_metadata()

                __query_backoff__ = IMPORT_reboot_aio_backoff.Backoff()
                while True:
                    try:
                        async with __context__.channel_manager.get_channel_to_state(
                            IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.Node'),
                            __this__._state_ref,
                        ) as __channel__:

                            __call__ = IMPORT_rbt_v1alpha1.react_pb2_grpc.ReactStub(
                                __channel__
                            ).Query(
                                IMPORT_rbt_v1alpha1.react_pb2.QueryRequest(
                                    method='ReverseRange',
                                    request=__request__.SerializeToString(),
                                ),
                                metadata=__metadata__,
                            )

                            async for __query_response__ in __call__:
                                # Clear the backoff so we don't wait
                                # as long the next time we get
                                # disconnected.
                                __query_backoff__.clear()

                                # The backend may have sent us this query
                                # response only to let us know that a new
                                # idempotency key has been recorded; there may
                                # not be a new response. Python callers don't
                                # (currently) care about such an event, so we
                                # simply ignore it.
                                if not __query_response__.HasField("response"):
                                    continue

                                __response__ = rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeReverseRangeResponse()
                                __response__.ParseFromString(__query_response__.response)
                                yield __response__

                    except BaseException as exception:
                        # We expect to get disconnected from the server
                        # from time to time, e.g., when it is being
                        # updated, but we don't want that error to
                        # propagate, we just want to retry.
                        if IMPORT_rebootdev.aio.aborted.is_grpc_retryable_exception(exception):
                            logger.debug(
                                "Reactive read to 'rbt.std.collections.ordered_map.v1.NodeMethods.ReverseRange' "
                                f"failed with a retryable error: '{exception}'; "
                                "will retry..."
                            )
                            await __query_backoff__()
                            continue
                        raise

            # Keep the original functions on the client, so old code will
            # continue to work, but use the new 'snake_case' method in
            # the new code.
            reverse_range = ReverseRange
            async def Stringify(
                # In methods which are dealing with user input, (i.e.,
                # proto message field names), we should use '__double_underscored__'
                # variables to avoid any potential name conflicts with the method's
                # parameters.
                # The '__self__' parameter is a convention in Python to
                # indicate that this method is a bound method, so we use
                # '__this__' instead.
                __this__,
                # Explicitly-reactive calls only make sense in the context of either...
                # (A) an external client, or...
                # (B) methods that may reasonably run for a long time, which in Reboot means: readers or workflows.
                __context__: IMPORT_reboot_aio_external.ExternalContext | IMPORT_reboot_aio_contexts.ReaderContext | IMPORT_reboot_aio_contexts.WorkflowContext,
                __options__: IMPORT_typing.Optional[IMPORT_reboot_aio_call.Options] = None,
                *,
                level: IMPORT_typing.Optional[int] = None,
            ) -> IMPORT_typing.AsyncIterator[rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeStringifyResponse]:
                # UX improvement: check that neither positional argument was accidentally
                # given a gRPC request type.
                IMPORT_reboot_aio_types.assert_not_request_type(__context__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeStringifyRequest)
                IMPORT_reboot_aio_types.assert_not_request_type(__options__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeStringifyRequest)
                IMPORT_reboot_aio_types.assert_type(__context__, [IMPORT_reboot_aio_external.ExternalContext, IMPORT_reboot_aio_contexts.ReaderContext, IMPORT_reboot_aio_contexts.WorkflowContext])

                # TODO: mypy-protobuf declares that
                # `google.protobuf.message.Message` constructor arguments are
                # always non-None, when in reality they are optional.
                __request__ = rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeStringifyRequest(
                    level=level,  # type: ignore[arg-type]
                )

                __metadata__: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None
                __bearer_token__: IMPORT_typing.Optional[str] = None
                __app_internal_authorization__: IMPORT_typing.Optional[str] = None

                if __options__ is not None:
                    IMPORT_reboot_aio_types.assert_type(__options__, [IMPORT_reboot_aio_call.Options])
                    if __options__.metadata is not None:
                        __metadata__ = __options__.metadata
                    if __options__.bearer_token is not None:
                        __bearer_token__ = __options__.bearer_token

                if __bearer_token__ is None:
                    __bearer_token__ = __this__._bearer_token
                if __bearer_token__ is None and isinstance(__context__, IMPORT_reboot_aio_external.ExternalContext):
                    # Within a Reboot context we do not pass on the caller's bearer token, as that might
                    # have security implications - we cannot simply trust any service we are calling with
                    # the user's credentials. Instead, the developer can rely on the default app-internal
                    # auth, or override that and set an explicit bearer token.
                    #
                    # In the case of `ExternalContext`, however, its `bearer_token` was set specifically
                    # by the developer for the purpose of making these calls. Note that only
                    # `ExternalContext` even has a `bearer_token` field.
                    __bearer_token__ = __context__.bearer_token

                if __metadata__ is None:
                    __metadata__ = ()

                if isinstance(__context__, IMPORT_reboot_aio_contexts.Context):
                    if __this__._application_id is None:
                        # Given our context type (inside a Reboot application) we can default to
                        # making the application send traffic to itself.
                        __this__._application_id = __context__.application_id
                        # It is safe to use app-internal auth given we know we're talking to
                        # the same application.
                        __app_internal_authorization__ = __context__._app_internal_api_key_secret

                __headers__ = IMPORT_reboot_aio_headers.Headers(
                    bearer_token=__bearer_token__,
                    state_ref=__this__._state_ref,
                    application_id=__this__._application_id,
                    app_internal_authorization=__app_internal_authorization__
                )

                __metadata__ += __headers__.to_grpc_metadata()

                __query_backoff__ = IMPORT_reboot_aio_backoff.Backoff()
                while True:
                    try:
                        async with __context__.channel_manager.get_channel_to_state(
                            IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.Node'),
                            __this__._state_ref,
                        ) as __channel__:

                            __call__ = IMPORT_rbt_v1alpha1.react_pb2_grpc.ReactStub(
                                __channel__
                            ).Query(
                                IMPORT_rbt_v1alpha1.react_pb2.QueryRequest(
                                    method='Stringify',
                                    request=__request__.SerializeToString(),
                                ),
                                metadata=__metadata__,
                            )

                            async for __query_response__ in __call__:
                                # Clear the backoff so we don't wait
                                # as long the next time we get
                                # disconnected.
                                __query_backoff__.clear()

                                # The backend may have sent us this query
                                # response only to let us know that a new
                                # idempotency key has been recorded; there may
                                # not be a new response. Python callers don't
                                # (currently) care about such an event, so we
                                # simply ignore it.
                                if not __query_response__.HasField("response"):
                                    continue

                                __response__ = rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeStringifyResponse()
                                __response__.ParseFromString(__query_response__.response)
                                yield __response__

                    except BaseException as exception:
                        # We expect to get disconnected from the server
                        # from time to time, e.g., when it is being
                        # updated, but we don't want that error to
                        # propagate, we just want to retry.
                        if IMPORT_rebootdev.aio.aborted.is_grpc_retryable_exception(exception):
                            logger.debug(
                                "Reactive read to 'rbt.std.collections.ordered_map.v1.NodeMethods.Stringify' "
                                f"failed with a retryable error: '{exception}'; "
                                "will retry..."
                            )
                            await __query_backoff__()
                            continue
                        raise

            # Keep the original functions on the client, so old code will
            # continue to work, but use the new 'snake_case' method in
            # the new code.
            stringify = Stringify

        def reactively(self):
            return Node.WeakReference._Reactively(
                application_id=self._application_id,
                state_ref=self._state_ref,
                bearer_token=self._bearer_token,
            )

        class _Idempotently(IMPORT_typing.Generic[Node_IdempotentlyScheduleTypeVar]):

            _weak_reference: Node.WeakReference[Node_IdempotentlyScheduleTypeVar]

            def __init__(
                self,
                *,
                weak_reference: Node.WeakReference[Node_IdempotentlyScheduleTypeVar],
                idempotency: IMPORT_reboot_aio_idempotency.Idempotency,
            ):
                self._weak_reference = weak_reference
                self._idempotency = idempotency

            def schedule(
                self,
                *,
                when: IMPORT_typing.Optional[IMPORT_datetime_datetime | IMPORT_datetime_timedelta] = None,
            ) -> Node_IdempotentlyScheduleTypeVar:
                return self._weak_reference._schedule_type(
                    self._weak_reference._application_id,
                    self._weak_reference._tasks,
                    when=when,
                    idempotency=self._idempotency,
                )

            def spawn(
                self,
                *,
                when: IMPORT_typing.Optional[IMPORT_datetime_datetime | IMPORT_datetime_timedelta] = None,
            ) -> Node.WeakReference._Spawn:
                return Node.WeakReference._Spawn(
                    self._weak_reference._application_id,
                    self._weak_reference._tasks,
                    when=when,
                    idempotency=self._idempotency,
                )

            async def read(
                self,
                context: IMPORT_reboot_aio_contexts.WorkflowContext,
            ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node:
                if self._weak_reference._servicer is None:
                    raise RuntimeError(
                        "`read()` is currently only supported within workflows; "
                        "Please reach out and let us know your use case if this "
                        "is important for you!"
                    )

                return await NodeBaseServicer.WorkflowState._Idempotently._read(
                    self._weak_reference._servicer,
                    self._idempotency,
                    context,
                )

            @IMPORT_typing.overload
            async def write(
                self,
                context: IMPORT_reboot_aio_contexts.WorkflowContext,
                writer: NodeBaseServicer.InlineWriterCallable[None],
                *,
                type: type = type(None),
            ) -> None:
                ...

            @IMPORT_typing.overload
            async def write(
                self,
                context: IMPORT_reboot_aio_contexts.WorkflowContext,
                writer: NodeBaseServicer.InlineWriterCallable[NodeBaseServicer.InlineWriterCallableResult],
                *,
                type: type[NodeBaseServicer.InlineWriterCallableResult],
            ) -> NodeBaseServicer.InlineWriterCallableResult:
                ...

            async def write(
                self,
                context: IMPORT_reboot_aio_contexts.WorkflowContext,
                writer: NodeBaseServicer.InlineWriterCallable[NodeBaseServicer.InlineWriterCallableResult],
                *,
                type: type = type(None),
            ) -> NodeBaseServicer.InlineWriterCallableResult:
                if self._weak_reference._servicer is None:
                    raise RuntimeError(
                        "`write()` is currently only supported within workflows; "
                        "Please reach out and let us know your use case if this "
                        "is important for you!"
                    )

                return await NodeBaseServicer.WorkflowState._Idempotently._write_validating_effects(
                    self._weak_reference._servicer,
                    self._idempotency,
                    context,
                    writer,
                    type_result=type,
                    check_type=not self._idempotency.always,
                    unidempotently=self._idempotency.always,
                    checkpoint=context.checkpoint(),
                )

            async def Create(
                # In methods which are dealing with user input, (i.e.,
                # proto message field names), we should use '__double_underscored__'
                # variables to avoid any potential name conflicts with the method's
                # parameters.
                # The '__self__' parameter is a convention in Python to
                # indicate that this method is a bound method, so we use
                # '__this__' instead.
                __this__,
                __context__: IMPORT_reboot_aio_contexts.TransactionContext | IMPORT_reboot_aio_contexts.WorkflowContext | IMPORT_reboot_aio_external.ExternalContext,
                __options__: IMPORT_reboot_aio_call.Options = IMPORT_reboot_aio_call.Options(),
                *,
                order: IMPORT_typing.Optional[int] = None,
                is_leaf: IMPORT_typing.Optional[bool] = None,
                keys: IMPORT_typing.Optional[IMPORT_typing.Iterable[str]] = None,
                children_ids: IMPORT_typing.Optional[IMPORT_typing.Iterable[str]] = None,
                values: IMPORT_typing.Optional[IMPORT_typing.Iterable[bytes]] = None,
                next_id: IMPORT_typing.Optional[str] = None,
                prev_id: IMPORT_typing.Optional[str] = None,
            ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateResponse:
                # UX improvement: check that neither positional argument was accidentally
                # given a gRPC request type.
                IMPORT_reboot_aio_types.assert_not_request_type(__context__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateRequest)
                IMPORT_reboot_aio_types.assert_not_request_type(__options__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateRequest)

                IMPORT_reboot_aio_types.assert_type(__options__, [IMPORT_reboot_aio_call.Options])
                if __options__.idempotency is not None:
                    raise RuntimeError(
                        'Found redundant idempotency in `Options`'
                    )

                __options__ = IMPORT_dataclasses.replace(
                    __options__,
                    idempotency_key=__this__._idempotency.key,
                    idempotency_alias=__this__._idempotency.alias,
                    generate_idempotency=__this__._idempotency.generate,
                    generated_idempotency=__this__._idempotency.generated,
                )

                return await __this__._weak_reference.Create(
                    __context__,
                    __options__,
                    order=order,
                    is_leaf=is_leaf,
                    keys=keys,
                    children_ids=children_ids,
                    values=values,
                    next_id=next_id,
                    prev_id=prev_id,
                )

            # Keep the original functions on the client, so old code will
            # continue to work, but use the new 'snake_case' method in
            # the new code.
            create = Create
            async def Search(
                # In methods which are dealing with user input, (i.e.,
                # proto message field names), we should use '__double_underscored__'
                # variables to avoid any potential name conflicts with the method's
                # parameters.
                # The '__self__' parameter is a convention in Python to
                # indicate that this method is a bound method, so we use
                # '__this__' instead.
                __this__,
                __context__: IMPORT_reboot_aio_contexts.TransactionContext | IMPORT_reboot_aio_contexts.WorkflowContext | IMPORT_reboot_aio_external.ExternalContext,
                __options__: IMPORT_reboot_aio_call.Options = IMPORT_reboot_aio_call.Options(),
                *,
                key: IMPORT_typing.Optional[str] = None,
            ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeSearchResponse:
                # UX improvement: check that neither positional argument was accidentally
                # given a gRPC request type.
                IMPORT_reboot_aio_types.assert_not_request_type(__context__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeSearchRequest)
                IMPORT_reboot_aio_types.assert_not_request_type(__options__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeSearchRequest)

                IMPORT_reboot_aio_types.assert_type(__options__, [IMPORT_reboot_aio_call.Options])
                if __options__.idempotency is not None:
                    raise RuntimeError(
                        'Found redundant idempotency in `Options`'
                    )

                __options__ = IMPORT_dataclasses.replace(
                    __options__,
                    idempotency_key=__this__._idempotency.key,
                    idempotency_alias=__this__._idempotency.alias,
                    generate_idempotency=__this__._idempotency.generate,
                    generated_idempotency=__this__._idempotency.generated,
                )

                return await __this__._weak_reference.Search(
                    __context__,
                    __options__,
                    key=key,
                )

            # Keep the original functions on the client, so old code will
            # continue to work, but use the new 'snake_case' method in
            # the new code.
            search = Search
            async def Insert(
                # In methods which are dealing with user input, (i.e.,
                # proto message field names), we should use '__double_underscored__'
                # variables to avoid any potential name conflicts with the method's
                # parameters.
                # The '__self__' parameter is a convention in Python to
                # indicate that this method is a bound method, so we use
                # '__this__' instead.
                __this__,
                __context__: IMPORT_reboot_aio_contexts.TransactionContext | IMPORT_reboot_aio_contexts.WorkflowContext | IMPORT_reboot_aio_external.ExternalContext,
                __options__: IMPORT_reboot_aio_call.Options = IMPORT_reboot_aio_call.Options(),
                *,
                key: IMPORT_typing.Optional[str] = None,
                value: IMPORT_typing.Optional[bytes] = None,
            ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeInsertResponse:
                # UX improvement: check that neither positional argument was accidentally
                # given a gRPC request type.
                IMPORT_reboot_aio_types.assert_not_request_type(__context__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeInsertRequest)
                IMPORT_reboot_aio_types.assert_not_request_type(__options__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeInsertRequest)

                IMPORT_reboot_aio_types.assert_type(__options__, [IMPORT_reboot_aio_call.Options])
                if __options__.idempotency is not None:
                    raise RuntimeError(
                        'Found redundant idempotency in `Options`'
                    )

                __options__ = IMPORT_dataclasses.replace(
                    __options__,
                    idempotency_key=__this__._idempotency.key,
                    idempotency_alias=__this__._idempotency.alias,
                    generate_idempotency=__this__._idempotency.generate,
                    generated_idempotency=__this__._idempotency.generated,
                )

                return await __this__._weak_reference.Insert(
                    __context__,
                    __options__,
                    key=key,
                    value=value,
                )

            # Keep the original functions on the client, so old code will
            # continue to work, but use the new 'snake_case' method in
            # the new code.
            insert = Insert
            async def Remove(
                # In methods which are dealing with user input, (i.e.,
                # proto message field names), we should use '__double_underscored__'
                # variables to avoid any potential name conflicts with the method's
                # parameters.
                # The '__self__' parameter is a convention in Python to
                # indicate that this method is a bound method, so we use
                # '__this__' instead.
                __this__,
                __context__: IMPORT_reboot_aio_contexts.TransactionContext | IMPORT_reboot_aio_contexts.WorkflowContext | IMPORT_reboot_aio_external.ExternalContext,
                __options__: IMPORT_reboot_aio_call.Options = IMPORT_reboot_aio_call.Options(),
                *,
                key: IMPORT_typing.Optional[str] = None,
            ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRemoveResponse:
                # UX improvement: check that neither positional argument was accidentally
                # given a gRPC request type.
                IMPORT_reboot_aio_types.assert_not_request_type(__context__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRemoveRequest)
                IMPORT_reboot_aio_types.assert_not_request_type(__options__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRemoveRequest)

                IMPORT_reboot_aio_types.assert_type(__options__, [IMPORT_reboot_aio_call.Options])
                if __options__.idempotency is not None:
                    raise RuntimeError(
                        'Found redundant idempotency in `Options`'
                    )

                __options__ = IMPORT_dataclasses.replace(
                    __options__,
                    idempotency_key=__this__._idempotency.key,
                    idempotency_alias=__this__._idempotency.alias,
                    generate_idempotency=__this__._idempotency.generate,
                    generated_idempotency=__this__._idempotency.generated,
                )

                return await __this__._weak_reference.Remove(
                    __context__,
                    __options__,
                    key=key,
                )

            # Keep the original functions on the client, so old code will
            # continue to work, but use the new 'snake_case' method in
            # the new code.
            remove = Remove
            async def Range(
                # In methods which are dealing with user input, (i.e.,
                # proto message field names), we should use '__double_underscored__'
                # variables to avoid any potential name conflicts with the method's
                # parameters.
                # The '__self__' parameter is a convention in Python to
                # indicate that this method is a bound method, so we use
                # '__this__' instead.
                __this__,
                __context__: IMPORT_reboot_aio_contexts.TransactionContext | IMPORT_reboot_aio_contexts.WorkflowContext | IMPORT_reboot_aio_external.ExternalContext,
                __options__: IMPORT_reboot_aio_call.Options = IMPORT_reboot_aio_call.Options(),
                *,
                start_key: IMPORT_typing.Optional[str] = None,
                limit: IMPORT_typing.Optional[int] = None,
            ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRangeResponse:
                # UX improvement: check that neither positional argument was accidentally
                # given a gRPC request type.
                IMPORT_reboot_aio_types.assert_not_request_type(__context__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRangeRequest)
                IMPORT_reboot_aio_types.assert_not_request_type(__options__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRangeRequest)

                IMPORT_reboot_aio_types.assert_type(__options__, [IMPORT_reboot_aio_call.Options])
                if __options__.idempotency is not None:
                    raise RuntimeError(
                        'Found redundant idempotency in `Options`'
                    )

                __options__ = IMPORT_dataclasses.replace(
                    __options__,
                    idempotency_key=__this__._idempotency.key,
                    idempotency_alias=__this__._idempotency.alias,
                    generate_idempotency=__this__._idempotency.generate,
                    generated_idempotency=__this__._idempotency.generated,
                )

                return await __this__._weak_reference.Range(
                    __context__,
                    __options__,
                    start_key=start_key,
                    limit=limit,
                )

            # Keep the original functions on the client, so old code will
            # continue to work, but use the new 'snake_case' method in
            # the new code.
            range = Range
            async def ReverseRange(
                # In methods which are dealing with user input, (i.e.,
                # proto message field names), we should use '__double_underscored__'
                # variables to avoid any potential name conflicts with the method's
                # parameters.
                # The '__self__' parameter is a convention in Python to
                # indicate that this method is a bound method, so we use
                # '__this__' instead.
                __this__,
                __context__: IMPORT_reboot_aio_contexts.TransactionContext | IMPORT_reboot_aio_contexts.WorkflowContext | IMPORT_reboot_aio_external.ExternalContext,
                __options__: IMPORT_reboot_aio_call.Options = IMPORT_reboot_aio_call.Options(),
                *,
                start_key: IMPORT_typing.Optional[str] = None,
                limit: IMPORT_typing.Optional[int] = None,
            ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeReverseRangeResponse:
                # UX improvement: check that neither positional argument was accidentally
                # given a gRPC request type.
                IMPORT_reboot_aio_types.assert_not_request_type(__context__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeReverseRangeRequest)
                IMPORT_reboot_aio_types.assert_not_request_type(__options__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeReverseRangeRequest)

                IMPORT_reboot_aio_types.assert_type(__options__, [IMPORT_reboot_aio_call.Options])
                if __options__.idempotency is not None:
                    raise RuntimeError(
                        'Found redundant idempotency in `Options`'
                    )

                __options__ = IMPORT_dataclasses.replace(
                    __options__,
                    idempotency_key=__this__._idempotency.key,
                    idempotency_alias=__this__._idempotency.alias,
                    generate_idempotency=__this__._idempotency.generate,
                    generated_idempotency=__this__._idempotency.generated,
                )

                return await __this__._weak_reference.ReverseRange(
                    __context__,
                    __options__,
                    start_key=start_key,
                    limit=limit,
                )

            # Keep the original functions on the client, so old code will
            # continue to work, but use the new 'snake_case' method in
            # the new code.
            reverse_range = ReverseRange
            async def Stringify(
                # In methods which are dealing with user input, (i.e.,
                # proto message field names), we should use '__double_underscored__'
                # variables to avoid any potential name conflicts with the method's
                # parameters.
                # The '__self__' parameter is a convention in Python to
                # indicate that this method is a bound method, so we use
                # '__this__' instead.
                __this__,
                __context__: IMPORT_reboot_aio_contexts.TransactionContext | IMPORT_reboot_aio_contexts.WorkflowContext | IMPORT_reboot_aio_external.ExternalContext,
                __options__: IMPORT_reboot_aio_call.Options = IMPORT_reboot_aio_call.Options(),
                *,
                level: IMPORT_typing.Optional[int] = None,
            ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeStringifyResponse:
                # UX improvement: check that neither positional argument was accidentally
                # given a gRPC request type.
                IMPORT_reboot_aio_types.assert_not_request_type(__context__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeStringifyRequest)
                IMPORT_reboot_aio_types.assert_not_request_type(__options__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeStringifyRequest)

                IMPORT_reboot_aio_types.assert_type(__options__, [IMPORT_reboot_aio_call.Options])
                if __options__.idempotency is not None:
                    raise RuntimeError(
                        'Found redundant idempotency in `Options`'
                    )

                __options__ = IMPORT_dataclasses.replace(
                    __options__,
                    idempotency_key=__this__._idempotency.key,
                    idempotency_alias=__this__._idempotency.alias,
                    generate_idempotency=__this__._idempotency.generate,
                    generated_idempotency=__this__._idempotency.generated,
                )

                return await __this__._weak_reference.Stringify(
                    __context__,
                    __options__,
                    level=level,
                )

            # Keep the original functions on the client, so old code will
            # continue to work, but use the new 'snake_case' method in
            # the new code.
            stringify = Stringify

        @IMPORT_typing.overload
        def idempotently(self, alias: IMPORT_typing.Optional[str] = None, *, each_iteration: bool = False) -> Node.WeakReference._Idempotently[Node_ScheduleTypeVar]:
            ...

        @IMPORT_typing.overload
        def idempotently(self, *, key: IMPORT_uuid.UUID, generated: bool = False) -> Node.WeakReference._Idempotently[Node_ScheduleTypeVar]:
            ...

        def idempotently(
            self,
            alias: IMPORT_typing.Optional[str] = None,
            *,
            key: IMPORT_typing.Optional[IMPORT_uuid.UUID] = None,
            each_iteration: IMPORT_typing.Optional[bool] = None,
            generated: bool = False,
        ) -> Node.WeakReference._Idempotently[Node_ScheduleTypeVar]:
            return Node.WeakReference._Idempotently(
                weak_reference=self,
                idempotency=IMPORT_reboot_aio_contexts.Context.idempotency(
                    alias=alias,
                    key=key,
                    each_iteration=each_iteration,
                    generated=generated,
                )
            )

        def per_workflow(self, alias: IMPORT_typing.Optional[str] = None):
            return self.idempotently(alias)

        def per_iteration(self, alias: IMPORT_typing.Optional[str] = None):
            return self.idempotently(alias, each_iteration=True)

        def always(self):
            return self.idempotently(key=IMPORT_uuid.uuid4(), generated=True)

        class _UntilChangesSatisfies(IMPORT_typing.Generic[Node_UntilCallableType]):

            _idempotency_alias: str
            _context: IMPORT_reboot_aio_contexts.WorkflowContext
            _callable: IMPORT_typing.Callable[[], IMPORT_typing.Awaitable[Node_UntilCallableType]]
            _type: type[Node_UntilCallableType]

            def __init__(
                self,
                *,
                idempotency_alias: str,
                context: IMPORT_reboot_aio_contexts.WorkflowContext,
                callable: IMPORT_typing.Callable[[], IMPORT_typing.Awaitable[Node_UntilCallableType]],
                type: type[Node_UntilCallableType],
            ):
                self._idempotency_alias = idempotency_alias
                self._context = context
                self._callable = callable
                self._type = type

            async def changes(self):
                return await IMPORT_reboot_aio_workflows.until_changes(
                    self._idempotency_alias,
                    self._context,
                    self._callable,
                    type=self._type,
                )

            async def satisfies(
                self,
                condition: IMPORT_typing.Callable[[Node_UntilCallableType], bool],
            ):

                async def converge():
                    response = await self._callable()
                    if condition(response):
                        return response
                    return False

                return await IMPORT_reboot_aio_workflows.until(
                    self._idempotency_alias,
                    self._context,
                    converge,
                    type=self._type,
                )

        class _Until:

            _weak_reference: Node.WeakReference
            _idempotency_alias: str

            def __init__(
                self,
                *,
                weak_reference: Node.WeakReference,
                idempotency_alias: str,
            ):
                self._weak_reference = weak_reference
                self._idempotency_alias = idempotency_alias

            def read(
                self,
                context: IMPORT_reboot_aio_contexts.WorkflowContext,
            ) -> Node.WeakReference._UntilChangesSatisfies[rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node]:
                IMPORT_reboot_aio_types.assert_type(
                    context,
                    [IMPORT_reboot_aio_contexts.WorkflowContext],
                )

                async def callable():
                    return await self._weak_reference.read(context)

                return Node.WeakReference._UntilChangesSatisfies(
                    idempotency_alias=self._idempotency_alias,
                    context=context,
                    callable=callable,
                    type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node,
                )

            def Search(
                # In methods which are dealing with user input, (i.e.,
                # proto message field names), we should use '__double_underscored__'
                # variables to avoid any potential name conflicts with the method's
                # parameters.
                # The '__self__' parameter is a convention in Python to
                # indicate that this method is a bound method, so we use
                # '__this__' instead.
                __this__,
                __context__: IMPORT_reboot_aio_contexts.WorkflowContext,
                __options__: IMPORT_reboot_aio_call.Options = IMPORT_reboot_aio_call.Options(),
                *,
                key: IMPORT_typing.Optional[str] = None,
            ) -> Node.WeakReference._UntilChangesSatisfies[rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeSearchResponse]:
                IMPORT_reboot_aio_types.assert_type(
                    __context__,
                    [IMPORT_reboot_aio_contexts.WorkflowContext],
                )

                async def callable():
                    return await __this__._weak_reference.Search(
                        __context__,
                        __options__,
                        key=key,
                    )

                return Node.WeakReference._UntilChangesSatisfies(
                    idempotency_alias=__this__._idempotency_alias,
                    context=__context__,
                    callable=callable,
                    type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeSearchResponse,
                )

            # Keep the original functions on the client, so old code will
            # continue to work, but use the new 'snake_case' method in
            # the new code.
            search = Search
            def Range(
                # In methods which are dealing with user input, (i.e.,
                # proto message field names), we should use '__double_underscored__'
                # variables to avoid any potential name conflicts with the method's
                # parameters.
                # The '__self__' parameter is a convention in Python to
                # indicate that this method is a bound method, so we use
                # '__this__' instead.
                __this__,
                __context__: IMPORT_reboot_aio_contexts.WorkflowContext,
                __options__: IMPORT_reboot_aio_call.Options = IMPORT_reboot_aio_call.Options(),
                *,
                start_key: IMPORT_typing.Optional[str] = None,
                limit: IMPORT_typing.Optional[int] = None,
            ) -> Node.WeakReference._UntilChangesSatisfies[rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRangeResponse]:
                IMPORT_reboot_aio_types.assert_type(
                    __context__,
                    [IMPORT_reboot_aio_contexts.WorkflowContext],
                )

                async def callable():
                    return await __this__._weak_reference.Range(
                        __context__,
                        __options__,
                        start_key=start_key,
                        limit=limit,
                    )

                return Node.WeakReference._UntilChangesSatisfies(
                    idempotency_alias=__this__._idempotency_alias,
                    context=__context__,
                    callable=callable,
                    type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRangeResponse,
                )

            # Keep the original functions on the client, so old code will
            # continue to work, but use the new 'snake_case' method in
            # the new code.
            range = Range
            def ReverseRange(
                # In methods which are dealing with user input, (i.e.,
                # proto message field names), we should use '__double_underscored__'
                # variables to avoid any potential name conflicts with the method's
                # parameters.
                # The '__self__' parameter is a convention in Python to
                # indicate that this method is a bound method, so we use
                # '__this__' instead.
                __this__,
                __context__: IMPORT_reboot_aio_contexts.WorkflowContext,
                __options__: IMPORT_reboot_aio_call.Options = IMPORT_reboot_aio_call.Options(),
                *,
                start_key: IMPORT_typing.Optional[str] = None,
                limit: IMPORT_typing.Optional[int] = None,
            ) -> Node.WeakReference._UntilChangesSatisfies[rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeReverseRangeResponse]:
                IMPORT_reboot_aio_types.assert_type(
                    __context__,
                    [IMPORT_reboot_aio_contexts.WorkflowContext],
                )

                async def callable():
                    return await __this__._weak_reference.ReverseRange(
                        __context__,
                        __options__,
                        start_key=start_key,
                        limit=limit,
                    )

                return Node.WeakReference._UntilChangesSatisfies(
                    idempotency_alias=__this__._idempotency_alias,
                    context=__context__,
                    callable=callable,
                    type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeReverseRangeResponse,
                )

            # Keep the original functions on the client, so old code will
            # continue to work, but use the new 'snake_case' method in
            # the new code.
            reverse_range = ReverseRange
            def Stringify(
                # In methods which are dealing with user input, (i.e.,
                # proto message field names), we should use '__double_underscored__'
                # variables to avoid any potential name conflicts with the method's
                # parameters.
                # The '__self__' parameter is a convention in Python to
                # indicate that this method is a bound method, so we use
                # '__this__' instead.
                __this__,
                __context__: IMPORT_reboot_aio_contexts.WorkflowContext,
                __options__: IMPORT_reboot_aio_call.Options = IMPORT_reboot_aio_call.Options(),
                *,
                level: IMPORT_typing.Optional[int] = None,
            ) -> Node.WeakReference._UntilChangesSatisfies[rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeStringifyResponse]:
                IMPORT_reboot_aio_types.assert_type(
                    __context__,
                    [IMPORT_reboot_aio_contexts.WorkflowContext],
                )

                async def callable():
                    return await __this__._weak_reference.Stringify(
                        __context__,
                        __options__,
                        level=level,
                    )

                return Node.WeakReference._UntilChangesSatisfies(
                    idempotency_alias=__this__._idempotency_alias,
                    context=__context__,
                    callable=callable,
                    type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeStringifyResponse,
                )

            # Keep the original functions on the client, so old code will
            # continue to work, but use the new 'snake_case' method in
            # the new code.
            stringify = Stringify

        def until(self, alias: str):
            return Node.WeakReference._Until(
                weak_reference=self,
                idempotency_alias=alias,
            )

        def schedule(
            self,
            *,
            when: IMPORT_typing.Optional[IMPORT_datetime_datetime | IMPORT_datetime_timedelta] = None,
        ) -> Node_ScheduleTypeVar:
            return self._schedule_type(self._application_id, self._tasks, when=when)

        class _Schedule:

            def __init__(
                self,
                application_id: IMPORT_typing.Optional[IMPORT_reboot_aio_types.ApplicationId],
                tasks: IMPORT_typing.Callable[[IMPORT_reboot_aio_contexts.TransactionContext | IMPORT_reboot_aio_contexts.WorkflowContext | IMPORT_reboot_aio_external.ExternalContext], NodeTasksStub],
                *,
                when: IMPORT_typing.Optional[IMPORT_datetime_datetime | IMPORT_datetime_timedelta] = None,
                idempotency: IMPORT_typing.Optional[IMPORT_reboot_aio_idempotency.Idempotency] = None,
            ) -> None:
                self._application_id = application_id
                self._tasks = tasks
                self._when = ensure_has_timezone(when=when)
                self._idempotency = idempotency

            # Node callable tasks:
            async def Create(
                __this__,
                __context__: IMPORT_reboot_aio_contexts.TransactionContext,
                __options__: IMPORT_typing.Optional[IMPORT_reboot_aio_call.Options] = None,
                *,
                order: IMPORT_typing.Optional[int] = None,
                is_leaf: IMPORT_typing.Optional[bool] = None,
                keys: IMPORT_typing.Optional[IMPORT_typing.Iterable[str]] = None,
                children_ids: IMPORT_typing.Optional[IMPORT_typing.Iterable[str]] = None,
                values: IMPORT_typing.Optional[IMPORT_typing.Iterable[bytes]] = None,
                next_id: IMPORT_typing.Optional[str] = None,
                prev_id: IMPORT_typing.Optional[str] = None,
            ) -> IMPORT_rbt_v1alpha1.tasks_pb2.TaskId:
                # UX improvement: check that neither positional argument was accidentally
                # given a gRPC request type.
                IMPORT_reboot_aio_types.assert_not_request_type(__context__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateRequest)
                IMPORT_reboot_aio_types.assert_not_request_type(__options__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateRequest)
                IMPORT_reboot_aio_types.assert_type(__context__, [IMPORT_reboot_aio_contexts.TransactionContext])

                if order is not None and not isinstance(
                    order,
                    int,
                ):
                    raise TypeError(
                        f"Can not construct protobuf message of type "
                        f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateRequest': field 'order' is not "
                        f"of required type 'int'"
                    )
                if is_leaf is not None and not isinstance(
                    is_leaf,
                    bool,
                ):
                    raise TypeError(
                        f"Can not construct protobuf message of type "
                        f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateRequest': field 'is_leaf' is not "
                        f"of required type 'bool'"
                    )
                if keys is not None and not isinstance(
                    keys,
                    IMPORT_typing.Iterable,
                ):
                    raise TypeError(
                        f"Can not construct protobuf message of type "
                        f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateRequest': field 'keys' is not "
                        f"of required type 'typing.Iterable[str]'"
                    )
                if children_ids is not None and not isinstance(
                    children_ids,
                    IMPORT_typing.Iterable,
                ):
                    raise TypeError(
                        f"Can not construct protobuf message of type "
                        f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateRequest': field 'children_ids' is not "
                        f"of required type 'typing.Iterable[str]'"
                    )
                if values is not None and not isinstance(
                    values,
                    IMPORT_typing.Iterable,
                ):
                    raise TypeError(
                        f"Can not construct protobuf message of type "
                        f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateRequest': field 'values' is not "
                        f"of required type 'typing.Iterable[bytes]'"
                    )
                if next_id is not None and not isinstance(
                    next_id,
                    str,
                ):
                    raise TypeError(
                        f"Can not construct protobuf message of type "
                        f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateRequest': field 'next_id' is not "
                        f"of required type 'str'"
                    )
                if prev_id is not None and not isinstance(
                    prev_id,
                    str,
                ):
                    raise TypeError(
                        f"Can not construct protobuf message of type "
                        f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateRequest': field 'prev_id' is not "
                        f"of required type 'str'"
                    )
                # TODO: mypy-protobuf declares that
                # `IMPORT_google_protobuf_message.Message` constructor arguments are
                # always non-None, when in reality they are optional.
                __request__ = rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateRequest(
                    order=order,  # type: ignore[arg-type]
                    is_leaf=is_leaf,  # type: ignore[arg-type]
                    keys=keys,  # type: ignore[arg-type]
                    children_ids=children_ids,  # type: ignore[arg-type]
                    values=values,  # type: ignore[arg-type]
                    next_id=next_id,  # type: ignore[arg-type]
                    prev_id=prev_id,  # type: ignore[arg-type]
                )

                __schedule__: IMPORT_typing.Optional[IMPORT_reboot_time_DateTimeWithTimeZone] = (IMPORT_reboot_time_DateTimeWithTimeZone.now() + __this__._when) if isinstance(
                    __this__._when, IMPORT_datetime_timedelta
                ) else __this__._when

                __metadata__: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None
                __idempotency__: IMPORT_typing.Optional[IMPORT_reboot_aio_idempotency.Idempotency] = __this__._idempotency
                __bearer_token__: IMPORT_typing.Optional[str] = None

                if __options__ is not None:
                    IMPORT_reboot_aio_types.assert_type(__options__, [IMPORT_reboot_aio_call.Options])
                    if __options__.idempotency:
                        if __idempotency__ is not None:
                            raise RuntimeError(
                                'Found redundant idempotency in `Options`'
                            )
                        __idempotency__ = __options__.idempotency
                    if __options__.metadata is not None:
                        __metadata__ = __options__.metadata
                    if __options__.bearer_token is not None:
                        __bearer_token__ = __options__.bearer_token

                # Add scheduling information to the metadata.
                __metadata__ = (
                    (IMPORT_reboot_aio_headers.TASK_SCHEDULE,
                    __schedule__.isoformat() if __schedule__ else ''),
                ) + (__metadata__ or tuple())

                __task_id__ = await __this__._tasks(
                    __context__
                ).Create(
                    __request__,
                    idempotency=__idempotency__,
                    metadata=__metadata__,
                    bearer_token=__bearer_token__,
                )

                return __task_id__

            # Keep the original functions on the client, so old code will
            # continue to work, but use the new 'snake_case' method in
            # the new code.
            create = Create
            async def Search(
                __this__,
                __context__: IMPORT_reboot_aio_contexts.TransactionContext,
                __options__: IMPORT_typing.Optional[IMPORT_reboot_aio_call.Options] = None,
                *,
                key: IMPORT_typing.Optional[str] = None,
            ) -> IMPORT_rbt_v1alpha1.tasks_pb2.TaskId:
                # UX improvement: check that neither positional argument was accidentally
                # given a gRPC request type.
                IMPORT_reboot_aio_types.assert_not_request_type(__context__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeSearchRequest)
                IMPORT_reboot_aio_types.assert_not_request_type(__options__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeSearchRequest)
                IMPORT_reboot_aio_types.assert_type(__context__, [IMPORT_reboot_aio_contexts.TransactionContext])

                if key is not None and not isinstance(
                    key,
                    str,
                ):
                    raise TypeError(
                        f"Can not construct protobuf message of type "
                        f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeSearchRequest': field 'key' is not "
                        f"of required type 'str'"
                    )
                # TODO: mypy-protobuf declares that
                # `IMPORT_google_protobuf_message.Message` constructor arguments are
                # always non-None, when in reality they are optional.
                __request__ = rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeSearchRequest(
                    key=key,  # type: ignore[arg-type]
                )

                __schedule__: IMPORT_typing.Optional[IMPORT_reboot_time_DateTimeWithTimeZone] = (IMPORT_reboot_time_DateTimeWithTimeZone.now() + __this__._when) if isinstance(
                    __this__._when, IMPORT_datetime_timedelta
                ) else __this__._when

                __metadata__: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None
                __idempotency__: IMPORT_typing.Optional[IMPORT_reboot_aio_idempotency.Idempotency] = __this__._idempotency
                __bearer_token__: IMPORT_typing.Optional[str] = None

                if __options__ is not None:
                    IMPORT_reboot_aio_types.assert_type(__options__, [IMPORT_reboot_aio_call.Options])
                    if __options__.idempotency:
                        if __idempotency__ is not None:
                            raise RuntimeError(
                                'Found redundant idempotency in `Options`'
                            )
                        __idempotency__ = __options__.idempotency
                    if __options__.metadata is not None:
                        __metadata__ = __options__.metadata
                    if __options__.bearer_token is not None:
                        __bearer_token__ = __options__.bearer_token

                # Add scheduling information to the metadata.
                __metadata__ = (
                    (IMPORT_reboot_aio_headers.TASK_SCHEDULE,
                    __schedule__.isoformat() if __schedule__ else ''),
                ) + (__metadata__ or tuple())

                __task_id__ = await __this__._tasks(
                    __context__
                ).Search(
                    __request__,
                    idempotency=__idempotency__,
                    metadata=__metadata__,
                    bearer_token=__bearer_token__,
                )

                return __task_id__

            # Keep the original functions on the client, so old code will
            # continue to work, but use the new 'snake_case' method in
            # the new code.
            search = Search
            async def Insert(
                __this__,
                __context__: IMPORT_reboot_aio_contexts.TransactionContext,
                __options__: IMPORT_typing.Optional[IMPORT_reboot_aio_call.Options] = None,
                *,
                key: IMPORT_typing.Optional[str] = None,
                value: IMPORT_typing.Optional[bytes] = None,
            ) -> IMPORT_rbt_v1alpha1.tasks_pb2.TaskId:
                # UX improvement: check that neither positional argument was accidentally
                # given a gRPC request type.
                IMPORT_reboot_aio_types.assert_not_request_type(__context__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeInsertRequest)
                IMPORT_reboot_aio_types.assert_not_request_type(__options__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeInsertRequest)
                IMPORT_reboot_aio_types.assert_type(__context__, [IMPORT_reboot_aio_contexts.TransactionContext])

                if key is not None and not isinstance(
                    key,
                    str,
                ):
                    raise TypeError(
                        f"Can not construct protobuf message of type "
                        f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeInsertRequest': field 'key' is not "
                        f"of required type 'str'"
                    )
                if value is not None and not isinstance(
                    value,
                    bytes,
                ):
                    raise TypeError(
                        f"Can not construct protobuf message of type "
                        f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeInsertRequest': field 'value' is not "
                        f"of required type 'bytes'"
                    )
                # TODO: mypy-protobuf declares that
                # `IMPORT_google_protobuf_message.Message` constructor arguments are
                # always non-None, when in reality they are optional.
                __request__ = rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeInsertRequest(
                    key=key,  # type: ignore[arg-type]
                    value=value,  # type: ignore[arg-type]
                )

                __schedule__: IMPORT_typing.Optional[IMPORT_reboot_time_DateTimeWithTimeZone] = (IMPORT_reboot_time_DateTimeWithTimeZone.now() + __this__._when) if isinstance(
                    __this__._when, IMPORT_datetime_timedelta
                ) else __this__._when

                __metadata__: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None
                __idempotency__: IMPORT_typing.Optional[IMPORT_reboot_aio_idempotency.Idempotency] = __this__._idempotency
                __bearer_token__: IMPORT_typing.Optional[str] = None

                if __options__ is not None:
                    IMPORT_reboot_aio_types.assert_type(__options__, [IMPORT_reboot_aio_call.Options])
                    if __options__.idempotency:
                        if __idempotency__ is not None:
                            raise RuntimeError(
                                'Found redundant idempotency in `Options`'
                            )
                        __idempotency__ = __options__.idempotency
                    if __options__.metadata is not None:
                        __metadata__ = __options__.metadata
                    if __options__.bearer_token is not None:
                        __bearer_token__ = __options__.bearer_token

                # Add scheduling information to the metadata.
                __metadata__ = (
                    (IMPORT_reboot_aio_headers.TASK_SCHEDULE,
                    __schedule__.isoformat() if __schedule__ else ''),
                ) + (__metadata__ or tuple())

                __task_id__ = await __this__._tasks(
                    __context__
                ).Insert(
                    __request__,
                    idempotency=__idempotency__,
                    metadata=__metadata__,
                    bearer_token=__bearer_token__,
                )

                return __task_id__

            # Keep the original functions on the client, so old code will
            # continue to work, but use the new 'snake_case' method in
            # the new code.
            insert = Insert
            async def Remove(
                __this__,
                __context__: IMPORT_reboot_aio_contexts.TransactionContext,
                __options__: IMPORT_typing.Optional[IMPORT_reboot_aio_call.Options] = None,
                *,
                key: IMPORT_typing.Optional[str] = None,
            ) -> IMPORT_rbt_v1alpha1.tasks_pb2.TaskId:
                # UX improvement: check that neither positional argument was accidentally
                # given a gRPC request type.
                IMPORT_reboot_aio_types.assert_not_request_type(__context__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRemoveRequest)
                IMPORT_reboot_aio_types.assert_not_request_type(__options__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRemoveRequest)
                IMPORT_reboot_aio_types.assert_type(__context__, [IMPORT_reboot_aio_contexts.TransactionContext])

                if key is not None and not isinstance(
                    key,
                    str,
                ):
                    raise TypeError(
                        f"Can not construct protobuf message of type "
                        f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRemoveRequest': field 'key' is not "
                        f"of required type 'str'"
                    )
                # TODO: mypy-protobuf declares that
                # `IMPORT_google_protobuf_message.Message` constructor arguments are
                # always non-None, when in reality they are optional.
                __request__ = rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRemoveRequest(
                    key=key,  # type: ignore[arg-type]
                )

                __schedule__: IMPORT_typing.Optional[IMPORT_reboot_time_DateTimeWithTimeZone] = (IMPORT_reboot_time_DateTimeWithTimeZone.now() + __this__._when) if isinstance(
                    __this__._when, IMPORT_datetime_timedelta
                ) else __this__._when

                __metadata__: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None
                __idempotency__: IMPORT_typing.Optional[IMPORT_reboot_aio_idempotency.Idempotency] = __this__._idempotency
                __bearer_token__: IMPORT_typing.Optional[str] = None

                if __options__ is not None:
                    IMPORT_reboot_aio_types.assert_type(__options__, [IMPORT_reboot_aio_call.Options])
                    if __options__.idempotency:
                        if __idempotency__ is not None:
                            raise RuntimeError(
                                'Found redundant idempotency in `Options`'
                            )
                        __idempotency__ = __options__.idempotency
                    if __options__.metadata is not None:
                        __metadata__ = __options__.metadata
                    if __options__.bearer_token is not None:
                        __bearer_token__ = __options__.bearer_token

                # Add scheduling information to the metadata.
                __metadata__ = (
                    (IMPORT_reboot_aio_headers.TASK_SCHEDULE,
                    __schedule__.isoformat() if __schedule__ else ''),
                ) + (__metadata__ or tuple())

                __task_id__ = await __this__._tasks(
                    __context__
                ).Remove(
                    __request__,
                    idempotency=__idempotency__,
                    metadata=__metadata__,
                    bearer_token=__bearer_token__,
                )

                return __task_id__

            # Keep the original functions on the client, so old code will
            # continue to work, but use the new 'snake_case' method in
            # the new code.
            remove = Remove
            async def Range(
                __this__,
                __context__: IMPORT_reboot_aio_contexts.TransactionContext,
                __options__: IMPORT_typing.Optional[IMPORT_reboot_aio_call.Options] = None,
                *,
                start_key: IMPORT_typing.Optional[str] = None,
                limit: IMPORT_typing.Optional[int] = None,
            ) -> IMPORT_rbt_v1alpha1.tasks_pb2.TaskId:
                # UX improvement: check that neither positional argument was accidentally
                # given a gRPC request type.
                IMPORT_reboot_aio_types.assert_not_request_type(__context__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRangeRequest)
                IMPORT_reboot_aio_types.assert_not_request_type(__options__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRangeRequest)
                IMPORT_reboot_aio_types.assert_type(__context__, [IMPORT_reboot_aio_contexts.TransactionContext])

                if start_key is not None and not isinstance(
                    start_key,
                    str,
                ):
                    raise TypeError(
                        f"Can not construct protobuf message of type "
                        f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRangeRequest': field 'start_key' is not "
                        f"of required type 'str'"
                    )
                if limit is not None and not isinstance(
                    limit,
                    int,
                ):
                    raise TypeError(
                        f"Can not construct protobuf message of type "
                        f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRangeRequest': field 'limit' is not "
                        f"of required type 'int'"
                    )
                # TODO: mypy-protobuf declares that
                # `IMPORT_google_protobuf_message.Message` constructor arguments are
                # always non-None, when in reality they are optional.
                __request__ = rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRangeRequest(
                    start_key=start_key,  # type: ignore[arg-type]
                    limit=limit,  # type: ignore[arg-type]
                )

                __schedule__: IMPORT_typing.Optional[IMPORT_reboot_time_DateTimeWithTimeZone] = (IMPORT_reboot_time_DateTimeWithTimeZone.now() + __this__._when) if isinstance(
                    __this__._when, IMPORT_datetime_timedelta
                ) else __this__._when

                __metadata__: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None
                __idempotency__: IMPORT_typing.Optional[IMPORT_reboot_aio_idempotency.Idempotency] = __this__._idempotency
                __bearer_token__: IMPORT_typing.Optional[str] = None

                if __options__ is not None:
                    IMPORT_reboot_aio_types.assert_type(__options__, [IMPORT_reboot_aio_call.Options])
                    if __options__.idempotency:
                        if __idempotency__ is not None:
                            raise RuntimeError(
                                'Found redundant idempotency in `Options`'
                            )
                        __idempotency__ = __options__.idempotency
                    if __options__.metadata is not None:
                        __metadata__ = __options__.metadata
                    if __options__.bearer_token is not None:
                        __bearer_token__ = __options__.bearer_token

                # Add scheduling information to the metadata.
                __metadata__ = (
                    (IMPORT_reboot_aio_headers.TASK_SCHEDULE,
                    __schedule__.isoformat() if __schedule__ else ''),
                ) + (__metadata__ or tuple())

                __task_id__ = await __this__._tasks(
                    __context__
                ).Range(
                    __request__,
                    idempotency=__idempotency__,
                    metadata=__metadata__,
                    bearer_token=__bearer_token__,
                )

                return __task_id__

            # Keep the original functions on the client, so old code will
            # continue to work, but use the new 'snake_case' method in
            # the new code.
            range = Range
            async def ReverseRange(
                __this__,
                __context__: IMPORT_reboot_aio_contexts.TransactionContext,
                __options__: IMPORT_typing.Optional[IMPORT_reboot_aio_call.Options] = None,
                *,
                start_key: IMPORT_typing.Optional[str] = None,
                limit: IMPORT_typing.Optional[int] = None,
            ) -> IMPORT_rbt_v1alpha1.tasks_pb2.TaskId:
                # UX improvement: check that neither positional argument was accidentally
                # given a gRPC request type.
                IMPORT_reboot_aio_types.assert_not_request_type(__context__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeReverseRangeRequest)
                IMPORT_reboot_aio_types.assert_not_request_type(__options__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeReverseRangeRequest)
                IMPORT_reboot_aio_types.assert_type(__context__, [IMPORT_reboot_aio_contexts.TransactionContext])

                if start_key is not None and not isinstance(
                    start_key,
                    str,
                ):
                    raise TypeError(
                        f"Can not construct protobuf message of type "
                        f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeReverseRangeRequest': field 'start_key' is not "
                        f"of required type 'str'"
                    )
                if limit is not None and not isinstance(
                    limit,
                    int,
                ):
                    raise TypeError(
                        f"Can not construct protobuf message of type "
                        f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeReverseRangeRequest': field 'limit' is not "
                        f"of required type 'int'"
                    )
                # TODO: mypy-protobuf declares that
                # `IMPORT_google_protobuf_message.Message` constructor arguments are
                # always non-None, when in reality they are optional.
                __request__ = rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeReverseRangeRequest(
                    start_key=start_key,  # type: ignore[arg-type]
                    limit=limit,  # type: ignore[arg-type]
                )

                __schedule__: IMPORT_typing.Optional[IMPORT_reboot_time_DateTimeWithTimeZone] = (IMPORT_reboot_time_DateTimeWithTimeZone.now() + __this__._when) if isinstance(
                    __this__._when, IMPORT_datetime_timedelta
                ) else __this__._when

                __metadata__: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None
                __idempotency__: IMPORT_typing.Optional[IMPORT_reboot_aio_idempotency.Idempotency] = __this__._idempotency
                __bearer_token__: IMPORT_typing.Optional[str] = None

                if __options__ is not None:
                    IMPORT_reboot_aio_types.assert_type(__options__, [IMPORT_reboot_aio_call.Options])
                    if __options__.idempotency:
                        if __idempotency__ is not None:
                            raise RuntimeError(
                                'Found redundant idempotency in `Options`'
                            )
                        __idempotency__ = __options__.idempotency
                    if __options__.metadata is not None:
                        __metadata__ = __options__.metadata
                    if __options__.bearer_token is not None:
                        __bearer_token__ = __options__.bearer_token

                # Add scheduling information to the metadata.
                __metadata__ = (
                    (IMPORT_reboot_aio_headers.TASK_SCHEDULE,
                    __schedule__.isoformat() if __schedule__ else ''),
                ) + (__metadata__ or tuple())

                __task_id__ = await __this__._tasks(
                    __context__
                ).ReverseRange(
                    __request__,
                    idempotency=__idempotency__,
                    metadata=__metadata__,
                    bearer_token=__bearer_token__,
                )

                return __task_id__

            # Keep the original functions on the client, so old code will
            # continue to work, but use the new 'snake_case' method in
            # the new code.
            reverse_range = ReverseRange
            async def Stringify(
                __this__,
                __context__: IMPORT_reboot_aio_contexts.TransactionContext,
                __options__: IMPORT_typing.Optional[IMPORT_reboot_aio_call.Options] = None,
                *,
                level: IMPORT_typing.Optional[int] = None,
            ) -> IMPORT_rbt_v1alpha1.tasks_pb2.TaskId:
                # UX improvement: check that neither positional argument was accidentally
                # given a gRPC request type.
                IMPORT_reboot_aio_types.assert_not_request_type(__context__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeStringifyRequest)
                IMPORT_reboot_aio_types.assert_not_request_type(__options__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeStringifyRequest)
                IMPORT_reboot_aio_types.assert_type(__context__, [IMPORT_reboot_aio_contexts.TransactionContext])

                if level is not None and not isinstance(
                    level,
                    int,
                ):
                    raise TypeError(
                        f"Can not construct protobuf message of type "
                        f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeStringifyRequest': field 'level' is not "
                        f"of required type 'int'"
                    )
                # TODO: mypy-protobuf declares that
                # `IMPORT_google_protobuf_message.Message` constructor arguments are
                # always non-None, when in reality they are optional.
                __request__ = rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeStringifyRequest(
                    level=level,  # type: ignore[arg-type]
                )

                __schedule__: IMPORT_typing.Optional[IMPORT_reboot_time_DateTimeWithTimeZone] = (IMPORT_reboot_time_DateTimeWithTimeZone.now() + __this__._when) if isinstance(
                    __this__._when, IMPORT_datetime_timedelta
                ) else __this__._when

                __metadata__: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None
                __idempotency__: IMPORT_typing.Optional[IMPORT_reboot_aio_idempotency.Idempotency] = __this__._idempotency
                __bearer_token__: IMPORT_typing.Optional[str] = None

                if __options__ is not None:
                    IMPORT_reboot_aio_types.assert_type(__options__, [IMPORT_reboot_aio_call.Options])
                    if __options__.idempotency:
                        if __idempotency__ is not None:
                            raise RuntimeError(
                                'Found redundant idempotency in `Options`'
                            )
                        __idempotency__ = __options__.idempotency
                    if __options__.metadata is not None:
                        __metadata__ = __options__.metadata
                    if __options__.bearer_token is not None:
                        __bearer_token__ = __options__.bearer_token

                # Add scheduling information to the metadata.
                __metadata__ = (
                    (IMPORT_reboot_aio_headers.TASK_SCHEDULE,
                    __schedule__.isoformat() if __schedule__ else ''),
                ) + (__metadata__ or tuple())

                __task_id__ = await __this__._tasks(
                    __context__
                ).Stringify(
                    __request__,
                    idempotency=__idempotency__,
                    metadata=__metadata__,
                    bearer_token=__bearer_token__,
                )

                return __task_id__

            # Keep the original functions on the client, so old code will
            # continue to work, but use the new 'snake_case' method in
            # the new code.
            stringify = Stringify

        # A `WriterContext` can not call any methods in `_Schedule` to
        # prevent a writer from doing a `Foo.ref()` and trying to
        # schedule. However, we want to allow a writer to schedule
        # when we are constructing a `WeakReference` from
        # `self.ref()` so instead we return a `_WriterSchedule` to
        # provide type safety that allows a `WriterContext` to
        # schedule (for itself).
        class _WriterSchedule:

            def __init__(
                self,
                application_id: IMPORT_typing.Optional[IMPORT_reboot_aio_types.ApplicationId],
                tasks: IMPORT_typing.Callable[[IMPORT_reboot_aio_contexts.TransactionContext | IMPORT_reboot_aio_contexts.WorkflowContext | IMPORT_reboot_aio_external.ExternalContext], NodeTasksStub],
                *,
                when: IMPORT_typing.Optional[IMPORT_datetime_datetime | IMPORT_datetime_timedelta] = None,
                idempotency: IMPORT_typing.Optional[IMPORT_reboot_aio_idempotency.Idempotency] = None,
            ) -> None:
                self._tasks = tasks
                self._when = ensure_has_timezone(when=when)
                self._idempotency = idempotency

            # Node callable tasks:
            async def Create(
                __this__,
                __context__: IMPORT_reboot_aio_contexts.WriterContext | IMPORT_reboot_aio_contexts.TransactionContext,
                __options__: IMPORT_typing.Optional[IMPORT_reboot_aio_call.Options] = None,
                *,
                order: IMPORT_typing.Optional[int] = None,
                is_leaf: IMPORT_typing.Optional[bool] = None,
                keys: IMPORT_typing.Optional[IMPORT_typing.Iterable[str]] = None,
                children_ids: IMPORT_typing.Optional[IMPORT_typing.Iterable[str]] = None,
                values: IMPORT_typing.Optional[IMPORT_typing.Iterable[bytes]] = None,
                next_id: IMPORT_typing.Optional[str] = None,
                prev_id: IMPORT_typing.Optional[str] = None,
            ) -> IMPORT_rbt_v1alpha1.tasks_pb2.TaskId:
                # Only `writer`s and `transaction`s should ``schedule()`, a
                # `workflow` should `spawn()`.
                IMPORT_reboot_aio_types.assert_type(__context__, [IMPORT_reboot_aio_contexts.WriterContext, IMPORT_reboot_aio_contexts.TransactionContext])

                # UX improvement: check that neither positional argument was accidentally
                # given a gRPC request type.
                IMPORT_reboot_aio_types.assert_not_request_type(__context__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateRequest)
                IMPORT_reboot_aio_types.assert_not_request_type(__options__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateRequest)

                if order is not None and not isinstance(
                    order,
                    int,
                ):
                    raise TypeError(
                        f"Can not construct protobuf message of type "
                        f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateRequest': field 'order' is not "
                        f"of required type 'int'"
                    )
                if is_leaf is not None and not isinstance(
                    is_leaf,
                    bool,
                ):
                    raise TypeError(
                        f"Can not construct protobuf message of type "
                        f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateRequest': field 'is_leaf' is not "
                        f"of required type 'bool'"
                    )
                if keys is not None and not isinstance(
                    keys,
                    IMPORT_typing.Iterable,
                ):
                    raise TypeError(
                        f"Can not construct protobuf message of type "
                        f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateRequest': field 'keys' is not "
                        f"of required type 'typing.Iterable[str]'"
                    )
                if children_ids is not None and not isinstance(
                    children_ids,
                    IMPORT_typing.Iterable,
                ):
                    raise TypeError(
                        f"Can not construct protobuf message of type "
                        f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateRequest': field 'children_ids' is not "
                        f"of required type 'typing.Iterable[str]'"
                    )
                if values is not None and not isinstance(
                    values,
                    IMPORT_typing.Iterable,
                ):
                    raise TypeError(
                        f"Can not construct protobuf message of type "
                        f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateRequest': field 'values' is not "
                        f"of required type 'typing.Iterable[bytes]'"
                    )
                if next_id is not None and not isinstance(
                    next_id,
                    str,
                ):
                    raise TypeError(
                        f"Can not construct protobuf message of type "
                        f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateRequest': field 'next_id' is not "
                        f"of required type 'str'"
                    )
                if prev_id is not None and not isinstance(
                    prev_id,
                    str,
                ):
                    raise TypeError(
                        f"Can not construct protobuf message of type "
                        f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateRequest': field 'prev_id' is not "
                        f"of required type 'str'"
                    )
                # TODO: mypy-protobuf declares that
                # `google.protobuf.message.Message` constructor arguments are
                # always non-None, when in reality they are optional.
                __request__ = rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateRequest(
                    order=order,  # type: ignore[arg-type]
                    is_leaf=is_leaf,  # type: ignore[arg-type]
                    keys=keys,  # type: ignore[arg-type]
                    children_ids=children_ids,  # type: ignore[arg-type]
                    values=values,  # type: ignore[arg-type]
                    next_id=next_id,  # type: ignore[arg-type]
                    prev_id=prev_id,  # type: ignore[arg-type]
                )
                if isinstance(__context__, IMPORT_reboot_aio_contexts.WriterContext):
                    return (await NodeServicerTasks(
                        context=__context__,
                        state_ref=__context__._state_ref,
                    ).Create(
                        __request__,
                        schedule=__this__._when,
                    )).task_id

                __schedule__: IMPORT_typing.Optional[IMPORT_reboot_time_DateTimeWithTimeZone] = (IMPORT_reboot_time_DateTimeWithTimeZone.now() + __this__._when) if isinstance(
                    __this__._when, IMPORT_datetime_timedelta
                ) else __this__._when

                __metadata__: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None
                __idempotency__: IMPORT_typing.Optional[IMPORT_reboot_aio_idempotency.Idempotency] = __this__._idempotency
                __bearer_token__: IMPORT_typing.Optional[str] = None

                if __options__ is not None:
                    IMPORT_reboot_aio_types.assert_type(__options__, [IMPORT_reboot_aio_call.Options])
                    if __options__.idempotency is not None:
                        if __idempotency__ is not None:
                            raise RuntimeError(
                                'Found redundant idempotency in `Options`'
                            )
                        __idempotency__ = __options__.idempotency
                    if __options__.metadata is not None:
                        __metadata__ = __options__.metadata
                    if __options__.bearer_token is not None:
                        __bearer_token__ = __options__.bearer_token

                # Add scheduling information to the metadata.
                __metadata__ = (
                    (IMPORT_reboot_aio_headers.TASK_SCHEDULE,
                    __schedule__.isoformat() if __schedule__ else ''),
                ) + (__metadata__ or tuple())

                return await __this__._tasks(
                    __context__
                ).Create(
                    __request__,
                    idempotency=__idempotency__,
                    metadata=__metadata__,
                    bearer_token=__bearer_token__,
                )

            # Keep the original functions on the client, so old code will
            # continue to work, but use the new 'snake_case' method in
            # the new code.
            create = Create
            async def Search(
                __this__,
                __context__: IMPORT_reboot_aio_contexts.WriterContext | IMPORT_reboot_aio_contexts.TransactionContext,
                __options__: IMPORT_typing.Optional[IMPORT_reboot_aio_call.Options] = None,
                *,
                key: IMPORT_typing.Optional[str] = None,
            ) -> IMPORT_rbt_v1alpha1.tasks_pb2.TaskId:
                # Only `writer`s and `transaction`s should ``schedule()`, a
                # `workflow` should `spawn()`.
                IMPORT_reboot_aio_types.assert_type(__context__, [IMPORT_reboot_aio_contexts.WriterContext, IMPORT_reboot_aio_contexts.TransactionContext])

                # UX improvement: check that neither positional argument was accidentally
                # given a gRPC request type.
                IMPORT_reboot_aio_types.assert_not_request_type(__context__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeSearchRequest)
                IMPORT_reboot_aio_types.assert_not_request_type(__options__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeSearchRequest)

                if key is not None and not isinstance(
                    key,
                    str,
                ):
                    raise TypeError(
                        f"Can not construct protobuf message of type "
                        f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeSearchRequest': field 'key' is not "
                        f"of required type 'str'"
                    )
                # TODO: mypy-protobuf declares that
                # `google.protobuf.message.Message` constructor arguments are
                # always non-None, when in reality they are optional.
                __request__ = rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeSearchRequest(
                    key=key,  # type: ignore[arg-type]
                )
                if isinstance(__context__, IMPORT_reboot_aio_contexts.WriterContext):
                    return (await NodeServicerTasks(
                        context=__context__,
                        state_ref=__context__._state_ref,
                    ).Search(
                        __request__,
                        schedule=__this__._when,
                    )).task_id

                __schedule__: IMPORT_typing.Optional[IMPORT_reboot_time_DateTimeWithTimeZone] = (IMPORT_reboot_time_DateTimeWithTimeZone.now() + __this__._when) if isinstance(
                    __this__._when, IMPORT_datetime_timedelta
                ) else __this__._when

                __metadata__: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None
                __idempotency__: IMPORT_typing.Optional[IMPORT_reboot_aio_idempotency.Idempotency] = __this__._idempotency
                __bearer_token__: IMPORT_typing.Optional[str] = None

                if __options__ is not None:
                    IMPORT_reboot_aio_types.assert_type(__options__, [IMPORT_reboot_aio_call.Options])
                    if __options__.idempotency is not None:
                        if __idempotency__ is not None:
                            raise RuntimeError(
                                'Found redundant idempotency in `Options`'
                            )
                        __idempotency__ = __options__.idempotency
                    if __options__.metadata is not None:
                        __metadata__ = __options__.metadata
                    if __options__.bearer_token is not None:
                        __bearer_token__ = __options__.bearer_token

                # Add scheduling information to the metadata.
                __metadata__ = (
                    (IMPORT_reboot_aio_headers.TASK_SCHEDULE,
                    __schedule__.isoformat() if __schedule__ else ''),
                ) + (__metadata__ or tuple())

                return await __this__._tasks(
                    __context__
                ).Search(
                    __request__,
                    idempotency=__idempotency__,
                    metadata=__metadata__,
                    bearer_token=__bearer_token__,
                )

            # Keep the original functions on the client, so old code will
            # continue to work, but use the new 'snake_case' method in
            # the new code.
            search = Search
            async def Insert(
                __this__,
                __context__: IMPORT_reboot_aio_contexts.WriterContext | IMPORT_reboot_aio_contexts.TransactionContext,
                __options__: IMPORT_typing.Optional[IMPORT_reboot_aio_call.Options] = None,
                *,
                key: IMPORT_typing.Optional[str] = None,
                value: IMPORT_typing.Optional[bytes] = None,
            ) -> IMPORT_rbt_v1alpha1.tasks_pb2.TaskId:
                # Only `writer`s and `transaction`s should ``schedule()`, a
                # `workflow` should `spawn()`.
                IMPORT_reboot_aio_types.assert_type(__context__, [IMPORT_reboot_aio_contexts.WriterContext, IMPORT_reboot_aio_contexts.TransactionContext])

                # UX improvement: check that neither positional argument was accidentally
                # given a gRPC request type.
                IMPORT_reboot_aio_types.assert_not_request_type(__context__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeInsertRequest)
                IMPORT_reboot_aio_types.assert_not_request_type(__options__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeInsertRequest)

                if key is not None and not isinstance(
                    key,
                    str,
                ):
                    raise TypeError(
                        f"Can not construct protobuf message of type "
                        f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeInsertRequest': field 'key' is not "
                        f"of required type 'str'"
                    )
                if value is not None and not isinstance(
                    value,
                    bytes,
                ):
                    raise TypeError(
                        f"Can not construct protobuf message of type "
                        f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeInsertRequest': field 'value' is not "
                        f"of required type 'bytes'"
                    )
                # TODO: mypy-protobuf declares that
                # `google.protobuf.message.Message` constructor arguments are
                # always non-None, when in reality they are optional.
                __request__ = rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeInsertRequest(
                    key=key,  # type: ignore[arg-type]
                    value=value,  # type: ignore[arg-type]
                )
                if isinstance(__context__, IMPORT_reboot_aio_contexts.WriterContext):
                    return (await NodeServicerTasks(
                        context=__context__,
                        state_ref=__context__._state_ref,
                    ).Insert(
                        __request__,
                        schedule=__this__._when,
                    )).task_id

                __schedule__: IMPORT_typing.Optional[IMPORT_reboot_time_DateTimeWithTimeZone] = (IMPORT_reboot_time_DateTimeWithTimeZone.now() + __this__._when) if isinstance(
                    __this__._when, IMPORT_datetime_timedelta
                ) else __this__._when

                __metadata__: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None
                __idempotency__: IMPORT_typing.Optional[IMPORT_reboot_aio_idempotency.Idempotency] = __this__._idempotency
                __bearer_token__: IMPORT_typing.Optional[str] = None

                if __options__ is not None:
                    IMPORT_reboot_aio_types.assert_type(__options__, [IMPORT_reboot_aio_call.Options])
                    if __options__.idempotency is not None:
                        if __idempotency__ is not None:
                            raise RuntimeError(
                                'Found redundant idempotency in `Options`'
                            )
                        __idempotency__ = __options__.idempotency
                    if __options__.metadata is not None:
                        __metadata__ = __options__.metadata
                    if __options__.bearer_token is not None:
                        __bearer_token__ = __options__.bearer_token

                # Add scheduling information to the metadata.
                __metadata__ = (
                    (IMPORT_reboot_aio_headers.TASK_SCHEDULE,
                    __schedule__.isoformat() if __schedule__ else ''),
                ) + (__metadata__ or tuple())

                return await __this__._tasks(
                    __context__
                ).Insert(
                    __request__,
                    idempotency=__idempotency__,
                    metadata=__metadata__,
                    bearer_token=__bearer_token__,
                )

            # Keep the original functions on the client, so old code will
            # continue to work, but use the new 'snake_case' method in
            # the new code.
            insert = Insert
            async def Remove(
                __this__,
                __context__: IMPORT_reboot_aio_contexts.WriterContext | IMPORT_reboot_aio_contexts.TransactionContext,
                __options__: IMPORT_typing.Optional[IMPORT_reboot_aio_call.Options] = None,
                *,
                key: IMPORT_typing.Optional[str] = None,
            ) -> IMPORT_rbt_v1alpha1.tasks_pb2.TaskId:
                # Only `writer`s and `transaction`s should ``schedule()`, a
                # `workflow` should `spawn()`.
                IMPORT_reboot_aio_types.assert_type(__context__, [IMPORT_reboot_aio_contexts.WriterContext, IMPORT_reboot_aio_contexts.TransactionContext])

                # UX improvement: check that neither positional argument was accidentally
                # given a gRPC request type.
                IMPORT_reboot_aio_types.assert_not_request_type(__context__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRemoveRequest)
                IMPORT_reboot_aio_types.assert_not_request_type(__options__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRemoveRequest)

                if key is not None and not isinstance(
                    key,
                    str,
                ):
                    raise TypeError(
                        f"Can not construct protobuf message of type "
                        f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRemoveRequest': field 'key' is not "
                        f"of required type 'str'"
                    )
                # TODO: mypy-protobuf declares that
                # `google.protobuf.message.Message` constructor arguments are
                # always non-None, when in reality they are optional.
                __request__ = rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRemoveRequest(
                    key=key,  # type: ignore[arg-type]
                )
                if isinstance(__context__, IMPORT_reboot_aio_contexts.WriterContext):
                    return (await NodeServicerTasks(
                        context=__context__,
                        state_ref=__context__._state_ref,
                    ).Remove(
                        __request__,
                        schedule=__this__._when,
                    )).task_id

                __schedule__: IMPORT_typing.Optional[IMPORT_reboot_time_DateTimeWithTimeZone] = (IMPORT_reboot_time_DateTimeWithTimeZone.now() + __this__._when) if isinstance(
                    __this__._when, IMPORT_datetime_timedelta
                ) else __this__._when

                __metadata__: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None
                __idempotency__: IMPORT_typing.Optional[IMPORT_reboot_aio_idempotency.Idempotency] = __this__._idempotency
                __bearer_token__: IMPORT_typing.Optional[str] = None

                if __options__ is not None:
                    IMPORT_reboot_aio_types.assert_type(__options__, [IMPORT_reboot_aio_call.Options])
                    if __options__.idempotency is not None:
                        if __idempotency__ is not None:
                            raise RuntimeError(
                                'Found redundant idempotency in `Options`'
                            )
                        __idempotency__ = __options__.idempotency
                    if __options__.metadata is not None:
                        __metadata__ = __options__.metadata
                    if __options__.bearer_token is not None:
                        __bearer_token__ = __options__.bearer_token

                # Add scheduling information to the metadata.
                __metadata__ = (
                    (IMPORT_reboot_aio_headers.TASK_SCHEDULE,
                    __schedule__.isoformat() if __schedule__ else ''),
                ) + (__metadata__ or tuple())

                return await __this__._tasks(
                    __context__
                ).Remove(
                    __request__,
                    idempotency=__idempotency__,
                    metadata=__metadata__,
                    bearer_token=__bearer_token__,
                )

            # Keep the original functions on the client, so old code will
            # continue to work, but use the new 'snake_case' method in
            # the new code.
            remove = Remove
            async def Range(
                __this__,
                __context__: IMPORT_reboot_aio_contexts.WriterContext | IMPORT_reboot_aio_contexts.TransactionContext,
                __options__: IMPORT_typing.Optional[IMPORT_reboot_aio_call.Options] = None,
                *,
                start_key: IMPORT_typing.Optional[str] = None,
                limit: IMPORT_typing.Optional[int] = None,
            ) -> IMPORT_rbt_v1alpha1.tasks_pb2.TaskId:
                # Only `writer`s and `transaction`s should ``schedule()`, a
                # `workflow` should `spawn()`.
                IMPORT_reboot_aio_types.assert_type(__context__, [IMPORT_reboot_aio_contexts.WriterContext, IMPORT_reboot_aio_contexts.TransactionContext])

                # UX improvement: check that neither positional argument was accidentally
                # given a gRPC request type.
                IMPORT_reboot_aio_types.assert_not_request_type(__context__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRangeRequest)
                IMPORT_reboot_aio_types.assert_not_request_type(__options__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRangeRequest)

                if start_key is not None and not isinstance(
                    start_key,
                    str,
                ):
                    raise TypeError(
                        f"Can not construct protobuf message of type "
                        f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRangeRequest': field 'start_key' is not "
                        f"of required type 'str'"
                    )
                if limit is not None and not isinstance(
                    limit,
                    int,
                ):
                    raise TypeError(
                        f"Can not construct protobuf message of type "
                        f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRangeRequest': field 'limit' is not "
                        f"of required type 'int'"
                    )
                # TODO: mypy-protobuf declares that
                # `google.protobuf.message.Message` constructor arguments are
                # always non-None, when in reality they are optional.
                __request__ = rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRangeRequest(
                    start_key=start_key,  # type: ignore[arg-type]
                    limit=limit,  # type: ignore[arg-type]
                )
                if isinstance(__context__, IMPORT_reboot_aio_contexts.WriterContext):
                    return (await NodeServicerTasks(
                        context=__context__,
                        state_ref=__context__._state_ref,
                    ).Range(
                        __request__,
                        schedule=__this__._when,
                    )).task_id

                __schedule__: IMPORT_typing.Optional[IMPORT_reboot_time_DateTimeWithTimeZone] = (IMPORT_reboot_time_DateTimeWithTimeZone.now() + __this__._when) if isinstance(
                    __this__._when, IMPORT_datetime_timedelta
                ) else __this__._when

                __metadata__: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None
                __idempotency__: IMPORT_typing.Optional[IMPORT_reboot_aio_idempotency.Idempotency] = __this__._idempotency
                __bearer_token__: IMPORT_typing.Optional[str] = None

                if __options__ is not None:
                    IMPORT_reboot_aio_types.assert_type(__options__, [IMPORT_reboot_aio_call.Options])
                    if __options__.idempotency is not None:
                        if __idempotency__ is not None:
                            raise RuntimeError(
                                'Found redundant idempotency in `Options`'
                            )
                        __idempotency__ = __options__.idempotency
                    if __options__.metadata is not None:
                        __metadata__ = __options__.metadata
                    if __options__.bearer_token is not None:
                        __bearer_token__ = __options__.bearer_token

                # Add scheduling information to the metadata.
                __metadata__ = (
                    (IMPORT_reboot_aio_headers.TASK_SCHEDULE,
                    __schedule__.isoformat() if __schedule__ else ''),
                ) + (__metadata__ or tuple())

                return await __this__._tasks(
                    __context__
                ).Range(
                    __request__,
                    idempotency=__idempotency__,
                    metadata=__metadata__,
                    bearer_token=__bearer_token__,
                )

            # Keep the original functions on the client, so old code will
            # continue to work, but use the new 'snake_case' method in
            # the new code.
            range = Range
            async def ReverseRange(
                __this__,
                __context__: IMPORT_reboot_aio_contexts.WriterContext | IMPORT_reboot_aio_contexts.TransactionContext,
                __options__: IMPORT_typing.Optional[IMPORT_reboot_aio_call.Options] = None,
                *,
                start_key: IMPORT_typing.Optional[str] = None,
                limit: IMPORT_typing.Optional[int] = None,
            ) -> IMPORT_rbt_v1alpha1.tasks_pb2.TaskId:
                # Only `writer`s and `transaction`s should ``schedule()`, a
                # `workflow` should `spawn()`.
                IMPORT_reboot_aio_types.assert_type(__context__, [IMPORT_reboot_aio_contexts.WriterContext, IMPORT_reboot_aio_contexts.TransactionContext])

                # UX improvement: check that neither positional argument was accidentally
                # given a gRPC request type.
                IMPORT_reboot_aio_types.assert_not_request_type(__context__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeReverseRangeRequest)
                IMPORT_reboot_aio_types.assert_not_request_type(__options__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeReverseRangeRequest)

                if start_key is not None and not isinstance(
                    start_key,
                    str,
                ):
                    raise TypeError(
                        f"Can not construct protobuf message of type "
                        f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeReverseRangeRequest': field 'start_key' is not "
                        f"of required type 'str'"
                    )
                if limit is not None and not isinstance(
                    limit,
                    int,
                ):
                    raise TypeError(
                        f"Can not construct protobuf message of type "
                        f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeReverseRangeRequest': field 'limit' is not "
                        f"of required type 'int'"
                    )
                # TODO: mypy-protobuf declares that
                # `google.protobuf.message.Message` constructor arguments are
                # always non-None, when in reality they are optional.
                __request__ = rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeReverseRangeRequest(
                    start_key=start_key,  # type: ignore[arg-type]
                    limit=limit,  # type: ignore[arg-type]
                )
                if isinstance(__context__, IMPORT_reboot_aio_contexts.WriterContext):
                    return (await NodeServicerTasks(
                        context=__context__,
                        state_ref=__context__._state_ref,
                    ).ReverseRange(
                        __request__,
                        schedule=__this__._when,
                    )).task_id

                __schedule__: IMPORT_typing.Optional[IMPORT_reboot_time_DateTimeWithTimeZone] = (IMPORT_reboot_time_DateTimeWithTimeZone.now() + __this__._when) if isinstance(
                    __this__._when, IMPORT_datetime_timedelta
                ) else __this__._when

                __metadata__: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None
                __idempotency__: IMPORT_typing.Optional[IMPORT_reboot_aio_idempotency.Idempotency] = __this__._idempotency
                __bearer_token__: IMPORT_typing.Optional[str] = None

                if __options__ is not None:
                    IMPORT_reboot_aio_types.assert_type(__options__, [IMPORT_reboot_aio_call.Options])
                    if __options__.idempotency is not None:
                        if __idempotency__ is not None:
                            raise RuntimeError(
                                'Found redundant idempotency in `Options`'
                            )
                        __idempotency__ = __options__.idempotency
                    if __options__.metadata is not None:
                        __metadata__ = __options__.metadata
                    if __options__.bearer_token is not None:
                        __bearer_token__ = __options__.bearer_token

                # Add scheduling information to the metadata.
                __metadata__ = (
                    (IMPORT_reboot_aio_headers.TASK_SCHEDULE,
                    __schedule__.isoformat() if __schedule__ else ''),
                ) + (__metadata__ or tuple())

                return await __this__._tasks(
                    __context__
                ).ReverseRange(
                    __request__,
                    idempotency=__idempotency__,
                    metadata=__metadata__,
                    bearer_token=__bearer_token__,
                )

            # Keep the original functions on the client, so old code will
            # continue to work, but use the new 'snake_case' method in
            # the new code.
            reverse_range = ReverseRange
            async def Stringify(
                __this__,
                __context__: IMPORT_reboot_aio_contexts.WriterContext | IMPORT_reboot_aio_contexts.TransactionContext,
                __options__: IMPORT_typing.Optional[IMPORT_reboot_aio_call.Options] = None,
                *,
                level: IMPORT_typing.Optional[int] = None,
            ) -> IMPORT_rbt_v1alpha1.tasks_pb2.TaskId:
                # Only `writer`s and `transaction`s should ``schedule()`, a
                # `workflow` should `spawn()`.
                IMPORT_reboot_aio_types.assert_type(__context__, [IMPORT_reboot_aio_contexts.WriterContext, IMPORT_reboot_aio_contexts.TransactionContext])

                # UX improvement: check that neither positional argument was accidentally
                # given a gRPC request type.
                IMPORT_reboot_aio_types.assert_not_request_type(__context__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeStringifyRequest)
                IMPORT_reboot_aio_types.assert_not_request_type(__options__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeStringifyRequest)

                if level is not None and not isinstance(
                    level,
                    int,
                ):
                    raise TypeError(
                        f"Can not construct protobuf message of type "
                        f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeStringifyRequest': field 'level' is not "
                        f"of required type 'int'"
                    )
                # TODO: mypy-protobuf declares that
                # `google.protobuf.message.Message` constructor arguments are
                # always non-None, when in reality they are optional.
                __request__ = rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeStringifyRequest(
                    level=level,  # type: ignore[arg-type]
                )
                if isinstance(__context__, IMPORT_reboot_aio_contexts.WriterContext):
                    return (await NodeServicerTasks(
                        context=__context__,
                        state_ref=__context__._state_ref,
                    ).Stringify(
                        __request__,
                        schedule=__this__._when,
                    )).task_id

                __schedule__: IMPORT_typing.Optional[IMPORT_reboot_time_DateTimeWithTimeZone] = (IMPORT_reboot_time_DateTimeWithTimeZone.now() + __this__._when) if isinstance(
                    __this__._when, IMPORT_datetime_timedelta
                ) else __this__._when

                __metadata__: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None
                __idempotency__: IMPORT_typing.Optional[IMPORT_reboot_aio_idempotency.Idempotency] = __this__._idempotency
                __bearer_token__: IMPORT_typing.Optional[str] = None

                if __options__ is not None:
                    IMPORT_reboot_aio_types.assert_type(__options__, [IMPORT_reboot_aio_call.Options])
                    if __options__.idempotency is not None:
                        if __idempotency__ is not None:
                            raise RuntimeError(
                                'Found redundant idempotency in `Options`'
                            )
                        __idempotency__ = __options__.idempotency
                    if __options__.metadata is not None:
                        __metadata__ = __options__.metadata
                    if __options__.bearer_token is not None:
                        __bearer_token__ = __options__.bearer_token

                # Add scheduling information to the metadata.
                __metadata__ = (
                    (IMPORT_reboot_aio_headers.TASK_SCHEDULE,
                    __schedule__.isoformat() if __schedule__ else ''),
                ) + (__metadata__ or tuple())

                return await __this__._tasks(
                    __context__
                ).Stringify(
                    __request__,
                    idempotency=__idempotency__,
                    metadata=__metadata__,
                    bearer_token=__bearer_token__,
                )

            # Keep the original functions on the client, so old code will
            # continue to work, but use the new 'snake_case' method in
            # the new code.
            stringify = Stringify

        def spawn(
            self,
            *,
            when: IMPORT_typing.Optional[IMPORT_datetime_datetime | IMPORT_datetime_timedelta] = None,
        ) -> Node.WeakReference._Spawn:
            # Within a `workflow`, all "bare" `spawn()` calls are
            # syntactic sugar for `per_workflow()`, unless we're
            # within a control loop, in which case they are syntactic
            # sugar for `per_iteration()`.
            context = IMPORT_reboot_aio_contexts.Context.get()
            if context is not None:
                if isinstance(context, IMPORT_reboot_aio_contexts.WorkflowContext):
                    return (
                        self.per_iteration() if context.within_loop()
                        else self.per_workflow()
                    ).spawn(when=when)
                elif isinstance(context, IMPORT_reboot_aio_external.InitializeContext):
                    return self.idempotently().spawn(when=when)

            return Node.WeakReference._Spawn(
                self._application_id, self._tasks, when=when
            )

        class _Spawn:

            def __init__(
                self,
                application_id: IMPORT_typing.Optional[IMPORT_reboot_aio_types.ApplicationId],
                tasks: IMPORT_typing.Callable[[IMPORT_reboot_aio_contexts.TransactionContext | IMPORT_reboot_aio_contexts.WorkflowContext | IMPORT_reboot_aio_external.ExternalContext], NodeTasksStub],
                *,
                when: IMPORT_typing.Optional[IMPORT_datetime_datetime | IMPORT_datetime_timedelta] = None,
                idempotency: IMPORT_typing.Optional[IMPORT_reboot_aio_idempotency.Idempotency] = None,
            ) -> None:
                self._application_id = application_id
                self._tasks = tasks
                self._when = ensure_has_timezone(when=when)
                self._idempotency = idempotency

            # Node callable tasks:
            async def Create(
                __this__,
                __context__: IMPORT_reboot_aio_contexts.WorkflowContext | IMPORT_reboot_aio_external.ExternalContext,
                __options__: IMPORT_typing.Optional[IMPORT_reboot_aio_call.Options] = None,
                *,
                order: IMPORT_typing.Optional[int] = None,
                is_leaf: IMPORT_typing.Optional[bool] = None,
                keys: IMPORT_typing.Optional[IMPORT_typing.Iterable[str]] = None,
                children_ids: IMPORT_typing.Optional[IMPORT_typing.Iterable[str]] = None,
                values: IMPORT_typing.Optional[IMPORT_typing.Iterable[bytes]] = None,
                next_id: IMPORT_typing.Optional[str] = None,
                prev_id: IMPORT_typing.Optional[str] = None,
            ) -> Node.CreateTask:
                # UX improvement: check that neither positional argument was accidentally
                # given a gRPC request type.
                IMPORT_reboot_aio_types.assert_not_request_type(__context__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateRequest)
                IMPORT_reboot_aio_types.assert_not_request_type(__options__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateRequest)
                IMPORT_reboot_aio_types.assert_type(__context__, [IMPORT_reboot_aio_contexts.WorkflowContext, IMPORT_reboot_aio_external.ExternalContext])

                if order is not None and not isinstance(
                    order,
                    int,
                ):
                    raise TypeError(
                        f"Can not construct protobuf message of type "
                        f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateRequest': field 'order' is not "
                        f"of required type 'int'"
                    )
                if is_leaf is not None and not isinstance(
                    is_leaf,
                    bool,
                ):
                    raise TypeError(
                        f"Can not construct protobuf message of type "
                        f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateRequest': field 'is_leaf' is not "
                        f"of required type 'bool'"
                    )
                if keys is not None and not isinstance(
                    keys,
                    IMPORT_typing.Iterable,
                ):
                    raise TypeError(
                        f"Can not construct protobuf message of type "
                        f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateRequest': field 'keys' is not "
                        f"of required type 'typing.Iterable[str]'"
                    )
                if children_ids is not None and not isinstance(
                    children_ids,
                    IMPORT_typing.Iterable,
                ):
                    raise TypeError(
                        f"Can not construct protobuf message of type "
                        f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateRequest': field 'children_ids' is not "
                        f"of required type 'typing.Iterable[str]'"
                    )
                if values is not None and not isinstance(
                    values,
                    IMPORT_typing.Iterable,
                ):
                    raise TypeError(
                        f"Can not construct protobuf message of type "
                        f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateRequest': field 'values' is not "
                        f"of required type 'typing.Iterable[bytes]'"
                    )
                if next_id is not None and not isinstance(
                    next_id,
                    str,
                ):
                    raise TypeError(
                        f"Can not construct protobuf message of type "
                        f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateRequest': field 'next_id' is not "
                        f"of required type 'str'"
                    )
                if prev_id is not None and not isinstance(
                    prev_id,
                    str,
                ):
                    raise TypeError(
                        f"Can not construct protobuf message of type "
                        f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateRequest': field 'prev_id' is not "
                        f"of required type 'str'"
                    )
                # TODO: mypy-protobuf declares that
                # `IMPORT_google_protobuf_message.Message` constructor arguments are
                # always non-None, when in reality they are optional.
                __request__ = rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateRequest(
                    order=order,  # type: ignore[arg-type]
                    is_leaf=is_leaf,  # type: ignore[arg-type]
                    keys=keys,  # type: ignore[arg-type]
                    children_ids=children_ids,  # type: ignore[arg-type]
                    values=values,  # type: ignore[arg-type]
                    next_id=next_id,  # type: ignore[arg-type]
                    prev_id=prev_id,  # type: ignore[arg-type]
                )

                __schedule__: IMPORT_typing.Optional[IMPORT_reboot_time_DateTimeWithTimeZone] = (IMPORT_reboot_time_DateTimeWithTimeZone.now() + __this__._when) if isinstance(
                    __this__._when, IMPORT_datetime_timedelta
                ) else __this__._when

                __metadata__: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None
                __idempotency__: IMPORT_typing.Optional[IMPORT_reboot_aio_idempotency.Idempotency] = __this__._idempotency
                __bearer_token__: IMPORT_typing.Optional[str] = None

                if __options__ is not None:
                    IMPORT_reboot_aio_types.assert_type(__options__, [IMPORT_reboot_aio_call.Options])
                    if __options__.idempotency:
                        if __idempotency__ is not None:
                            raise RuntimeError(
                                'Found redundant idempotency in `Options`'
                            )
                        __idempotency__ = __options__.idempotency
                    if __options__.metadata is not None:
                        __metadata__ = __options__.metadata
                    if __options__.bearer_token is not None:
                        __bearer_token__ = __options__.bearer_token

                # Add scheduling information to the metadata.
                __metadata__ = (
                    (IMPORT_reboot_aio_headers.TASK_SCHEDULE,
                    __schedule__.isoformat() if __schedule__ else ''),
                ) + (__metadata__ or tuple())

                __task_id__ = await __this__._tasks(
                    __context__
                ).Create(
                    __request__,
                    idempotency=__idempotency__,
                    metadata=__metadata__,
                    bearer_token=__bearer_token__,
                )

                return Node.CreateTask(
                    __context__,
                    task_id=__task_id__,
                )

            # Keep the original functions on the client, so old code will
            # continue to work, but use the new 'snake_case' method in
            # the new code.
            create = Create
            async def Search(
                __this__,
                __context__: IMPORT_reboot_aio_contexts.WorkflowContext | IMPORT_reboot_aio_external.ExternalContext,
                __options__: IMPORT_typing.Optional[IMPORT_reboot_aio_call.Options] = None,
                *,
                key: IMPORT_typing.Optional[str] = None,
            ) -> Node.SearchTask:
                # UX improvement: check that neither positional argument was accidentally
                # given a gRPC request type.
                IMPORT_reboot_aio_types.assert_not_request_type(__context__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeSearchRequest)
                IMPORT_reboot_aio_types.assert_not_request_type(__options__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeSearchRequest)
                IMPORT_reboot_aio_types.assert_type(__context__, [IMPORT_reboot_aio_contexts.WorkflowContext, IMPORT_reboot_aio_external.ExternalContext])

                if key is not None and not isinstance(
                    key,
                    str,
                ):
                    raise TypeError(
                        f"Can not construct protobuf message of type "
                        f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeSearchRequest': field 'key' is not "
                        f"of required type 'str'"
                    )
                # TODO: mypy-protobuf declares that
                # `IMPORT_google_protobuf_message.Message` constructor arguments are
                # always non-None, when in reality they are optional.
                __request__ = rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeSearchRequest(
                    key=key,  # type: ignore[arg-type]
                )

                __schedule__: IMPORT_typing.Optional[IMPORT_reboot_time_DateTimeWithTimeZone] = (IMPORT_reboot_time_DateTimeWithTimeZone.now() + __this__._when) if isinstance(
                    __this__._when, IMPORT_datetime_timedelta
                ) else __this__._when

                __metadata__: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None
                __idempotency__: IMPORT_typing.Optional[IMPORT_reboot_aio_idempotency.Idempotency] = __this__._idempotency
                __bearer_token__: IMPORT_typing.Optional[str] = None

                if __options__ is not None:
                    IMPORT_reboot_aio_types.assert_type(__options__, [IMPORT_reboot_aio_call.Options])
                    if __options__.idempotency:
                        if __idempotency__ is not None:
                            raise RuntimeError(
                                'Found redundant idempotency in `Options`'
                            )
                        __idempotency__ = __options__.idempotency
                    if __options__.metadata is not None:
                        __metadata__ = __options__.metadata
                    if __options__.bearer_token is not None:
                        __bearer_token__ = __options__.bearer_token

                # Add scheduling information to the metadata.
                __metadata__ = (
                    (IMPORT_reboot_aio_headers.TASK_SCHEDULE,
                    __schedule__.isoformat() if __schedule__ else ''),
                ) + (__metadata__ or tuple())

                __task_id__ = await __this__._tasks(
                    __context__
                ).Search(
                    __request__,
                    idempotency=__idempotency__,
                    metadata=__metadata__,
                    bearer_token=__bearer_token__,
                )

                return Node.SearchTask(
                    __context__,
                    task_id=__task_id__,
                )

            # Keep the original functions on the client, so old code will
            # continue to work, but use the new 'snake_case' method in
            # the new code.
            search = Search
            async def Insert(
                __this__,
                __context__: IMPORT_reboot_aio_contexts.WorkflowContext | IMPORT_reboot_aio_external.ExternalContext,
                __options__: IMPORT_typing.Optional[IMPORT_reboot_aio_call.Options] = None,
                *,
                key: IMPORT_typing.Optional[str] = None,
                value: IMPORT_typing.Optional[bytes] = None,
            ) -> Node.InsertTask:
                # UX improvement: check that neither positional argument was accidentally
                # given a gRPC request type.
                IMPORT_reboot_aio_types.assert_not_request_type(__context__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeInsertRequest)
                IMPORT_reboot_aio_types.assert_not_request_type(__options__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeInsertRequest)
                IMPORT_reboot_aio_types.assert_type(__context__, [IMPORT_reboot_aio_contexts.WorkflowContext, IMPORT_reboot_aio_external.ExternalContext])

                if key is not None and not isinstance(
                    key,
                    str,
                ):
                    raise TypeError(
                        f"Can not construct protobuf message of type "
                        f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeInsertRequest': field 'key' is not "
                        f"of required type 'str'"
                    )
                if value is not None and not isinstance(
                    value,
                    bytes,
                ):
                    raise TypeError(
                        f"Can not construct protobuf message of type "
                        f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeInsertRequest': field 'value' is not "
                        f"of required type 'bytes'"
                    )
                # TODO: mypy-protobuf declares that
                # `IMPORT_google_protobuf_message.Message` constructor arguments are
                # always non-None, when in reality they are optional.
                __request__ = rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeInsertRequest(
                    key=key,  # type: ignore[arg-type]
                    value=value,  # type: ignore[arg-type]
                )

                __schedule__: IMPORT_typing.Optional[IMPORT_reboot_time_DateTimeWithTimeZone] = (IMPORT_reboot_time_DateTimeWithTimeZone.now() + __this__._when) if isinstance(
                    __this__._when, IMPORT_datetime_timedelta
                ) else __this__._when

                __metadata__: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None
                __idempotency__: IMPORT_typing.Optional[IMPORT_reboot_aio_idempotency.Idempotency] = __this__._idempotency
                __bearer_token__: IMPORT_typing.Optional[str] = None

                if __options__ is not None:
                    IMPORT_reboot_aio_types.assert_type(__options__, [IMPORT_reboot_aio_call.Options])
                    if __options__.idempotency:
                        if __idempotency__ is not None:
                            raise RuntimeError(
                                'Found redundant idempotency in `Options`'
                            )
                        __idempotency__ = __options__.idempotency
                    if __options__.metadata is not None:
                        __metadata__ = __options__.metadata
                    if __options__.bearer_token is not None:
                        __bearer_token__ = __options__.bearer_token

                # Add scheduling information to the metadata.
                __metadata__ = (
                    (IMPORT_reboot_aio_headers.TASK_SCHEDULE,
                    __schedule__.isoformat() if __schedule__ else ''),
                ) + (__metadata__ or tuple())

                __task_id__ = await __this__._tasks(
                    __context__
                ).Insert(
                    __request__,
                    idempotency=__idempotency__,
                    metadata=__metadata__,
                    bearer_token=__bearer_token__,
                )

                return Node.InsertTask(
                    __context__,
                    task_id=__task_id__,
                )

            # Keep the original functions on the client, so old code will
            # continue to work, but use the new 'snake_case' method in
            # the new code.
            insert = Insert
            async def Remove(
                __this__,
                __context__: IMPORT_reboot_aio_contexts.WorkflowContext | IMPORT_reboot_aio_external.ExternalContext,
                __options__: IMPORT_typing.Optional[IMPORT_reboot_aio_call.Options] = None,
                *,
                key: IMPORT_typing.Optional[str] = None,
            ) -> Node.RemoveTask:
                # UX improvement: check that neither positional argument was accidentally
                # given a gRPC request type.
                IMPORT_reboot_aio_types.assert_not_request_type(__context__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRemoveRequest)
                IMPORT_reboot_aio_types.assert_not_request_type(__options__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRemoveRequest)
                IMPORT_reboot_aio_types.assert_type(__context__, [IMPORT_reboot_aio_contexts.WorkflowContext, IMPORT_reboot_aio_external.ExternalContext])

                if key is not None and not isinstance(
                    key,
                    str,
                ):
                    raise TypeError(
                        f"Can not construct protobuf message of type "
                        f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRemoveRequest': field 'key' is not "
                        f"of required type 'str'"
                    )
                # TODO: mypy-protobuf declares that
                # `IMPORT_google_protobuf_message.Message` constructor arguments are
                # always non-None, when in reality they are optional.
                __request__ = rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRemoveRequest(
                    key=key,  # type: ignore[arg-type]
                )

                __schedule__: IMPORT_typing.Optional[IMPORT_reboot_time_DateTimeWithTimeZone] = (IMPORT_reboot_time_DateTimeWithTimeZone.now() + __this__._when) if isinstance(
                    __this__._when, IMPORT_datetime_timedelta
                ) else __this__._when

                __metadata__: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None
                __idempotency__: IMPORT_typing.Optional[IMPORT_reboot_aio_idempotency.Idempotency] = __this__._idempotency
                __bearer_token__: IMPORT_typing.Optional[str] = None

                if __options__ is not None:
                    IMPORT_reboot_aio_types.assert_type(__options__, [IMPORT_reboot_aio_call.Options])
                    if __options__.idempotency:
                        if __idempotency__ is not None:
                            raise RuntimeError(
                                'Found redundant idempotency in `Options`'
                            )
                        __idempotency__ = __options__.idempotency
                    if __options__.metadata is not None:
                        __metadata__ = __options__.metadata
                    if __options__.bearer_token is not None:
                        __bearer_token__ = __options__.bearer_token

                # Add scheduling information to the metadata.
                __metadata__ = (
                    (IMPORT_reboot_aio_headers.TASK_SCHEDULE,
                    __schedule__.isoformat() if __schedule__ else ''),
                ) + (__metadata__ or tuple())

                __task_id__ = await __this__._tasks(
                    __context__
                ).Remove(
                    __request__,
                    idempotency=__idempotency__,
                    metadata=__metadata__,
                    bearer_token=__bearer_token__,
                )

                return Node.RemoveTask(
                    __context__,
                    task_id=__task_id__,
                )

            # Keep the original functions on the client, so old code will
            # continue to work, but use the new 'snake_case' method in
            # the new code.
            remove = Remove
            async def Range(
                __this__,
                __context__: IMPORT_reboot_aio_contexts.WorkflowContext | IMPORT_reboot_aio_external.ExternalContext,
                __options__: IMPORT_typing.Optional[IMPORT_reboot_aio_call.Options] = None,
                *,
                start_key: IMPORT_typing.Optional[str] = None,
                limit: IMPORT_typing.Optional[int] = None,
            ) -> Node.RangeTask:
                # UX improvement: check that neither positional argument was accidentally
                # given a gRPC request type.
                IMPORT_reboot_aio_types.assert_not_request_type(__context__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRangeRequest)
                IMPORT_reboot_aio_types.assert_not_request_type(__options__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRangeRequest)
                IMPORT_reboot_aio_types.assert_type(__context__, [IMPORT_reboot_aio_contexts.WorkflowContext, IMPORT_reboot_aio_external.ExternalContext])

                if start_key is not None and not isinstance(
                    start_key,
                    str,
                ):
                    raise TypeError(
                        f"Can not construct protobuf message of type "
                        f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRangeRequest': field 'start_key' is not "
                        f"of required type 'str'"
                    )
                if limit is not None and not isinstance(
                    limit,
                    int,
                ):
                    raise TypeError(
                        f"Can not construct protobuf message of type "
                        f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRangeRequest': field 'limit' is not "
                        f"of required type 'int'"
                    )
                # TODO: mypy-protobuf declares that
                # `IMPORT_google_protobuf_message.Message` constructor arguments are
                # always non-None, when in reality they are optional.
                __request__ = rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRangeRequest(
                    start_key=start_key,  # type: ignore[arg-type]
                    limit=limit,  # type: ignore[arg-type]
                )

                __schedule__: IMPORT_typing.Optional[IMPORT_reboot_time_DateTimeWithTimeZone] = (IMPORT_reboot_time_DateTimeWithTimeZone.now() + __this__._when) if isinstance(
                    __this__._when, IMPORT_datetime_timedelta
                ) else __this__._when

                __metadata__: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None
                __idempotency__: IMPORT_typing.Optional[IMPORT_reboot_aio_idempotency.Idempotency] = __this__._idempotency
                __bearer_token__: IMPORT_typing.Optional[str] = None

                if __options__ is not None:
                    IMPORT_reboot_aio_types.assert_type(__options__, [IMPORT_reboot_aio_call.Options])
                    if __options__.idempotency:
                        if __idempotency__ is not None:
                            raise RuntimeError(
                                'Found redundant idempotency in `Options`'
                            )
                        __idempotency__ = __options__.idempotency
                    if __options__.metadata is not None:
                        __metadata__ = __options__.metadata
                    if __options__.bearer_token is not None:
                        __bearer_token__ = __options__.bearer_token

                # Add scheduling information to the metadata.
                __metadata__ = (
                    (IMPORT_reboot_aio_headers.TASK_SCHEDULE,
                    __schedule__.isoformat() if __schedule__ else ''),
                ) + (__metadata__ or tuple())

                __task_id__ = await __this__._tasks(
                    __context__
                ).Range(
                    __request__,
                    idempotency=__idempotency__,
                    metadata=__metadata__,
                    bearer_token=__bearer_token__,
                )

                return Node.RangeTask(
                    __context__,
                    task_id=__task_id__,
                )

            # Keep the original functions on the client, so old code will
            # continue to work, but use the new 'snake_case' method in
            # the new code.
            range = Range
            async def ReverseRange(
                __this__,
                __context__: IMPORT_reboot_aio_contexts.WorkflowContext | IMPORT_reboot_aio_external.ExternalContext,
                __options__: IMPORT_typing.Optional[IMPORT_reboot_aio_call.Options] = None,
                *,
                start_key: IMPORT_typing.Optional[str] = None,
                limit: IMPORT_typing.Optional[int] = None,
            ) -> Node.ReverseRangeTask:
                # UX improvement: check that neither positional argument was accidentally
                # given a gRPC request type.
                IMPORT_reboot_aio_types.assert_not_request_type(__context__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeReverseRangeRequest)
                IMPORT_reboot_aio_types.assert_not_request_type(__options__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeReverseRangeRequest)
                IMPORT_reboot_aio_types.assert_type(__context__, [IMPORT_reboot_aio_contexts.WorkflowContext, IMPORT_reboot_aio_external.ExternalContext])

                if start_key is not None and not isinstance(
                    start_key,
                    str,
                ):
                    raise TypeError(
                        f"Can not construct protobuf message of type "
                        f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeReverseRangeRequest': field 'start_key' is not "
                        f"of required type 'str'"
                    )
                if limit is not None and not isinstance(
                    limit,
                    int,
                ):
                    raise TypeError(
                        f"Can not construct protobuf message of type "
                        f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeReverseRangeRequest': field 'limit' is not "
                        f"of required type 'int'"
                    )
                # TODO: mypy-protobuf declares that
                # `IMPORT_google_protobuf_message.Message` constructor arguments are
                # always non-None, when in reality they are optional.
                __request__ = rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeReverseRangeRequest(
                    start_key=start_key,  # type: ignore[arg-type]
                    limit=limit,  # type: ignore[arg-type]
                )

                __schedule__: IMPORT_typing.Optional[IMPORT_reboot_time_DateTimeWithTimeZone] = (IMPORT_reboot_time_DateTimeWithTimeZone.now() + __this__._when) if isinstance(
                    __this__._when, IMPORT_datetime_timedelta
                ) else __this__._when

                __metadata__: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None
                __idempotency__: IMPORT_typing.Optional[IMPORT_reboot_aio_idempotency.Idempotency] = __this__._idempotency
                __bearer_token__: IMPORT_typing.Optional[str] = None

                if __options__ is not None:
                    IMPORT_reboot_aio_types.assert_type(__options__, [IMPORT_reboot_aio_call.Options])
                    if __options__.idempotency:
                        if __idempotency__ is not None:
                            raise RuntimeError(
                                'Found redundant idempotency in `Options`'
                            )
                        __idempotency__ = __options__.idempotency
                    if __options__.metadata is not None:
                        __metadata__ = __options__.metadata
                    if __options__.bearer_token is not None:
                        __bearer_token__ = __options__.bearer_token

                # Add scheduling information to the metadata.
                __metadata__ = (
                    (IMPORT_reboot_aio_headers.TASK_SCHEDULE,
                    __schedule__.isoformat() if __schedule__ else ''),
                ) + (__metadata__ or tuple())

                __task_id__ = await __this__._tasks(
                    __context__
                ).ReverseRange(
                    __request__,
                    idempotency=__idempotency__,
                    metadata=__metadata__,
                    bearer_token=__bearer_token__,
                )

                return Node.ReverseRangeTask(
                    __context__,
                    task_id=__task_id__,
                )

            # Keep the original functions on the client, so old code will
            # continue to work, but use the new 'snake_case' method in
            # the new code.
            reverse_range = ReverseRange
            async def Stringify(
                __this__,
                __context__: IMPORT_reboot_aio_contexts.WorkflowContext | IMPORT_reboot_aio_external.ExternalContext,
                __options__: IMPORT_typing.Optional[IMPORT_reboot_aio_call.Options] = None,
                *,
                level: IMPORT_typing.Optional[int] = None,
            ) -> Node.StringifyTask:
                # UX improvement: check that neither positional argument was accidentally
                # given a gRPC request type.
                IMPORT_reboot_aio_types.assert_not_request_type(__context__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeStringifyRequest)
                IMPORT_reboot_aio_types.assert_not_request_type(__options__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeStringifyRequest)
                IMPORT_reboot_aio_types.assert_type(__context__, [IMPORT_reboot_aio_contexts.WorkflowContext, IMPORT_reboot_aio_external.ExternalContext])

                if level is not None and not isinstance(
                    level,
                    int,
                ):
                    raise TypeError(
                        f"Can not construct protobuf message of type "
                        f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeStringifyRequest': field 'level' is not "
                        f"of required type 'int'"
                    )
                # TODO: mypy-protobuf declares that
                # `IMPORT_google_protobuf_message.Message` constructor arguments are
                # always non-None, when in reality they are optional.
                __request__ = rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeStringifyRequest(
                    level=level,  # type: ignore[arg-type]
                )

                __schedule__: IMPORT_typing.Optional[IMPORT_reboot_time_DateTimeWithTimeZone] = (IMPORT_reboot_time_DateTimeWithTimeZone.now() + __this__._when) if isinstance(
                    __this__._when, IMPORT_datetime_timedelta
                ) else __this__._when

                __metadata__: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None
                __idempotency__: IMPORT_typing.Optional[IMPORT_reboot_aio_idempotency.Idempotency] = __this__._idempotency
                __bearer_token__: IMPORT_typing.Optional[str] = None

                if __options__ is not None:
                    IMPORT_reboot_aio_types.assert_type(__options__, [IMPORT_reboot_aio_call.Options])
                    if __options__.idempotency:
                        if __idempotency__ is not None:
                            raise RuntimeError(
                                'Found redundant idempotency in `Options`'
                            )
                        __idempotency__ = __options__.idempotency
                    if __options__.metadata is not None:
                        __metadata__ = __options__.metadata
                    if __options__.bearer_token is not None:
                        __bearer_token__ = __options__.bearer_token

                # Add scheduling information to the metadata.
                __metadata__ = (
                    (IMPORT_reboot_aio_headers.TASK_SCHEDULE,
                    __schedule__.isoformat() if __schedule__ else ''),
                ) + (__metadata__ or tuple())

                __task_id__ = await __this__._tasks(
                    __context__
                ).Stringify(
                    __request__,
                    idempotency=__idempotency__,
                    metadata=__metadata__,
                    bearer_token=__bearer_token__,
                )

                return Node.StringifyTask(
                    __context__,
                    task_id=__task_id__,
                )

            # Keep the original functions on the client, so old code will
            # continue to work, but use the new 'snake_case' method in
            # the new code.
            stringify = Stringify

        async def read(
            self,
            context: IMPORT_reboot_aio_contexts.WorkflowContext,
        ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node:
            return await (
                self.always() if context.within_until()
                else (
                    self.per_iteration() if context.within_loop()
                    else self.per_workflow()
                )
            ).read(context)

        @IMPORT_typing.overload
        async def write(
            self,
            context: IMPORT_reboot_aio_contexts.WorkflowContext,
            writer: NodeBaseServicer.InlineWriterCallable[None],
            *,
            type: type = type(None),
        ) -> None:
            ...

        @IMPORT_typing.overload
        async def write(
            self,
            context: IMPORT_reboot_aio_contexts.WorkflowContext,
            writer: NodeBaseServicer.InlineWriterCallable[NodeBaseServicer.InlineWriterCallableResult],
            *,
            type: type[NodeBaseServicer.InlineWriterCallableResult],
        ) -> NodeBaseServicer.InlineWriterCallableResult:
            ...

        async def write(
            self,
            context: IMPORT_reboot_aio_contexts.WorkflowContext,
            writer: NodeBaseServicer.InlineWriterCallable[NodeBaseServicer.InlineWriterCallableResult],
            *,
            type: type = type(None),
        ) -> NodeBaseServicer.InlineWriterCallableResult:
            """Perform an "inline write" within a workflow."""
            return await (
                self.always() if context.within_until()
                else (
                    self.per_iteration() if context.within_loop()
                    else self.per_workflow()
                )
            ).write(
                context, writer, type=type
            )

        # Node specific methods:
        async def Create(
            __this__,
            __context__: IMPORT_reboot_aio_contexts.TransactionContext | IMPORT_reboot_aio_contexts.WorkflowContext | IMPORT_reboot_aio_external.ExternalContext,
            __options__: IMPORT_typing.Optional[IMPORT_reboot_aio_call.Options] = None,
            *,
            order: IMPORT_typing.Optional[int] = None,
            is_leaf: IMPORT_typing.Optional[bool] = None,
            keys: IMPORT_typing.Optional[IMPORT_typing.Iterable[str]] = None,
            children_ids: IMPORT_typing.Optional[IMPORT_typing.Iterable[str]] = None,
            values: IMPORT_typing.Optional[IMPORT_typing.Iterable[bytes]] = None,
            next_id: IMPORT_typing.Optional[str] = None,
            prev_id: IMPORT_typing.Optional[str] = None,
        ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateResponse:
            # UX improvement: check that neither positional argument was accidentally
            # given a gRPC request type.
            IMPORT_reboot_aio_types.assert_not_request_type(__context__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateRequest)
            IMPORT_reboot_aio_types.assert_not_request_type(__options__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateRequest)

            # Within a `workflow`, all "bare" calls are
            # `per_workflow()` calls, unless we're within a control
            # loop, in which case they are syntactic sugar for
            # `per_iteration()`.
            if __options__ is None or __options__.idempotency is None:
                if isinstance(__context__, IMPORT_reboot_aio_contexts.WorkflowContext):
                    return await (
                        __this__.per_iteration() if __context__.within_loop()
                        else __this__.per_workflow()
                    ).Create(
                        __context__,
                        __options__ or IMPORT_reboot_aio_call.Options(),
                        order=order,
                        is_leaf=is_leaf,
                        keys=keys,
                        children_ids=children_ids,
                        values=values,
                        next_id=next_id,
                        prev_id=prev_id,
                    )
                elif isinstance(__context__, IMPORT_reboot_aio_external.InitializeContext):
                    return await __this__.idempotently().Create(
                        __context__,
                        __options__ or IMPORT_reboot_aio_call.Options(),
                        order=order,
                        is_leaf=is_leaf,
                        keys=keys,
                        children_ids=children_ids,
                        values=values,
                        next_id=next_id,
                        prev_id=prev_id,
                    )

            if order is not None and not isinstance(
                order,
                int,
            ):
                raise TypeError(
                    f"Can not construct protobuf message of type "
                    f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateRequest': field 'order' is not "
                    f"of required type 'int'"
                )
            if is_leaf is not None and not isinstance(
                is_leaf,
                bool,
            ):
                raise TypeError(
                    f"Can not construct protobuf message of type "
                    f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateRequest': field 'is_leaf' is not "
                    f"of required type 'bool'"
                )
            if keys is not None and not isinstance(
                keys,
                IMPORT_typing.Iterable,
            ):
                raise TypeError(
                    f"Can not construct protobuf message of type "
                    f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateRequest': field 'keys' is not "
                    f"of required type 'typing.Iterable[str]'"
                )
            if children_ids is not None and not isinstance(
                children_ids,
                IMPORT_typing.Iterable,
            ):
                raise TypeError(
                    f"Can not construct protobuf message of type "
                    f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateRequest': field 'children_ids' is not "
                    f"of required type 'typing.Iterable[str]'"
                )
            if values is not None and not isinstance(
                values,
                IMPORT_typing.Iterable,
            ):
                raise TypeError(
                    f"Can not construct protobuf message of type "
                    f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateRequest': field 'values' is not "
                    f"of required type 'typing.Iterable[bytes]'"
                )
            if next_id is not None and not isinstance(
                next_id,
                str,
            ):
                raise TypeError(
                    f"Can not construct protobuf message of type "
                    f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateRequest': field 'next_id' is not "
                    f"of required type 'str'"
                )
            if prev_id is not None and not isinstance(
                prev_id,
                str,
            ):
                raise TypeError(
                    f"Can not construct protobuf message of type "
                    f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateRequest': field 'prev_id' is not "
                    f"of required type 'str'"
                )
            # TODO: mypy-protobuf declares that
            # `google.protobuf.message.Message` constructor arguments are
            # always non-None, when in reality they are optional.
            __request__ = rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateRequest(
                order=order,  # type: ignore[arg-type]
                is_leaf=is_leaf,  # type: ignore[arg-type]
                keys=keys,  # type: ignore[arg-type]
                children_ids=children_ids,  # type: ignore[arg-type]
                values=values,  # type: ignore[arg-type]
                next_id=next_id,  # type: ignore[arg-type]
                prev_id=prev_id,  # type: ignore[arg-type]
            )
            __idempotency__: IMPORT_typing.Optional[IMPORT_reboot_aio_idempotency.Idempotency] = None
            __metadata__: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None
            __bearer_token__: IMPORT_typing.Optional[str] = None
            if __options__ is not None:
                IMPORT_reboot_aio_types.assert_type(__options__, [IMPORT_reboot_aio_call.Options])
                if __options__.idempotency is not None:
                    __idempotency__ = __options__.idempotency
                if __options__.metadata is not None:
                    __metadata__ = __options__.metadata
                if __options__.bearer_token is not None:
                    __bearer_token__ = __options__.bearer_token

            return await __this__._writer(__context__).Create(
                __request__,
                idempotency=__idempotency__,
                metadata=__metadata__,
                bearer_token=__bearer_token__,
            )

        # Keep the original functions on the client, so old code will
        # continue to work, but use the new 'snake_case' method in
        # the new code.
        create = Create
        async def Search(
            __this__,
            __context__: IMPORT_reboot_aio_contexts.ReaderContext | IMPORT_reboot_aio_contexts.WriterContext | IMPORT_reboot_aio_contexts.TransactionContext | IMPORT_reboot_aio_contexts.WorkflowContext | IMPORT_reboot_aio_external.ExternalContext,
            __options__: IMPORT_typing.Optional[IMPORT_reboot_aio_call.Options] = None,
            *,
            key: IMPORT_typing.Optional[str] = None,
        ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeSearchResponse:
            # UX improvement: check that neither positional argument was accidentally
            # given a gRPC request type.
            IMPORT_reboot_aio_types.assert_not_request_type(__context__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeSearchRequest)
            IMPORT_reboot_aio_types.assert_not_request_type(__options__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeSearchRequest)

            # Within a `workflow`, all "bare" calls are
            # `per_workflow()` calls, unless we're within a control
            # loop, in which case they are syntactic sugar for
            # `per_iteration()`.
            #
            # Unless we are "within until" in which case all "bare"
            # calls are `.always().
            if __options__ is None or __options__.idempotency is None:
                if isinstance(__context__, IMPORT_reboot_aio_contexts.WorkflowContext):
                    return await (
                        __this__.always() if __context__.within_until()
                        else (
                            __this__.per_iteration() if __context__.within_loop()
                            else __this__.per_workflow()
                        )
                    ).Search(
                        __context__,
                        __options__ or IMPORT_reboot_aio_call.Options(),
                        key=key,
                    )
                elif isinstance(__context__, IMPORT_reboot_aio_external.InitializeContext):
                    return await __this__.idempotently().Search(
                        __context__,
                        __options__ or IMPORT_reboot_aio_call.Options(),
                        key=key,
                    )

            if key is not None and not isinstance(
                key,
                str,
            ):
                raise TypeError(
                    f"Can not construct protobuf message of type "
                    f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeSearchRequest': field 'key' is not "
                    f"of required type 'str'"
                )
            # TODO: mypy-protobuf declares that
            # `google.protobuf.message.Message` constructor arguments are
            # always non-None, when in reality they are optional.
            __request__ = rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeSearchRequest(
                key=key,  # type: ignore[arg-type]
            )
            __metadata__: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None
            __bearer_token__: IMPORT_typing.Optional[str] = None
            if __options__ is not None:
                IMPORT_reboot_aio_types.assert_type(__options__, [IMPORT_reboot_aio_call.Options])
                if __options__.metadata is not None:
                    __metadata__ = __options__.metadata
                if __options__.bearer_token is not None:
                    __bearer_token__ = __options__.bearer_token
            return await __this__._reader(__context__).Search(
                __request__,
                metadata=__metadata__,
                bearer_token=__bearer_token__,
                idempotency=__options__.idempotency if __options__ is not None else None,
            )
        # Keep the original functions on the client, so old code will
        # continue to work, but use the new 'snake_case' method in
        # the new code.
        search = Search
        async def Insert(
            __this__,
            __context__: IMPORT_reboot_aio_contexts.TransactionContext | IMPORT_reboot_aio_contexts.WorkflowContext | IMPORT_reboot_aio_external.ExternalContext,
            __options__: IMPORT_typing.Optional[IMPORT_reboot_aio_call.Options] = None,
            *,
            key: IMPORT_typing.Optional[str] = None,
            value: IMPORT_typing.Optional[bytes] = None,
        ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeInsertResponse:
            # UX improvement: check that neither positional argument was accidentally
            # given a gRPC request type.
            IMPORT_reboot_aio_types.assert_not_request_type(__context__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeInsertRequest)
            IMPORT_reboot_aio_types.assert_not_request_type(__options__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeInsertRequest)

            # Within a `workflow`, all "bare" calls are
            # `per_workflow()` calls, unless we're within a control
            # loop, in which case they are syntactic sugar for
            # `per_iteration()`.
            if __options__ is None or __options__.idempotency is None:
                if isinstance(__context__, IMPORT_reboot_aio_contexts.WorkflowContext):
                    return await (
                        __this__.per_iteration() if __context__.within_loop()
                        else __this__.per_workflow()
                    ).Insert(
                        __context__,
                        __options__ or IMPORT_reboot_aio_call.Options(),
                        key=key,
                        value=value,
                    )
                elif isinstance(__context__, IMPORT_reboot_aio_external.InitializeContext):
                    return await __this__.idempotently().Insert(
                        __context__,
                        __options__ or IMPORT_reboot_aio_call.Options(),
                        key=key,
                        value=value,
                    )

            if key is not None and not isinstance(
                key,
                str,
            ):
                raise TypeError(
                    f"Can not construct protobuf message of type "
                    f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeInsertRequest': field 'key' is not "
                    f"of required type 'str'"
                )
            if value is not None and not isinstance(
                value,
                bytes,
            ):
                raise TypeError(
                    f"Can not construct protobuf message of type "
                    f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeInsertRequest': field 'value' is not "
                    f"of required type 'bytes'"
                )
            # TODO: mypy-protobuf declares that
            # `google.protobuf.message.Message` constructor arguments are
            # always non-None, when in reality they are optional.
            __request__ = rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeInsertRequest(
                key=key,  # type: ignore[arg-type]
                value=value,  # type: ignore[arg-type]
            )
            __idempotency__: IMPORT_typing.Optional[IMPORT_reboot_aio_idempotency.Idempotency] = None
            __metadata__: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None
            __bearer_token__: IMPORT_typing.Optional[str] = None
            if __options__ is not None:
                IMPORT_reboot_aio_types.assert_type(__options__, [IMPORT_reboot_aio_call.Options])
                if __options__.idempotency is not None:
                    __idempotency__ = __options__.idempotency
                if __options__.metadata is not None:
                    __metadata__ = __options__.metadata
                if __options__.bearer_token is not None:
                    __bearer_token__ = __options__.bearer_token

            return await __this__._workflow(__context__).Insert(
                __request__,
                idempotency=__idempotency__,
                metadata=__metadata__,
                bearer_token=__bearer_token__,
            )

        # Keep the original functions on the client, so old code will
        # continue to work, but use the new 'snake_case' method in
        # the new code.
        insert = Insert
        async def Remove(
            __this__,
            __context__: IMPORT_reboot_aio_contexts.TransactionContext | IMPORT_reboot_aio_contexts.WorkflowContext | IMPORT_reboot_aio_external.ExternalContext,
            __options__: IMPORT_typing.Optional[IMPORT_reboot_aio_call.Options] = None,
            *,
            key: IMPORT_typing.Optional[str] = None,
        ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRemoveResponse:
            # UX improvement: check that neither positional argument was accidentally
            # given a gRPC request type.
            IMPORT_reboot_aio_types.assert_not_request_type(__context__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRemoveRequest)
            IMPORT_reboot_aio_types.assert_not_request_type(__options__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRemoveRequest)

            # Within a `workflow`, all "bare" calls are
            # `per_workflow()` calls, unless we're within a control
            # loop, in which case they are syntactic sugar for
            # `per_iteration()`.
            if __options__ is None or __options__.idempotency is None:
                if isinstance(__context__, IMPORT_reboot_aio_contexts.WorkflowContext):
                    return await (
                        __this__.per_iteration() if __context__.within_loop()
                        else __this__.per_workflow()
                    ).Remove(
                        __context__,
                        __options__ or IMPORT_reboot_aio_call.Options(),
                        key=key,
                    )
                elif isinstance(__context__, IMPORT_reboot_aio_external.InitializeContext):
                    return await __this__.idempotently().Remove(
                        __context__,
                        __options__ or IMPORT_reboot_aio_call.Options(),
                        key=key,
                    )

            if key is not None and not isinstance(
                key,
                str,
            ):
                raise TypeError(
                    f"Can not construct protobuf message of type "
                    f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRemoveRequest': field 'key' is not "
                    f"of required type 'str'"
                )
            # TODO: mypy-protobuf declares that
            # `google.protobuf.message.Message` constructor arguments are
            # always non-None, when in reality they are optional.
            __request__ = rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRemoveRequest(
                key=key,  # type: ignore[arg-type]
            )
            __idempotency__: IMPORT_typing.Optional[IMPORT_reboot_aio_idempotency.Idempotency] = None
            __metadata__: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None
            __bearer_token__: IMPORT_typing.Optional[str] = None
            if __options__ is not None:
                IMPORT_reboot_aio_types.assert_type(__options__, [IMPORT_reboot_aio_call.Options])
                if __options__.idempotency is not None:
                    __idempotency__ = __options__.idempotency
                if __options__.metadata is not None:
                    __metadata__ = __options__.metadata
                if __options__.bearer_token is not None:
                    __bearer_token__ = __options__.bearer_token

            return await __this__._workflow(__context__).Remove(
                __request__,
                idempotency=__idempotency__,
                metadata=__metadata__,
                bearer_token=__bearer_token__,
            )

        # Keep the original functions on the client, so old code will
        # continue to work, but use the new 'snake_case' method in
        # the new code.
        remove = Remove
        async def Range(
            __this__,
            __context__: IMPORT_reboot_aio_contexts.ReaderContext | IMPORT_reboot_aio_contexts.WriterContext | IMPORT_reboot_aio_contexts.TransactionContext | IMPORT_reboot_aio_contexts.WorkflowContext | IMPORT_reboot_aio_external.ExternalContext,
            __options__: IMPORT_typing.Optional[IMPORT_reboot_aio_call.Options] = None,
            *,
            start_key: IMPORT_typing.Optional[str] = None,
            limit: IMPORT_typing.Optional[int] = None,
        ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRangeResponse:
            # UX improvement: check that neither positional argument was accidentally
            # given a gRPC request type.
            IMPORT_reboot_aio_types.assert_not_request_type(__context__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRangeRequest)
            IMPORT_reboot_aio_types.assert_not_request_type(__options__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRangeRequest)

            # Within a `workflow`, all "bare" calls are
            # `per_workflow()` calls, unless we're within a control
            # loop, in which case they are syntactic sugar for
            # `per_iteration()`.
            #
            # Unless we are "within until" in which case all "bare"
            # calls are `.always().
            if __options__ is None or __options__.idempotency is None:
                if isinstance(__context__, IMPORT_reboot_aio_contexts.WorkflowContext):
                    return await (
                        __this__.always() if __context__.within_until()
                        else (
                            __this__.per_iteration() if __context__.within_loop()
                            else __this__.per_workflow()
                        )
                    ).Range(
                        __context__,
                        __options__ or IMPORT_reboot_aio_call.Options(),
                        start_key=start_key,
                        limit=limit,
                    )
                elif isinstance(__context__, IMPORT_reboot_aio_external.InitializeContext):
                    return await __this__.idempotently().Range(
                        __context__,
                        __options__ or IMPORT_reboot_aio_call.Options(),
                        start_key=start_key,
                        limit=limit,
                    )

            if start_key is not None and not isinstance(
                start_key,
                str,
            ):
                raise TypeError(
                    f"Can not construct protobuf message of type "
                    f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRangeRequest': field 'start_key' is not "
                    f"of required type 'str'"
                )
            if limit is not None and not isinstance(
                limit,
                int,
            ):
                raise TypeError(
                    f"Can not construct protobuf message of type "
                    f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRangeRequest': field 'limit' is not "
                    f"of required type 'int'"
                )
            # TODO: mypy-protobuf declares that
            # `google.protobuf.message.Message` constructor arguments are
            # always non-None, when in reality they are optional.
            __request__ = rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRangeRequest(
                start_key=start_key,  # type: ignore[arg-type]
                limit=limit,  # type: ignore[arg-type]
            )
            __metadata__: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None
            __bearer_token__: IMPORT_typing.Optional[str] = None
            if __options__ is not None:
                IMPORT_reboot_aio_types.assert_type(__options__, [IMPORT_reboot_aio_call.Options])
                if __options__.metadata is not None:
                    __metadata__ = __options__.metadata
                if __options__.bearer_token is not None:
                    __bearer_token__ = __options__.bearer_token
            return await __this__._reader(__context__).Range(
                __request__,
                metadata=__metadata__,
                bearer_token=__bearer_token__,
                idempotency=__options__.idempotency if __options__ is not None else None,
            )
        # Keep the original functions on the client, so old code will
        # continue to work, but use the new 'snake_case' method in
        # the new code.
        range = Range
        async def ReverseRange(
            __this__,
            __context__: IMPORT_reboot_aio_contexts.ReaderContext | IMPORT_reboot_aio_contexts.WriterContext | IMPORT_reboot_aio_contexts.TransactionContext | IMPORT_reboot_aio_contexts.WorkflowContext | IMPORT_reboot_aio_external.ExternalContext,
            __options__: IMPORT_typing.Optional[IMPORT_reboot_aio_call.Options] = None,
            *,
            start_key: IMPORT_typing.Optional[str] = None,
            limit: IMPORT_typing.Optional[int] = None,
        ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeReverseRangeResponse:
            # UX improvement: check that neither positional argument was accidentally
            # given a gRPC request type.
            IMPORT_reboot_aio_types.assert_not_request_type(__context__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeReverseRangeRequest)
            IMPORT_reboot_aio_types.assert_not_request_type(__options__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeReverseRangeRequest)

            # Within a `workflow`, all "bare" calls are
            # `per_workflow()` calls, unless we're within a control
            # loop, in which case they are syntactic sugar for
            # `per_iteration()`.
            #
            # Unless we are "within until" in which case all "bare"
            # calls are `.always().
            if __options__ is None or __options__.idempotency is None:
                if isinstance(__context__, IMPORT_reboot_aio_contexts.WorkflowContext):
                    return await (
                        __this__.always() if __context__.within_until()
                        else (
                            __this__.per_iteration() if __context__.within_loop()
                            else __this__.per_workflow()
                        )
                    ).ReverseRange(
                        __context__,
                        __options__ or IMPORT_reboot_aio_call.Options(),
                        start_key=start_key,
                        limit=limit,
                    )
                elif isinstance(__context__, IMPORT_reboot_aio_external.InitializeContext):
                    return await __this__.idempotently().ReverseRange(
                        __context__,
                        __options__ or IMPORT_reboot_aio_call.Options(),
                        start_key=start_key,
                        limit=limit,
                    )

            if start_key is not None and not isinstance(
                start_key,
                str,
            ):
                raise TypeError(
                    f"Can not construct protobuf message of type "
                    f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeReverseRangeRequest': field 'start_key' is not "
                    f"of required type 'str'"
                )
            if limit is not None and not isinstance(
                limit,
                int,
            ):
                raise TypeError(
                    f"Can not construct protobuf message of type "
                    f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeReverseRangeRequest': field 'limit' is not "
                    f"of required type 'int'"
                )
            # TODO: mypy-protobuf declares that
            # `google.protobuf.message.Message` constructor arguments are
            # always non-None, when in reality they are optional.
            __request__ = rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeReverseRangeRequest(
                start_key=start_key,  # type: ignore[arg-type]
                limit=limit,  # type: ignore[arg-type]
            )
            __metadata__: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None
            __bearer_token__: IMPORT_typing.Optional[str] = None
            if __options__ is not None:
                IMPORT_reboot_aio_types.assert_type(__options__, [IMPORT_reboot_aio_call.Options])
                if __options__.metadata is not None:
                    __metadata__ = __options__.metadata
                if __options__.bearer_token is not None:
                    __bearer_token__ = __options__.bearer_token
            return await __this__._reader(__context__).ReverseRange(
                __request__,
                metadata=__metadata__,
                bearer_token=__bearer_token__,
                idempotency=__options__.idempotency if __options__ is not None else None,
            )
        # Keep the original functions on the client, so old code will
        # continue to work, but use the new 'snake_case' method in
        # the new code.
        reverse_range = ReverseRange
        async def Stringify(
            __this__,
            __context__: IMPORT_reboot_aio_contexts.ReaderContext | IMPORT_reboot_aio_contexts.WriterContext | IMPORT_reboot_aio_contexts.TransactionContext | IMPORT_reboot_aio_contexts.WorkflowContext | IMPORT_reboot_aio_external.ExternalContext,
            __options__: IMPORT_typing.Optional[IMPORT_reboot_aio_call.Options] = None,
            *,
            level: IMPORT_typing.Optional[int] = None,
        ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeStringifyResponse:
            # UX improvement: check that neither positional argument was accidentally
            # given a gRPC request type.
            IMPORT_reboot_aio_types.assert_not_request_type(__context__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeStringifyRequest)
            IMPORT_reboot_aio_types.assert_not_request_type(__options__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeStringifyRequest)

            # Within a `workflow`, all "bare" calls are
            # `per_workflow()` calls, unless we're within a control
            # loop, in which case they are syntactic sugar for
            # `per_iteration()`.
            #
            # Unless we are "within until" in which case all "bare"
            # calls are `.always().
            if __options__ is None or __options__.idempotency is None:
                if isinstance(__context__, IMPORT_reboot_aio_contexts.WorkflowContext):
                    return await (
                        __this__.always() if __context__.within_until()
                        else (
                            __this__.per_iteration() if __context__.within_loop()
                            else __this__.per_workflow()
                        )
                    ).Stringify(
                        __context__,
                        __options__ or IMPORT_reboot_aio_call.Options(),
                        level=level,
                    )
                elif isinstance(__context__, IMPORT_reboot_aio_external.InitializeContext):
                    return await __this__.idempotently().Stringify(
                        __context__,
                        __options__ or IMPORT_reboot_aio_call.Options(),
                        level=level,
                    )

            if level is not None and not isinstance(
                level,
                int,
            ):
                raise TypeError(
                    f"Can not construct protobuf message of type "
                    f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeStringifyRequest': field 'level' is not "
                    f"of required type 'int'"
                )
            # TODO: mypy-protobuf declares that
            # `google.protobuf.message.Message` constructor arguments are
            # always non-None, when in reality they are optional.
            __request__ = rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeStringifyRequest(
                level=level,  # type: ignore[arg-type]
            )
            __metadata__: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None
            __bearer_token__: IMPORT_typing.Optional[str] = None
            if __options__ is not None:
                IMPORT_reboot_aio_types.assert_type(__options__, [IMPORT_reboot_aio_call.Options])
                if __options__.metadata is not None:
                    __metadata__ = __options__.metadata
                if __options__.bearer_token is not None:
                    __bearer_token__ = __options__.bearer_token
            return await __this__._reader(__context__).Stringify(
                __request__,
                metadata=__metadata__,
                bearer_token=__bearer_token__,
                idempotency=__options__.idempotency if __options__ is not None else None,
            )
        # Keep the original functions on the client, so old code will
        # continue to work, but use the new 'snake_case' method in
        # the new code.
        stringify = Stringify

    class _Forall:

        _ids: list[str]

        def __init__(self, ids: list[str]):
            self._ids = ids

        async def Create(
            # In methods which are dealing with user input, (i.e.,
            # proto message field names), we should use '__double_underscored__'
            # variables to avoid any potential name conflicts with the method's
            # parameters.
            # The '__self__' parameter is a convention in Python to
            # indicate that this method is a bound method, so we use
            # '__this__' instead.
            __this__,
            __context__: IMPORT_reboot_aio_contexts.TransactionContext | IMPORT_reboot_aio_contexts.WorkflowContext | IMPORT_reboot_aio_external.ExternalContext,
            __options__: IMPORT_reboot_aio_call.Options = IMPORT_reboot_aio_call.Options(),
            *,
            order: IMPORT_typing.Optional[int] = None,
            is_leaf: IMPORT_typing.Optional[bool] = None,
            keys: IMPORT_typing.Optional[IMPORT_typing.Iterable[str]] = None,
            children_ids: IMPORT_typing.Optional[IMPORT_typing.Iterable[str]] = None,
            values: IMPORT_typing.Optional[IMPORT_typing.Iterable[bytes]] = None,
            next_id: IMPORT_typing.Optional[str] = None,
            prev_id: IMPORT_typing.Optional[str] = None,
        ) -> list[rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateResponse]:
            return await IMPORT_asyncio.gather(
                *[
                    Node.ref(
                        id
                    ).Create(
                        __context__,
                        __options__,
                        order=order,
                        is_leaf=is_leaf,
                        keys=keys,
                        children_ids=children_ids,
                        values=values,
                        next_id=next_id,
                        prev_id=prev_id,
                    ) for id in __this__._ids
                ]
            )

        # Keep the original functions on the client, so old code will
        # continue to work, but use the new 'snake_case' method in
        # the new code.
        create = Create
        async def Search(
            # In methods which are dealing with user input, (i.e.,
            # proto message field names), we should use '__double_underscored__'
            # variables to avoid any potential name conflicts with the method's
            # parameters.
            # The '__self__' parameter is a convention in Python to
            # indicate that this method is a bound method, so we use
            # '__this__' instead.
            __this__,
            __context__: IMPORT_reboot_aio_contexts.ReaderContext | IMPORT_reboot_aio_contexts.WriterContext | IMPORT_reboot_aio_contexts.TransactionContext | IMPORT_reboot_aio_contexts.WorkflowContext | IMPORT_reboot_aio_external.ExternalContext,
            __options__: IMPORT_reboot_aio_call.Options = IMPORT_reboot_aio_call.Options(),
            *,
            key: IMPORT_typing.Optional[str] = None,
        ) -> list[rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeSearchResponse]:
            return await IMPORT_asyncio.gather(
                *[
                    Node.ref(
                        id
                    ).Search(
                        __context__,
                        __options__,
                        key=key,
                    ) for id in __this__._ids
                ]
            )

        # Keep the original functions on the client, so old code will
        # continue to work, but use the new 'snake_case' method in
        # the new code.
        search = Search
        async def Insert(
            # In methods which are dealing with user input, (i.e.,
            # proto message field names), we should use '__double_underscored__'
            # variables to avoid any potential name conflicts with the method's
            # parameters.
            # The '__self__' parameter is a convention in Python to
            # indicate that this method is a bound method, so we use
            # '__this__' instead.
            __this__,
            __context__: IMPORT_reboot_aio_contexts.TransactionContext | IMPORT_reboot_aio_contexts.WorkflowContext | IMPORT_reboot_aio_external.ExternalContext,
            __options__: IMPORT_reboot_aio_call.Options = IMPORT_reboot_aio_call.Options(),
            *,
            key: IMPORT_typing.Optional[str] = None,
            value: IMPORT_typing.Optional[bytes] = None,
        ) -> list[rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeInsertResponse]:
            return await IMPORT_asyncio.gather(
                *[
                    Node.ref(
                        id
                    ).Insert(
                        __context__,
                        __options__,
                        key=key,
                        value=value,
                    ) for id in __this__._ids
                ]
            )

        # Keep the original functions on the client, so old code will
        # continue to work, but use the new 'snake_case' method in
        # the new code.
        insert = Insert
        async def Remove(
            # In methods which are dealing with user input, (i.e.,
            # proto message field names), we should use '__double_underscored__'
            # variables to avoid any potential name conflicts with the method's
            # parameters.
            # The '__self__' parameter is a convention in Python to
            # indicate that this method is a bound method, so we use
            # '__this__' instead.
            __this__,
            __context__: IMPORT_reboot_aio_contexts.TransactionContext | IMPORT_reboot_aio_contexts.WorkflowContext | IMPORT_reboot_aio_external.ExternalContext,
            __options__: IMPORT_reboot_aio_call.Options = IMPORT_reboot_aio_call.Options(),
            *,
            key: IMPORT_typing.Optional[str] = None,
        ) -> list[rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRemoveResponse]:
            return await IMPORT_asyncio.gather(
                *[
                    Node.ref(
                        id
                    ).Remove(
                        __context__,
                        __options__,
                        key=key,
                    ) for id in __this__._ids
                ]
            )

        # Keep the original functions on the client, so old code will
        # continue to work, but use the new 'snake_case' method in
        # the new code.
        remove = Remove
        async def Range(
            # In methods which are dealing with user input, (i.e.,
            # proto message field names), we should use '__double_underscored__'
            # variables to avoid any potential name conflicts with the method's
            # parameters.
            # The '__self__' parameter is a convention in Python to
            # indicate that this method is a bound method, so we use
            # '__this__' instead.
            __this__,
            __context__: IMPORT_reboot_aio_contexts.ReaderContext | IMPORT_reboot_aio_contexts.WriterContext | IMPORT_reboot_aio_contexts.TransactionContext | IMPORT_reboot_aio_contexts.WorkflowContext | IMPORT_reboot_aio_external.ExternalContext,
            __options__: IMPORT_reboot_aio_call.Options = IMPORT_reboot_aio_call.Options(),
            *,
            start_key: IMPORT_typing.Optional[str] = None,
            limit: IMPORT_typing.Optional[int] = None,
        ) -> list[rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRangeResponse]:
            return await IMPORT_asyncio.gather(
                *[
                    Node.ref(
                        id
                    ).Range(
                        __context__,
                        __options__,
                        start_key=start_key,
                        limit=limit,
                    ) for id in __this__._ids
                ]
            )

        # Keep the original functions on the client, so old code will
        # continue to work, but use the new 'snake_case' method in
        # the new code.
        range = Range
        async def ReverseRange(
            # In methods which are dealing with user input, (i.e.,
            # proto message field names), we should use '__double_underscored__'
            # variables to avoid any potential name conflicts with the method's
            # parameters.
            # The '__self__' parameter is a convention in Python to
            # indicate that this method is a bound method, so we use
            # '__this__' instead.
            __this__,
            __context__: IMPORT_reboot_aio_contexts.ReaderContext | IMPORT_reboot_aio_contexts.WriterContext | IMPORT_reboot_aio_contexts.TransactionContext | IMPORT_reboot_aio_contexts.WorkflowContext | IMPORT_reboot_aio_external.ExternalContext,
            __options__: IMPORT_reboot_aio_call.Options = IMPORT_reboot_aio_call.Options(),
            *,
            start_key: IMPORT_typing.Optional[str] = None,
            limit: IMPORT_typing.Optional[int] = None,
        ) -> list[rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeReverseRangeResponse]:
            return await IMPORT_asyncio.gather(
                *[
                    Node.ref(
                        id
                    ).ReverseRange(
                        __context__,
                        __options__,
                        start_key=start_key,
                        limit=limit,
                    ) for id in __this__._ids
                ]
            )

        # Keep the original functions on the client, so old code will
        # continue to work, but use the new 'snake_case' method in
        # the new code.
        reverse_range = ReverseRange
        async def Stringify(
            # In methods which are dealing with user input, (i.e.,
            # proto message field names), we should use '__double_underscored__'
            # variables to avoid any potential name conflicts with the method's
            # parameters.
            # The '__self__' parameter is a convention in Python to
            # indicate that this method is a bound method, so we use
            # '__this__' instead.
            __this__,
            __context__: IMPORT_reboot_aio_contexts.ReaderContext | IMPORT_reboot_aio_contexts.WriterContext | IMPORT_reboot_aio_contexts.TransactionContext | IMPORT_reboot_aio_contexts.WorkflowContext | IMPORT_reboot_aio_external.ExternalContext,
            __options__: IMPORT_reboot_aio_call.Options = IMPORT_reboot_aio_call.Options(),
            *,
            level: IMPORT_typing.Optional[int] = None,
        ) -> list[rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeStringifyResponse]:
            return await IMPORT_asyncio.gather(
                *[
                    Node.ref(
                        id
                    ).Stringify(
                        __context__,
                        __options__,
                        level=level,
                    ) for id in __this__._ids
                ]
            )

        # Keep the original functions on the client, so old code will
        # continue to work, but use the new 'snake_case' method in
        # the new code.
        stringify = Stringify

    @classmethod
    def forall(cls, ids: list[str]) -> Node._Forall:
        return Node._Forall(ids)

    @classmethod
    def ref(
        cls,
        state_id: IMPORT_reboot_aio_types.StateId,
        *,
        bearer_token: IMPORT_typing.Optional[str] = None,
    ) -> Node.WeakReference[Node.WeakReference._Schedule]:
        return Node.WeakReference(
            # TODO(https://github.com/reboot-dev/mono/issues/3226): add support for calling other applications.
            # For now this always stays within the application that creates the context.
            application_id=None,
            state_id=state_id,
            schedule_type=Node.WeakReference._Schedule,
            bearer_token=bearer_token,
        )


    @IMPORT_typing.overload
    @classmethod
    def idempotently(cls, alias: IMPORT_typing.Optional[str] = None, *, each_iteration: bool = False) -> Node._ConstructIdempotently:
        ...

    @IMPORT_typing.overload
    @classmethod
    def idempotently(cls, *, key: IMPORT_uuid.UUID, generated: bool = False) -> Node._ConstructIdempotently:
        ...

    @classmethod
    def idempotently(
        cls,
        alias: IMPORT_typing.Optional[str] = None,
        *,
        key: IMPORT_typing.Optional[IMPORT_uuid.UUID] = None,
        each_iteration: IMPORT_typing.Optional[bool] = None,
        generated: bool = False,
    ) -> Node._ConstructIdempotently:
        return Node._ConstructIdempotently(
            _idempotency=IMPORT_reboot_aio_contexts.Context.idempotency(
                alias=alias,
                key=key,
                each_iteration=each_iteration,
                generated=generated,
            ),
        )

    @classmethod
    def per_workflow(
        cls,
        alias: IMPORT_typing.Optional[str] = None,
    ):
        return cls.idempotently(alias)

    @classmethod
    def per_iteration(
        cls,
        alias: IMPORT_typing.Optional[str] = None,
    ):
        return cls.idempotently(alias, each_iteration=True)

    @classmethod
    def always(cls):
        return cls.idempotently(key=IMPORT_uuid.uuid4(), generated=True)

    @IMPORT_dataclasses.dataclass(frozen=True)
    class _ConstructIdempotently:

        _idempotency: IMPORT_reboot_aio_idempotency.Idempotency


OrderedMap_ScheduleTypeVar = IMPORT_typing.TypeVar('OrderedMap_ScheduleTypeVar', 'OrderedMap.WeakReference._Schedule', 'OrderedMap.WeakReference._WriterSchedule')
OrderedMap_IdempotentlyScheduleTypeVar = IMPORT_typing.TypeVar('OrderedMap_IdempotentlyScheduleTypeVar', 'OrderedMap.WeakReference._Schedule', 'OrderedMap.WeakReference._WriterSchedule')

OrderedMap_UntilCallableType = IMPORT_typing.TypeVar('OrderedMap_UntilCallableType')

class OrderedMapSingleton:
    Servicer: IMPORT_typing.TypeAlias = OrderedMapSingletonServicer


class OrderedMap:


    Servicer: IMPORT_typing.TypeAlias = OrderedMapServicer

    singleton: IMPORT_typing.TypeAlias = OrderedMapSingleton

    Effects: IMPORT_typing.TypeAlias = OrderedMapBaseServicer.Effects

    Authorizer: IMPORT_typing.TypeAlias = OrderedMapAuthorizer

    State: IMPORT_typing.TypeAlias = rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap

    __state_type_name__ = IMPORT_reboot_aio_types.StateTypeName("rbt.std.collections.ordered_map.v1.OrderedMap")

    class CreateTask:
        """Represents a scheduled task running for the
        state. Note that this is not a coroutine because we are trying
        to convey the semantics that the task is already running (or
        will soon be).
        """

        @classmethod
        def retrieve(
            cls,
            context: IMPORT_reboot_aio_contexts.Context | IMPORT_reboot_aio_external.ExternalContext,
            *,
            task_id: IMPORT_rbt_v1alpha1.tasks_pb2.TaskId,
        ):
            return cls(context, task_id=task_id)

        def __init__(
            self,
            context: IMPORT_reboot_aio_contexts.Context | IMPORT_reboot_aio_external.ExternalContext,
            *,
            task_id: IMPORT_rbt_v1alpha1.tasks_pb2.TaskId,
        ) -> None:
            # Depending on the context type (inside or outside a Reboot application)
            # we may or may not know the application ID. If we don't know it, then
            # the `ExternalContext.gateway` will determine it.
            #
            # TODO: in the future we expect to support cross-application calls, in
            #       which case the developer may explicitly pass in an application ID
            #       here.
            self._application_id: IMPORT_typing.Optional[IMPORT_reboot_aio_types.ApplicationId] = None
            if isinstance(context, IMPORT_reboot_aio_contexts.Context):
                self._application_id = context.application_id
            self._channel_manager = context.channel_manager
            self._task_id = task_id

        @property
        def task_id(self) -> IMPORT_rbt_v1alpha1.tasks_pb2.TaskId:
            return self._task_id

        def __await__(self) -> IMPORT_typing.Generator[None, None, rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapCreateResponse]:
            """Awaits for task to finish and returns its response."""
            async def wait_for_task() -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapCreateResponse:
                channel = self._channel_manager.get_channel_to_state(
                    IMPORT_reboot_aio_types.StateTypeName(self._task_id.state_type),
                    IMPORT_reboot_aio_types.StateRef(self._task_id.state_ref),
                )

                stub = IMPORT_rbt_v1alpha1.tasks_pb2_grpc.TasksStub(channel)

                try:
                    call = IMPORT_reboot_aio_stubs.UnaryRetriedCall(
                        call=None,  # `RetriedCall` can create the call itself.
                        stub_method=stub.Wait,
                        method_name="Wait",
                        request=IMPORT_rbt_v1alpha1.tasks_pb2.WaitRequest(task_id=self._task_id),
                        metadata=IMPORT_reboot_aio_headers.Headers(
                            state_ref=IMPORT_reboot_aio_types.StateRef(self._task_id.state_ref),
                            application_id=self._application_id,
                        ).to_grpc_metadata(),
                        aborted_type=IMPORT_rebootdev.aio.aborted.SystemAborted,
                    )

                    wait_for_task_response = await call
                except IMPORT_rebootdev.aio.aborted.SystemAborted as error:
                    if error.code == IMPORT_grpc.StatusCode.NOT_FOUND:
                        raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                            IMPORT_rbt_v1alpha1.errors_pb2.UnknownTask()
                        ) from None

                    raise
                else:
                    response_or_error: IMPORT_typing.Optional[IMPORT_google_protobuf_any_pb2.Any] = None
                    is_error = False

                    if wait_for_task_response.response_or_error.WhichOneof("response_or_error") == "response":
                        response_or_error = wait_for_task_response.response_or_error.response
                    else:
                        is_error = True
                        response_or_error = wait_for_task_response.response_or_error.error

                    assert response_or_error is not None
                    assert response_or_error.TypeName() != ""

                    response = rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapCreateResponse()

                    if (
                        not is_error and response_or_error.TypeName() != response.DESCRIPTOR.full_name
                    ):
                        raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                            IMPORT_rbt_v1alpha1.errors_pb2.InvalidArgument(),
                            message=
                            f"task with UUID {IMPORT_uuid.UUID(bytes=self._task_id.task_uuid)} "
                            f"has a response of type '{response_or_error.TypeName()}' "
                            "but expecting type 'rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapCreateResponse'; "
                            "are you waiting on a task of the correct method?",
                        ) from None

                    if is_error:
                        # Currently only cancelled error is supported.
                        raise IMPORT_asyncio.CancelledError("Task was cancelled by a TasksServicer")
                    else:
                        response_or_error.Unpack(response)
                        return response

            return wait_for_task().__await__()


    class CreateAborted(IMPORT_rebootdev.aio.aborted.Aborted):

        MethodError = IMPORT_typing.Union[
            rbt.v1alpha1.errors_pb2.StateAlreadyConstructed        ]

        Error = IMPORT_typing.Union[
            MethodError,
            IMPORT_rebootdev.aio.aborted.GrpcError,
            IMPORT_rebootdev.aio.aborted.RebootError,
        ]

        METHOD_ERROR_TYPES: list[type[IMPORT_google_protobuf_message.Message]] = [
            rbt.v1alpha1.errors_pb2.StateAlreadyConstructed        ]

        ERROR_TYPES: list[type[IMPORT_google_protobuf_message.Message]] = (
            METHOD_ERROR_TYPES +
            IMPORT_rebootdev.aio.aborted.GRPC_ERROR_TYPES +
            IMPORT_rebootdev.aio.aborted.REBOOT_ERROR_TYPES
        )

        _error: Error
        _code: IMPORT_grpc.StatusCode
        _message: IMPORT_typing.Optional[str]

        def __init__(
            self,
            error: MethodError |  IMPORT_rebootdev.aio.aborted.GrpcError,
            *,
            message: IMPORT_typing.Optional[str] = None,
            # Do not set this value when constructing in order to
            # raise. This is only used internally when constructing
            # from aborted calls.
            error_types: IMPORT_typing.Sequence[type[Error]] = (
                METHOD_ERROR_TYPES + IMPORT_rebootdev.aio.aborted.GRPC_ERROR_TYPES
            ),
        ):
            super().__init__()

            IMPORT_reboot_aio_types.assert_type(error, error_types)

            self._error = error

            code = self.grpc_status_code_from_error(self._error)

            if code is None:
                # Must be a Reboot specific or declared method error.
                code = IMPORT_grpc.StatusCode.ABORTED

            self._code = code

            self._message = message

        @property
        def error(self) -> Error:
            return self._error

        @property
        def code(self) -> IMPORT_grpc.StatusCode:
            return self._code

        @property
        def message(self) -> IMPORT_typing.Optional[str]:
            return self._message

        @classmethod
        def from_status(cls, status: IMPORT_google_rpc_status_pb2.Status):
            error = cls.error_from_google_rpc_status_details(
                status,
                cls.ERROR_TYPES,
            )

            message = status.message if len(status.message) > 0 else None

            if error is not None:
                return cls(error, message=message, error_types=cls.ERROR_TYPES)

            error = cls.error_from_google_rpc_status_code(status)

            assert error is not None

            # TODO(benh): also consider getting the type names from
            # `status.details` and including that in `message` to make
            # debugging easier.

            return cls(error, message=message)

        @classmethod
        def from_grpc_aio_rpc_error(cls, aio_rpc_error: IMPORT_grpc.aio.AioRpcError):
            return cls(
                cls.error_from_grpc_aio_rpc_error(aio_rpc_error),
                message=aio_rpc_error.details(),
            )

        @classmethod
        def is_declared_error(cls, message: IMPORT_google_protobuf_message.Message) -> bool:
            if message.DESCRIPTOR.full_name == 'rbt.v1alpha1.StateAlreadyConstructed':
                return True
            return False

    class SearchTask:
        """Represents a scheduled task running for the
        state. Note that this is not a coroutine because we are trying
        to convey the semantics that the task is already running (or
        will soon be).
        """

        @classmethod
        def retrieve(
            cls,
            context: IMPORT_reboot_aio_contexts.Context | IMPORT_reboot_aio_external.ExternalContext,
            *,
            task_id: IMPORT_rbt_v1alpha1.tasks_pb2.TaskId,
        ):
            return cls(context, task_id=task_id)

        def __init__(
            self,
            context: IMPORT_reboot_aio_contexts.Context | IMPORT_reboot_aio_external.ExternalContext,
            *,
            task_id: IMPORT_rbt_v1alpha1.tasks_pb2.TaskId,
        ) -> None:
            # Depending on the context type (inside or outside a Reboot application)
            # we may or may not know the application ID. If we don't know it, then
            # the `ExternalContext.gateway` will determine it.
            #
            # TODO: in the future we expect to support cross-application calls, in
            #       which case the developer may explicitly pass in an application ID
            #       here.
            self._application_id: IMPORT_typing.Optional[IMPORT_reboot_aio_types.ApplicationId] = None
            if isinstance(context, IMPORT_reboot_aio_contexts.Context):
                self._application_id = context.application_id
            self._channel_manager = context.channel_manager
            self._task_id = task_id

        @property
        def task_id(self) -> IMPORT_rbt_v1alpha1.tasks_pb2.TaskId:
            return self._task_id

        def __await__(self) -> IMPORT_typing.Generator[None, None, rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapSearchResponse]:
            """Awaits for task to finish and returns its response."""
            async def wait_for_task() -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapSearchResponse:
                channel = self._channel_manager.get_channel_to_state(
                    IMPORT_reboot_aio_types.StateTypeName(self._task_id.state_type),
                    IMPORT_reboot_aio_types.StateRef(self._task_id.state_ref),
                )

                stub = IMPORT_rbt_v1alpha1.tasks_pb2_grpc.TasksStub(channel)

                try:
                    call = IMPORT_reboot_aio_stubs.UnaryRetriedCall(
                        call=None,  # `RetriedCall` can create the call itself.
                        stub_method=stub.Wait,
                        method_name="Wait",
                        request=IMPORT_rbt_v1alpha1.tasks_pb2.WaitRequest(task_id=self._task_id),
                        metadata=IMPORT_reboot_aio_headers.Headers(
                            state_ref=IMPORT_reboot_aio_types.StateRef(self._task_id.state_ref),
                            application_id=self._application_id,
                        ).to_grpc_metadata(),
                        aborted_type=IMPORT_rebootdev.aio.aborted.SystemAborted,
                    )

                    wait_for_task_response = await call
                except IMPORT_rebootdev.aio.aborted.SystemAborted as error:
                    if error.code == IMPORT_grpc.StatusCode.NOT_FOUND:
                        raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                            IMPORT_rbt_v1alpha1.errors_pb2.UnknownTask()
                        ) from None

                    raise
                else:
                    response_or_error: IMPORT_typing.Optional[IMPORT_google_protobuf_any_pb2.Any] = None
                    is_error = False

                    if wait_for_task_response.response_or_error.WhichOneof("response_or_error") == "response":
                        response_or_error = wait_for_task_response.response_or_error.response
                    else:
                        is_error = True
                        response_or_error = wait_for_task_response.response_or_error.error

                    assert response_or_error is not None
                    assert response_or_error.TypeName() != ""

                    response = rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapSearchResponse()

                    if (
                        not is_error and response_or_error.TypeName() != response.DESCRIPTOR.full_name
                    ):
                        raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                            IMPORT_rbt_v1alpha1.errors_pb2.InvalidArgument(),
                            message=
                            f"task with UUID {IMPORT_uuid.UUID(bytes=self._task_id.task_uuid)} "
                            f"has a response of type '{response_or_error.TypeName()}' "
                            "but expecting type 'rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapSearchResponse'; "
                            "are you waiting on a task of the correct method?",
                        ) from None

                    if is_error:
                        # Currently only cancelled error is supported.
                        raise IMPORT_asyncio.CancelledError("Task was cancelled by a TasksServicer")
                    else:
                        response_or_error.Unpack(response)
                        return response

            return wait_for_task().__await__()


    class SearchAborted(IMPORT_rebootdev.aio.aborted.Aborted):


        Error = IMPORT_typing.Union[
            IMPORT_rebootdev.aio.aborted.GrpcError,
            IMPORT_rebootdev.aio.aborted.RebootError,
        ]

        METHOD_ERROR_TYPES: list[type[IMPORT_google_protobuf_message.Message]] = [
        ]

        ERROR_TYPES: list[type[IMPORT_google_protobuf_message.Message]] = (
            METHOD_ERROR_TYPES +
            IMPORT_rebootdev.aio.aborted.GRPC_ERROR_TYPES +
            IMPORT_rebootdev.aio.aborted.REBOOT_ERROR_TYPES
        )

        _error: Error
        _code: IMPORT_grpc.StatusCode
        _message: IMPORT_typing.Optional[str]

        def __init__(
            self,
            error:  IMPORT_rebootdev.aio.aborted.GrpcError,
            *,
            message: IMPORT_typing.Optional[str] = None,
            # Do not set this value when constructing in order to
            # raise. This is only used internally when constructing
            # from aborted calls.
            error_types: IMPORT_typing.Sequence[type[Error]] = (
                METHOD_ERROR_TYPES + IMPORT_rebootdev.aio.aborted.GRPC_ERROR_TYPES
            ),
        ):
            super().__init__()

            IMPORT_reboot_aio_types.assert_type(error, error_types)

            self._error = error

            code = self.grpc_status_code_from_error(self._error)

            if code is None:
                # Must be a Reboot specific or declared method error.
                code = IMPORT_grpc.StatusCode.ABORTED

            self._code = code

            self._message = message

        @property
        def error(self) -> Error:
            return self._error

        @property
        def code(self) -> IMPORT_grpc.StatusCode:
            return self._code

        @property
        def message(self) -> IMPORT_typing.Optional[str]:
            return self._message

        @classmethod
        def from_status(cls, status: IMPORT_google_rpc_status_pb2.Status):
            error = cls.error_from_google_rpc_status_details(
                status,
                cls.ERROR_TYPES,
            )

            message = status.message if len(status.message) > 0 else None

            if error is not None:
                return cls(error, message=message, error_types=cls.ERROR_TYPES)

            error = cls.error_from_google_rpc_status_code(status)

            assert error is not None

            # TODO(benh): also consider getting the type names from
            # `status.details` and including that in `message` to make
            # debugging easier.

            return cls(error, message=message)

        @classmethod
        def from_grpc_aio_rpc_error(cls, aio_rpc_error: IMPORT_grpc.aio.AioRpcError):
            return cls(
                cls.error_from_grpc_aio_rpc_error(aio_rpc_error),
                message=aio_rpc_error.details(),
            )

        @classmethod
        def is_declared_error(cls, message: IMPORT_google_protobuf_message.Message) -> bool:
            return False

    class InsertTask:
        """Represents a scheduled task running for the
        state. Note that this is not a coroutine because we are trying
        to convey the semantics that the task is already running (or
        will soon be).
        """

        @classmethod
        def retrieve(
            cls,
            context: IMPORT_reboot_aio_contexts.Context | IMPORT_reboot_aio_external.ExternalContext,
            *,
            task_id: IMPORT_rbt_v1alpha1.tasks_pb2.TaskId,
        ):
            return cls(context, task_id=task_id)

        def __init__(
            self,
            context: IMPORT_reboot_aio_contexts.Context | IMPORT_reboot_aio_external.ExternalContext,
            *,
            task_id: IMPORT_rbt_v1alpha1.tasks_pb2.TaskId,
        ) -> None:
            # Depending on the context type (inside or outside a Reboot application)
            # we may or may not know the application ID. If we don't know it, then
            # the `ExternalContext.gateway` will determine it.
            #
            # TODO: in the future we expect to support cross-application calls, in
            #       which case the developer may explicitly pass in an application ID
            #       here.
            self._application_id: IMPORT_typing.Optional[IMPORT_reboot_aio_types.ApplicationId] = None
            if isinstance(context, IMPORT_reboot_aio_contexts.Context):
                self._application_id = context.application_id
            self._channel_manager = context.channel_manager
            self._task_id = task_id

        @property
        def task_id(self) -> IMPORT_rbt_v1alpha1.tasks_pb2.TaskId:
            return self._task_id

        def __await__(self) -> IMPORT_typing.Generator[None, None, rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapInsertResponse]:
            """Awaits for task to finish and returns its response."""
            async def wait_for_task() -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapInsertResponse:
                channel = self._channel_manager.get_channel_to_state(
                    IMPORT_reboot_aio_types.StateTypeName(self._task_id.state_type),
                    IMPORT_reboot_aio_types.StateRef(self._task_id.state_ref),
                )

                stub = IMPORT_rbt_v1alpha1.tasks_pb2_grpc.TasksStub(channel)

                try:
                    call = IMPORT_reboot_aio_stubs.UnaryRetriedCall(
                        call=None,  # `RetriedCall` can create the call itself.
                        stub_method=stub.Wait,
                        method_name="Wait",
                        request=IMPORT_rbt_v1alpha1.tasks_pb2.WaitRequest(task_id=self._task_id),
                        metadata=IMPORT_reboot_aio_headers.Headers(
                            state_ref=IMPORT_reboot_aio_types.StateRef(self._task_id.state_ref),
                            application_id=self._application_id,
                        ).to_grpc_metadata(),
                        aborted_type=IMPORT_rebootdev.aio.aborted.SystemAborted,
                    )

                    wait_for_task_response = await call
                except IMPORT_rebootdev.aio.aborted.SystemAborted as error:
                    if error.code == IMPORT_grpc.StatusCode.NOT_FOUND:
                        raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                            IMPORT_rbt_v1alpha1.errors_pb2.UnknownTask()
                        ) from None

                    raise
                else:
                    response_or_error: IMPORT_typing.Optional[IMPORT_google_protobuf_any_pb2.Any] = None
                    is_error = False

                    if wait_for_task_response.response_or_error.WhichOneof("response_or_error") == "response":
                        response_or_error = wait_for_task_response.response_or_error.response
                    else:
                        is_error = True
                        response_or_error = wait_for_task_response.response_or_error.error

                    assert response_or_error is not None
                    assert response_or_error.TypeName() != ""

                    response = rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapInsertResponse()

                    if (
                        not is_error and response_or_error.TypeName() != response.DESCRIPTOR.full_name
                    ):
                        raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                            IMPORT_rbt_v1alpha1.errors_pb2.InvalidArgument(),
                            message=
                            f"task with UUID {IMPORT_uuid.UUID(bytes=self._task_id.task_uuid)} "
                            f"has a response of type '{response_or_error.TypeName()}' "
                            "but expecting type 'rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapInsertResponse'; "
                            "are you waiting on a task of the correct method?",
                        ) from None

                    if is_error:
                        # Currently only cancelled error is supported.
                        raise IMPORT_asyncio.CancelledError("Task was cancelled by a TasksServicer")
                    else:
                        response_or_error.Unpack(response)
                        return response

            return wait_for_task().__await__()


    class InsertAborted(IMPORT_rebootdev.aio.aborted.Aborted):


        Error = IMPORT_typing.Union[
            IMPORT_rebootdev.aio.aborted.GrpcError,
            IMPORT_rebootdev.aio.aborted.RebootError,
        ]

        METHOD_ERROR_TYPES: list[type[IMPORT_google_protobuf_message.Message]] = [
        ]

        ERROR_TYPES: list[type[IMPORT_google_protobuf_message.Message]] = (
            METHOD_ERROR_TYPES +
            IMPORT_rebootdev.aio.aborted.GRPC_ERROR_TYPES +
            IMPORT_rebootdev.aio.aborted.REBOOT_ERROR_TYPES
        )

        _error: Error
        _code: IMPORT_grpc.StatusCode
        _message: IMPORT_typing.Optional[str]

        def __init__(
            self,
            error:  IMPORT_rebootdev.aio.aborted.GrpcError,
            *,
            message: IMPORT_typing.Optional[str] = None,
            # Do not set this value when constructing in order to
            # raise. This is only used internally when constructing
            # from aborted calls.
            error_types: IMPORT_typing.Sequence[type[Error]] = (
                METHOD_ERROR_TYPES + IMPORT_rebootdev.aio.aborted.GRPC_ERROR_TYPES
            ),
        ):
            super().__init__()

            IMPORT_reboot_aio_types.assert_type(error, error_types)

            self._error = error

            code = self.grpc_status_code_from_error(self._error)

            if code is None:
                # Must be a Reboot specific or declared method error.
                code = IMPORT_grpc.StatusCode.ABORTED

            self._code = code

            self._message = message

        @property
        def error(self) -> Error:
            return self._error

        @property
        def code(self) -> IMPORT_grpc.StatusCode:
            return self._code

        @property
        def message(self) -> IMPORT_typing.Optional[str]:
            return self._message

        @classmethod
        def from_status(cls, status: IMPORT_google_rpc_status_pb2.Status):
            error = cls.error_from_google_rpc_status_details(
                status,
                cls.ERROR_TYPES,
            )

            message = status.message if len(status.message) > 0 else None

            if error is not None:
                return cls(error, message=message, error_types=cls.ERROR_TYPES)

            error = cls.error_from_google_rpc_status_code(status)

            assert error is not None

            # TODO(benh): also consider getting the type names from
            # `status.details` and including that in `message` to make
            # debugging easier.

            return cls(error, message=message)

        @classmethod
        def from_grpc_aio_rpc_error(cls, aio_rpc_error: IMPORT_grpc.aio.AioRpcError):
            return cls(
                cls.error_from_grpc_aio_rpc_error(aio_rpc_error),
                message=aio_rpc_error.details(),
            )

        @classmethod
        def is_declared_error(cls, message: IMPORT_google_protobuf_message.Message) -> bool:
            return False

    class RemoveTask:
        """Represents a scheduled task running for the
        state. Note that this is not a coroutine because we are trying
        to convey the semantics that the task is already running (or
        will soon be).
        """

        @classmethod
        def retrieve(
            cls,
            context: IMPORT_reboot_aio_contexts.Context | IMPORT_reboot_aio_external.ExternalContext,
            *,
            task_id: IMPORT_rbt_v1alpha1.tasks_pb2.TaskId,
        ):
            return cls(context, task_id=task_id)

        def __init__(
            self,
            context: IMPORT_reboot_aio_contexts.Context | IMPORT_reboot_aio_external.ExternalContext,
            *,
            task_id: IMPORT_rbt_v1alpha1.tasks_pb2.TaskId,
        ) -> None:
            # Depending on the context type (inside or outside a Reboot application)
            # we may or may not know the application ID. If we don't know it, then
            # the `ExternalContext.gateway` will determine it.
            #
            # TODO: in the future we expect to support cross-application calls, in
            #       which case the developer may explicitly pass in an application ID
            #       here.
            self._application_id: IMPORT_typing.Optional[IMPORT_reboot_aio_types.ApplicationId] = None
            if isinstance(context, IMPORT_reboot_aio_contexts.Context):
                self._application_id = context.application_id
            self._channel_manager = context.channel_manager
            self._task_id = task_id

        @property
        def task_id(self) -> IMPORT_rbt_v1alpha1.tasks_pb2.TaskId:
            return self._task_id

        def __await__(self) -> IMPORT_typing.Generator[None, None, rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRemoveResponse]:
            """Awaits for task to finish and returns its response."""
            async def wait_for_task() -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRemoveResponse:
                channel = self._channel_manager.get_channel_to_state(
                    IMPORT_reboot_aio_types.StateTypeName(self._task_id.state_type),
                    IMPORT_reboot_aio_types.StateRef(self._task_id.state_ref),
                )

                stub = IMPORT_rbt_v1alpha1.tasks_pb2_grpc.TasksStub(channel)

                try:
                    call = IMPORT_reboot_aio_stubs.UnaryRetriedCall(
                        call=None,  # `RetriedCall` can create the call itself.
                        stub_method=stub.Wait,
                        method_name="Wait",
                        request=IMPORT_rbt_v1alpha1.tasks_pb2.WaitRequest(task_id=self._task_id),
                        metadata=IMPORT_reboot_aio_headers.Headers(
                            state_ref=IMPORT_reboot_aio_types.StateRef(self._task_id.state_ref),
                            application_id=self._application_id,
                        ).to_grpc_metadata(),
                        aborted_type=IMPORT_rebootdev.aio.aborted.SystemAborted,
                    )

                    wait_for_task_response = await call
                except IMPORT_rebootdev.aio.aborted.SystemAborted as error:
                    if error.code == IMPORT_grpc.StatusCode.NOT_FOUND:
                        raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                            IMPORT_rbt_v1alpha1.errors_pb2.UnknownTask()
                        ) from None

                    raise
                else:
                    response_or_error: IMPORT_typing.Optional[IMPORT_google_protobuf_any_pb2.Any] = None
                    is_error = False

                    if wait_for_task_response.response_or_error.WhichOneof("response_or_error") == "response":
                        response_or_error = wait_for_task_response.response_or_error.response
                    else:
                        is_error = True
                        response_or_error = wait_for_task_response.response_or_error.error

                    assert response_or_error is not None
                    assert response_or_error.TypeName() != ""

                    response = rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRemoveResponse()

                    if (
                        not is_error and response_or_error.TypeName() != response.DESCRIPTOR.full_name
                    ):
                        raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                            IMPORT_rbt_v1alpha1.errors_pb2.InvalidArgument(),
                            message=
                            f"task with UUID {IMPORT_uuid.UUID(bytes=self._task_id.task_uuid)} "
                            f"has a response of type '{response_or_error.TypeName()}' "
                            "but expecting type 'rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRemoveResponse'; "
                            "are you waiting on a task of the correct method?",
                        ) from None

                    if is_error:
                        # Currently only cancelled error is supported.
                        raise IMPORT_asyncio.CancelledError("Task was cancelled by a TasksServicer")
                    else:
                        response_or_error.Unpack(response)
                        return response

            return wait_for_task().__await__()


    class RemoveAborted(IMPORT_rebootdev.aio.aborted.Aborted):


        Error = IMPORT_typing.Union[
            IMPORT_rebootdev.aio.aborted.GrpcError,
            IMPORT_rebootdev.aio.aborted.RebootError,
        ]

        METHOD_ERROR_TYPES: list[type[IMPORT_google_protobuf_message.Message]] = [
        ]

        ERROR_TYPES: list[type[IMPORT_google_protobuf_message.Message]] = (
            METHOD_ERROR_TYPES +
            IMPORT_rebootdev.aio.aborted.GRPC_ERROR_TYPES +
            IMPORT_rebootdev.aio.aborted.REBOOT_ERROR_TYPES
        )

        _error: Error
        _code: IMPORT_grpc.StatusCode
        _message: IMPORT_typing.Optional[str]

        def __init__(
            self,
            error:  IMPORT_rebootdev.aio.aborted.GrpcError,
            *,
            message: IMPORT_typing.Optional[str] = None,
            # Do not set this value when constructing in order to
            # raise. This is only used internally when constructing
            # from aborted calls.
            error_types: IMPORT_typing.Sequence[type[Error]] = (
                METHOD_ERROR_TYPES + IMPORT_rebootdev.aio.aborted.GRPC_ERROR_TYPES
            ),
        ):
            super().__init__()

            IMPORT_reboot_aio_types.assert_type(error, error_types)

            self._error = error

            code = self.grpc_status_code_from_error(self._error)

            if code is None:
                # Must be a Reboot specific or declared method error.
                code = IMPORT_grpc.StatusCode.ABORTED

            self._code = code

            self._message = message

        @property
        def error(self) -> Error:
            return self._error

        @property
        def code(self) -> IMPORT_grpc.StatusCode:
            return self._code

        @property
        def message(self) -> IMPORT_typing.Optional[str]:
            return self._message

        @classmethod
        def from_status(cls, status: IMPORT_google_rpc_status_pb2.Status):
            error = cls.error_from_google_rpc_status_details(
                status,
                cls.ERROR_TYPES,
            )

            message = status.message if len(status.message) > 0 else None

            if error is not None:
                return cls(error, message=message, error_types=cls.ERROR_TYPES)

            error = cls.error_from_google_rpc_status_code(status)

            assert error is not None

            # TODO(benh): also consider getting the type names from
            # `status.details` and including that in `message` to make
            # debugging easier.

            return cls(error, message=message)

        @classmethod
        def from_grpc_aio_rpc_error(cls, aio_rpc_error: IMPORT_grpc.aio.AioRpcError):
            return cls(
                cls.error_from_grpc_aio_rpc_error(aio_rpc_error),
                message=aio_rpc_error.details(),
            )

        @classmethod
        def is_declared_error(cls, message: IMPORT_google_protobuf_message.Message) -> bool:
            return False

    class RangeTask:
        """Represents a scheduled task running for the
        state. Note that this is not a coroutine because we are trying
        to convey the semantics that the task is already running (or
        will soon be).
        """

        @classmethod
        def retrieve(
            cls,
            context: IMPORT_reboot_aio_contexts.Context | IMPORT_reboot_aio_external.ExternalContext,
            *,
            task_id: IMPORT_rbt_v1alpha1.tasks_pb2.TaskId,
        ):
            return cls(context, task_id=task_id)

        def __init__(
            self,
            context: IMPORT_reboot_aio_contexts.Context | IMPORT_reboot_aio_external.ExternalContext,
            *,
            task_id: IMPORT_rbt_v1alpha1.tasks_pb2.TaskId,
        ) -> None:
            # Depending on the context type (inside or outside a Reboot application)
            # we may or may not know the application ID. If we don't know it, then
            # the `ExternalContext.gateway` will determine it.
            #
            # TODO: in the future we expect to support cross-application calls, in
            #       which case the developer may explicitly pass in an application ID
            #       here.
            self._application_id: IMPORT_typing.Optional[IMPORT_reboot_aio_types.ApplicationId] = None
            if isinstance(context, IMPORT_reboot_aio_contexts.Context):
                self._application_id = context.application_id
            self._channel_manager = context.channel_manager
            self._task_id = task_id

        @property
        def task_id(self) -> IMPORT_rbt_v1alpha1.tasks_pb2.TaskId:
            return self._task_id

        def __await__(self) -> IMPORT_typing.Generator[None, None, rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRangeResponse]:
            """Awaits for task to finish and returns its response."""
            async def wait_for_task() -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRangeResponse:
                channel = self._channel_manager.get_channel_to_state(
                    IMPORT_reboot_aio_types.StateTypeName(self._task_id.state_type),
                    IMPORT_reboot_aio_types.StateRef(self._task_id.state_ref),
                )

                stub = IMPORT_rbt_v1alpha1.tasks_pb2_grpc.TasksStub(channel)

                try:
                    call = IMPORT_reboot_aio_stubs.UnaryRetriedCall(
                        call=None,  # `RetriedCall` can create the call itself.
                        stub_method=stub.Wait,
                        method_name="Wait",
                        request=IMPORT_rbt_v1alpha1.tasks_pb2.WaitRequest(task_id=self._task_id),
                        metadata=IMPORT_reboot_aio_headers.Headers(
                            state_ref=IMPORT_reboot_aio_types.StateRef(self._task_id.state_ref),
                            application_id=self._application_id,
                        ).to_grpc_metadata(),
                        aborted_type=IMPORT_rebootdev.aio.aborted.SystemAborted,
                    )

                    wait_for_task_response = await call
                except IMPORT_rebootdev.aio.aborted.SystemAborted as error:
                    if error.code == IMPORT_grpc.StatusCode.NOT_FOUND:
                        raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                            IMPORT_rbt_v1alpha1.errors_pb2.UnknownTask()
                        ) from None

                    raise
                else:
                    response_or_error: IMPORT_typing.Optional[IMPORT_google_protobuf_any_pb2.Any] = None
                    is_error = False

                    if wait_for_task_response.response_or_error.WhichOneof("response_or_error") == "response":
                        response_or_error = wait_for_task_response.response_or_error.response
                    else:
                        is_error = True
                        response_or_error = wait_for_task_response.response_or_error.error

                    assert response_or_error is not None
                    assert response_or_error.TypeName() != ""

                    response = rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRangeResponse()

                    if (
                        not is_error and response_or_error.TypeName() != response.DESCRIPTOR.full_name
                    ):
                        raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                            IMPORT_rbt_v1alpha1.errors_pb2.InvalidArgument(),
                            message=
                            f"task with UUID {IMPORT_uuid.UUID(bytes=self._task_id.task_uuid)} "
                            f"has a response of type '{response_or_error.TypeName()}' "
                            "but expecting type 'rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRangeResponse'; "
                            "are you waiting on a task of the correct method?",
                        ) from None

                    if is_error:
                        # Currently only cancelled error is supported.
                        raise IMPORT_asyncio.CancelledError("Task was cancelled by a TasksServicer")
                    else:
                        response_or_error.Unpack(response)
                        return response

            return wait_for_task().__await__()


    class RangeAborted(IMPORT_rebootdev.aio.aborted.Aborted):

        MethodError = IMPORT_typing.Union[
            rbt.std.collections.ordered_map.v1.ordered_map_pb2.InvalidRangeError        ]

        Error = IMPORT_typing.Union[
            MethodError,
            IMPORT_rebootdev.aio.aborted.GrpcError,
            IMPORT_rebootdev.aio.aborted.RebootError,
        ]

        METHOD_ERROR_TYPES: list[type[IMPORT_google_protobuf_message.Message]] = [
            rbt.std.collections.ordered_map.v1.ordered_map_pb2.InvalidRangeError        ]

        ERROR_TYPES: list[type[IMPORT_google_protobuf_message.Message]] = (
            METHOD_ERROR_TYPES +
            IMPORT_rebootdev.aio.aborted.GRPC_ERROR_TYPES +
            IMPORT_rebootdev.aio.aborted.REBOOT_ERROR_TYPES
        )

        _error: Error
        _code: IMPORT_grpc.StatusCode
        _message: IMPORT_typing.Optional[str]

        def __init__(
            self,
            error: MethodError |  IMPORT_rebootdev.aio.aborted.GrpcError,
            *,
            message: IMPORT_typing.Optional[str] = None,
            # Do not set this value when constructing in order to
            # raise. This is only used internally when constructing
            # from aborted calls.
            error_types: IMPORT_typing.Sequence[type[Error]] = (
                METHOD_ERROR_TYPES + IMPORT_rebootdev.aio.aborted.GRPC_ERROR_TYPES
            ),
        ):
            super().__init__()

            IMPORT_reboot_aio_types.assert_type(error, error_types)

            self._error = error

            code = self.grpc_status_code_from_error(self._error)

            if code is None:
                # Must be a Reboot specific or declared method error.
                code = IMPORT_grpc.StatusCode.ABORTED

            self._code = code

            self._message = message

        @property
        def error(self) -> Error:
            return self._error

        @property
        def code(self) -> IMPORT_grpc.StatusCode:
            return self._code

        @property
        def message(self) -> IMPORT_typing.Optional[str]:
            return self._message

        @classmethod
        def from_status(cls, status: IMPORT_google_rpc_status_pb2.Status):
            error = cls.error_from_google_rpc_status_details(
                status,
                cls.ERROR_TYPES,
            )

            message = status.message if len(status.message) > 0 else None

            if error is not None:
                return cls(error, message=message, error_types=cls.ERROR_TYPES)

            error = cls.error_from_google_rpc_status_code(status)

            assert error is not None

            # TODO(benh): also consider getting the type names from
            # `status.details` and including that in `message` to make
            # debugging easier.

            return cls(error, message=message)

        @classmethod
        def from_grpc_aio_rpc_error(cls, aio_rpc_error: IMPORT_grpc.aio.AioRpcError):
            return cls(
                cls.error_from_grpc_aio_rpc_error(aio_rpc_error),
                message=aio_rpc_error.details(),
            )

        @classmethod
        def is_declared_error(cls, message: IMPORT_google_protobuf_message.Message) -> bool:
            if message.DESCRIPTOR.full_name == 'rbt.std.collections.ordered_map.v1.InvalidRangeError':
                return True
            return False

    class ReverseRangeTask:
        """Represents a scheduled task running for the
        state. Note that this is not a coroutine because we are trying
        to convey the semantics that the task is already running (or
        will soon be).
        """

        @classmethod
        def retrieve(
            cls,
            context: IMPORT_reboot_aio_contexts.Context | IMPORT_reboot_aio_external.ExternalContext,
            *,
            task_id: IMPORT_rbt_v1alpha1.tasks_pb2.TaskId,
        ):
            return cls(context, task_id=task_id)

        def __init__(
            self,
            context: IMPORT_reboot_aio_contexts.Context | IMPORT_reboot_aio_external.ExternalContext,
            *,
            task_id: IMPORT_rbt_v1alpha1.tasks_pb2.TaskId,
        ) -> None:
            # Depending on the context type (inside or outside a Reboot application)
            # we may or may not know the application ID. If we don't know it, then
            # the `ExternalContext.gateway` will determine it.
            #
            # TODO: in the future we expect to support cross-application calls, in
            #       which case the developer may explicitly pass in an application ID
            #       here.
            self._application_id: IMPORT_typing.Optional[IMPORT_reboot_aio_types.ApplicationId] = None
            if isinstance(context, IMPORT_reboot_aio_contexts.Context):
                self._application_id = context.application_id
            self._channel_manager = context.channel_manager
            self._task_id = task_id

        @property
        def task_id(self) -> IMPORT_rbt_v1alpha1.tasks_pb2.TaskId:
            return self._task_id

        def __await__(self) -> IMPORT_typing.Generator[None, None, rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapReverseRangeResponse]:
            """Awaits for task to finish and returns its response."""
            async def wait_for_task() -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapReverseRangeResponse:
                channel = self._channel_manager.get_channel_to_state(
                    IMPORT_reboot_aio_types.StateTypeName(self._task_id.state_type),
                    IMPORT_reboot_aio_types.StateRef(self._task_id.state_ref),
                )

                stub = IMPORT_rbt_v1alpha1.tasks_pb2_grpc.TasksStub(channel)

                try:
                    call = IMPORT_reboot_aio_stubs.UnaryRetriedCall(
                        call=None,  # `RetriedCall` can create the call itself.
                        stub_method=stub.Wait,
                        method_name="Wait",
                        request=IMPORT_rbt_v1alpha1.tasks_pb2.WaitRequest(task_id=self._task_id),
                        metadata=IMPORT_reboot_aio_headers.Headers(
                            state_ref=IMPORT_reboot_aio_types.StateRef(self._task_id.state_ref),
                            application_id=self._application_id,
                        ).to_grpc_metadata(),
                        aborted_type=IMPORT_rebootdev.aio.aborted.SystemAborted,
                    )

                    wait_for_task_response = await call
                except IMPORT_rebootdev.aio.aborted.SystemAborted as error:
                    if error.code == IMPORT_grpc.StatusCode.NOT_FOUND:
                        raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                            IMPORT_rbt_v1alpha1.errors_pb2.UnknownTask()
                        ) from None

                    raise
                else:
                    response_or_error: IMPORT_typing.Optional[IMPORT_google_protobuf_any_pb2.Any] = None
                    is_error = False

                    if wait_for_task_response.response_or_error.WhichOneof("response_or_error") == "response":
                        response_or_error = wait_for_task_response.response_or_error.response
                    else:
                        is_error = True
                        response_or_error = wait_for_task_response.response_or_error.error

                    assert response_or_error is not None
                    assert response_or_error.TypeName() != ""

                    response = rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapReverseRangeResponse()

                    if (
                        not is_error and response_or_error.TypeName() != response.DESCRIPTOR.full_name
                    ):
                        raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                            IMPORT_rbt_v1alpha1.errors_pb2.InvalidArgument(),
                            message=
                            f"task with UUID {IMPORT_uuid.UUID(bytes=self._task_id.task_uuid)} "
                            f"has a response of type '{response_or_error.TypeName()}' "
                            "but expecting type 'rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapReverseRangeResponse'; "
                            "are you waiting on a task of the correct method?",
                        ) from None

                    if is_error:
                        # Currently only cancelled error is supported.
                        raise IMPORT_asyncio.CancelledError("Task was cancelled by a TasksServicer")
                    else:
                        response_or_error.Unpack(response)
                        return response

            return wait_for_task().__await__()


    class ReverseRangeAborted(IMPORT_rebootdev.aio.aborted.Aborted):

        MethodError = IMPORT_typing.Union[
            rbt.std.collections.ordered_map.v1.ordered_map_pb2.InvalidRangeError        ]

        Error = IMPORT_typing.Union[
            MethodError,
            IMPORT_rebootdev.aio.aborted.GrpcError,
            IMPORT_rebootdev.aio.aborted.RebootError,
        ]

        METHOD_ERROR_TYPES: list[type[IMPORT_google_protobuf_message.Message]] = [
            rbt.std.collections.ordered_map.v1.ordered_map_pb2.InvalidRangeError        ]

        ERROR_TYPES: list[type[IMPORT_google_protobuf_message.Message]] = (
            METHOD_ERROR_TYPES +
            IMPORT_rebootdev.aio.aborted.GRPC_ERROR_TYPES +
            IMPORT_rebootdev.aio.aborted.REBOOT_ERROR_TYPES
        )

        _error: Error
        _code: IMPORT_grpc.StatusCode
        _message: IMPORT_typing.Optional[str]

        def __init__(
            self,
            error: MethodError |  IMPORT_rebootdev.aio.aborted.GrpcError,
            *,
            message: IMPORT_typing.Optional[str] = None,
            # Do not set this value when constructing in order to
            # raise. This is only used internally when constructing
            # from aborted calls.
            error_types: IMPORT_typing.Sequence[type[Error]] = (
                METHOD_ERROR_TYPES + IMPORT_rebootdev.aio.aborted.GRPC_ERROR_TYPES
            ),
        ):
            super().__init__()

            IMPORT_reboot_aio_types.assert_type(error, error_types)

            self._error = error

            code = self.grpc_status_code_from_error(self._error)

            if code is None:
                # Must be a Reboot specific or declared method error.
                code = IMPORT_grpc.StatusCode.ABORTED

            self._code = code

            self._message = message

        @property
        def error(self) -> Error:
            return self._error

        @property
        def code(self) -> IMPORT_grpc.StatusCode:
            return self._code

        @property
        def message(self) -> IMPORT_typing.Optional[str]:
            return self._message

        @classmethod
        def from_status(cls, status: IMPORT_google_rpc_status_pb2.Status):
            error = cls.error_from_google_rpc_status_details(
                status,
                cls.ERROR_TYPES,
            )

            message = status.message if len(status.message) > 0 else None

            if error is not None:
                return cls(error, message=message, error_types=cls.ERROR_TYPES)

            error = cls.error_from_google_rpc_status_code(status)

            assert error is not None

            # TODO(benh): also consider getting the type names from
            # `status.details` and including that in `message` to make
            # debugging easier.

            return cls(error, message=message)

        @classmethod
        def from_grpc_aio_rpc_error(cls, aio_rpc_error: IMPORT_grpc.aio.AioRpcError):
            return cls(
                cls.error_from_grpc_aio_rpc_error(aio_rpc_error),
                message=aio_rpc_error.details(),
            )

        @classmethod
        def is_declared_error(cls, message: IMPORT_google_protobuf_message.Message) -> bool:
            if message.DESCRIPTOR.full_name == 'rbt.std.collections.ordered_map.v1.InvalidRangeError':
                return True
            return False

    class StringifyTask:
        """Represents a scheduled task running for the
        state. Note that this is not a coroutine because we are trying
        to convey the semantics that the task is already running (or
        will soon be).
        """

        @classmethod
        def retrieve(
            cls,
            context: IMPORT_reboot_aio_contexts.Context | IMPORT_reboot_aio_external.ExternalContext,
            *,
            task_id: IMPORT_rbt_v1alpha1.tasks_pb2.TaskId,
        ):
            return cls(context, task_id=task_id)

        def __init__(
            self,
            context: IMPORT_reboot_aio_contexts.Context | IMPORT_reboot_aio_external.ExternalContext,
            *,
            task_id: IMPORT_rbt_v1alpha1.tasks_pb2.TaskId,
        ) -> None:
            # Depending on the context type (inside or outside a Reboot application)
            # we may or may not know the application ID. If we don't know it, then
            # the `ExternalContext.gateway` will determine it.
            #
            # TODO: in the future we expect to support cross-application calls, in
            #       which case the developer may explicitly pass in an application ID
            #       here.
            self._application_id: IMPORT_typing.Optional[IMPORT_reboot_aio_types.ApplicationId] = None
            if isinstance(context, IMPORT_reboot_aio_contexts.Context):
                self._application_id = context.application_id
            self._channel_manager = context.channel_manager
            self._task_id = task_id

        @property
        def task_id(self) -> IMPORT_rbt_v1alpha1.tasks_pb2.TaskId:
            return self._task_id

        def __await__(self) -> IMPORT_typing.Generator[None, None, rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapStringifyResponse]:
            """Awaits for task to finish and returns its response."""
            async def wait_for_task() -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapStringifyResponse:
                channel = self._channel_manager.get_channel_to_state(
                    IMPORT_reboot_aio_types.StateTypeName(self._task_id.state_type),
                    IMPORT_reboot_aio_types.StateRef(self._task_id.state_ref),
                )

                stub = IMPORT_rbt_v1alpha1.tasks_pb2_grpc.TasksStub(channel)

                try:
                    call = IMPORT_reboot_aio_stubs.UnaryRetriedCall(
                        call=None,  # `RetriedCall` can create the call itself.
                        stub_method=stub.Wait,
                        method_name="Wait",
                        request=IMPORT_rbt_v1alpha1.tasks_pb2.WaitRequest(task_id=self._task_id),
                        metadata=IMPORT_reboot_aio_headers.Headers(
                            state_ref=IMPORT_reboot_aio_types.StateRef(self._task_id.state_ref),
                            application_id=self._application_id,
                        ).to_grpc_metadata(),
                        aborted_type=IMPORT_rebootdev.aio.aborted.SystemAborted,
                    )

                    wait_for_task_response = await call
                except IMPORT_rebootdev.aio.aborted.SystemAborted as error:
                    if error.code == IMPORT_grpc.StatusCode.NOT_FOUND:
                        raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                            IMPORT_rbt_v1alpha1.errors_pb2.UnknownTask()
                        ) from None

                    raise
                else:
                    response_or_error: IMPORT_typing.Optional[IMPORT_google_protobuf_any_pb2.Any] = None
                    is_error = False

                    if wait_for_task_response.response_or_error.WhichOneof("response_or_error") == "response":
                        response_or_error = wait_for_task_response.response_or_error.response
                    else:
                        is_error = True
                        response_or_error = wait_for_task_response.response_or_error.error

                    assert response_or_error is not None
                    assert response_or_error.TypeName() != ""

                    response = rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapStringifyResponse()

                    if (
                        not is_error and response_or_error.TypeName() != response.DESCRIPTOR.full_name
                    ):
                        raise IMPORT_rebootdev.aio.aborted.SystemAborted(
                            IMPORT_rbt_v1alpha1.errors_pb2.InvalidArgument(),
                            message=
                            f"task with UUID {IMPORT_uuid.UUID(bytes=self._task_id.task_uuid)} "
                            f"has a response of type '{response_or_error.TypeName()}' "
                            "but expecting type 'rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapStringifyResponse'; "
                            "are you waiting on a task of the correct method?",
                        ) from None

                    if is_error:
                        # Currently only cancelled error is supported.
                        raise IMPORT_asyncio.CancelledError("Task was cancelled by a TasksServicer")
                    else:
                        response_or_error.Unpack(response)
                        return response

            return wait_for_task().__await__()


    class StringifyAborted(IMPORT_rebootdev.aio.aborted.Aborted):


        Error = IMPORT_typing.Union[
            IMPORT_rebootdev.aio.aborted.GrpcError,
            IMPORT_rebootdev.aio.aborted.RebootError,
        ]

        METHOD_ERROR_TYPES: list[type[IMPORT_google_protobuf_message.Message]] = [
        ]

        ERROR_TYPES: list[type[IMPORT_google_protobuf_message.Message]] = (
            METHOD_ERROR_TYPES +
            IMPORT_rebootdev.aio.aborted.GRPC_ERROR_TYPES +
            IMPORT_rebootdev.aio.aborted.REBOOT_ERROR_TYPES
        )

        _error: Error
        _code: IMPORT_grpc.StatusCode
        _message: IMPORT_typing.Optional[str]

        def __init__(
            self,
            error:  IMPORT_rebootdev.aio.aborted.GrpcError,
            *,
            message: IMPORT_typing.Optional[str] = None,
            # Do not set this value when constructing in order to
            # raise. This is only used internally when constructing
            # from aborted calls.
            error_types: IMPORT_typing.Sequence[type[Error]] = (
                METHOD_ERROR_TYPES + IMPORT_rebootdev.aio.aborted.GRPC_ERROR_TYPES
            ),
        ):
            super().__init__()

            IMPORT_reboot_aio_types.assert_type(error, error_types)

            self._error = error

            code = self.grpc_status_code_from_error(self._error)

            if code is None:
                # Must be a Reboot specific or declared method error.
                code = IMPORT_grpc.StatusCode.ABORTED

            self._code = code

            self._message = message

        @property
        def error(self) -> Error:
            return self._error

        @property
        def code(self) -> IMPORT_grpc.StatusCode:
            return self._code

        @property
        def message(self) -> IMPORT_typing.Optional[str]:
            return self._message

        @classmethod
        def from_status(cls, status: IMPORT_google_rpc_status_pb2.Status):
            error = cls.error_from_google_rpc_status_details(
                status,
                cls.ERROR_TYPES,
            )

            message = status.message if len(status.message) > 0 else None

            if error is not None:
                return cls(error, message=message, error_types=cls.ERROR_TYPES)

            error = cls.error_from_google_rpc_status_code(status)

            assert error is not None

            # TODO(benh): also consider getting the type names from
            # `status.details` and including that in `message` to make
            # debugging easier.

            return cls(error, message=message)

        @classmethod
        def from_grpc_aio_rpc_error(cls, aio_rpc_error: IMPORT_grpc.aio.AioRpcError):
            return cls(
                cls.error_from_grpc_aio_rpc_error(aio_rpc_error),
                message=aio_rpc_error.details(),
            )

        @classmethod
        def is_declared_error(cls, message: IMPORT_google_protobuf_message.Message) -> bool:
            return False


    class WeakReference(IMPORT_typing.Generic[OrderedMap_ScheduleTypeVar]):

        _schedule_type: type[OrderedMap_ScheduleTypeVar]

        def __init__(
            self,
            # When application ID is None, refers to a state within the application given by the context.
            application_id: IMPORT_typing.Optional[IMPORT_reboot_aio_types.ApplicationId],
            state_id: IMPORT_reboot_aio_types.StateId,
            *,
            schedule_type: type[OrderedMap_ScheduleTypeVar],
            bearer_token: IMPORT_typing.Optional[str] = None,
            servicer: IMPORT_typing.Optional[OrderedMapBaseServicer] = None,
        ):
            self._application_id = application_id
            self._state_ref = IMPORT_reboot_aio_types.StateRef.from_id(
              OrderedMap.__state_type_name__,
              state_id,
            )
            self._schedule_type = schedule_type
            self._idempotency_manager: IMPORT_typing.Optional[IMPORT_reboot_aio_idempotency.IdempotencyManager] = None
            self._reader_stub: IMPORT_typing.Optional[OrderedMapReaderStub] = None
            self._writer_stub: IMPORT_typing.Optional[OrderedMapWriterStub] = None
            self._workflow_stub: IMPORT_typing.Optional[OrderedMapWorkflowStub] = None
            self._tasks_stub: IMPORT_typing.Optional[OrderedMapTasksStub] = None
            self._bearer_token = bearer_token
            self._servicer = servicer

        @property
        def state_id(self) -> IMPORT_reboot_aio_types.StateId:
            return self._state_ref.id

        def _reader(
            self,
            context: IMPORT_reboot_aio_contexts.ReaderContext | IMPORT_reboot_aio_contexts.WriterContext | IMPORT_reboot_aio_contexts.TransactionContext | IMPORT_reboot_aio_contexts.WorkflowContext | IMPORT_reboot_aio_external.ExternalContext,
        ) -> OrderedMapReaderStub:
            if self._reader_stub is None:
                self._reader_stub = OrderedMapReaderStub(
                    context=context,
                    state_ref=self._state_ref,
                    bearer_token=self._bearer_token,
                )
            assert self._reader_stub is not None
            if self._idempotency_manager is None:
                self._idempotency_manager = context
            elif self._idempotency_manager != context:
                raise IMPORT_reboot_aio_call.MixedContextsError(
                    "This `WeakReference` for `OrderedMap` with ID "
                    f"'{self.state_id}' has previously been used by a "
                    "different `Context`. That is not allowed. "
                    "Instead create a new `WeakReference` for every `Context` by calling "
                    f"`OrderedMap.ref('{self.state_id}')`."
                )
            return self._reader_stub

        def _writer(
            self,
            context: IMPORT_reboot_aio_contexts.TransactionContext | IMPORT_reboot_aio_contexts.WorkflowContext | IMPORT_reboot_aio_external.ExternalContext,
        ) -> OrderedMapWriterStub:
            if self._writer_stub is None:
                self._writer_stub = OrderedMapWriterStub(
                    context=context,
                    state_ref=self._state_ref,
                    bearer_token=self._bearer_token,
                )
            assert self._writer_stub is not None
            if self._idempotency_manager is None:
                self._idempotency_manager = context
            elif self._idempotency_manager != context:
                raise IMPORT_reboot_aio_call.MixedContextsError(
                    "This `WeakReference` for `OrderedMap` with ID "
                    f"'{self.state_id}' has previously been used by a "
                    "different `Context`. That is not allowed. "
                    "Instead create a new `WeakReference` for every `Context` by calling "
                    f"`OrderedMap.ref('{self.state_id}')`."
                )
            return self._writer_stub

        def _workflow(
            self,
            context: IMPORT_reboot_aio_contexts.TransactionContext | IMPORT_reboot_aio_contexts.WorkflowContext | IMPORT_reboot_aio_external.ExternalContext,
        ) -> OrderedMapWorkflowStub:
            if self._workflow_stub is None:
                self._workflow_stub = OrderedMapWorkflowStub(
                    context=context,
                    state_ref=self._state_ref,
                    bearer_token=self._bearer_token,
                )
            assert self._workflow_stub is not None
            if self._idempotency_manager is None:
                self._idempotency_manager = context
            elif self._idempotency_manager != context:
                raise IMPORT_reboot_aio_call.MixedContextsError(
                    "This `WeakReference` for `OrderedMap` with ID "
                    f"'{self.state_id}' has previously been used by a "
                    "different `Context`. That is not allowed. "
                    "Instead create a new `WeakReference` for every `Context` by calling "
                    f"`OrderedMap.ref('{self.state_id}')`."
                )
            return self._workflow_stub

        def _tasks(
            self,
            context: IMPORT_reboot_aio_contexts.TransactionContext | IMPORT_reboot_aio_contexts.WorkflowContext | IMPORT_reboot_aio_external.ExternalContext,
        ) -> OrderedMapTasksStub:
            if self._tasks_stub is None:
                self._tasks_stub = OrderedMapTasksStub(
                    context=context,
                    state_ref=self._state_ref,
                    bearer_token=self._bearer_token,
                )
            assert self._tasks_stub is not None
            if self._idempotency_manager is None:
                self._idempotency_manager = context
            elif self._idempotency_manager != context:
                raise IMPORT_reboot_aio_call.MixedContextsError(
                    "This `WeakReference` for `OrderedMap` with ID "
                    f"'{self.state_id}' has previously been used by a "
                    "different `Context`. That is not allowed. "
                    "Instead create a new `WeakReference` for every `Context` by calling "
                    f"`OrderedMap.ref('{self.state_id}')`."
                )
            return self._tasks_stub

        class _Reactively:

            def __init__(
                self,
                *,
                application_id: IMPORT_typing.Optional[IMPORT_reboot_aio_types.ApplicationId],
                state_ref: IMPORT_reboot_aio_types.StateRef,
                bearer_token: IMPORT_typing.Optional[str] = None,
            ):
                self._application_id = application_id
                self._state_ref = state_ref
                self._bearer_token = bearer_token

            async def Search(
                # In methods which are dealing with user input, (i.e.,
                # proto message field names), we should use '__double_underscored__'
                # variables to avoid any potential name conflicts with the method's
                # parameters.
                # The '__self__' parameter is a convention in Python to
                # indicate that this method is a bound method, so we use
                # '__this__' instead.
                __this__,
                # Explicitly-reactive calls only make sense in the context of either...
                # (A) an external client, or...
                # (B) methods that may reasonably run for a long time, which in Reboot means: readers or workflows.
                __context__: IMPORT_reboot_aio_external.ExternalContext | IMPORT_reboot_aio_contexts.ReaderContext | IMPORT_reboot_aio_contexts.WorkflowContext,
                __options__: IMPORT_typing.Optional[IMPORT_reboot_aio_call.Options] = None,
                *,
                key: IMPORT_typing.Optional[str] = None,
            ) -> IMPORT_typing.AsyncIterator[rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapSearchResponse]:
                # UX improvement: check that neither positional argument was accidentally
                # given a gRPC request type.
                IMPORT_reboot_aio_types.assert_not_request_type(__context__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapSearchRequest)
                IMPORT_reboot_aio_types.assert_not_request_type(__options__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapSearchRequest)
                IMPORT_reboot_aio_types.assert_type(__context__, [IMPORT_reboot_aio_external.ExternalContext, IMPORT_reboot_aio_contexts.ReaderContext, IMPORT_reboot_aio_contexts.WorkflowContext])

                # TODO: mypy-protobuf declares that
                # `google.protobuf.message.Message` constructor arguments are
                # always non-None, when in reality they are optional.
                __request__ = rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapSearchRequest(
                    key=key,  # type: ignore[arg-type]
                )

                __metadata__: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None
                __bearer_token__: IMPORT_typing.Optional[str] = None
                __app_internal_authorization__: IMPORT_typing.Optional[str] = None

                if __options__ is not None:
                    IMPORT_reboot_aio_types.assert_type(__options__, [IMPORT_reboot_aio_call.Options])
                    if __options__.metadata is not None:
                        __metadata__ = __options__.metadata
                    if __options__.bearer_token is not None:
                        __bearer_token__ = __options__.bearer_token

                if __bearer_token__ is None:
                    __bearer_token__ = __this__._bearer_token
                if __bearer_token__ is None and isinstance(__context__, IMPORT_reboot_aio_external.ExternalContext):
                    # Within a Reboot context we do not pass on the caller's bearer token, as that might
                    # have security implications - we cannot simply trust any service we are calling with
                    # the user's credentials. Instead, the developer can rely on the default app-internal
                    # auth, or override that and set an explicit bearer token.
                    #
                    # In the case of `ExternalContext`, however, its `bearer_token` was set specifically
                    # by the developer for the purpose of making these calls. Note that only
                    # `ExternalContext` even has a `bearer_token` field.
                    __bearer_token__ = __context__.bearer_token

                if __metadata__ is None:
                    __metadata__ = ()

                if isinstance(__context__, IMPORT_reboot_aio_contexts.Context):
                    if __this__._application_id is None:
                        # Given our context type (inside a Reboot application) we can default to
                        # making the application send traffic to itself.
                        __this__._application_id = __context__.application_id
                        # It is safe to use app-internal auth given we know we're talking to
                        # the same application.
                        __app_internal_authorization__ = __context__._app_internal_api_key_secret

                __headers__ = IMPORT_reboot_aio_headers.Headers(
                    bearer_token=__bearer_token__,
                    state_ref=__this__._state_ref,
                    application_id=__this__._application_id,
                    app_internal_authorization=__app_internal_authorization__
                )

                __metadata__ += __headers__.to_grpc_metadata()

                __query_backoff__ = IMPORT_reboot_aio_backoff.Backoff()
                while True:
                    try:
                        async with __context__.channel_manager.get_channel_to_state(
                            IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.OrderedMap'),
                            __this__._state_ref,
                        ) as __channel__:

                            __call__ = IMPORT_rbt_v1alpha1.react_pb2_grpc.ReactStub(
                                __channel__
                            ).Query(
                                IMPORT_rbt_v1alpha1.react_pb2.QueryRequest(
                                    method='Search',
                                    request=__request__.SerializeToString(),
                                ),
                                metadata=__metadata__,
                            )

                            async for __query_response__ in __call__:
                                # Clear the backoff so we don't wait
                                # as long the next time we get
                                # disconnected.
                                __query_backoff__.clear()

                                # The backend may have sent us this query
                                # response only to let us know that a new
                                # idempotency key has been recorded; there may
                                # not be a new response. Python callers don't
                                # (currently) care about such an event, so we
                                # simply ignore it.
                                if not __query_response__.HasField("response"):
                                    continue

                                __response__ = rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapSearchResponse()
                                __response__.ParseFromString(__query_response__.response)
                                yield __response__

                    except BaseException as exception:
                        # We expect to get disconnected from the server
                        # from time to time, e.g., when it is being
                        # updated, but we don't want that error to
                        # propagate, we just want to retry.
                        if IMPORT_rebootdev.aio.aborted.is_grpc_retryable_exception(exception):
                            logger.debug(
                                "Reactive read to 'rbt.std.collections.ordered_map.v1.OrderedMapMethods.Search' "
                                f"failed with a retryable error: '{exception}'; "
                                "will retry..."
                            )
                            await __query_backoff__()
                            continue
                        raise

            # Keep the original functions on the client, so old code will
            # continue to work, but use the new 'snake_case' method in
            # the new code.
            search = Search
            async def Range(
                # In methods which are dealing with user input, (i.e.,
                # proto message field names), we should use '__double_underscored__'
                # variables to avoid any potential name conflicts with the method's
                # parameters.
                # The '__self__' parameter is a convention in Python to
                # indicate that this method is a bound method, so we use
                # '__this__' instead.
                __this__,
                # Explicitly-reactive calls only make sense in the context of either...
                # (A) an external client, or...
                # (B) methods that may reasonably run for a long time, which in Reboot means: readers or workflows.
                __context__: IMPORT_reboot_aio_external.ExternalContext | IMPORT_reboot_aio_contexts.ReaderContext | IMPORT_reboot_aio_contexts.WorkflowContext,
                __options__: IMPORT_typing.Optional[IMPORT_reboot_aio_call.Options] = None,
                *,
                start_key: IMPORT_typing.Optional[str] = None,
                limit: IMPORT_typing.Optional[int] = None,
            ) -> IMPORT_typing.AsyncIterator[rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRangeResponse]:
                # UX improvement: check that neither positional argument was accidentally
                # given a gRPC request type.
                IMPORT_reboot_aio_types.assert_not_request_type(__context__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRangeRequest)
                IMPORT_reboot_aio_types.assert_not_request_type(__options__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRangeRequest)
                IMPORT_reboot_aio_types.assert_type(__context__, [IMPORT_reboot_aio_external.ExternalContext, IMPORT_reboot_aio_contexts.ReaderContext, IMPORT_reboot_aio_contexts.WorkflowContext])

                # TODO: mypy-protobuf declares that
                # `google.protobuf.message.Message` constructor arguments are
                # always non-None, when in reality they are optional.
                __request__ = rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRangeRequest(
                    start_key=start_key,  # type: ignore[arg-type]
                    limit=limit,  # type: ignore[arg-type]
                )

                __metadata__: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None
                __bearer_token__: IMPORT_typing.Optional[str] = None
                __app_internal_authorization__: IMPORT_typing.Optional[str] = None

                if __options__ is not None:
                    IMPORT_reboot_aio_types.assert_type(__options__, [IMPORT_reboot_aio_call.Options])
                    if __options__.metadata is not None:
                        __metadata__ = __options__.metadata
                    if __options__.bearer_token is not None:
                        __bearer_token__ = __options__.bearer_token

                if __bearer_token__ is None:
                    __bearer_token__ = __this__._bearer_token
                if __bearer_token__ is None and isinstance(__context__, IMPORT_reboot_aio_external.ExternalContext):
                    # Within a Reboot context we do not pass on the caller's bearer token, as that might
                    # have security implications - we cannot simply trust any service we are calling with
                    # the user's credentials. Instead, the developer can rely on the default app-internal
                    # auth, or override that and set an explicit bearer token.
                    #
                    # In the case of `ExternalContext`, however, its `bearer_token` was set specifically
                    # by the developer for the purpose of making these calls. Note that only
                    # `ExternalContext` even has a `bearer_token` field.
                    __bearer_token__ = __context__.bearer_token

                if __metadata__ is None:
                    __metadata__ = ()

                if isinstance(__context__, IMPORT_reboot_aio_contexts.Context):
                    if __this__._application_id is None:
                        # Given our context type (inside a Reboot application) we can default to
                        # making the application send traffic to itself.
                        __this__._application_id = __context__.application_id
                        # It is safe to use app-internal auth given we know we're talking to
                        # the same application.
                        __app_internal_authorization__ = __context__._app_internal_api_key_secret

                __headers__ = IMPORT_reboot_aio_headers.Headers(
                    bearer_token=__bearer_token__,
                    state_ref=__this__._state_ref,
                    application_id=__this__._application_id,
                    app_internal_authorization=__app_internal_authorization__
                )

                __metadata__ += __headers__.to_grpc_metadata()

                __query_backoff__ = IMPORT_reboot_aio_backoff.Backoff()
                while True:
                    try:
                        async with __context__.channel_manager.get_channel_to_state(
                            IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.OrderedMap'),
                            __this__._state_ref,
                        ) as __channel__:

                            __call__ = IMPORT_rbt_v1alpha1.react_pb2_grpc.ReactStub(
                                __channel__
                            ).Query(
                                IMPORT_rbt_v1alpha1.react_pb2.QueryRequest(
                                    method='Range',
                                    request=__request__.SerializeToString(),
                                ),
                                metadata=__metadata__,
                            )

                            async for __query_response__ in __call__:
                                # Clear the backoff so we don't wait
                                # as long the next time we get
                                # disconnected.
                                __query_backoff__.clear()

                                # The backend may have sent us this query
                                # response only to let us know that a new
                                # idempotency key has been recorded; there may
                                # not be a new response. Python callers don't
                                # (currently) care about such an event, so we
                                # simply ignore it.
                                if not __query_response__.HasField("response"):
                                    continue

                                __response__ = rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRangeResponse()
                                __response__.ParseFromString(__query_response__.response)
                                yield __response__

                    except BaseException as exception:
                        # We expect to get disconnected from the server
                        # from time to time, e.g., when it is being
                        # updated, but we don't want that error to
                        # propagate, we just want to retry.
                        if IMPORT_rebootdev.aio.aborted.is_grpc_retryable_exception(exception):
                            logger.debug(
                                "Reactive read to 'rbt.std.collections.ordered_map.v1.OrderedMapMethods.Range' "
                                f"failed with a retryable error: '{exception}'; "
                                "will retry..."
                            )
                            await __query_backoff__()
                            continue
                        raise

            # Keep the original functions on the client, so old code will
            # continue to work, but use the new 'snake_case' method in
            # the new code.
            range = Range
            async def ReverseRange(
                # In methods which are dealing with user input, (i.e.,
                # proto message field names), we should use '__double_underscored__'
                # variables to avoid any potential name conflicts with the method's
                # parameters.
                # The '__self__' parameter is a convention in Python to
                # indicate that this method is a bound method, so we use
                # '__this__' instead.
                __this__,
                # Explicitly-reactive calls only make sense in the context of either...
                # (A) an external client, or...
                # (B) methods that may reasonably run for a long time, which in Reboot means: readers or workflows.
                __context__: IMPORT_reboot_aio_external.ExternalContext | IMPORT_reboot_aio_contexts.ReaderContext | IMPORT_reboot_aio_contexts.WorkflowContext,
                __options__: IMPORT_typing.Optional[IMPORT_reboot_aio_call.Options] = None,
                *,
                start_key: IMPORT_typing.Optional[str] = None,
                limit: IMPORT_typing.Optional[int] = None,
            ) -> IMPORT_typing.AsyncIterator[rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapReverseRangeResponse]:
                # UX improvement: check that neither positional argument was accidentally
                # given a gRPC request type.
                IMPORT_reboot_aio_types.assert_not_request_type(__context__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapReverseRangeRequest)
                IMPORT_reboot_aio_types.assert_not_request_type(__options__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapReverseRangeRequest)
                IMPORT_reboot_aio_types.assert_type(__context__, [IMPORT_reboot_aio_external.ExternalContext, IMPORT_reboot_aio_contexts.ReaderContext, IMPORT_reboot_aio_contexts.WorkflowContext])

                # TODO: mypy-protobuf declares that
                # `google.protobuf.message.Message` constructor arguments are
                # always non-None, when in reality they are optional.
                __request__ = rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapReverseRangeRequest(
                    start_key=start_key,  # type: ignore[arg-type]
                    limit=limit,  # type: ignore[arg-type]
                )

                __metadata__: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None
                __bearer_token__: IMPORT_typing.Optional[str] = None
                __app_internal_authorization__: IMPORT_typing.Optional[str] = None

                if __options__ is not None:
                    IMPORT_reboot_aio_types.assert_type(__options__, [IMPORT_reboot_aio_call.Options])
                    if __options__.metadata is not None:
                        __metadata__ = __options__.metadata
                    if __options__.bearer_token is not None:
                        __bearer_token__ = __options__.bearer_token

                if __bearer_token__ is None:
                    __bearer_token__ = __this__._bearer_token
                if __bearer_token__ is None and isinstance(__context__, IMPORT_reboot_aio_external.ExternalContext):
                    # Within a Reboot context we do not pass on the caller's bearer token, as that might
                    # have security implications - we cannot simply trust any service we are calling with
                    # the user's credentials. Instead, the developer can rely on the default app-internal
                    # auth, or override that and set an explicit bearer token.
                    #
                    # In the case of `ExternalContext`, however, its `bearer_token` was set specifically
                    # by the developer for the purpose of making these calls. Note that only
                    # `ExternalContext` even has a `bearer_token` field.
                    __bearer_token__ = __context__.bearer_token

                if __metadata__ is None:
                    __metadata__ = ()

                if isinstance(__context__, IMPORT_reboot_aio_contexts.Context):
                    if __this__._application_id is None:
                        # Given our context type (inside a Reboot application) we can default to
                        # making the application send traffic to itself.
                        __this__._application_id = __context__.application_id
                        # It is safe to use app-internal auth given we know we're talking to
                        # the same application.
                        __app_internal_authorization__ = __context__._app_internal_api_key_secret

                __headers__ = IMPORT_reboot_aio_headers.Headers(
                    bearer_token=__bearer_token__,
                    state_ref=__this__._state_ref,
                    application_id=__this__._application_id,
                    app_internal_authorization=__app_internal_authorization__
                )

                __metadata__ += __headers__.to_grpc_metadata()

                __query_backoff__ = IMPORT_reboot_aio_backoff.Backoff()
                while True:
                    try:
                        async with __context__.channel_manager.get_channel_to_state(
                            IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.OrderedMap'),
                            __this__._state_ref,
                        ) as __channel__:

                            __call__ = IMPORT_rbt_v1alpha1.react_pb2_grpc.ReactStub(
                                __channel__
                            ).Query(
                                IMPORT_rbt_v1alpha1.react_pb2.QueryRequest(
                                    method='ReverseRange',
                                    request=__request__.SerializeToString(),
                                ),
                                metadata=__metadata__,
                            )

                            async for __query_response__ in __call__:
                                # Clear the backoff so we don't wait
                                # as long the next time we get
                                # disconnected.
                                __query_backoff__.clear()

                                # The backend may have sent us this query
                                # response only to let us know that a new
                                # idempotency key has been recorded; there may
                                # not be a new response. Python callers don't
                                # (currently) care about such an event, so we
                                # simply ignore it.
                                if not __query_response__.HasField("response"):
                                    continue

                                __response__ = rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapReverseRangeResponse()
                                __response__.ParseFromString(__query_response__.response)
                                yield __response__

                    except BaseException as exception:
                        # We expect to get disconnected from the server
                        # from time to time, e.g., when it is being
                        # updated, but we don't want that error to
                        # propagate, we just want to retry.
                        if IMPORT_rebootdev.aio.aborted.is_grpc_retryable_exception(exception):
                            logger.debug(
                                "Reactive read to 'rbt.std.collections.ordered_map.v1.OrderedMapMethods.ReverseRange' "
                                f"failed with a retryable error: '{exception}'; "
                                "will retry..."
                            )
                            await __query_backoff__()
                            continue
                        raise

            # Keep the original functions on the client, so old code will
            # continue to work, but use the new 'snake_case' method in
            # the new code.
            reverse_range = ReverseRange
            async def Stringify(
                # In methods which are dealing with user input, (i.e.,
                # proto message field names), we should use '__double_underscored__'
                # variables to avoid any potential name conflicts with the method's
                # parameters.
                # The '__self__' parameter is a convention in Python to
                # indicate that this method is a bound method, so we use
                # '__this__' instead.
                __this__,
                # Explicitly-reactive calls only make sense in the context of either...
                # (A) an external client, or...
                # (B) methods that may reasonably run for a long time, which in Reboot means: readers or workflows.
                __context__: IMPORT_reboot_aio_external.ExternalContext | IMPORT_reboot_aio_contexts.ReaderContext | IMPORT_reboot_aio_contexts.WorkflowContext,
                __options__: IMPORT_typing.Optional[IMPORT_reboot_aio_call.Options] = None,
            ) -> IMPORT_typing.AsyncIterator[rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapStringifyResponse]:
                # UX improvement: check that neither positional argument was accidentally
                # given a gRPC request type.
                IMPORT_reboot_aio_types.assert_not_request_type(__context__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapStringifyRequest)
                IMPORT_reboot_aio_types.assert_not_request_type(__options__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapStringifyRequest)
                IMPORT_reboot_aio_types.assert_type(__context__, [IMPORT_reboot_aio_external.ExternalContext, IMPORT_reboot_aio_contexts.ReaderContext, IMPORT_reboot_aio_contexts.WorkflowContext])

                # TODO: mypy-protobuf declares that
                # `google.protobuf.message.Message` constructor arguments are
                # always non-None, when in reality they are optional.
                __request__ = rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapStringifyRequest(
                )

                __metadata__: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None
                __bearer_token__: IMPORT_typing.Optional[str] = None
                __app_internal_authorization__: IMPORT_typing.Optional[str] = None

                if __options__ is not None:
                    IMPORT_reboot_aio_types.assert_type(__options__, [IMPORT_reboot_aio_call.Options])
                    if __options__.metadata is not None:
                        __metadata__ = __options__.metadata
                    if __options__.bearer_token is not None:
                        __bearer_token__ = __options__.bearer_token

                if __bearer_token__ is None:
                    __bearer_token__ = __this__._bearer_token
                if __bearer_token__ is None and isinstance(__context__, IMPORT_reboot_aio_external.ExternalContext):
                    # Within a Reboot context we do not pass on the caller's bearer token, as that might
                    # have security implications - we cannot simply trust any service we are calling with
                    # the user's credentials. Instead, the developer can rely on the default app-internal
                    # auth, or override that and set an explicit bearer token.
                    #
                    # In the case of `ExternalContext`, however, its `bearer_token` was set specifically
                    # by the developer for the purpose of making these calls. Note that only
                    # `ExternalContext` even has a `bearer_token` field.
                    __bearer_token__ = __context__.bearer_token

                if __metadata__ is None:
                    __metadata__ = ()

                if isinstance(__context__, IMPORT_reboot_aio_contexts.Context):
                    if __this__._application_id is None:
                        # Given our context type (inside a Reboot application) we can default to
                        # making the application send traffic to itself.
                        __this__._application_id = __context__.application_id
                        # It is safe to use app-internal auth given we know we're talking to
                        # the same application.
                        __app_internal_authorization__ = __context__._app_internal_api_key_secret

                __headers__ = IMPORT_reboot_aio_headers.Headers(
                    bearer_token=__bearer_token__,
                    state_ref=__this__._state_ref,
                    application_id=__this__._application_id,
                    app_internal_authorization=__app_internal_authorization__
                )

                __metadata__ += __headers__.to_grpc_metadata()

                __query_backoff__ = IMPORT_reboot_aio_backoff.Backoff()
                while True:
                    try:
                        async with __context__.channel_manager.get_channel_to_state(
                            IMPORT_reboot_aio_types.StateTypeName('rbt.std.collections.ordered_map.v1.OrderedMap'),
                            __this__._state_ref,
                        ) as __channel__:

                            __call__ = IMPORT_rbt_v1alpha1.react_pb2_grpc.ReactStub(
                                __channel__
                            ).Query(
                                IMPORT_rbt_v1alpha1.react_pb2.QueryRequest(
                                    method='Stringify',
                                    request=__request__.SerializeToString(),
                                ),
                                metadata=__metadata__,
                            )

                            async for __query_response__ in __call__:
                                # Clear the backoff so we don't wait
                                # as long the next time we get
                                # disconnected.
                                __query_backoff__.clear()

                                # The backend may have sent us this query
                                # response only to let us know that a new
                                # idempotency key has been recorded; there may
                                # not be a new response. Python callers don't
                                # (currently) care about such an event, so we
                                # simply ignore it.
                                if not __query_response__.HasField("response"):
                                    continue

                                __response__ = rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapStringifyResponse()
                                __response__.ParseFromString(__query_response__.response)
                                yield __response__

                    except BaseException as exception:
                        # We expect to get disconnected from the server
                        # from time to time, e.g., when it is being
                        # updated, but we don't want that error to
                        # propagate, we just want to retry.
                        if IMPORT_rebootdev.aio.aborted.is_grpc_retryable_exception(exception):
                            logger.debug(
                                "Reactive read to 'rbt.std.collections.ordered_map.v1.OrderedMapMethods.Stringify' "
                                f"failed with a retryable error: '{exception}'; "
                                "will retry..."
                            )
                            await __query_backoff__()
                            continue
                        raise

            # Keep the original functions on the client, so old code will
            # continue to work, but use the new 'snake_case' method in
            # the new code.
            stringify = Stringify

        def reactively(self):
            return OrderedMap.WeakReference._Reactively(
                application_id=self._application_id,
                state_ref=self._state_ref,
                bearer_token=self._bearer_token,
            )

        class _Idempotently(IMPORT_typing.Generic[OrderedMap_IdempotentlyScheduleTypeVar]):

            _weak_reference: OrderedMap.WeakReference[OrderedMap_IdempotentlyScheduleTypeVar]

            def __init__(
                self,
                *,
                weak_reference: OrderedMap.WeakReference[OrderedMap_IdempotentlyScheduleTypeVar],
                idempotency: IMPORT_reboot_aio_idempotency.Idempotency,
            ):
                self._weak_reference = weak_reference
                self._idempotency = idempotency

            def schedule(
                self,
                *,
                when: IMPORT_typing.Optional[IMPORT_datetime_datetime | IMPORT_datetime_timedelta] = None,
            ) -> OrderedMap_IdempotentlyScheduleTypeVar:
                return self._weak_reference._schedule_type(
                    self._weak_reference._application_id,
                    self._weak_reference._tasks,
                    when=when,
                    idempotency=self._idempotency,
                )

            def spawn(
                self,
                *,
                when: IMPORT_typing.Optional[IMPORT_datetime_datetime | IMPORT_datetime_timedelta] = None,
            ) -> OrderedMap.WeakReference._Spawn:
                return OrderedMap.WeakReference._Spawn(
                    self._weak_reference._application_id,
                    self._weak_reference._tasks,
                    when=when,
                    idempotency=self._idempotency,
                )

            async def read(
                self,
                context: IMPORT_reboot_aio_contexts.WorkflowContext,
            ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap:
                if self._weak_reference._servicer is None:
                    raise RuntimeError(
                        "`read()` is currently only supported within workflows; "
                        "Please reach out and let us know your use case if this "
                        "is important for you!"
                    )

                return await OrderedMapBaseServicer.WorkflowState._Idempotently._read(
                    self._weak_reference._servicer,
                    self._idempotency,
                    context,
                )

            @IMPORT_typing.overload
            async def write(
                self,
                context: IMPORT_reboot_aio_contexts.WorkflowContext,
                writer: OrderedMapBaseServicer.InlineWriterCallable[None],
                *,
                type: type = type(None),
            ) -> None:
                ...

            @IMPORT_typing.overload
            async def write(
                self,
                context: IMPORT_reboot_aio_contexts.WorkflowContext,
                writer: OrderedMapBaseServicer.InlineWriterCallable[OrderedMapBaseServicer.InlineWriterCallableResult],
                *,
                type: type[OrderedMapBaseServicer.InlineWriterCallableResult],
            ) -> OrderedMapBaseServicer.InlineWriterCallableResult:
                ...

            async def write(
                self,
                context: IMPORT_reboot_aio_contexts.WorkflowContext,
                writer: OrderedMapBaseServicer.InlineWriterCallable[OrderedMapBaseServicer.InlineWriterCallableResult],
                *,
                type: type = type(None),
            ) -> OrderedMapBaseServicer.InlineWriterCallableResult:
                if self._weak_reference._servicer is None:
                    raise RuntimeError(
                        "`write()` is currently only supported within workflows; "
                        "Please reach out and let us know your use case if this "
                        "is important for you!"
                    )

                return await OrderedMapBaseServicer.WorkflowState._Idempotently._write_validating_effects(
                    self._weak_reference._servicer,
                    self._idempotency,
                    context,
                    writer,
                    type_result=type,
                    check_type=not self._idempotency.always,
                    unidempotently=self._idempotency.always,
                    checkpoint=context.checkpoint(),
                )

            async def Create(
                # In methods which are dealing with user input, (i.e.,
                # proto message field names), we should use '__double_underscored__'
                # variables to avoid any potential name conflicts with the method's
                # parameters.
                # The '__self__' parameter is a convention in Python to
                # indicate that this method is a bound method, so we use
                # '__this__' instead.
                __this__,
                __context__: IMPORT_reboot_aio_contexts.TransactionContext | IMPORT_reboot_aio_contexts.WorkflowContext | IMPORT_reboot_aio_external.ExternalContext,
                __options__: IMPORT_reboot_aio_call.Options = IMPORT_reboot_aio_call.Options(),
                *,
                order: IMPORT_typing.Optional[int] = None,
            ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapCreateResponse:
                # UX improvement: check that neither positional argument was accidentally
                # given a gRPC request type.
                IMPORT_reboot_aio_types.assert_not_request_type(__context__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapCreateRequest)
                IMPORT_reboot_aio_types.assert_not_request_type(__options__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapCreateRequest)

                IMPORT_reboot_aio_types.assert_type(__options__, [IMPORT_reboot_aio_call.Options])
                if __options__.idempotency is not None:
                    raise RuntimeError(
                        'Found redundant idempotency in `Options`'
                    )

                __options__ = IMPORT_dataclasses.replace(
                    __options__,
                    idempotency_key=__this__._idempotency.key,
                    idempotency_alias=__this__._idempotency.alias,
                    generate_idempotency=__this__._idempotency.generate,
                    generated_idempotency=__this__._idempotency.generated,
                )

                return await __this__._weak_reference.Create(
                    __context__,
                    __options__,
                    order=order,
                )

            # Keep the original functions on the client, so old code will
            # continue to work, but use the new 'snake_case' method in
            # the new code.
            create = Create
            async def Search(
                # In methods which are dealing with user input, (i.e.,
                # proto message field names), we should use '__double_underscored__'
                # variables to avoid any potential name conflicts with the method's
                # parameters.
                # The '__self__' parameter is a convention in Python to
                # indicate that this method is a bound method, so we use
                # '__this__' instead.
                __this__,
                __context__: IMPORT_reboot_aio_contexts.TransactionContext | IMPORT_reboot_aio_contexts.WorkflowContext | IMPORT_reboot_aio_external.ExternalContext,
                __options__: IMPORT_reboot_aio_call.Options = IMPORT_reboot_aio_call.Options(),
                *,
                key: IMPORT_typing.Optional[str] = None,
            ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapSearchResponse:
                # UX improvement: check that neither positional argument was accidentally
                # given a gRPC request type.
                IMPORT_reboot_aio_types.assert_not_request_type(__context__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapSearchRequest)
                IMPORT_reboot_aio_types.assert_not_request_type(__options__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapSearchRequest)

                IMPORT_reboot_aio_types.assert_type(__options__, [IMPORT_reboot_aio_call.Options])
                if __options__.idempotency is not None:
                    raise RuntimeError(
                        'Found redundant idempotency in `Options`'
                    )

                __options__ = IMPORT_dataclasses.replace(
                    __options__,
                    idempotency_key=__this__._idempotency.key,
                    idempotency_alias=__this__._idempotency.alias,
                    generate_idempotency=__this__._idempotency.generate,
                    generated_idempotency=__this__._idempotency.generated,
                )

                return await __this__._weak_reference.Search(
                    __context__,
                    __options__,
                    key=key,
                )

            # Keep the original functions on the client, so old code will
            # continue to work, but use the new 'snake_case' method in
            # the new code.
            search = Search
            async def Insert(
                # In methods which are dealing with user input, (i.e.,
                # proto message field names), we should use '__double_underscored__'
                # variables to avoid any potential name conflicts with the method's
                # parameters.
                # The '__self__' parameter is a convention in Python to
                # indicate that this method is a bound method, so we use
                # '__this__' instead.
                __this__,
                __context__: IMPORT_reboot_aio_contexts.TransactionContext | IMPORT_reboot_aio_contexts.WorkflowContext | IMPORT_reboot_aio_external.ExternalContext,
                __options__: IMPORT_reboot_aio_call.Options = IMPORT_reboot_aio_call.Options(),
                *,
                key: IMPORT_typing.Optional[str] = None,
                value: IMPORT_typing.Optional[google.protobuf.struct_pb2.Value] = None,
                bytes: IMPORT_typing.Optional[bytes] = None,
                any: IMPORT_typing.Optional[google.protobuf.any_pb2.Any] = None,
            ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapInsertResponse:
                # UX improvement: check that neither positional argument was accidentally
                # given a gRPC request type.
                IMPORT_reboot_aio_types.assert_not_request_type(__context__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapInsertRequest)
                IMPORT_reboot_aio_types.assert_not_request_type(__options__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapInsertRequest)

                IMPORT_reboot_aio_types.assert_type(__options__, [IMPORT_reboot_aio_call.Options])
                if __options__.idempotency is not None:
                    raise RuntimeError(
                        'Found redundant idempotency in `Options`'
                    )

                __options__ = IMPORT_dataclasses.replace(
                    __options__,
                    idempotency_key=__this__._idempotency.key,
                    idempotency_alias=__this__._idempotency.alias,
                    generate_idempotency=__this__._idempotency.generate,
                    generated_idempotency=__this__._idempotency.generated,
                )

                return await __this__._weak_reference.Insert(
                    __context__,
                    __options__,
                    key=key,
                    value=value,
                    bytes=bytes,
                    any=any,
                )

            # Keep the original functions on the client, so old code will
            # continue to work, but use the new 'snake_case' method in
            # the new code.
            insert = Insert
            async def Remove(
                # In methods which are dealing with user input, (i.e.,
                # proto message field names), we should use '__double_underscored__'
                # variables to avoid any potential name conflicts with the method's
                # parameters.
                # The '__self__' parameter is a convention in Python to
                # indicate that this method is a bound method, so we use
                # '__this__' instead.
                __this__,
                __context__: IMPORT_reboot_aio_contexts.TransactionContext | IMPORT_reboot_aio_contexts.WorkflowContext | IMPORT_reboot_aio_external.ExternalContext,
                __options__: IMPORT_reboot_aio_call.Options = IMPORT_reboot_aio_call.Options(),
                *,
                key: IMPORT_typing.Optional[str] = None,
            ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRemoveResponse:
                # UX improvement: check that neither positional argument was accidentally
                # given a gRPC request type.
                IMPORT_reboot_aio_types.assert_not_request_type(__context__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRemoveRequest)
                IMPORT_reboot_aio_types.assert_not_request_type(__options__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRemoveRequest)

                IMPORT_reboot_aio_types.assert_type(__options__, [IMPORT_reboot_aio_call.Options])
                if __options__.idempotency is not None:
                    raise RuntimeError(
                        'Found redundant idempotency in `Options`'
                    )

                __options__ = IMPORT_dataclasses.replace(
                    __options__,
                    idempotency_key=__this__._idempotency.key,
                    idempotency_alias=__this__._idempotency.alias,
                    generate_idempotency=__this__._idempotency.generate,
                    generated_idempotency=__this__._idempotency.generated,
                )

                return await __this__._weak_reference.Remove(
                    __context__,
                    __options__,
                    key=key,
                )

            # Keep the original functions on the client, so old code will
            # continue to work, but use the new 'snake_case' method in
            # the new code.
            remove = Remove
            async def Range(
                # In methods which are dealing with user input, (i.e.,
                # proto message field names), we should use '__double_underscored__'
                # variables to avoid any potential name conflicts with the method's
                # parameters.
                # The '__self__' parameter is a convention in Python to
                # indicate that this method is a bound method, so we use
                # '__this__' instead.
                __this__,
                __context__: IMPORT_reboot_aio_contexts.TransactionContext | IMPORT_reboot_aio_contexts.WorkflowContext | IMPORT_reboot_aio_external.ExternalContext,
                __options__: IMPORT_reboot_aio_call.Options = IMPORT_reboot_aio_call.Options(),
                *,
                start_key: IMPORT_typing.Optional[str] = None,
                limit: IMPORT_typing.Optional[int] = None,
            ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRangeResponse:
                # UX improvement: check that neither positional argument was accidentally
                # given a gRPC request type.
                IMPORT_reboot_aio_types.assert_not_request_type(__context__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRangeRequest)
                IMPORT_reboot_aio_types.assert_not_request_type(__options__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRangeRequest)

                IMPORT_reboot_aio_types.assert_type(__options__, [IMPORT_reboot_aio_call.Options])
                if __options__.idempotency is not None:
                    raise RuntimeError(
                        'Found redundant idempotency in `Options`'
                    )

                __options__ = IMPORT_dataclasses.replace(
                    __options__,
                    idempotency_key=__this__._idempotency.key,
                    idempotency_alias=__this__._idempotency.alias,
                    generate_idempotency=__this__._idempotency.generate,
                    generated_idempotency=__this__._idempotency.generated,
                )

                return await __this__._weak_reference.Range(
                    __context__,
                    __options__,
                    start_key=start_key,
                    limit=limit,
                )

            # Keep the original functions on the client, so old code will
            # continue to work, but use the new 'snake_case' method in
            # the new code.
            range = Range
            async def ReverseRange(
                # In methods which are dealing with user input, (i.e.,
                # proto message field names), we should use '__double_underscored__'
                # variables to avoid any potential name conflicts with the method's
                # parameters.
                # The '__self__' parameter is a convention in Python to
                # indicate that this method is a bound method, so we use
                # '__this__' instead.
                __this__,
                __context__: IMPORT_reboot_aio_contexts.TransactionContext | IMPORT_reboot_aio_contexts.WorkflowContext | IMPORT_reboot_aio_external.ExternalContext,
                __options__: IMPORT_reboot_aio_call.Options = IMPORT_reboot_aio_call.Options(),
                *,
                start_key: IMPORT_typing.Optional[str] = None,
                limit: IMPORT_typing.Optional[int] = None,
            ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapReverseRangeResponse:
                # UX improvement: check that neither positional argument was accidentally
                # given a gRPC request type.
                IMPORT_reboot_aio_types.assert_not_request_type(__context__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapReverseRangeRequest)
                IMPORT_reboot_aio_types.assert_not_request_type(__options__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapReverseRangeRequest)

                IMPORT_reboot_aio_types.assert_type(__options__, [IMPORT_reboot_aio_call.Options])
                if __options__.idempotency is not None:
                    raise RuntimeError(
                        'Found redundant idempotency in `Options`'
                    )

                __options__ = IMPORT_dataclasses.replace(
                    __options__,
                    idempotency_key=__this__._idempotency.key,
                    idempotency_alias=__this__._idempotency.alias,
                    generate_idempotency=__this__._idempotency.generate,
                    generated_idempotency=__this__._idempotency.generated,
                )

                return await __this__._weak_reference.ReverseRange(
                    __context__,
                    __options__,
                    start_key=start_key,
                    limit=limit,
                )

            # Keep the original functions on the client, so old code will
            # continue to work, but use the new 'snake_case' method in
            # the new code.
            reverse_range = ReverseRange
            async def Stringify(
                # In methods which are dealing with user input, (i.e.,
                # proto message field names), we should use '__double_underscored__'
                # variables to avoid any potential name conflicts with the method's
                # parameters.
                # The '__self__' parameter is a convention in Python to
                # indicate that this method is a bound method, so we use
                # '__this__' instead.
                __this__,
                __context__: IMPORT_reboot_aio_contexts.TransactionContext | IMPORT_reboot_aio_contexts.WorkflowContext | IMPORT_reboot_aio_external.ExternalContext,
                __options__: IMPORT_reboot_aio_call.Options = IMPORT_reboot_aio_call.Options(),
            ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapStringifyResponse:
                # UX improvement: check that neither positional argument was accidentally
                # given a gRPC request type.
                IMPORT_reboot_aio_types.assert_not_request_type(__context__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapStringifyRequest)
                IMPORT_reboot_aio_types.assert_not_request_type(__options__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapStringifyRequest)

                IMPORT_reboot_aio_types.assert_type(__options__, [IMPORT_reboot_aio_call.Options])
                if __options__.idempotency is not None:
                    raise RuntimeError(
                        'Found redundant idempotency in `Options`'
                    )

                __options__ = IMPORT_dataclasses.replace(
                    __options__,
                    idempotency_key=__this__._idempotency.key,
                    idempotency_alias=__this__._idempotency.alias,
                    generate_idempotency=__this__._idempotency.generate,
                    generated_idempotency=__this__._idempotency.generated,
                )

                return await __this__._weak_reference.Stringify(
                    __context__,
                    __options__,
                )

            # Keep the original functions on the client, so old code will
            # continue to work, but use the new 'snake_case' method in
            # the new code.
            stringify = Stringify

        @IMPORT_typing.overload
        def idempotently(self, alias: IMPORT_typing.Optional[str] = None, *, each_iteration: bool = False) -> OrderedMap.WeakReference._Idempotently[OrderedMap_ScheduleTypeVar]:
            ...

        @IMPORT_typing.overload
        def idempotently(self, *, key: IMPORT_uuid.UUID, generated: bool = False) -> OrderedMap.WeakReference._Idempotently[OrderedMap_ScheduleTypeVar]:
            ...

        def idempotently(
            self,
            alias: IMPORT_typing.Optional[str] = None,
            *,
            key: IMPORT_typing.Optional[IMPORT_uuid.UUID] = None,
            each_iteration: IMPORT_typing.Optional[bool] = None,
            generated: bool = False,
        ) -> OrderedMap.WeakReference._Idempotently[OrderedMap_ScheduleTypeVar]:
            return OrderedMap.WeakReference._Idempotently(
                weak_reference=self,
                idempotency=IMPORT_reboot_aio_contexts.Context.idempotency(
                    alias=alias,
                    key=key,
                    each_iteration=each_iteration,
                    generated=generated,
                )
            )

        def per_workflow(self, alias: IMPORT_typing.Optional[str] = None):
            return self.idempotently(alias)

        def per_iteration(self, alias: IMPORT_typing.Optional[str] = None):
            return self.idempotently(alias, each_iteration=True)

        def always(self):
            return self.idempotently(key=IMPORT_uuid.uuid4(), generated=True)

        class _UntilChangesSatisfies(IMPORT_typing.Generic[OrderedMap_UntilCallableType]):

            _idempotency_alias: str
            _context: IMPORT_reboot_aio_contexts.WorkflowContext
            _callable: IMPORT_typing.Callable[[], IMPORT_typing.Awaitable[OrderedMap_UntilCallableType]]
            _type: type[OrderedMap_UntilCallableType]

            def __init__(
                self,
                *,
                idempotency_alias: str,
                context: IMPORT_reboot_aio_contexts.WorkflowContext,
                callable: IMPORT_typing.Callable[[], IMPORT_typing.Awaitable[OrderedMap_UntilCallableType]],
                type: type[OrderedMap_UntilCallableType],
            ):
                self._idempotency_alias = idempotency_alias
                self._context = context
                self._callable = callable
                self._type = type

            async def changes(self):
                return await IMPORT_reboot_aio_workflows.until_changes(
                    self._idempotency_alias,
                    self._context,
                    self._callable,
                    type=self._type,
                )

            async def satisfies(
                self,
                condition: IMPORT_typing.Callable[[OrderedMap_UntilCallableType], bool],
            ):

                async def converge():
                    response = await self._callable()
                    if condition(response):
                        return response
                    return False

                return await IMPORT_reboot_aio_workflows.until(
                    self._idempotency_alias,
                    self._context,
                    converge,
                    type=self._type,
                )

        class _Until:

            _weak_reference: OrderedMap.WeakReference
            _idempotency_alias: str

            def __init__(
                self,
                *,
                weak_reference: OrderedMap.WeakReference,
                idempotency_alias: str,
            ):
                self._weak_reference = weak_reference
                self._idempotency_alias = idempotency_alias

            def read(
                self,
                context: IMPORT_reboot_aio_contexts.WorkflowContext,
            ) -> OrderedMap.WeakReference._UntilChangesSatisfies[rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap]:
                IMPORT_reboot_aio_types.assert_type(
                    context,
                    [IMPORT_reboot_aio_contexts.WorkflowContext],
                )

                async def callable():
                    return await self._weak_reference.read(context)

                return OrderedMap.WeakReference._UntilChangesSatisfies(
                    idempotency_alias=self._idempotency_alias,
                    context=context,
                    callable=callable,
                    type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap,
                )

            def Search(
                # In methods which are dealing with user input, (i.e.,
                # proto message field names), we should use '__double_underscored__'
                # variables to avoid any potential name conflicts with the method's
                # parameters.
                # The '__self__' parameter is a convention in Python to
                # indicate that this method is a bound method, so we use
                # '__this__' instead.
                __this__,
                __context__: IMPORT_reboot_aio_contexts.WorkflowContext,
                __options__: IMPORT_reboot_aio_call.Options = IMPORT_reboot_aio_call.Options(),
                *,
                key: IMPORT_typing.Optional[str] = None,
            ) -> OrderedMap.WeakReference._UntilChangesSatisfies[rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapSearchResponse]:
                IMPORT_reboot_aio_types.assert_type(
                    __context__,
                    [IMPORT_reboot_aio_contexts.WorkflowContext],
                )

                async def callable():
                    return await __this__._weak_reference.Search(
                        __context__,
                        __options__,
                        key=key,
                    )

                return OrderedMap.WeakReference._UntilChangesSatisfies(
                    idempotency_alias=__this__._idempotency_alias,
                    context=__context__,
                    callable=callable,
                    type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapSearchResponse,
                )

            # Keep the original functions on the client, so old code will
            # continue to work, but use the new 'snake_case' method in
            # the new code.
            search = Search
            def Range(
                # In methods which are dealing with user input, (i.e.,
                # proto message field names), we should use '__double_underscored__'
                # variables to avoid any potential name conflicts with the method's
                # parameters.
                # The '__self__' parameter is a convention in Python to
                # indicate that this method is a bound method, so we use
                # '__this__' instead.
                __this__,
                __context__: IMPORT_reboot_aio_contexts.WorkflowContext,
                __options__: IMPORT_reboot_aio_call.Options = IMPORT_reboot_aio_call.Options(),
                *,
                start_key: IMPORT_typing.Optional[str] = None,
                limit: IMPORT_typing.Optional[int] = None,
            ) -> OrderedMap.WeakReference._UntilChangesSatisfies[rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRangeResponse]:
                IMPORT_reboot_aio_types.assert_type(
                    __context__,
                    [IMPORT_reboot_aio_contexts.WorkflowContext],
                )

                async def callable():
                    return await __this__._weak_reference.Range(
                        __context__,
                        __options__,
                        start_key=start_key,
                        limit=limit,
                    )

                return OrderedMap.WeakReference._UntilChangesSatisfies(
                    idempotency_alias=__this__._idempotency_alias,
                    context=__context__,
                    callable=callable,
                    type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRangeResponse,
                )

            # Keep the original functions on the client, so old code will
            # continue to work, but use the new 'snake_case' method in
            # the new code.
            range = Range
            def ReverseRange(
                # In methods which are dealing with user input, (i.e.,
                # proto message field names), we should use '__double_underscored__'
                # variables to avoid any potential name conflicts with the method's
                # parameters.
                # The '__self__' parameter is a convention in Python to
                # indicate that this method is a bound method, so we use
                # '__this__' instead.
                __this__,
                __context__: IMPORT_reboot_aio_contexts.WorkflowContext,
                __options__: IMPORT_reboot_aio_call.Options = IMPORT_reboot_aio_call.Options(),
                *,
                start_key: IMPORT_typing.Optional[str] = None,
                limit: IMPORT_typing.Optional[int] = None,
            ) -> OrderedMap.WeakReference._UntilChangesSatisfies[rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapReverseRangeResponse]:
                IMPORT_reboot_aio_types.assert_type(
                    __context__,
                    [IMPORT_reboot_aio_contexts.WorkflowContext],
                )

                async def callable():
                    return await __this__._weak_reference.ReverseRange(
                        __context__,
                        __options__,
                        start_key=start_key,
                        limit=limit,
                    )

                return OrderedMap.WeakReference._UntilChangesSatisfies(
                    idempotency_alias=__this__._idempotency_alias,
                    context=__context__,
                    callable=callable,
                    type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapReverseRangeResponse,
                )

            # Keep the original functions on the client, so old code will
            # continue to work, but use the new 'snake_case' method in
            # the new code.
            reverse_range = ReverseRange
            def Stringify(
                # In methods which are dealing with user input, (i.e.,
                # proto message field names), we should use '__double_underscored__'
                # variables to avoid any potential name conflicts with the method's
                # parameters.
                # The '__self__' parameter is a convention in Python to
                # indicate that this method is a bound method, so we use
                # '__this__' instead.
                __this__,
                __context__: IMPORT_reboot_aio_contexts.WorkflowContext,
                __options__: IMPORT_reboot_aio_call.Options = IMPORT_reboot_aio_call.Options(),
            ) -> OrderedMap.WeakReference._UntilChangesSatisfies[rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapStringifyResponse]:
                IMPORT_reboot_aio_types.assert_type(
                    __context__,
                    [IMPORT_reboot_aio_contexts.WorkflowContext],
                )

                async def callable():
                    return await __this__._weak_reference.Stringify(
                        __context__,
                        __options__,
                    )

                return OrderedMap.WeakReference._UntilChangesSatisfies(
                    idempotency_alias=__this__._idempotency_alias,
                    context=__context__,
                    callable=callable,
                    type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapStringifyResponse,
                )

            # Keep the original functions on the client, so old code will
            # continue to work, but use the new 'snake_case' method in
            # the new code.
            stringify = Stringify

        def until(self, alias: str):
            return OrderedMap.WeakReference._Until(
                weak_reference=self,
                idempotency_alias=alias,
            )

        def schedule(
            self,
            *,
            when: IMPORT_typing.Optional[IMPORT_datetime_datetime | IMPORT_datetime_timedelta] = None,
        ) -> OrderedMap_ScheduleTypeVar:
            return self._schedule_type(self._application_id, self._tasks, when=when)

        class _Schedule:

            def __init__(
                self,
                application_id: IMPORT_typing.Optional[IMPORT_reboot_aio_types.ApplicationId],
                tasks: IMPORT_typing.Callable[[IMPORT_reboot_aio_contexts.TransactionContext | IMPORT_reboot_aio_contexts.WorkflowContext | IMPORT_reboot_aio_external.ExternalContext], OrderedMapTasksStub],
                *,
                when: IMPORT_typing.Optional[IMPORT_datetime_datetime | IMPORT_datetime_timedelta] = None,
                idempotency: IMPORT_typing.Optional[IMPORT_reboot_aio_idempotency.Idempotency] = None,
            ) -> None:
                self._application_id = application_id
                self._tasks = tasks
                self._when = ensure_has_timezone(when=when)
                self._idempotency = idempotency

            # OrderedMap callable tasks:
            async def Create(
                __this__,
                __context__: IMPORT_reboot_aio_contexts.TransactionContext,
                __options__: IMPORT_typing.Optional[IMPORT_reboot_aio_call.Options] = None,
                *,
                order: IMPORT_typing.Optional[int] = None,
            ) -> IMPORT_rbt_v1alpha1.tasks_pb2.TaskId:
                # UX improvement: check that neither positional argument was accidentally
                # given a gRPC request type.
                IMPORT_reboot_aio_types.assert_not_request_type(__context__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapCreateRequest)
                IMPORT_reboot_aio_types.assert_not_request_type(__options__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapCreateRequest)
                IMPORT_reboot_aio_types.assert_type(__context__, [IMPORT_reboot_aio_contexts.TransactionContext])

                if order is not None and not isinstance(
                    order,
                    int,
                ):
                    raise TypeError(
                        f"Can not construct protobuf message of type "
                        f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapCreateRequest': field 'order' is not "
                        f"of required type 'int'"
                    )
                # TODO: mypy-protobuf declares that
                # `IMPORT_google_protobuf_message.Message` constructor arguments are
                # always non-None, when in reality they are optional.
                __request__ = rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapCreateRequest(
                    order=order,  # type: ignore[arg-type]
                )

                __schedule__: IMPORT_typing.Optional[IMPORT_reboot_time_DateTimeWithTimeZone] = (IMPORT_reboot_time_DateTimeWithTimeZone.now() + __this__._when) if isinstance(
                    __this__._when, IMPORT_datetime_timedelta
                ) else __this__._when

                __metadata__: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None
                __idempotency__: IMPORT_typing.Optional[IMPORT_reboot_aio_idempotency.Idempotency] = __this__._idempotency
                __bearer_token__: IMPORT_typing.Optional[str] = None

                if __options__ is not None:
                    IMPORT_reboot_aio_types.assert_type(__options__, [IMPORT_reboot_aio_call.Options])
                    if __options__.idempotency:
                        if __idempotency__ is not None:
                            raise RuntimeError(
                                'Found redundant idempotency in `Options`'
                            )
                        __idempotency__ = __options__.idempotency
                    if __options__.metadata is not None:
                        __metadata__ = __options__.metadata
                    if __options__.bearer_token is not None:
                        __bearer_token__ = __options__.bearer_token

                # Add scheduling information to the metadata.
                __metadata__ = (
                    (IMPORT_reboot_aio_headers.TASK_SCHEDULE,
                    __schedule__.isoformat() if __schedule__ else ''),
                ) + (__metadata__ or tuple())

                __task_id__ = await __this__._tasks(
                    __context__
                ).Create(
                    __request__,
                    idempotency=__idempotency__,
                    metadata=__metadata__,
                    bearer_token=__bearer_token__,
                )

                return __task_id__

            # Keep the original functions on the client, so old code will
            # continue to work, but use the new 'snake_case' method in
            # the new code.
            create = Create
            async def Search(
                __this__,
                __context__: IMPORT_reboot_aio_contexts.TransactionContext,
                __options__: IMPORT_typing.Optional[IMPORT_reboot_aio_call.Options] = None,
                *,
                key: IMPORT_typing.Optional[str] = None,
            ) -> IMPORT_rbt_v1alpha1.tasks_pb2.TaskId:
                # UX improvement: check that neither positional argument was accidentally
                # given a gRPC request type.
                IMPORT_reboot_aio_types.assert_not_request_type(__context__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapSearchRequest)
                IMPORT_reboot_aio_types.assert_not_request_type(__options__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapSearchRequest)
                IMPORT_reboot_aio_types.assert_type(__context__, [IMPORT_reboot_aio_contexts.TransactionContext])

                if key is not None and not isinstance(
                    key,
                    str,
                ):
                    raise TypeError(
                        f"Can not construct protobuf message of type "
                        f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapSearchRequest': field 'key' is not "
                        f"of required type 'str'"
                    )
                # TODO: mypy-protobuf declares that
                # `IMPORT_google_protobuf_message.Message` constructor arguments are
                # always non-None, when in reality they are optional.
                __request__ = rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapSearchRequest(
                    key=key,  # type: ignore[arg-type]
                )

                __schedule__: IMPORT_typing.Optional[IMPORT_reboot_time_DateTimeWithTimeZone] = (IMPORT_reboot_time_DateTimeWithTimeZone.now() + __this__._when) if isinstance(
                    __this__._when, IMPORT_datetime_timedelta
                ) else __this__._when

                __metadata__: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None
                __idempotency__: IMPORT_typing.Optional[IMPORT_reboot_aio_idempotency.Idempotency] = __this__._idempotency
                __bearer_token__: IMPORT_typing.Optional[str] = None

                if __options__ is not None:
                    IMPORT_reboot_aio_types.assert_type(__options__, [IMPORT_reboot_aio_call.Options])
                    if __options__.idempotency:
                        if __idempotency__ is not None:
                            raise RuntimeError(
                                'Found redundant idempotency in `Options`'
                            )
                        __idempotency__ = __options__.idempotency
                    if __options__.metadata is not None:
                        __metadata__ = __options__.metadata
                    if __options__.bearer_token is not None:
                        __bearer_token__ = __options__.bearer_token

                # Add scheduling information to the metadata.
                __metadata__ = (
                    (IMPORT_reboot_aio_headers.TASK_SCHEDULE,
                    __schedule__.isoformat() if __schedule__ else ''),
                ) + (__metadata__ or tuple())

                __task_id__ = await __this__._tasks(
                    __context__
                ).Search(
                    __request__,
                    idempotency=__idempotency__,
                    metadata=__metadata__,
                    bearer_token=__bearer_token__,
                )

                return __task_id__

            # Keep the original functions on the client, so old code will
            # continue to work, but use the new 'snake_case' method in
            # the new code.
            search = Search
            async def Insert(
                __this__,
                __context__: IMPORT_reboot_aio_contexts.TransactionContext,
                __options__: IMPORT_typing.Optional[IMPORT_reboot_aio_call.Options] = None,
                *,
                key: IMPORT_typing.Optional[str] = None,
                value: IMPORT_typing.Optional[google.protobuf.struct_pb2.Value] = None,
                bytes: IMPORT_typing.Optional[bytes] = None,
                any: IMPORT_typing.Optional[google.protobuf.any_pb2.Any] = None,
            ) -> IMPORT_rbt_v1alpha1.tasks_pb2.TaskId:
                # UX improvement: check that neither positional argument was accidentally
                # given a gRPC request type.
                IMPORT_reboot_aio_types.assert_not_request_type(__context__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapInsertRequest)
                IMPORT_reboot_aio_types.assert_not_request_type(__options__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapInsertRequest)
                IMPORT_reboot_aio_types.assert_type(__context__, [IMPORT_reboot_aio_contexts.TransactionContext])

                if key is not None and not isinstance(
                    key,
                    str,
                ):
                    raise TypeError(
                        f"Can not construct protobuf message of type "
                        f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapInsertRequest': field 'key' is not "
                        f"of required type 'str'"
                    )
                if value is not None and not isinstance(
                    value,
                    google.protobuf.struct_pb2.Value,
                ):
                    raise TypeError(
                        f"Can not construct protobuf message of type "
                        f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapInsertRequest': field 'value' is not "
                        f"of required type 'google.protobuf.struct_pb2.Value'"
                    )
                if bytes is not None and not isinstance(
                    bytes,
                    IMPORT_builtins.bytes
                ):
                    raise TypeError(
                        f"Can not construct protobuf message of type "
                        f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapInsertRequest': field 'bytes' is not "
                        f"of required type 'bytes'"
                    )
                if any is not None and not isinstance(
                    any,
                    google.protobuf.any_pb2.Any,
                ):
                    raise TypeError(
                        f"Can not construct protobuf message of type "
                        f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapInsertRequest': field 'any' is not "
                        f"of required type 'google.protobuf.any_pb2.Any'"
                    )
                # TODO: mypy-protobuf declares that
                # `IMPORT_google_protobuf_message.Message` constructor arguments are
                # always non-None, when in reality they are optional.
                __request__ = rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapInsertRequest(
                    key=key,  # type: ignore[arg-type]
                    value=value,  # type: ignore[arg-type]
                    bytes=bytes,  # type: ignore[arg-type]
                    any=any,  # type: ignore[arg-type]
                )

                __schedule__: IMPORT_typing.Optional[IMPORT_reboot_time_DateTimeWithTimeZone] = (IMPORT_reboot_time_DateTimeWithTimeZone.now() + __this__._when) if isinstance(
                    __this__._when, IMPORT_datetime_timedelta
                ) else __this__._when

                __metadata__: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None
                __idempotency__: IMPORT_typing.Optional[IMPORT_reboot_aio_idempotency.Idempotency] = __this__._idempotency
                __bearer_token__: IMPORT_typing.Optional[str] = None

                if __options__ is not None:
                    IMPORT_reboot_aio_types.assert_type(__options__, [IMPORT_reboot_aio_call.Options])
                    if __options__.idempotency:
                        if __idempotency__ is not None:
                            raise RuntimeError(
                                'Found redundant idempotency in `Options`'
                            )
                        __idempotency__ = __options__.idempotency
                    if __options__.metadata is not None:
                        __metadata__ = __options__.metadata
                    if __options__.bearer_token is not None:
                        __bearer_token__ = __options__.bearer_token

                # Add scheduling information to the metadata.
                __metadata__ = (
                    (IMPORT_reboot_aio_headers.TASK_SCHEDULE,
                    __schedule__.isoformat() if __schedule__ else ''),
                ) + (__metadata__ or tuple())

                __task_id__ = await __this__._tasks(
                    __context__
                ).Insert(
                    __request__,
                    idempotency=__idempotency__,
                    metadata=__metadata__,
                    bearer_token=__bearer_token__,
                )

                return __task_id__

            # Keep the original functions on the client, so old code will
            # continue to work, but use the new 'snake_case' method in
            # the new code.
            insert = Insert
            async def Remove(
                __this__,
                __context__: IMPORT_reboot_aio_contexts.TransactionContext,
                __options__: IMPORT_typing.Optional[IMPORT_reboot_aio_call.Options] = None,
                *,
                key: IMPORT_typing.Optional[str] = None,
            ) -> IMPORT_rbt_v1alpha1.tasks_pb2.TaskId:
                # UX improvement: check that neither positional argument was accidentally
                # given a gRPC request type.
                IMPORT_reboot_aio_types.assert_not_request_type(__context__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRemoveRequest)
                IMPORT_reboot_aio_types.assert_not_request_type(__options__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRemoveRequest)
                IMPORT_reboot_aio_types.assert_type(__context__, [IMPORT_reboot_aio_contexts.TransactionContext])

                if key is not None and not isinstance(
                    key,
                    str,
                ):
                    raise TypeError(
                        f"Can not construct protobuf message of type "
                        f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRemoveRequest': field 'key' is not "
                        f"of required type 'str'"
                    )
                # TODO: mypy-protobuf declares that
                # `IMPORT_google_protobuf_message.Message` constructor arguments are
                # always non-None, when in reality they are optional.
                __request__ = rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRemoveRequest(
                    key=key,  # type: ignore[arg-type]
                )

                __schedule__: IMPORT_typing.Optional[IMPORT_reboot_time_DateTimeWithTimeZone] = (IMPORT_reboot_time_DateTimeWithTimeZone.now() + __this__._when) if isinstance(
                    __this__._when, IMPORT_datetime_timedelta
                ) else __this__._when

                __metadata__: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None
                __idempotency__: IMPORT_typing.Optional[IMPORT_reboot_aio_idempotency.Idempotency] = __this__._idempotency
                __bearer_token__: IMPORT_typing.Optional[str] = None

                if __options__ is not None:
                    IMPORT_reboot_aio_types.assert_type(__options__, [IMPORT_reboot_aio_call.Options])
                    if __options__.idempotency:
                        if __idempotency__ is not None:
                            raise RuntimeError(
                                'Found redundant idempotency in `Options`'
                            )
                        __idempotency__ = __options__.idempotency
                    if __options__.metadata is not None:
                        __metadata__ = __options__.metadata
                    if __options__.bearer_token is not None:
                        __bearer_token__ = __options__.bearer_token

                # Add scheduling information to the metadata.
                __metadata__ = (
                    (IMPORT_reboot_aio_headers.TASK_SCHEDULE,
                    __schedule__.isoformat() if __schedule__ else ''),
                ) + (__metadata__ or tuple())

                __task_id__ = await __this__._tasks(
                    __context__
                ).Remove(
                    __request__,
                    idempotency=__idempotency__,
                    metadata=__metadata__,
                    bearer_token=__bearer_token__,
                )

                return __task_id__

            # Keep the original functions on the client, so old code will
            # continue to work, but use the new 'snake_case' method in
            # the new code.
            remove = Remove
            async def Range(
                __this__,
                __context__: IMPORT_reboot_aio_contexts.TransactionContext,
                __options__: IMPORT_typing.Optional[IMPORT_reboot_aio_call.Options] = None,
                *,
                start_key: IMPORT_typing.Optional[str] = None,
                limit: IMPORT_typing.Optional[int] = None,
            ) -> IMPORT_rbt_v1alpha1.tasks_pb2.TaskId:
                # UX improvement: check that neither positional argument was accidentally
                # given a gRPC request type.
                IMPORT_reboot_aio_types.assert_not_request_type(__context__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRangeRequest)
                IMPORT_reboot_aio_types.assert_not_request_type(__options__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRangeRequest)
                IMPORT_reboot_aio_types.assert_type(__context__, [IMPORT_reboot_aio_contexts.TransactionContext])

                if start_key is not None and not isinstance(
                    start_key,
                    str,
                ):
                    raise TypeError(
                        f"Can not construct protobuf message of type "
                        f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRangeRequest': field 'start_key' is not "
                        f"of required type 'str'"
                    )
                if limit is not None and not isinstance(
                    limit,
                    int,
                ):
                    raise TypeError(
                        f"Can not construct protobuf message of type "
                        f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRangeRequest': field 'limit' is not "
                        f"of required type 'int'"
                    )
                # TODO: mypy-protobuf declares that
                # `IMPORT_google_protobuf_message.Message` constructor arguments are
                # always non-None, when in reality they are optional.
                __request__ = rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRangeRequest(
                    start_key=start_key,  # type: ignore[arg-type]
                    limit=limit,  # type: ignore[arg-type]
                )

                __schedule__: IMPORT_typing.Optional[IMPORT_reboot_time_DateTimeWithTimeZone] = (IMPORT_reboot_time_DateTimeWithTimeZone.now() + __this__._when) if isinstance(
                    __this__._when, IMPORT_datetime_timedelta
                ) else __this__._when

                __metadata__: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None
                __idempotency__: IMPORT_typing.Optional[IMPORT_reboot_aio_idempotency.Idempotency] = __this__._idempotency
                __bearer_token__: IMPORT_typing.Optional[str] = None

                if __options__ is not None:
                    IMPORT_reboot_aio_types.assert_type(__options__, [IMPORT_reboot_aio_call.Options])
                    if __options__.idempotency:
                        if __idempotency__ is not None:
                            raise RuntimeError(
                                'Found redundant idempotency in `Options`'
                            )
                        __idempotency__ = __options__.idempotency
                    if __options__.metadata is not None:
                        __metadata__ = __options__.metadata
                    if __options__.bearer_token is not None:
                        __bearer_token__ = __options__.bearer_token

                # Add scheduling information to the metadata.
                __metadata__ = (
                    (IMPORT_reboot_aio_headers.TASK_SCHEDULE,
                    __schedule__.isoformat() if __schedule__ else ''),
                ) + (__metadata__ or tuple())

                __task_id__ = await __this__._tasks(
                    __context__
                ).Range(
                    __request__,
                    idempotency=__idempotency__,
                    metadata=__metadata__,
                    bearer_token=__bearer_token__,
                )

                return __task_id__

            # Keep the original functions on the client, so old code will
            # continue to work, but use the new 'snake_case' method in
            # the new code.
            range = Range
            async def ReverseRange(
                __this__,
                __context__: IMPORT_reboot_aio_contexts.TransactionContext,
                __options__: IMPORT_typing.Optional[IMPORT_reboot_aio_call.Options] = None,
                *,
                start_key: IMPORT_typing.Optional[str] = None,
                limit: IMPORT_typing.Optional[int] = None,
            ) -> IMPORT_rbt_v1alpha1.tasks_pb2.TaskId:
                # UX improvement: check that neither positional argument was accidentally
                # given a gRPC request type.
                IMPORT_reboot_aio_types.assert_not_request_type(__context__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapReverseRangeRequest)
                IMPORT_reboot_aio_types.assert_not_request_type(__options__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapReverseRangeRequest)
                IMPORT_reboot_aio_types.assert_type(__context__, [IMPORT_reboot_aio_contexts.TransactionContext])

                if start_key is not None and not isinstance(
                    start_key,
                    str,
                ):
                    raise TypeError(
                        f"Can not construct protobuf message of type "
                        f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapReverseRangeRequest': field 'start_key' is not "
                        f"of required type 'str'"
                    )
                if limit is not None and not isinstance(
                    limit,
                    int,
                ):
                    raise TypeError(
                        f"Can not construct protobuf message of type "
                        f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapReverseRangeRequest': field 'limit' is not "
                        f"of required type 'int'"
                    )
                # TODO: mypy-protobuf declares that
                # `IMPORT_google_protobuf_message.Message` constructor arguments are
                # always non-None, when in reality they are optional.
                __request__ = rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapReverseRangeRequest(
                    start_key=start_key,  # type: ignore[arg-type]
                    limit=limit,  # type: ignore[arg-type]
                )

                __schedule__: IMPORT_typing.Optional[IMPORT_reboot_time_DateTimeWithTimeZone] = (IMPORT_reboot_time_DateTimeWithTimeZone.now() + __this__._when) if isinstance(
                    __this__._when, IMPORT_datetime_timedelta
                ) else __this__._when

                __metadata__: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None
                __idempotency__: IMPORT_typing.Optional[IMPORT_reboot_aio_idempotency.Idempotency] = __this__._idempotency
                __bearer_token__: IMPORT_typing.Optional[str] = None

                if __options__ is not None:
                    IMPORT_reboot_aio_types.assert_type(__options__, [IMPORT_reboot_aio_call.Options])
                    if __options__.idempotency:
                        if __idempotency__ is not None:
                            raise RuntimeError(
                                'Found redundant idempotency in `Options`'
                            )
                        __idempotency__ = __options__.idempotency
                    if __options__.metadata is not None:
                        __metadata__ = __options__.metadata
                    if __options__.bearer_token is not None:
                        __bearer_token__ = __options__.bearer_token

                # Add scheduling information to the metadata.
                __metadata__ = (
                    (IMPORT_reboot_aio_headers.TASK_SCHEDULE,
                    __schedule__.isoformat() if __schedule__ else ''),
                ) + (__metadata__ or tuple())

                __task_id__ = await __this__._tasks(
                    __context__
                ).ReverseRange(
                    __request__,
                    idempotency=__idempotency__,
                    metadata=__metadata__,
                    bearer_token=__bearer_token__,
                )

                return __task_id__

            # Keep the original functions on the client, so old code will
            # continue to work, but use the new 'snake_case' method in
            # the new code.
            reverse_range = ReverseRange
            async def Stringify(
                __this__,
                __context__: IMPORT_reboot_aio_contexts.TransactionContext,
                __options__: IMPORT_typing.Optional[IMPORT_reboot_aio_call.Options] = None,
            ) -> IMPORT_rbt_v1alpha1.tasks_pb2.TaskId:
                # UX improvement: check that neither positional argument was accidentally
                # given a gRPC request type.
                IMPORT_reboot_aio_types.assert_not_request_type(__context__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapStringifyRequest)
                IMPORT_reboot_aio_types.assert_not_request_type(__options__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapStringifyRequest)
                IMPORT_reboot_aio_types.assert_type(__context__, [IMPORT_reboot_aio_contexts.TransactionContext])

                # TODO: mypy-protobuf declares that
                # `IMPORT_google_protobuf_message.Message` constructor arguments are
                # always non-None, when in reality they are optional.
                __request__ = rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapStringifyRequest(
                )

                __schedule__: IMPORT_typing.Optional[IMPORT_reboot_time_DateTimeWithTimeZone] = (IMPORT_reboot_time_DateTimeWithTimeZone.now() + __this__._when) if isinstance(
                    __this__._when, IMPORT_datetime_timedelta
                ) else __this__._when

                __metadata__: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None
                __idempotency__: IMPORT_typing.Optional[IMPORT_reboot_aio_idempotency.Idempotency] = __this__._idempotency
                __bearer_token__: IMPORT_typing.Optional[str] = None

                if __options__ is not None:
                    IMPORT_reboot_aio_types.assert_type(__options__, [IMPORT_reboot_aio_call.Options])
                    if __options__.idempotency:
                        if __idempotency__ is not None:
                            raise RuntimeError(
                                'Found redundant idempotency in `Options`'
                            )
                        __idempotency__ = __options__.idempotency
                    if __options__.metadata is not None:
                        __metadata__ = __options__.metadata
                    if __options__.bearer_token is not None:
                        __bearer_token__ = __options__.bearer_token

                # Add scheduling information to the metadata.
                __metadata__ = (
                    (IMPORT_reboot_aio_headers.TASK_SCHEDULE,
                    __schedule__.isoformat() if __schedule__ else ''),
                ) + (__metadata__ or tuple())

                __task_id__ = await __this__._tasks(
                    __context__
                ).Stringify(
                    __request__,
                    idempotency=__idempotency__,
                    metadata=__metadata__,
                    bearer_token=__bearer_token__,
                )

                return __task_id__

            # Keep the original functions on the client, so old code will
            # continue to work, but use the new 'snake_case' method in
            # the new code.
            stringify = Stringify

        # A `WriterContext` can not call any methods in `_Schedule` to
        # prevent a writer from doing a `Foo.ref()` and trying to
        # schedule. However, we want to allow a writer to schedule
        # when we are constructing a `WeakReference` from
        # `self.ref()` so instead we return a `_WriterSchedule` to
        # provide type safety that allows a `WriterContext` to
        # schedule (for itself).
        class _WriterSchedule:

            def __init__(
                self,
                application_id: IMPORT_typing.Optional[IMPORT_reboot_aio_types.ApplicationId],
                tasks: IMPORT_typing.Callable[[IMPORT_reboot_aio_contexts.TransactionContext | IMPORT_reboot_aio_contexts.WorkflowContext | IMPORT_reboot_aio_external.ExternalContext], OrderedMapTasksStub],
                *,
                when: IMPORT_typing.Optional[IMPORT_datetime_datetime | IMPORT_datetime_timedelta] = None,
                idempotency: IMPORT_typing.Optional[IMPORT_reboot_aio_idempotency.Idempotency] = None,
            ) -> None:
                self._tasks = tasks
                self._when = ensure_has_timezone(when=when)
                self._idempotency = idempotency

            # OrderedMap callable tasks:
            async def Create(
                __this__,
                __context__: IMPORT_reboot_aio_contexts.WriterContext | IMPORT_reboot_aio_contexts.TransactionContext,
                __options__: IMPORT_typing.Optional[IMPORT_reboot_aio_call.Options] = None,
                *,
                order: IMPORT_typing.Optional[int] = None,
            ) -> IMPORT_rbt_v1alpha1.tasks_pb2.TaskId:
                # Only `writer`s and `transaction`s should ``schedule()`, a
                # `workflow` should `spawn()`.
                IMPORT_reboot_aio_types.assert_type(__context__, [IMPORT_reboot_aio_contexts.WriterContext, IMPORT_reboot_aio_contexts.TransactionContext])

                # UX improvement: check that neither positional argument was accidentally
                # given a gRPC request type.
                IMPORT_reboot_aio_types.assert_not_request_type(__context__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapCreateRequest)
                IMPORT_reboot_aio_types.assert_not_request_type(__options__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapCreateRequest)

                if order is not None and not isinstance(
                    order,
                    int,
                ):
                    raise TypeError(
                        f"Can not construct protobuf message of type "
                        f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapCreateRequest': field 'order' is not "
                        f"of required type 'int'"
                    )
                # TODO: mypy-protobuf declares that
                # `google.protobuf.message.Message` constructor arguments are
                # always non-None, when in reality they are optional.
                __request__ = rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapCreateRequest(
                    order=order,  # type: ignore[arg-type]
                )
                if isinstance(__context__, IMPORT_reboot_aio_contexts.WriterContext):
                    return (await OrderedMapServicerTasks(
                        context=__context__,
                        state_ref=__context__._state_ref,
                    ).Create(
                        __request__,
                        schedule=__this__._when,
                    )).task_id

                __schedule__: IMPORT_typing.Optional[IMPORT_reboot_time_DateTimeWithTimeZone] = (IMPORT_reboot_time_DateTimeWithTimeZone.now() + __this__._when) if isinstance(
                    __this__._when, IMPORT_datetime_timedelta
                ) else __this__._when

                __metadata__: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None
                __idempotency__: IMPORT_typing.Optional[IMPORT_reboot_aio_idempotency.Idempotency] = __this__._idempotency
                __bearer_token__: IMPORT_typing.Optional[str] = None

                if __options__ is not None:
                    IMPORT_reboot_aio_types.assert_type(__options__, [IMPORT_reboot_aio_call.Options])
                    if __options__.idempotency is not None:
                        if __idempotency__ is not None:
                            raise RuntimeError(
                                'Found redundant idempotency in `Options`'
                            )
                        __idempotency__ = __options__.idempotency
                    if __options__.metadata is not None:
                        __metadata__ = __options__.metadata
                    if __options__.bearer_token is not None:
                        __bearer_token__ = __options__.bearer_token

                # Add scheduling information to the metadata.
                __metadata__ = (
                    (IMPORT_reboot_aio_headers.TASK_SCHEDULE,
                    __schedule__.isoformat() if __schedule__ else ''),
                ) + (__metadata__ or tuple())

                return await __this__._tasks(
                    __context__
                ).Create(
                    __request__,
                    idempotency=__idempotency__,
                    metadata=__metadata__,
                    bearer_token=__bearer_token__,
                )

            # Keep the original functions on the client, so old code will
            # continue to work, but use the new 'snake_case' method in
            # the new code.
            create = Create
            async def Search(
                __this__,
                __context__: IMPORT_reboot_aio_contexts.WriterContext | IMPORT_reboot_aio_contexts.TransactionContext,
                __options__: IMPORT_typing.Optional[IMPORT_reboot_aio_call.Options] = None,
                *,
                key: IMPORT_typing.Optional[str] = None,
            ) -> IMPORT_rbt_v1alpha1.tasks_pb2.TaskId:
                # Only `writer`s and `transaction`s should ``schedule()`, a
                # `workflow` should `spawn()`.
                IMPORT_reboot_aio_types.assert_type(__context__, [IMPORT_reboot_aio_contexts.WriterContext, IMPORT_reboot_aio_contexts.TransactionContext])

                # UX improvement: check that neither positional argument was accidentally
                # given a gRPC request type.
                IMPORT_reboot_aio_types.assert_not_request_type(__context__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapSearchRequest)
                IMPORT_reboot_aio_types.assert_not_request_type(__options__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapSearchRequest)

                if key is not None and not isinstance(
                    key,
                    str,
                ):
                    raise TypeError(
                        f"Can not construct protobuf message of type "
                        f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapSearchRequest': field 'key' is not "
                        f"of required type 'str'"
                    )
                # TODO: mypy-protobuf declares that
                # `google.protobuf.message.Message` constructor arguments are
                # always non-None, when in reality they are optional.
                __request__ = rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapSearchRequest(
                    key=key,  # type: ignore[arg-type]
                )
                if isinstance(__context__, IMPORT_reboot_aio_contexts.WriterContext):
                    return (await OrderedMapServicerTasks(
                        context=__context__,
                        state_ref=__context__._state_ref,
                    ).Search(
                        __request__,
                        schedule=__this__._when,
                    )).task_id

                __schedule__: IMPORT_typing.Optional[IMPORT_reboot_time_DateTimeWithTimeZone] = (IMPORT_reboot_time_DateTimeWithTimeZone.now() + __this__._when) if isinstance(
                    __this__._when, IMPORT_datetime_timedelta
                ) else __this__._when

                __metadata__: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None
                __idempotency__: IMPORT_typing.Optional[IMPORT_reboot_aio_idempotency.Idempotency] = __this__._idempotency
                __bearer_token__: IMPORT_typing.Optional[str] = None

                if __options__ is not None:
                    IMPORT_reboot_aio_types.assert_type(__options__, [IMPORT_reboot_aio_call.Options])
                    if __options__.idempotency is not None:
                        if __idempotency__ is not None:
                            raise RuntimeError(
                                'Found redundant idempotency in `Options`'
                            )
                        __idempotency__ = __options__.idempotency
                    if __options__.metadata is not None:
                        __metadata__ = __options__.metadata
                    if __options__.bearer_token is not None:
                        __bearer_token__ = __options__.bearer_token

                # Add scheduling information to the metadata.
                __metadata__ = (
                    (IMPORT_reboot_aio_headers.TASK_SCHEDULE,
                    __schedule__.isoformat() if __schedule__ else ''),
                ) + (__metadata__ or tuple())

                return await __this__._tasks(
                    __context__
                ).Search(
                    __request__,
                    idempotency=__idempotency__,
                    metadata=__metadata__,
                    bearer_token=__bearer_token__,
                )

            # Keep the original functions on the client, so old code will
            # continue to work, but use the new 'snake_case' method in
            # the new code.
            search = Search
            async def Insert(
                __this__,
                __context__: IMPORT_reboot_aio_contexts.WriterContext | IMPORT_reboot_aio_contexts.TransactionContext,
                __options__: IMPORT_typing.Optional[IMPORT_reboot_aio_call.Options] = None,
                *,
                key: IMPORT_typing.Optional[str] = None,
                value: IMPORT_typing.Optional[google.protobuf.struct_pb2.Value] = None,
                bytes: IMPORT_typing.Optional[bytes] = None,
                any: IMPORT_typing.Optional[google.protobuf.any_pb2.Any] = None,
            ) -> IMPORT_rbt_v1alpha1.tasks_pb2.TaskId:
                # Only `writer`s and `transaction`s should ``schedule()`, a
                # `workflow` should `spawn()`.
                IMPORT_reboot_aio_types.assert_type(__context__, [IMPORT_reboot_aio_contexts.WriterContext, IMPORT_reboot_aio_contexts.TransactionContext])

                # UX improvement: check that neither positional argument was accidentally
                # given a gRPC request type.
                IMPORT_reboot_aio_types.assert_not_request_type(__context__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapInsertRequest)
                IMPORT_reboot_aio_types.assert_not_request_type(__options__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapInsertRequest)

                if key is not None and not isinstance(
                    key,
                    str,
                ):
                    raise TypeError(
                        f"Can not construct protobuf message of type "
                        f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapInsertRequest': field 'key' is not "
                        f"of required type 'str'"
                    )
                if value is not None and not isinstance(
                    value,
                    google.protobuf.struct_pb2.Value,
                ):
                    raise TypeError(
                        f"Can not construct protobuf message of type "
                        f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapInsertRequest': field 'value' is not "
                        f"of required type 'google.protobuf.struct_pb2.Value'"
                    )
                if bytes is not None and not isinstance(
                    bytes,
                    IMPORT_builtins.bytes
                ):
                    raise TypeError(
                        f"Can not construct protobuf message of type "
                        f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapInsertRequest': field 'bytes' is not "
                        f"of required type 'bytes'"
                    )
                if any is not None and not isinstance(
                    any,
                    google.protobuf.any_pb2.Any,
                ):
                    raise TypeError(
                        f"Can not construct protobuf message of type "
                        f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapInsertRequest': field 'any' is not "
                        f"of required type 'google.protobuf.any_pb2.Any'"
                    )
                # TODO: mypy-protobuf declares that
                # `google.protobuf.message.Message` constructor arguments are
                # always non-None, when in reality they are optional.
                __request__ = rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapInsertRequest(
                    key=key,  # type: ignore[arg-type]
                    value=value,  # type: ignore[arg-type]
                    bytes=bytes,  # type: ignore[arg-type]
                    any=any,  # type: ignore[arg-type]
                )
                if isinstance(__context__, IMPORT_reboot_aio_contexts.WriterContext):
                    return (await OrderedMapServicerTasks(
                        context=__context__,
                        state_ref=__context__._state_ref,
                    ).Insert(
                        __request__,
                        schedule=__this__._when,
                    )).task_id

                __schedule__: IMPORT_typing.Optional[IMPORT_reboot_time_DateTimeWithTimeZone] = (IMPORT_reboot_time_DateTimeWithTimeZone.now() + __this__._when) if isinstance(
                    __this__._when, IMPORT_datetime_timedelta
                ) else __this__._when

                __metadata__: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None
                __idempotency__: IMPORT_typing.Optional[IMPORT_reboot_aio_idempotency.Idempotency] = __this__._idempotency
                __bearer_token__: IMPORT_typing.Optional[str] = None

                if __options__ is not None:
                    IMPORT_reboot_aio_types.assert_type(__options__, [IMPORT_reboot_aio_call.Options])
                    if __options__.idempotency is not None:
                        if __idempotency__ is not None:
                            raise RuntimeError(
                                'Found redundant idempotency in `Options`'
                            )
                        __idempotency__ = __options__.idempotency
                    if __options__.metadata is not None:
                        __metadata__ = __options__.metadata
                    if __options__.bearer_token is not None:
                        __bearer_token__ = __options__.bearer_token

                # Add scheduling information to the metadata.
                __metadata__ = (
                    (IMPORT_reboot_aio_headers.TASK_SCHEDULE,
                    __schedule__.isoformat() if __schedule__ else ''),
                ) + (__metadata__ or tuple())

                return await __this__._tasks(
                    __context__
                ).Insert(
                    __request__,
                    idempotency=__idempotency__,
                    metadata=__metadata__,
                    bearer_token=__bearer_token__,
                )

            # Keep the original functions on the client, so old code will
            # continue to work, but use the new 'snake_case' method in
            # the new code.
            insert = Insert
            async def Remove(
                __this__,
                __context__: IMPORT_reboot_aio_contexts.WriterContext | IMPORT_reboot_aio_contexts.TransactionContext,
                __options__: IMPORT_typing.Optional[IMPORT_reboot_aio_call.Options] = None,
                *,
                key: IMPORT_typing.Optional[str] = None,
            ) -> IMPORT_rbt_v1alpha1.tasks_pb2.TaskId:
                # Only `writer`s and `transaction`s should ``schedule()`, a
                # `workflow` should `spawn()`.
                IMPORT_reboot_aio_types.assert_type(__context__, [IMPORT_reboot_aio_contexts.WriterContext, IMPORT_reboot_aio_contexts.TransactionContext])

                # UX improvement: check that neither positional argument was accidentally
                # given a gRPC request type.
                IMPORT_reboot_aio_types.assert_not_request_type(__context__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRemoveRequest)
                IMPORT_reboot_aio_types.assert_not_request_type(__options__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRemoveRequest)

                if key is not None and not isinstance(
                    key,
                    str,
                ):
                    raise TypeError(
                        f"Can not construct protobuf message of type "
                        f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRemoveRequest': field 'key' is not "
                        f"of required type 'str'"
                    )
                # TODO: mypy-protobuf declares that
                # `google.protobuf.message.Message` constructor arguments are
                # always non-None, when in reality they are optional.
                __request__ = rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRemoveRequest(
                    key=key,  # type: ignore[arg-type]
                )
                if isinstance(__context__, IMPORT_reboot_aio_contexts.WriterContext):
                    return (await OrderedMapServicerTasks(
                        context=__context__,
                        state_ref=__context__._state_ref,
                    ).Remove(
                        __request__,
                        schedule=__this__._when,
                    )).task_id

                __schedule__: IMPORT_typing.Optional[IMPORT_reboot_time_DateTimeWithTimeZone] = (IMPORT_reboot_time_DateTimeWithTimeZone.now() + __this__._when) if isinstance(
                    __this__._when, IMPORT_datetime_timedelta
                ) else __this__._when

                __metadata__: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None
                __idempotency__: IMPORT_typing.Optional[IMPORT_reboot_aio_idempotency.Idempotency] = __this__._idempotency
                __bearer_token__: IMPORT_typing.Optional[str] = None

                if __options__ is not None:
                    IMPORT_reboot_aio_types.assert_type(__options__, [IMPORT_reboot_aio_call.Options])
                    if __options__.idempotency is not None:
                        if __idempotency__ is not None:
                            raise RuntimeError(
                                'Found redundant idempotency in `Options`'
                            )
                        __idempotency__ = __options__.idempotency
                    if __options__.metadata is not None:
                        __metadata__ = __options__.metadata
                    if __options__.bearer_token is not None:
                        __bearer_token__ = __options__.bearer_token

                # Add scheduling information to the metadata.
                __metadata__ = (
                    (IMPORT_reboot_aio_headers.TASK_SCHEDULE,
                    __schedule__.isoformat() if __schedule__ else ''),
                ) + (__metadata__ or tuple())

                return await __this__._tasks(
                    __context__
                ).Remove(
                    __request__,
                    idempotency=__idempotency__,
                    metadata=__metadata__,
                    bearer_token=__bearer_token__,
                )

            # Keep the original functions on the client, so old code will
            # continue to work, but use the new 'snake_case' method in
            # the new code.
            remove = Remove
            async def Range(
                __this__,
                __context__: IMPORT_reboot_aio_contexts.WriterContext | IMPORT_reboot_aio_contexts.TransactionContext,
                __options__: IMPORT_typing.Optional[IMPORT_reboot_aio_call.Options] = None,
                *,
                start_key: IMPORT_typing.Optional[str] = None,
                limit: IMPORT_typing.Optional[int] = None,
            ) -> IMPORT_rbt_v1alpha1.tasks_pb2.TaskId:
                # Only `writer`s and `transaction`s should ``schedule()`, a
                # `workflow` should `spawn()`.
                IMPORT_reboot_aio_types.assert_type(__context__, [IMPORT_reboot_aio_contexts.WriterContext, IMPORT_reboot_aio_contexts.TransactionContext])

                # UX improvement: check that neither positional argument was accidentally
                # given a gRPC request type.
                IMPORT_reboot_aio_types.assert_not_request_type(__context__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRangeRequest)
                IMPORT_reboot_aio_types.assert_not_request_type(__options__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRangeRequest)

                if start_key is not None and not isinstance(
                    start_key,
                    str,
                ):
                    raise TypeError(
                        f"Can not construct protobuf message of type "
                        f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRangeRequest': field 'start_key' is not "
                        f"of required type 'str'"
                    )
                if limit is not None and not isinstance(
                    limit,
                    int,
                ):
                    raise TypeError(
                        f"Can not construct protobuf message of type "
                        f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRangeRequest': field 'limit' is not "
                        f"of required type 'int'"
                    )
                # TODO: mypy-protobuf declares that
                # `google.protobuf.message.Message` constructor arguments are
                # always non-None, when in reality they are optional.
                __request__ = rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRangeRequest(
                    start_key=start_key,  # type: ignore[arg-type]
                    limit=limit,  # type: ignore[arg-type]
                )
                if isinstance(__context__, IMPORT_reboot_aio_contexts.WriterContext):
                    return (await OrderedMapServicerTasks(
                        context=__context__,
                        state_ref=__context__._state_ref,
                    ).Range(
                        __request__,
                        schedule=__this__._when,
                    )).task_id

                __schedule__: IMPORT_typing.Optional[IMPORT_reboot_time_DateTimeWithTimeZone] = (IMPORT_reboot_time_DateTimeWithTimeZone.now() + __this__._when) if isinstance(
                    __this__._when, IMPORT_datetime_timedelta
                ) else __this__._when

                __metadata__: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None
                __idempotency__: IMPORT_typing.Optional[IMPORT_reboot_aio_idempotency.Idempotency] = __this__._idempotency
                __bearer_token__: IMPORT_typing.Optional[str] = None

                if __options__ is not None:
                    IMPORT_reboot_aio_types.assert_type(__options__, [IMPORT_reboot_aio_call.Options])
                    if __options__.idempotency is not None:
                        if __idempotency__ is not None:
                            raise RuntimeError(
                                'Found redundant idempotency in `Options`'
                            )
                        __idempotency__ = __options__.idempotency
                    if __options__.metadata is not None:
                        __metadata__ = __options__.metadata
                    if __options__.bearer_token is not None:
                        __bearer_token__ = __options__.bearer_token

                # Add scheduling information to the metadata.
                __metadata__ = (
                    (IMPORT_reboot_aio_headers.TASK_SCHEDULE,
                    __schedule__.isoformat() if __schedule__ else ''),
                ) + (__metadata__ or tuple())

                return await __this__._tasks(
                    __context__
                ).Range(
                    __request__,
                    idempotency=__idempotency__,
                    metadata=__metadata__,
                    bearer_token=__bearer_token__,
                )

            # Keep the original functions on the client, so old code will
            # continue to work, but use the new 'snake_case' method in
            # the new code.
            range = Range
            async def ReverseRange(
                __this__,
                __context__: IMPORT_reboot_aio_contexts.WriterContext | IMPORT_reboot_aio_contexts.TransactionContext,
                __options__: IMPORT_typing.Optional[IMPORT_reboot_aio_call.Options] = None,
                *,
                start_key: IMPORT_typing.Optional[str] = None,
                limit: IMPORT_typing.Optional[int] = None,
            ) -> IMPORT_rbt_v1alpha1.tasks_pb2.TaskId:
                # Only `writer`s and `transaction`s should ``schedule()`, a
                # `workflow` should `spawn()`.
                IMPORT_reboot_aio_types.assert_type(__context__, [IMPORT_reboot_aio_contexts.WriterContext, IMPORT_reboot_aio_contexts.TransactionContext])

                # UX improvement: check that neither positional argument was accidentally
                # given a gRPC request type.
                IMPORT_reboot_aio_types.assert_not_request_type(__context__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapReverseRangeRequest)
                IMPORT_reboot_aio_types.assert_not_request_type(__options__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapReverseRangeRequest)

                if start_key is not None and not isinstance(
                    start_key,
                    str,
                ):
                    raise TypeError(
                        f"Can not construct protobuf message of type "
                        f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapReverseRangeRequest': field 'start_key' is not "
                        f"of required type 'str'"
                    )
                if limit is not None and not isinstance(
                    limit,
                    int,
                ):
                    raise TypeError(
                        f"Can not construct protobuf message of type "
                        f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapReverseRangeRequest': field 'limit' is not "
                        f"of required type 'int'"
                    )
                # TODO: mypy-protobuf declares that
                # `google.protobuf.message.Message` constructor arguments are
                # always non-None, when in reality they are optional.
                __request__ = rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapReverseRangeRequest(
                    start_key=start_key,  # type: ignore[arg-type]
                    limit=limit,  # type: ignore[arg-type]
                )
                if isinstance(__context__, IMPORT_reboot_aio_contexts.WriterContext):
                    return (await OrderedMapServicerTasks(
                        context=__context__,
                        state_ref=__context__._state_ref,
                    ).ReverseRange(
                        __request__,
                        schedule=__this__._when,
                    )).task_id

                __schedule__: IMPORT_typing.Optional[IMPORT_reboot_time_DateTimeWithTimeZone] = (IMPORT_reboot_time_DateTimeWithTimeZone.now() + __this__._when) if isinstance(
                    __this__._when, IMPORT_datetime_timedelta
                ) else __this__._when

                __metadata__: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None
                __idempotency__: IMPORT_typing.Optional[IMPORT_reboot_aio_idempotency.Idempotency] = __this__._idempotency
                __bearer_token__: IMPORT_typing.Optional[str] = None

                if __options__ is not None:
                    IMPORT_reboot_aio_types.assert_type(__options__, [IMPORT_reboot_aio_call.Options])
                    if __options__.idempotency is not None:
                        if __idempotency__ is not None:
                            raise RuntimeError(
                                'Found redundant idempotency in `Options`'
                            )
                        __idempotency__ = __options__.idempotency
                    if __options__.metadata is not None:
                        __metadata__ = __options__.metadata
                    if __options__.bearer_token is not None:
                        __bearer_token__ = __options__.bearer_token

                # Add scheduling information to the metadata.
                __metadata__ = (
                    (IMPORT_reboot_aio_headers.TASK_SCHEDULE,
                    __schedule__.isoformat() if __schedule__ else ''),
                ) + (__metadata__ or tuple())

                return await __this__._tasks(
                    __context__
                ).ReverseRange(
                    __request__,
                    idempotency=__idempotency__,
                    metadata=__metadata__,
                    bearer_token=__bearer_token__,
                )

            # Keep the original functions on the client, so old code will
            # continue to work, but use the new 'snake_case' method in
            # the new code.
            reverse_range = ReverseRange
            async def Stringify(
                __this__,
                __context__: IMPORT_reboot_aio_contexts.WriterContext | IMPORT_reboot_aio_contexts.TransactionContext,
                __options__: IMPORT_typing.Optional[IMPORT_reboot_aio_call.Options] = None,
            ) -> IMPORT_rbt_v1alpha1.tasks_pb2.TaskId:
                # Only `writer`s and `transaction`s should ``schedule()`, a
                # `workflow` should `spawn()`.
                IMPORT_reboot_aio_types.assert_type(__context__, [IMPORT_reboot_aio_contexts.WriterContext, IMPORT_reboot_aio_contexts.TransactionContext])

                # UX improvement: check that neither positional argument was accidentally
                # given a gRPC request type.
                IMPORT_reboot_aio_types.assert_not_request_type(__context__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapStringifyRequest)
                IMPORT_reboot_aio_types.assert_not_request_type(__options__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapStringifyRequest)

                # TODO: mypy-protobuf declares that
                # `google.protobuf.message.Message` constructor arguments are
                # always non-None, when in reality they are optional.
                __request__ = rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapStringifyRequest(
                )
                if isinstance(__context__, IMPORT_reboot_aio_contexts.WriterContext):
                    return (await OrderedMapServicerTasks(
                        context=__context__,
                        state_ref=__context__._state_ref,
                    ).Stringify(
                        __request__,
                        schedule=__this__._when,
                    )).task_id

                __schedule__: IMPORT_typing.Optional[IMPORT_reboot_time_DateTimeWithTimeZone] = (IMPORT_reboot_time_DateTimeWithTimeZone.now() + __this__._when) if isinstance(
                    __this__._when, IMPORT_datetime_timedelta
                ) else __this__._when

                __metadata__: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None
                __idempotency__: IMPORT_typing.Optional[IMPORT_reboot_aio_idempotency.Idempotency] = __this__._idempotency
                __bearer_token__: IMPORT_typing.Optional[str] = None

                if __options__ is not None:
                    IMPORT_reboot_aio_types.assert_type(__options__, [IMPORT_reboot_aio_call.Options])
                    if __options__.idempotency is not None:
                        if __idempotency__ is not None:
                            raise RuntimeError(
                                'Found redundant idempotency in `Options`'
                            )
                        __idempotency__ = __options__.idempotency
                    if __options__.metadata is not None:
                        __metadata__ = __options__.metadata
                    if __options__.bearer_token is not None:
                        __bearer_token__ = __options__.bearer_token

                # Add scheduling information to the metadata.
                __metadata__ = (
                    (IMPORT_reboot_aio_headers.TASK_SCHEDULE,
                    __schedule__.isoformat() if __schedule__ else ''),
                ) + (__metadata__ or tuple())

                return await __this__._tasks(
                    __context__
                ).Stringify(
                    __request__,
                    idempotency=__idempotency__,
                    metadata=__metadata__,
                    bearer_token=__bearer_token__,
                )

            # Keep the original functions on the client, so old code will
            # continue to work, but use the new 'snake_case' method in
            # the new code.
            stringify = Stringify

        def spawn(
            self,
            *,
            when: IMPORT_typing.Optional[IMPORT_datetime_datetime | IMPORT_datetime_timedelta] = None,
        ) -> OrderedMap.WeakReference._Spawn:
            # Within a `workflow`, all "bare" `spawn()` calls are
            # syntactic sugar for `per_workflow()`, unless we're
            # within a control loop, in which case they are syntactic
            # sugar for `per_iteration()`.
            context = IMPORT_reboot_aio_contexts.Context.get()
            if context is not None:
                if isinstance(context, IMPORT_reboot_aio_contexts.WorkflowContext):
                    return (
                        self.per_iteration() if context.within_loop()
                        else self.per_workflow()
                    ).spawn(when=when)
                elif isinstance(context, IMPORT_reboot_aio_external.InitializeContext):
                    return self.idempotently().spawn(when=when)

            return OrderedMap.WeakReference._Spawn(
                self._application_id, self._tasks, when=when
            )

        class _Spawn:

            def __init__(
                self,
                application_id: IMPORT_typing.Optional[IMPORT_reboot_aio_types.ApplicationId],
                tasks: IMPORT_typing.Callable[[IMPORT_reboot_aio_contexts.TransactionContext | IMPORT_reboot_aio_contexts.WorkflowContext | IMPORT_reboot_aio_external.ExternalContext], OrderedMapTasksStub],
                *,
                when: IMPORT_typing.Optional[IMPORT_datetime_datetime | IMPORT_datetime_timedelta] = None,
                idempotency: IMPORT_typing.Optional[IMPORT_reboot_aio_idempotency.Idempotency] = None,
            ) -> None:
                self._application_id = application_id
                self._tasks = tasks
                self._when = ensure_has_timezone(when=when)
                self._idempotency = idempotency

            # OrderedMap callable tasks:
            async def Create(
                __this__,
                __context__: IMPORT_reboot_aio_contexts.WorkflowContext | IMPORT_reboot_aio_external.ExternalContext,
                __options__: IMPORT_typing.Optional[IMPORT_reboot_aio_call.Options] = None,
                *,
                order: IMPORT_typing.Optional[int] = None,
            ) -> OrderedMap.CreateTask:
                # UX improvement: check that neither positional argument was accidentally
                # given a gRPC request type.
                IMPORT_reboot_aio_types.assert_not_request_type(__context__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapCreateRequest)
                IMPORT_reboot_aio_types.assert_not_request_type(__options__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapCreateRequest)
                IMPORT_reboot_aio_types.assert_type(__context__, [IMPORT_reboot_aio_contexts.WorkflowContext, IMPORT_reboot_aio_external.ExternalContext])

                if order is not None and not isinstance(
                    order,
                    int,
                ):
                    raise TypeError(
                        f"Can not construct protobuf message of type "
                        f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapCreateRequest': field 'order' is not "
                        f"of required type 'int'"
                    )
                # TODO: mypy-protobuf declares that
                # `IMPORT_google_protobuf_message.Message` constructor arguments are
                # always non-None, when in reality they are optional.
                __request__ = rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapCreateRequest(
                    order=order,  # type: ignore[arg-type]
                )

                __schedule__: IMPORT_typing.Optional[IMPORT_reboot_time_DateTimeWithTimeZone] = (IMPORT_reboot_time_DateTimeWithTimeZone.now() + __this__._when) if isinstance(
                    __this__._when, IMPORT_datetime_timedelta
                ) else __this__._when

                __metadata__: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None
                __idempotency__: IMPORT_typing.Optional[IMPORT_reboot_aio_idempotency.Idempotency] = __this__._idempotency
                __bearer_token__: IMPORT_typing.Optional[str] = None

                if __options__ is not None:
                    IMPORT_reboot_aio_types.assert_type(__options__, [IMPORT_reboot_aio_call.Options])
                    if __options__.idempotency:
                        if __idempotency__ is not None:
                            raise RuntimeError(
                                'Found redundant idempotency in `Options`'
                            )
                        __idempotency__ = __options__.idempotency
                    if __options__.metadata is not None:
                        __metadata__ = __options__.metadata
                    if __options__.bearer_token is not None:
                        __bearer_token__ = __options__.bearer_token

                # Add scheduling information to the metadata.
                __metadata__ = (
                    (IMPORT_reboot_aio_headers.TASK_SCHEDULE,
                    __schedule__.isoformat() if __schedule__ else ''),
                ) + (__metadata__ or tuple())

                __task_id__ = await __this__._tasks(
                    __context__
                ).Create(
                    __request__,
                    idempotency=__idempotency__,
                    metadata=__metadata__,
                    bearer_token=__bearer_token__,
                )

                return OrderedMap.CreateTask(
                    __context__,
                    task_id=__task_id__,
                )

            # Keep the original functions on the client, so old code will
            # continue to work, but use the new 'snake_case' method in
            # the new code.
            create = Create
            async def Search(
                __this__,
                __context__: IMPORT_reboot_aio_contexts.WorkflowContext | IMPORT_reboot_aio_external.ExternalContext,
                __options__: IMPORT_typing.Optional[IMPORT_reboot_aio_call.Options] = None,
                *,
                key: IMPORT_typing.Optional[str] = None,
            ) -> OrderedMap.SearchTask:
                # UX improvement: check that neither positional argument was accidentally
                # given a gRPC request type.
                IMPORT_reboot_aio_types.assert_not_request_type(__context__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapSearchRequest)
                IMPORT_reboot_aio_types.assert_not_request_type(__options__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapSearchRequest)
                IMPORT_reboot_aio_types.assert_type(__context__, [IMPORT_reboot_aio_contexts.WorkflowContext, IMPORT_reboot_aio_external.ExternalContext])

                if key is not None and not isinstance(
                    key,
                    str,
                ):
                    raise TypeError(
                        f"Can not construct protobuf message of type "
                        f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapSearchRequest': field 'key' is not "
                        f"of required type 'str'"
                    )
                # TODO: mypy-protobuf declares that
                # `IMPORT_google_protobuf_message.Message` constructor arguments are
                # always non-None, when in reality they are optional.
                __request__ = rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapSearchRequest(
                    key=key,  # type: ignore[arg-type]
                )

                __schedule__: IMPORT_typing.Optional[IMPORT_reboot_time_DateTimeWithTimeZone] = (IMPORT_reboot_time_DateTimeWithTimeZone.now() + __this__._when) if isinstance(
                    __this__._when, IMPORT_datetime_timedelta
                ) else __this__._when

                __metadata__: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None
                __idempotency__: IMPORT_typing.Optional[IMPORT_reboot_aio_idempotency.Idempotency] = __this__._idempotency
                __bearer_token__: IMPORT_typing.Optional[str] = None

                if __options__ is not None:
                    IMPORT_reboot_aio_types.assert_type(__options__, [IMPORT_reboot_aio_call.Options])
                    if __options__.idempotency:
                        if __idempotency__ is not None:
                            raise RuntimeError(
                                'Found redundant idempotency in `Options`'
                            )
                        __idempotency__ = __options__.idempotency
                    if __options__.metadata is not None:
                        __metadata__ = __options__.metadata
                    if __options__.bearer_token is not None:
                        __bearer_token__ = __options__.bearer_token

                # Add scheduling information to the metadata.
                __metadata__ = (
                    (IMPORT_reboot_aio_headers.TASK_SCHEDULE,
                    __schedule__.isoformat() if __schedule__ else ''),
                ) + (__metadata__ or tuple())

                __task_id__ = await __this__._tasks(
                    __context__
                ).Search(
                    __request__,
                    idempotency=__idempotency__,
                    metadata=__metadata__,
                    bearer_token=__bearer_token__,
                )

                return OrderedMap.SearchTask(
                    __context__,
                    task_id=__task_id__,
                )

            # Keep the original functions on the client, so old code will
            # continue to work, but use the new 'snake_case' method in
            # the new code.
            search = Search
            async def Insert(
                __this__,
                __context__: IMPORT_reboot_aio_contexts.WorkflowContext | IMPORT_reboot_aio_external.ExternalContext,
                __options__: IMPORT_typing.Optional[IMPORT_reboot_aio_call.Options] = None,
                *,
                key: IMPORT_typing.Optional[str] = None,
                value: IMPORT_typing.Optional[google.protobuf.struct_pb2.Value] = None,
                bytes: IMPORT_typing.Optional[bytes] = None,
                any: IMPORT_typing.Optional[google.protobuf.any_pb2.Any] = None,
            ) -> OrderedMap.InsertTask:
                # UX improvement: check that neither positional argument was accidentally
                # given a gRPC request type.
                IMPORT_reboot_aio_types.assert_not_request_type(__context__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapInsertRequest)
                IMPORT_reboot_aio_types.assert_not_request_type(__options__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapInsertRequest)
                IMPORT_reboot_aio_types.assert_type(__context__, [IMPORT_reboot_aio_contexts.WorkflowContext, IMPORT_reboot_aio_external.ExternalContext])

                if key is not None and not isinstance(
                    key,
                    str,
                ):
                    raise TypeError(
                        f"Can not construct protobuf message of type "
                        f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapInsertRequest': field 'key' is not "
                        f"of required type 'str'"
                    )
                if value is not None and not isinstance(
                    value,
                    google.protobuf.struct_pb2.Value,
                ):
                    raise TypeError(
                        f"Can not construct protobuf message of type "
                        f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapInsertRequest': field 'value' is not "
                        f"of required type 'google.protobuf.struct_pb2.Value'"
                    )
                if bytes is not None and not isinstance(
                    bytes,
                    IMPORT_builtins.bytes
                ):
                    raise TypeError(
                        f"Can not construct protobuf message of type "
                        f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapInsertRequest': field 'bytes' is not "
                        f"of required type 'bytes'"
                    )
                if any is not None and not isinstance(
                    any,
                    google.protobuf.any_pb2.Any,
                ):
                    raise TypeError(
                        f"Can not construct protobuf message of type "
                        f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapInsertRequest': field 'any' is not "
                        f"of required type 'google.protobuf.any_pb2.Any'"
                    )
                # TODO: mypy-protobuf declares that
                # `IMPORT_google_protobuf_message.Message` constructor arguments are
                # always non-None, when in reality they are optional.
                __request__ = rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapInsertRequest(
                    key=key,  # type: ignore[arg-type]
                    value=value,  # type: ignore[arg-type]
                    bytes=bytes,  # type: ignore[arg-type]
                    any=any,  # type: ignore[arg-type]
                )

                __schedule__: IMPORT_typing.Optional[IMPORT_reboot_time_DateTimeWithTimeZone] = (IMPORT_reboot_time_DateTimeWithTimeZone.now() + __this__._when) if isinstance(
                    __this__._when, IMPORT_datetime_timedelta
                ) else __this__._when

                __metadata__: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None
                __idempotency__: IMPORT_typing.Optional[IMPORT_reboot_aio_idempotency.Idempotency] = __this__._idempotency
                __bearer_token__: IMPORT_typing.Optional[str] = None

                if __options__ is not None:
                    IMPORT_reboot_aio_types.assert_type(__options__, [IMPORT_reboot_aio_call.Options])
                    if __options__.idempotency:
                        if __idempotency__ is not None:
                            raise RuntimeError(
                                'Found redundant idempotency in `Options`'
                            )
                        __idempotency__ = __options__.idempotency
                    if __options__.metadata is not None:
                        __metadata__ = __options__.metadata
                    if __options__.bearer_token is not None:
                        __bearer_token__ = __options__.bearer_token

                # Add scheduling information to the metadata.
                __metadata__ = (
                    (IMPORT_reboot_aio_headers.TASK_SCHEDULE,
                    __schedule__.isoformat() if __schedule__ else ''),
                ) + (__metadata__ or tuple())

                __task_id__ = await __this__._tasks(
                    __context__
                ).Insert(
                    __request__,
                    idempotency=__idempotency__,
                    metadata=__metadata__,
                    bearer_token=__bearer_token__,
                )

                return OrderedMap.InsertTask(
                    __context__,
                    task_id=__task_id__,
                )

            # Keep the original functions on the client, so old code will
            # continue to work, but use the new 'snake_case' method in
            # the new code.
            insert = Insert
            async def Remove(
                __this__,
                __context__: IMPORT_reboot_aio_contexts.WorkflowContext | IMPORT_reboot_aio_external.ExternalContext,
                __options__: IMPORT_typing.Optional[IMPORT_reboot_aio_call.Options] = None,
                *,
                key: IMPORT_typing.Optional[str] = None,
            ) -> OrderedMap.RemoveTask:
                # UX improvement: check that neither positional argument was accidentally
                # given a gRPC request type.
                IMPORT_reboot_aio_types.assert_not_request_type(__context__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRemoveRequest)
                IMPORT_reboot_aio_types.assert_not_request_type(__options__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRemoveRequest)
                IMPORT_reboot_aio_types.assert_type(__context__, [IMPORT_reboot_aio_contexts.WorkflowContext, IMPORT_reboot_aio_external.ExternalContext])

                if key is not None and not isinstance(
                    key,
                    str,
                ):
                    raise TypeError(
                        f"Can not construct protobuf message of type "
                        f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRemoveRequest': field 'key' is not "
                        f"of required type 'str'"
                    )
                # TODO: mypy-protobuf declares that
                # `IMPORT_google_protobuf_message.Message` constructor arguments are
                # always non-None, when in reality they are optional.
                __request__ = rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRemoveRequest(
                    key=key,  # type: ignore[arg-type]
                )

                __schedule__: IMPORT_typing.Optional[IMPORT_reboot_time_DateTimeWithTimeZone] = (IMPORT_reboot_time_DateTimeWithTimeZone.now() + __this__._when) if isinstance(
                    __this__._when, IMPORT_datetime_timedelta
                ) else __this__._when

                __metadata__: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None
                __idempotency__: IMPORT_typing.Optional[IMPORT_reboot_aio_idempotency.Idempotency] = __this__._idempotency
                __bearer_token__: IMPORT_typing.Optional[str] = None

                if __options__ is not None:
                    IMPORT_reboot_aio_types.assert_type(__options__, [IMPORT_reboot_aio_call.Options])
                    if __options__.idempotency:
                        if __idempotency__ is not None:
                            raise RuntimeError(
                                'Found redundant idempotency in `Options`'
                            )
                        __idempotency__ = __options__.idempotency
                    if __options__.metadata is not None:
                        __metadata__ = __options__.metadata
                    if __options__.bearer_token is not None:
                        __bearer_token__ = __options__.bearer_token

                # Add scheduling information to the metadata.
                __metadata__ = (
                    (IMPORT_reboot_aio_headers.TASK_SCHEDULE,
                    __schedule__.isoformat() if __schedule__ else ''),
                ) + (__metadata__ or tuple())

                __task_id__ = await __this__._tasks(
                    __context__
                ).Remove(
                    __request__,
                    idempotency=__idempotency__,
                    metadata=__metadata__,
                    bearer_token=__bearer_token__,
                )

                return OrderedMap.RemoveTask(
                    __context__,
                    task_id=__task_id__,
                )

            # Keep the original functions on the client, so old code will
            # continue to work, but use the new 'snake_case' method in
            # the new code.
            remove = Remove
            async def Range(
                __this__,
                __context__: IMPORT_reboot_aio_contexts.WorkflowContext | IMPORT_reboot_aio_external.ExternalContext,
                __options__: IMPORT_typing.Optional[IMPORT_reboot_aio_call.Options] = None,
                *,
                start_key: IMPORT_typing.Optional[str] = None,
                limit: IMPORT_typing.Optional[int] = None,
            ) -> OrderedMap.RangeTask:
                # UX improvement: check that neither positional argument was accidentally
                # given a gRPC request type.
                IMPORT_reboot_aio_types.assert_not_request_type(__context__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRangeRequest)
                IMPORT_reboot_aio_types.assert_not_request_type(__options__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRangeRequest)
                IMPORT_reboot_aio_types.assert_type(__context__, [IMPORT_reboot_aio_contexts.WorkflowContext, IMPORT_reboot_aio_external.ExternalContext])

                if start_key is not None and not isinstance(
                    start_key,
                    str,
                ):
                    raise TypeError(
                        f"Can not construct protobuf message of type "
                        f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRangeRequest': field 'start_key' is not "
                        f"of required type 'str'"
                    )
                if limit is not None and not isinstance(
                    limit,
                    int,
                ):
                    raise TypeError(
                        f"Can not construct protobuf message of type "
                        f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRangeRequest': field 'limit' is not "
                        f"of required type 'int'"
                    )
                # TODO: mypy-protobuf declares that
                # `IMPORT_google_protobuf_message.Message` constructor arguments are
                # always non-None, when in reality they are optional.
                __request__ = rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRangeRequest(
                    start_key=start_key,  # type: ignore[arg-type]
                    limit=limit,  # type: ignore[arg-type]
                )

                __schedule__: IMPORT_typing.Optional[IMPORT_reboot_time_DateTimeWithTimeZone] = (IMPORT_reboot_time_DateTimeWithTimeZone.now() + __this__._when) if isinstance(
                    __this__._when, IMPORT_datetime_timedelta
                ) else __this__._when

                __metadata__: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None
                __idempotency__: IMPORT_typing.Optional[IMPORT_reboot_aio_idempotency.Idempotency] = __this__._idempotency
                __bearer_token__: IMPORT_typing.Optional[str] = None

                if __options__ is not None:
                    IMPORT_reboot_aio_types.assert_type(__options__, [IMPORT_reboot_aio_call.Options])
                    if __options__.idempotency:
                        if __idempotency__ is not None:
                            raise RuntimeError(
                                'Found redundant idempotency in `Options`'
                            )
                        __idempotency__ = __options__.idempotency
                    if __options__.metadata is not None:
                        __metadata__ = __options__.metadata
                    if __options__.bearer_token is not None:
                        __bearer_token__ = __options__.bearer_token

                # Add scheduling information to the metadata.
                __metadata__ = (
                    (IMPORT_reboot_aio_headers.TASK_SCHEDULE,
                    __schedule__.isoformat() if __schedule__ else ''),
                ) + (__metadata__ or tuple())

                __task_id__ = await __this__._tasks(
                    __context__
                ).Range(
                    __request__,
                    idempotency=__idempotency__,
                    metadata=__metadata__,
                    bearer_token=__bearer_token__,
                )

                return OrderedMap.RangeTask(
                    __context__,
                    task_id=__task_id__,
                )

            # Keep the original functions on the client, so old code will
            # continue to work, but use the new 'snake_case' method in
            # the new code.
            range = Range
            async def ReverseRange(
                __this__,
                __context__: IMPORT_reboot_aio_contexts.WorkflowContext | IMPORT_reboot_aio_external.ExternalContext,
                __options__: IMPORT_typing.Optional[IMPORT_reboot_aio_call.Options] = None,
                *,
                start_key: IMPORT_typing.Optional[str] = None,
                limit: IMPORT_typing.Optional[int] = None,
            ) -> OrderedMap.ReverseRangeTask:
                # UX improvement: check that neither positional argument was accidentally
                # given a gRPC request type.
                IMPORT_reboot_aio_types.assert_not_request_type(__context__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapReverseRangeRequest)
                IMPORT_reboot_aio_types.assert_not_request_type(__options__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapReverseRangeRequest)
                IMPORT_reboot_aio_types.assert_type(__context__, [IMPORT_reboot_aio_contexts.WorkflowContext, IMPORT_reboot_aio_external.ExternalContext])

                if start_key is not None and not isinstance(
                    start_key,
                    str,
                ):
                    raise TypeError(
                        f"Can not construct protobuf message of type "
                        f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapReverseRangeRequest': field 'start_key' is not "
                        f"of required type 'str'"
                    )
                if limit is not None and not isinstance(
                    limit,
                    int,
                ):
                    raise TypeError(
                        f"Can not construct protobuf message of type "
                        f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapReverseRangeRequest': field 'limit' is not "
                        f"of required type 'int'"
                    )
                # TODO: mypy-protobuf declares that
                # `IMPORT_google_protobuf_message.Message` constructor arguments are
                # always non-None, when in reality they are optional.
                __request__ = rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapReverseRangeRequest(
                    start_key=start_key,  # type: ignore[arg-type]
                    limit=limit,  # type: ignore[arg-type]
                )

                __schedule__: IMPORT_typing.Optional[IMPORT_reboot_time_DateTimeWithTimeZone] = (IMPORT_reboot_time_DateTimeWithTimeZone.now() + __this__._when) if isinstance(
                    __this__._when, IMPORT_datetime_timedelta
                ) else __this__._when

                __metadata__: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None
                __idempotency__: IMPORT_typing.Optional[IMPORT_reboot_aio_idempotency.Idempotency] = __this__._idempotency
                __bearer_token__: IMPORT_typing.Optional[str] = None

                if __options__ is not None:
                    IMPORT_reboot_aio_types.assert_type(__options__, [IMPORT_reboot_aio_call.Options])
                    if __options__.idempotency:
                        if __idempotency__ is not None:
                            raise RuntimeError(
                                'Found redundant idempotency in `Options`'
                            )
                        __idempotency__ = __options__.idempotency
                    if __options__.metadata is not None:
                        __metadata__ = __options__.metadata
                    if __options__.bearer_token is not None:
                        __bearer_token__ = __options__.bearer_token

                # Add scheduling information to the metadata.
                __metadata__ = (
                    (IMPORT_reboot_aio_headers.TASK_SCHEDULE,
                    __schedule__.isoformat() if __schedule__ else ''),
                ) + (__metadata__ or tuple())

                __task_id__ = await __this__._tasks(
                    __context__
                ).ReverseRange(
                    __request__,
                    idempotency=__idempotency__,
                    metadata=__metadata__,
                    bearer_token=__bearer_token__,
                )

                return OrderedMap.ReverseRangeTask(
                    __context__,
                    task_id=__task_id__,
                )

            # Keep the original functions on the client, so old code will
            # continue to work, but use the new 'snake_case' method in
            # the new code.
            reverse_range = ReverseRange
            async def Stringify(
                __this__,
                __context__: IMPORT_reboot_aio_contexts.WorkflowContext | IMPORT_reboot_aio_external.ExternalContext,
                __options__: IMPORT_typing.Optional[IMPORT_reboot_aio_call.Options] = None,
            ) -> OrderedMap.StringifyTask:
                # UX improvement: check that neither positional argument was accidentally
                # given a gRPC request type.
                IMPORT_reboot_aio_types.assert_not_request_type(__context__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapStringifyRequest)
                IMPORT_reboot_aio_types.assert_not_request_type(__options__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapStringifyRequest)
                IMPORT_reboot_aio_types.assert_type(__context__, [IMPORT_reboot_aio_contexts.WorkflowContext, IMPORT_reboot_aio_external.ExternalContext])

                # TODO: mypy-protobuf declares that
                # `IMPORT_google_protobuf_message.Message` constructor arguments are
                # always non-None, when in reality they are optional.
                __request__ = rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapStringifyRequest(
                )

                __schedule__: IMPORT_typing.Optional[IMPORT_reboot_time_DateTimeWithTimeZone] = (IMPORT_reboot_time_DateTimeWithTimeZone.now() + __this__._when) if isinstance(
                    __this__._when, IMPORT_datetime_timedelta
                ) else __this__._when

                __metadata__: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None
                __idempotency__: IMPORT_typing.Optional[IMPORT_reboot_aio_idempotency.Idempotency] = __this__._idempotency
                __bearer_token__: IMPORT_typing.Optional[str] = None

                if __options__ is not None:
                    IMPORT_reboot_aio_types.assert_type(__options__, [IMPORT_reboot_aio_call.Options])
                    if __options__.idempotency:
                        if __idempotency__ is not None:
                            raise RuntimeError(
                                'Found redundant idempotency in `Options`'
                            )
                        __idempotency__ = __options__.idempotency
                    if __options__.metadata is not None:
                        __metadata__ = __options__.metadata
                    if __options__.bearer_token is not None:
                        __bearer_token__ = __options__.bearer_token

                # Add scheduling information to the metadata.
                __metadata__ = (
                    (IMPORT_reboot_aio_headers.TASK_SCHEDULE,
                    __schedule__.isoformat() if __schedule__ else ''),
                ) + (__metadata__ or tuple())

                __task_id__ = await __this__._tasks(
                    __context__
                ).Stringify(
                    __request__,
                    idempotency=__idempotency__,
                    metadata=__metadata__,
                    bearer_token=__bearer_token__,
                )

                return OrderedMap.StringifyTask(
                    __context__,
                    task_id=__task_id__,
                )

            # Keep the original functions on the client, so old code will
            # continue to work, but use the new 'snake_case' method in
            # the new code.
            stringify = Stringify

        async def read(
            self,
            context: IMPORT_reboot_aio_contexts.WorkflowContext,
        ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap:
            return await (
                self.always() if context.within_until()
                else (
                    self.per_iteration() if context.within_loop()
                    else self.per_workflow()
                )
            ).read(context)

        @IMPORT_typing.overload
        async def write(
            self,
            context: IMPORT_reboot_aio_contexts.WorkflowContext,
            writer: OrderedMapBaseServicer.InlineWriterCallable[None],
            *,
            type: type = type(None),
        ) -> None:
            ...

        @IMPORT_typing.overload
        async def write(
            self,
            context: IMPORT_reboot_aio_contexts.WorkflowContext,
            writer: OrderedMapBaseServicer.InlineWriterCallable[OrderedMapBaseServicer.InlineWriterCallableResult],
            *,
            type: type[OrderedMapBaseServicer.InlineWriterCallableResult],
        ) -> OrderedMapBaseServicer.InlineWriterCallableResult:
            ...

        async def write(
            self,
            context: IMPORT_reboot_aio_contexts.WorkflowContext,
            writer: OrderedMapBaseServicer.InlineWriterCallable[OrderedMapBaseServicer.InlineWriterCallableResult],
            *,
            type: type = type(None),
        ) -> OrderedMapBaseServicer.InlineWriterCallableResult:
            """Perform an "inline write" within a workflow."""
            return await (
                self.always() if context.within_until()
                else (
                    self.per_iteration() if context.within_loop()
                    else self.per_workflow()
                )
            ).write(
                context, writer, type=type
            )

        # OrderedMap specific methods:
        async def Create(
            __this__,
            __context__: IMPORT_reboot_aio_contexts.TransactionContext | IMPORT_reboot_aio_contexts.WorkflowContext | IMPORT_reboot_aio_external.ExternalContext,
            __options__: IMPORT_typing.Optional[IMPORT_reboot_aio_call.Options] = None,
            *,
            order: IMPORT_typing.Optional[int] = None,
        ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapCreateResponse:
            # UX improvement: check that neither positional argument was accidentally
            # given a gRPC request type.
            IMPORT_reboot_aio_types.assert_not_request_type(__context__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapCreateRequest)
            IMPORT_reboot_aio_types.assert_not_request_type(__options__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapCreateRequest)

            # Within a `workflow`, all "bare" calls are
            # `per_workflow()` calls, unless we're within a control
            # loop, in which case they are syntactic sugar for
            # `per_iteration()`.
            if __options__ is None or __options__.idempotency is None:
                if isinstance(__context__, IMPORT_reboot_aio_contexts.WorkflowContext):
                    return await (
                        __this__.per_iteration() if __context__.within_loop()
                        else __this__.per_workflow()
                    ).Create(
                        __context__,
                        __options__ or IMPORT_reboot_aio_call.Options(),
                        order=order,
                    )
                elif isinstance(__context__, IMPORT_reboot_aio_external.InitializeContext):
                    return await __this__.idempotently().Create(
                        __context__,
                        __options__ or IMPORT_reboot_aio_call.Options(),
                        order=order,
                    )

            if order is not None and not isinstance(
                order,
                int,
            ):
                raise TypeError(
                    f"Can not construct protobuf message of type "
                    f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapCreateRequest': field 'order' is not "
                    f"of required type 'int'"
                )
            # TODO: mypy-protobuf declares that
            # `google.protobuf.message.Message` constructor arguments are
            # always non-None, when in reality they are optional.
            __request__ = rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapCreateRequest(
                order=order,  # type: ignore[arg-type]
            )
            __idempotency__: IMPORT_typing.Optional[IMPORT_reboot_aio_idempotency.Idempotency] = None
            __metadata__: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None
            __bearer_token__: IMPORT_typing.Optional[str] = None
            if __options__ is not None:
                IMPORT_reboot_aio_types.assert_type(__options__, [IMPORT_reboot_aio_call.Options])
                if __options__.idempotency is not None:
                    __idempotency__ = __options__.idempotency
                if __options__.metadata is not None:
                    __metadata__ = __options__.metadata
                if __options__.bearer_token is not None:
                    __bearer_token__ = __options__.bearer_token

            return await __this__._workflow(__context__).Create(
                __request__,
                idempotency=__idempotency__,
                metadata=__metadata__,
                bearer_token=__bearer_token__,
            )

        # Keep the original functions on the client, so old code will
        # continue to work, but use the new 'snake_case' method in
        # the new code.
        create = Create
        async def Search(
            __this__,
            __context__: IMPORT_reboot_aio_contexts.ReaderContext | IMPORT_reboot_aio_contexts.WriterContext | IMPORT_reboot_aio_contexts.TransactionContext | IMPORT_reboot_aio_contexts.WorkflowContext | IMPORT_reboot_aio_external.ExternalContext,
            __options__: IMPORT_typing.Optional[IMPORT_reboot_aio_call.Options] = None,
            *,
            key: IMPORT_typing.Optional[str] = None,
        ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapSearchResponse:
            # UX improvement: check that neither positional argument was accidentally
            # given a gRPC request type.
            IMPORT_reboot_aio_types.assert_not_request_type(__context__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapSearchRequest)
            IMPORT_reboot_aio_types.assert_not_request_type(__options__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapSearchRequest)

            # Within a `workflow`, all "bare" calls are
            # `per_workflow()` calls, unless we're within a control
            # loop, in which case they are syntactic sugar for
            # `per_iteration()`.
            #
            # Unless we are "within until" in which case all "bare"
            # calls are `.always().
            if __options__ is None or __options__.idempotency is None:
                if isinstance(__context__, IMPORT_reboot_aio_contexts.WorkflowContext):
                    return await (
                        __this__.always() if __context__.within_until()
                        else (
                            __this__.per_iteration() if __context__.within_loop()
                            else __this__.per_workflow()
                        )
                    ).Search(
                        __context__,
                        __options__ or IMPORT_reboot_aio_call.Options(),
                        key=key,
                    )
                elif isinstance(__context__, IMPORT_reboot_aio_external.InitializeContext):
                    return await __this__.idempotently().Search(
                        __context__,
                        __options__ or IMPORT_reboot_aio_call.Options(),
                        key=key,
                    )

            if key is not None and not isinstance(
                key,
                str,
            ):
                raise TypeError(
                    f"Can not construct protobuf message of type "
                    f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapSearchRequest': field 'key' is not "
                    f"of required type 'str'"
                )
            # TODO: mypy-protobuf declares that
            # `google.protobuf.message.Message` constructor arguments are
            # always non-None, when in reality they are optional.
            __request__ = rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapSearchRequest(
                key=key,  # type: ignore[arg-type]
            )
            __metadata__: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None
            __bearer_token__: IMPORT_typing.Optional[str] = None
            if __options__ is not None:
                IMPORT_reboot_aio_types.assert_type(__options__, [IMPORT_reboot_aio_call.Options])
                if __options__.metadata is not None:
                    __metadata__ = __options__.metadata
                if __options__.bearer_token is not None:
                    __bearer_token__ = __options__.bearer_token
            return await __this__._reader(__context__).Search(
                __request__,
                metadata=__metadata__,
                bearer_token=__bearer_token__,
                idempotency=__options__.idempotency if __options__ is not None else None,
            )
        # Keep the original functions on the client, so old code will
        # continue to work, but use the new 'snake_case' method in
        # the new code.
        search = Search
        async def Insert(
            __this__,
            __context__: IMPORT_reboot_aio_contexts.TransactionContext | IMPORT_reboot_aio_contexts.WorkflowContext | IMPORT_reboot_aio_external.ExternalContext,
            __options__: IMPORT_typing.Optional[IMPORT_reboot_aio_call.Options] = None,
            *,
            key: IMPORT_typing.Optional[str] = None,
            value: IMPORT_typing.Optional[google.protobuf.struct_pb2.Value] = None,
            bytes: IMPORT_typing.Optional[bytes] = None,
            any: IMPORT_typing.Optional[google.protobuf.any_pb2.Any] = None,
        ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapInsertResponse:
            # UX improvement: check that neither positional argument was accidentally
            # given a gRPC request type.
            IMPORT_reboot_aio_types.assert_not_request_type(__context__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapInsertRequest)
            IMPORT_reboot_aio_types.assert_not_request_type(__options__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapInsertRequest)

            # Within a `workflow`, all "bare" calls are
            # `per_workflow()` calls, unless we're within a control
            # loop, in which case they are syntactic sugar for
            # `per_iteration()`.
            if __options__ is None or __options__.idempotency is None:
                if isinstance(__context__, IMPORT_reboot_aio_contexts.WorkflowContext):
                    return await (
                        __this__.per_iteration() if __context__.within_loop()
                        else __this__.per_workflow()
                    ).Insert(
                        __context__,
                        __options__ or IMPORT_reboot_aio_call.Options(),
                        key=key,
                        value=value,
                        bytes=bytes,
                        any=any,
                    )
                elif isinstance(__context__, IMPORT_reboot_aio_external.InitializeContext):
                    return await __this__.idempotently().Insert(
                        __context__,
                        __options__ or IMPORT_reboot_aio_call.Options(),
                        key=key,
                        value=value,
                        bytes=bytes,
                        any=any,
                    )

            if key is not None and not isinstance(
                key,
                str,
            ):
                raise TypeError(
                    f"Can not construct protobuf message of type "
                    f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapInsertRequest': field 'key' is not "
                    f"of required type 'str'"
                )
            if value is not None and not isinstance(
                value,
                google.protobuf.struct_pb2.Value,
            ):
                raise TypeError(
                    f"Can not construct protobuf message of type "
                    f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapInsertRequest': field 'value' is not "
                    f"of required type 'google.protobuf.struct_pb2.Value'"
                )
            if bytes is not None and not isinstance(
                bytes,
                IMPORT_builtins.bytes
            ):
                raise TypeError(
                    f"Can not construct protobuf message of type "
                    f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapInsertRequest': field 'bytes' is not "
                    f"of required type 'bytes'"
                )
            if any is not None and not isinstance(
                any,
                google.protobuf.any_pb2.Any,
            ):
                raise TypeError(
                    f"Can not construct protobuf message of type "
                    f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapInsertRequest': field 'any' is not "
                    f"of required type 'google.protobuf.any_pb2.Any'"
                )
            # TODO: mypy-protobuf declares that
            # `google.protobuf.message.Message` constructor arguments are
            # always non-None, when in reality they are optional.
            __request__ = rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapInsertRequest(
                key=key,  # type: ignore[arg-type]
                value=value,  # type: ignore[arg-type]
                bytes=bytes,  # type: ignore[arg-type]
                any=any,  # type: ignore[arg-type]
            )
            __idempotency__: IMPORT_typing.Optional[IMPORT_reboot_aio_idempotency.Idempotency] = None
            __metadata__: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None
            __bearer_token__: IMPORT_typing.Optional[str] = None
            if __options__ is not None:
                IMPORT_reboot_aio_types.assert_type(__options__, [IMPORT_reboot_aio_call.Options])
                if __options__.idempotency is not None:
                    __idempotency__ = __options__.idempotency
                if __options__.metadata is not None:
                    __metadata__ = __options__.metadata
                if __options__.bearer_token is not None:
                    __bearer_token__ = __options__.bearer_token

            return await __this__._workflow(__context__).Insert(
                __request__,
                idempotency=__idempotency__,
                metadata=__metadata__,
                bearer_token=__bearer_token__,
            )

        # Keep the original functions on the client, so old code will
        # continue to work, but use the new 'snake_case' method in
        # the new code.
        insert = Insert
        async def Remove(
            __this__,
            __context__: IMPORT_reboot_aio_contexts.TransactionContext | IMPORT_reboot_aio_contexts.WorkflowContext | IMPORT_reboot_aio_external.ExternalContext,
            __options__: IMPORT_typing.Optional[IMPORT_reboot_aio_call.Options] = None,
            *,
            key: IMPORT_typing.Optional[str] = None,
        ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRemoveResponse:
            # UX improvement: check that neither positional argument was accidentally
            # given a gRPC request type.
            IMPORT_reboot_aio_types.assert_not_request_type(__context__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRemoveRequest)
            IMPORT_reboot_aio_types.assert_not_request_type(__options__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRemoveRequest)

            # Within a `workflow`, all "bare" calls are
            # `per_workflow()` calls, unless we're within a control
            # loop, in which case they are syntactic sugar for
            # `per_iteration()`.
            if __options__ is None or __options__.idempotency is None:
                if isinstance(__context__, IMPORT_reboot_aio_contexts.WorkflowContext):
                    return await (
                        __this__.per_iteration() if __context__.within_loop()
                        else __this__.per_workflow()
                    ).Remove(
                        __context__,
                        __options__ or IMPORT_reboot_aio_call.Options(),
                        key=key,
                    )
                elif isinstance(__context__, IMPORT_reboot_aio_external.InitializeContext):
                    return await __this__.idempotently().Remove(
                        __context__,
                        __options__ or IMPORT_reboot_aio_call.Options(),
                        key=key,
                    )

            if key is not None and not isinstance(
                key,
                str,
            ):
                raise TypeError(
                    f"Can not construct protobuf message of type "
                    f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRemoveRequest': field 'key' is not "
                    f"of required type 'str'"
                )
            # TODO: mypy-protobuf declares that
            # `google.protobuf.message.Message` constructor arguments are
            # always non-None, when in reality they are optional.
            __request__ = rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRemoveRequest(
                key=key,  # type: ignore[arg-type]
            )
            __idempotency__: IMPORT_typing.Optional[IMPORT_reboot_aio_idempotency.Idempotency] = None
            __metadata__: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None
            __bearer_token__: IMPORT_typing.Optional[str] = None
            if __options__ is not None:
                IMPORT_reboot_aio_types.assert_type(__options__, [IMPORT_reboot_aio_call.Options])
                if __options__.idempotency is not None:
                    __idempotency__ = __options__.idempotency
                if __options__.metadata is not None:
                    __metadata__ = __options__.metadata
                if __options__.bearer_token is not None:
                    __bearer_token__ = __options__.bearer_token

            return await __this__._workflow(__context__).Remove(
                __request__,
                idempotency=__idempotency__,
                metadata=__metadata__,
                bearer_token=__bearer_token__,
            )

        # Keep the original functions on the client, so old code will
        # continue to work, but use the new 'snake_case' method in
        # the new code.
        remove = Remove
        async def Range(
            __this__,
            __context__: IMPORT_reboot_aio_contexts.ReaderContext | IMPORT_reboot_aio_contexts.WriterContext | IMPORT_reboot_aio_contexts.TransactionContext | IMPORT_reboot_aio_contexts.WorkflowContext | IMPORT_reboot_aio_external.ExternalContext,
            __options__: IMPORT_typing.Optional[IMPORT_reboot_aio_call.Options] = None,
            *,
            start_key: IMPORT_typing.Optional[str] = None,
            limit: IMPORT_typing.Optional[int] = None,
        ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRangeResponse:
            # UX improvement: check that neither positional argument was accidentally
            # given a gRPC request type.
            IMPORT_reboot_aio_types.assert_not_request_type(__context__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRangeRequest)
            IMPORT_reboot_aio_types.assert_not_request_type(__options__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRangeRequest)

            # Within a `workflow`, all "bare" calls are
            # `per_workflow()` calls, unless we're within a control
            # loop, in which case they are syntactic sugar for
            # `per_iteration()`.
            #
            # Unless we are "within until" in which case all "bare"
            # calls are `.always().
            if __options__ is None or __options__.idempotency is None:
                if isinstance(__context__, IMPORT_reboot_aio_contexts.WorkflowContext):
                    return await (
                        __this__.always() if __context__.within_until()
                        else (
                            __this__.per_iteration() if __context__.within_loop()
                            else __this__.per_workflow()
                        )
                    ).Range(
                        __context__,
                        __options__ or IMPORT_reboot_aio_call.Options(),
                        start_key=start_key,
                        limit=limit,
                    )
                elif isinstance(__context__, IMPORT_reboot_aio_external.InitializeContext):
                    return await __this__.idempotently().Range(
                        __context__,
                        __options__ or IMPORT_reboot_aio_call.Options(),
                        start_key=start_key,
                        limit=limit,
                    )

            if start_key is not None and not isinstance(
                start_key,
                str,
            ):
                raise TypeError(
                    f"Can not construct protobuf message of type "
                    f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRangeRequest': field 'start_key' is not "
                    f"of required type 'str'"
                )
            if limit is not None and not isinstance(
                limit,
                int,
            ):
                raise TypeError(
                    f"Can not construct protobuf message of type "
                    f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRangeRequest': field 'limit' is not "
                    f"of required type 'int'"
                )
            # TODO: mypy-protobuf declares that
            # `google.protobuf.message.Message` constructor arguments are
            # always non-None, when in reality they are optional.
            __request__ = rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRangeRequest(
                start_key=start_key,  # type: ignore[arg-type]
                limit=limit,  # type: ignore[arg-type]
            )
            __metadata__: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None
            __bearer_token__: IMPORT_typing.Optional[str] = None
            if __options__ is not None:
                IMPORT_reboot_aio_types.assert_type(__options__, [IMPORT_reboot_aio_call.Options])
                if __options__.metadata is not None:
                    __metadata__ = __options__.metadata
                if __options__.bearer_token is not None:
                    __bearer_token__ = __options__.bearer_token
            return await __this__._reader(__context__).Range(
                __request__,
                metadata=__metadata__,
                bearer_token=__bearer_token__,
                idempotency=__options__.idempotency if __options__ is not None else None,
            )
        # Keep the original functions on the client, so old code will
        # continue to work, but use the new 'snake_case' method in
        # the new code.
        range = Range
        async def ReverseRange(
            __this__,
            __context__: IMPORT_reboot_aio_contexts.ReaderContext | IMPORT_reboot_aio_contexts.WriterContext | IMPORT_reboot_aio_contexts.TransactionContext | IMPORT_reboot_aio_contexts.WorkflowContext | IMPORT_reboot_aio_external.ExternalContext,
            __options__: IMPORT_typing.Optional[IMPORT_reboot_aio_call.Options] = None,
            *,
            start_key: IMPORT_typing.Optional[str] = None,
            limit: IMPORT_typing.Optional[int] = None,
        ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapReverseRangeResponse:
            # UX improvement: check that neither positional argument was accidentally
            # given a gRPC request type.
            IMPORT_reboot_aio_types.assert_not_request_type(__context__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapReverseRangeRequest)
            IMPORT_reboot_aio_types.assert_not_request_type(__options__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapReverseRangeRequest)

            # Within a `workflow`, all "bare" calls are
            # `per_workflow()` calls, unless we're within a control
            # loop, in which case they are syntactic sugar for
            # `per_iteration()`.
            #
            # Unless we are "within until" in which case all "bare"
            # calls are `.always().
            if __options__ is None or __options__.idempotency is None:
                if isinstance(__context__, IMPORT_reboot_aio_contexts.WorkflowContext):
                    return await (
                        __this__.always() if __context__.within_until()
                        else (
                            __this__.per_iteration() if __context__.within_loop()
                            else __this__.per_workflow()
                        )
                    ).ReverseRange(
                        __context__,
                        __options__ or IMPORT_reboot_aio_call.Options(),
                        start_key=start_key,
                        limit=limit,
                    )
                elif isinstance(__context__, IMPORT_reboot_aio_external.InitializeContext):
                    return await __this__.idempotently().ReverseRange(
                        __context__,
                        __options__ or IMPORT_reboot_aio_call.Options(),
                        start_key=start_key,
                        limit=limit,
                    )

            if start_key is not None and not isinstance(
                start_key,
                str,
            ):
                raise TypeError(
                    f"Can not construct protobuf message of type "
                    f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapReverseRangeRequest': field 'start_key' is not "
                    f"of required type 'str'"
                )
            if limit is not None and not isinstance(
                limit,
                int,
            ):
                raise TypeError(
                    f"Can not construct protobuf message of type "
                    f"'rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapReverseRangeRequest': field 'limit' is not "
                    f"of required type 'int'"
                )
            # TODO: mypy-protobuf declares that
            # `google.protobuf.message.Message` constructor arguments are
            # always non-None, when in reality they are optional.
            __request__ = rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapReverseRangeRequest(
                start_key=start_key,  # type: ignore[arg-type]
                limit=limit,  # type: ignore[arg-type]
            )
            __metadata__: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None
            __bearer_token__: IMPORT_typing.Optional[str] = None
            if __options__ is not None:
                IMPORT_reboot_aio_types.assert_type(__options__, [IMPORT_reboot_aio_call.Options])
                if __options__.metadata is not None:
                    __metadata__ = __options__.metadata
                if __options__.bearer_token is not None:
                    __bearer_token__ = __options__.bearer_token
            return await __this__._reader(__context__).ReverseRange(
                __request__,
                metadata=__metadata__,
                bearer_token=__bearer_token__,
                idempotency=__options__.idempotency if __options__ is not None else None,
            )
        # Keep the original functions on the client, so old code will
        # continue to work, but use the new 'snake_case' method in
        # the new code.
        reverse_range = ReverseRange
        async def Stringify(
            __this__,
            __context__: IMPORT_reboot_aio_contexts.ReaderContext | IMPORT_reboot_aio_contexts.WriterContext | IMPORT_reboot_aio_contexts.TransactionContext | IMPORT_reboot_aio_contexts.WorkflowContext | IMPORT_reboot_aio_external.ExternalContext,
            __options__: IMPORT_typing.Optional[IMPORT_reboot_aio_call.Options] = None,
        ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapStringifyResponse:
            # UX improvement: check that neither positional argument was accidentally
            # given a gRPC request type.
            IMPORT_reboot_aio_types.assert_not_request_type(__context__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapStringifyRequest)
            IMPORT_reboot_aio_types.assert_not_request_type(__options__, request_type=rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapStringifyRequest)

            # Within a `workflow`, all "bare" calls are
            # `per_workflow()` calls, unless we're within a control
            # loop, in which case they are syntactic sugar for
            # `per_iteration()`.
            #
            # Unless we are "within until" in which case all "bare"
            # calls are `.always().
            if __options__ is None or __options__.idempotency is None:
                if isinstance(__context__, IMPORT_reboot_aio_contexts.WorkflowContext):
                    return await (
                        __this__.always() if __context__.within_until()
                        else (
                            __this__.per_iteration() if __context__.within_loop()
                            else __this__.per_workflow()
                        )
                    ).Stringify(
                        __context__,
                        __options__ or IMPORT_reboot_aio_call.Options(),
                    )
                elif isinstance(__context__, IMPORT_reboot_aio_external.InitializeContext):
                    return await __this__.idempotently().Stringify(
                        __context__,
                        __options__ or IMPORT_reboot_aio_call.Options(),
                    )

            # TODO: mypy-protobuf declares that
            # `google.protobuf.message.Message` constructor arguments are
            # always non-None, when in reality they are optional.
            __request__ = rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapStringifyRequest(
            )
            __metadata__: IMPORT_typing.Optional[IMPORT_reboot_aio_types.GrpcMetadata] = None
            __bearer_token__: IMPORT_typing.Optional[str] = None
            if __options__ is not None:
                IMPORT_reboot_aio_types.assert_type(__options__, [IMPORT_reboot_aio_call.Options])
                if __options__.metadata is not None:
                    __metadata__ = __options__.metadata
                if __options__.bearer_token is not None:
                    __bearer_token__ = __options__.bearer_token
            return await __this__._reader(__context__).Stringify(
                __request__,
                metadata=__metadata__,
                bearer_token=__bearer_token__,
                idempotency=__options__.idempotency if __options__ is not None else None,
            )
        # Keep the original functions on the client, so old code will
        # continue to work, but use the new 'snake_case' method in
        # the new code.
        stringify = Stringify

    class _Forall:

        _ids: list[str]

        def __init__(self, ids: list[str]):
            self._ids = ids

        async def Create(
            # In methods which are dealing with user input, (i.e.,
            # proto message field names), we should use '__double_underscored__'
            # variables to avoid any potential name conflicts with the method's
            # parameters.
            # The '__self__' parameter is a convention in Python to
            # indicate that this method is a bound method, so we use
            # '__this__' instead.
            __this__,
            __context__: IMPORT_reboot_aio_contexts.TransactionContext | IMPORT_reboot_aio_contexts.WorkflowContext | IMPORT_reboot_aio_external.ExternalContext,
            __options__: IMPORT_reboot_aio_call.Options = IMPORT_reboot_aio_call.Options(),
            *,
            order: IMPORT_typing.Optional[int] = None,
        ) -> list[rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapCreateResponse]:
            return await IMPORT_asyncio.gather(
                *[
                    OrderedMap.ref(
                        id
                    ).Create(
                        __context__,
                        __options__,
                        order=order,
                    ) for id in __this__._ids
                ]
            )

        # Keep the original functions on the client, so old code will
        # continue to work, but use the new 'snake_case' method in
        # the new code.
        create = Create
        async def Search(
            # In methods which are dealing with user input, (i.e.,
            # proto message field names), we should use '__double_underscored__'
            # variables to avoid any potential name conflicts with the method's
            # parameters.
            # The '__self__' parameter is a convention in Python to
            # indicate that this method is a bound method, so we use
            # '__this__' instead.
            __this__,
            __context__: IMPORT_reboot_aio_contexts.ReaderContext | IMPORT_reboot_aio_contexts.WriterContext | IMPORT_reboot_aio_contexts.TransactionContext | IMPORT_reboot_aio_contexts.WorkflowContext | IMPORT_reboot_aio_external.ExternalContext,
            __options__: IMPORT_reboot_aio_call.Options = IMPORT_reboot_aio_call.Options(),
            *,
            key: IMPORT_typing.Optional[str] = None,
        ) -> list[rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapSearchResponse]:
            return await IMPORT_asyncio.gather(
                *[
                    OrderedMap.ref(
                        id
                    ).Search(
                        __context__,
                        __options__,
                        key=key,
                    ) for id in __this__._ids
                ]
            )

        # Keep the original functions on the client, so old code will
        # continue to work, but use the new 'snake_case' method in
        # the new code.
        search = Search
        async def Insert(
            # In methods which are dealing with user input, (i.e.,
            # proto message field names), we should use '__double_underscored__'
            # variables to avoid any potential name conflicts with the method's
            # parameters.
            # The '__self__' parameter is a convention in Python to
            # indicate that this method is a bound method, so we use
            # '__this__' instead.
            __this__,
            __context__: IMPORT_reboot_aio_contexts.TransactionContext | IMPORT_reboot_aio_contexts.WorkflowContext | IMPORT_reboot_aio_external.ExternalContext,
            __options__: IMPORT_reboot_aio_call.Options = IMPORT_reboot_aio_call.Options(),
            *,
            key: IMPORT_typing.Optional[str] = None,
            value: IMPORT_typing.Optional[google.protobuf.struct_pb2.Value] = None,
            bytes: IMPORT_typing.Optional[bytes] = None,
            any: IMPORT_typing.Optional[google.protobuf.any_pb2.Any] = None,
        ) -> list[rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapInsertResponse]:
            return await IMPORT_asyncio.gather(
                *[
                    OrderedMap.ref(
                        id
                    ).Insert(
                        __context__,
                        __options__,
                        key=key,
                        value=value,
                        bytes=bytes,
                        any=any,
                    ) for id in __this__._ids
                ]
            )

        # Keep the original functions on the client, so old code will
        # continue to work, but use the new 'snake_case' method in
        # the new code.
        insert = Insert
        async def Remove(
            # In methods which are dealing with user input, (i.e.,
            # proto message field names), we should use '__double_underscored__'
            # variables to avoid any potential name conflicts with the method's
            # parameters.
            # The '__self__' parameter is a convention in Python to
            # indicate that this method is a bound method, so we use
            # '__this__' instead.
            __this__,
            __context__: IMPORT_reboot_aio_contexts.TransactionContext | IMPORT_reboot_aio_contexts.WorkflowContext | IMPORT_reboot_aio_external.ExternalContext,
            __options__: IMPORT_reboot_aio_call.Options = IMPORT_reboot_aio_call.Options(),
            *,
            key: IMPORT_typing.Optional[str] = None,
        ) -> list[rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRemoveResponse]:
            return await IMPORT_asyncio.gather(
                *[
                    OrderedMap.ref(
                        id
                    ).Remove(
                        __context__,
                        __options__,
                        key=key,
                    ) for id in __this__._ids
                ]
            )

        # Keep the original functions on the client, so old code will
        # continue to work, but use the new 'snake_case' method in
        # the new code.
        remove = Remove
        async def Range(
            # In methods which are dealing with user input, (i.e.,
            # proto message field names), we should use '__double_underscored__'
            # variables to avoid any potential name conflicts with the method's
            # parameters.
            # The '__self__' parameter is a convention in Python to
            # indicate that this method is a bound method, so we use
            # '__this__' instead.
            __this__,
            __context__: IMPORT_reboot_aio_contexts.ReaderContext | IMPORT_reboot_aio_contexts.WriterContext | IMPORT_reboot_aio_contexts.TransactionContext | IMPORT_reboot_aio_contexts.WorkflowContext | IMPORT_reboot_aio_external.ExternalContext,
            __options__: IMPORT_reboot_aio_call.Options = IMPORT_reboot_aio_call.Options(),
            *,
            start_key: IMPORT_typing.Optional[str] = None,
            limit: IMPORT_typing.Optional[int] = None,
        ) -> list[rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRangeResponse]:
            return await IMPORT_asyncio.gather(
                *[
                    OrderedMap.ref(
                        id
                    ).Range(
                        __context__,
                        __options__,
                        start_key=start_key,
                        limit=limit,
                    ) for id in __this__._ids
                ]
            )

        # Keep the original functions on the client, so old code will
        # continue to work, but use the new 'snake_case' method in
        # the new code.
        range = Range
        async def ReverseRange(
            # In methods which are dealing with user input, (i.e.,
            # proto message field names), we should use '__double_underscored__'
            # variables to avoid any potential name conflicts with the method's
            # parameters.
            # The '__self__' parameter is a convention in Python to
            # indicate that this method is a bound method, so we use
            # '__this__' instead.
            __this__,
            __context__: IMPORT_reboot_aio_contexts.ReaderContext | IMPORT_reboot_aio_contexts.WriterContext | IMPORT_reboot_aio_contexts.TransactionContext | IMPORT_reboot_aio_contexts.WorkflowContext | IMPORT_reboot_aio_external.ExternalContext,
            __options__: IMPORT_reboot_aio_call.Options = IMPORT_reboot_aio_call.Options(),
            *,
            start_key: IMPORT_typing.Optional[str] = None,
            limit: IMPORT_typing.Optional[int] = None,
        ) -> list[rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapReverseRangeResponse]:
            return await IMPORT_asyncio.gather(
                *[
                    OrderedMap.ref(
                        id
                    ).ReverseRange(
                        __context__,
                        __options__,
                        start_key=start_key,
                        limit=limit,
                    ) for id in __this__._ids
                ]
            )

        # Keep the original functions on the client, so old code will
        # continue to work, but use the new 'snake_case' method in
        # the new code.
        reverse_range = ReverseRange
        async def Stringify(
            # In methods which are dealing with user input, (i.e.,
            # proto message field names), we should use '__double_underscored__'
            # variables to avoid any potential name conflicts with the method's
            # parameters.
            # The '__self__' parameter is a convention in Python to
            # indicate that this method is a bound method, so we use
            # '__this__' instead.
            __this__,
            __context__: IMPORT_reboot_aio_contexts.ReaderContext | IMPORT_reboot_aio_contexts.WriterContext | IMPORT_reboot_aio_contexts.TransactionContext | IMPORT_reboot_aio_contexts.WorkflowContext | IMPORT_reboot_aio_external.ExternalContext,
            __options__: IMPORT_reboot_aio_call.Options = IMPORT_reboot_aio_call.Options(),
        ) -> list[rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapStringifyResponse]:
            return await IMPORT_asyncio.gather(
                *[
                    OrderedMap.ref(
                        id
                    ).Stringify(
                        __context__,
                        __options__,
                    ) for id in __this__._ids
                ]
            )

        # Keep the original functions on the client, so old code will
        # continue to work, but use the new 'snake_case' method in
        # the new code.
        stringify = Stringify

    @classmethod
    def forall(cls, ids: list[str]) -> OrderedMap._Forall:
        return OrderedMap._Forall(ids)

    @classmethod
    def ref(
        cls,
        state_id: IMPORT_reboot_aio_types.StateId,
        *,
        bearer_token: IMPORT_typing.Optional[str] = None,
    ) -> OrderedMap.WeakReference[OrderedMap.WeakReference._Schedule]:
        return OrderedMap.WeakReference(
            # TODO(https://github.com/reboot-dev/mono/issues/3226): add support for calling other applications.
            # For now this always stays within the application that creates the context.
            application_id=None,
            state_id=state_id,
            schedule_type=OrderedMap.WeakReference._Schedule,
            bearer_token=bearer_token,
        )


    @IMPORT_typing.overload
    @classmethod
    def idempotently(cls, alias: IMPORT_typing.Optional[str] = None, *, each_iteration: bool = False) -> OrderedMap._ConstructIdempotently:
        ...

    @IMPORT_typing.overload
    @classmethod
    def idempotently(cls, *, key: IMPORT_uuid.UUID, generated: bool = False) -> OrderedMap._ConstructIdempotently:
        ...

    @classmethod
    def idempotently(
        cls,
        alias: IMPORT_typing.Optional[str] = None,
        *,
        key: IMPORT_typing.Optional[IMPORT_uuid.UUID] = None,
        each_iteration: IMPORT_typing.Optional[bool] = None,
        generated: bool = False,
    ) -> OrderedMap._ConstructIdempotently:
        return OrderedMap._ConstructIdempotently(
            _idempotency=IMPORT_reboot_aio_contexts.Context.idempotency(
                alias=alias,
                key=key,
                each_iteration=each_iteration,
                generated=generated,
            ),
        )

    @classmethod
    def per_workflow(
        cls,
        alias: IMPORT_typing.Optional[str] = None,
    ):
        return cls.idempotently(alias)

    @classmethod
    def per_iteration(
        cls,
        alias: IMPORT_typing.Optional[str] = None,
    ):
        return cls.idempotently(alias, each_iteration=True)

    @classmethod
    def always(cls):
        return cls.idempotently(key=IMPORT_uuid.uuid4(), generated=True)

    @IMPORT_dataclasses.dataclass(frozen=True)
    class _ConstructIdempotently:

        _idempotency: IMPORT_reboot_aio_idempotency.Idempotency



############################ Servicer Node adapters ############################
# Used by Node.js servicer implementations to access Python code and vice-versa.
# Relevant to servicers, irrelevant to clients.

class NodeServicerNodeAdaptor(Node.singleton.Servicer):

    async def _wait_for_cancelled(
        self,
        future: IMPORT_asyncio.Future,
        method: str,
    ):
        while True:
            done, pending = await IMPORT_asyncio.wait(
                [future],
                timeout=5,  # seconds
            )
            # Check if we've timed out and log a warning that their
            # call has been cancelled but it is still running.
            if len(done) == 0:
                logger.warning(
                    f"Call to method '{method}' has been cancelled by the caller, "
                    "BUT WE ARE STILL WAITING for it complete. You can use the promise "
                    "`context.cancelled` to check if the caller has cancelled so you "
                    "don't do unnecessary work or wait for something that may never occur."
                )
                continue
            break

        # Now need to actually `await` the future so that we don't
        # have an unretrieved exception that gets logged.
        #
        # NOTE: this will raise an exception if the method raised even
        # though the call has already been cancelled but it makes it
        # more clear that the method raised so that is why we're not
        # catching and swallowing any exception.
        await future

    def __init__(self):
        self._js_servicer_reference = self._construct_js_servicer()  # type: ignore[attr-defined]

    def authorizer(self) -> IMPORT_typing.Optional[IMPORT_rebootdev.aio.auth.authorizers.Authorizer]:
        return self._construct_authorizer(self._js_servicer_reference)  # type: ignore[attr-defined]

    async def _read(
        self,
        context: IMPORT_reboot_aio_contexts.WorkflowContext,
        json_options: str,
    ) -> str:
        options = IMPORT_json.loads(json_options)

        alias = options.get('alias')

        assert 'how' in options
        how = options['how']
        if how == IMPORT_reboot_aio_workflows.ALWAYS:
            assert alias is None
            return IMPORT_google_protobuf_json_format.MessageToJson(
                await super().state.always().read(context)
            )

        assert how in [
            IMPORT_reboot_aio_workflows.PER_WORKFLOW,
            IMPORT_reboot_aio_workflows.PER_ITERATION,
        ]

        return IMPORT_google_protobuf_json_format.MessageToJson(
            await (
                super().state.per_workflow(alias)
                if how == IMPORT_reboot_aio_workflows.PER_WORKFLOW
                else super().state.per_iteration(alias)
            ).read(context)
        )

    async def _write(
        self,
        context: IMPORT_reboot_aio_contexts.WorkflowContext,
        writer: IMPORT_typing.Callable[[str], IMPORT_typing.Awaitable[str]],
        json_options: str,
    ) -> str:

        async def _writer(state: IMPORT_google_protobuf_message.Message):
            with IMPORT_reboot_aio_tracing.span(
                state_name=f"{context.state_type_name}('{context.state_id}')",
                span_name="_writer on NodeAdaptor",
                # The naming above matches Python, but not TypeScript.
                python_specific=True,
                level=IMPORT_reboot_aio_tracing.TraceLevel.CUSTOMER,
            ):
                json_result_state = await writer(
                    IMPORT_google_protobuf_json_format.MessageToJson(state)
                )

                with IMPORT_reboot_aio_tracing.span(
                    state_name=f"{context.state_type_name}('{context.state_id}')",
                    span_name="_write - State Copy on NodeAdaptor",
                    # The naming above matches Python, but not TypeScript.
                    python_specific=True,
                    level=IMPORT_reboot_aio_tracing.TraceLevel.CUSTOMER,
                ):
                    result_state = IMPORT_json.loads(json_result_state)

                    state.CopyFrom(
                        IMPORT_google_protobuf_json_format.ParseDict(
                            result_state['state'],
                            self.__state_type__(),
                        )
                    )

                    assert 'result' in result_state
                    result = result_state['result']
                    assert type(result) == str
                    return result

        options = IMPORT_json.loads(json_options)

        alias = options.get('alias')

        assert 'how' in options
        how = options['how']

        if how == IMPORT_reboot_aio_workflows.ALWAYS:
            assert alias is None
            return await super().state.always().write(
                context,
                _writer,
            )

        assert how in [
            IMPORT_reboot_aio_workflows.PER_WORKFLOW,
            IMPORT_reboot_aio_workflows.PER_ITERATION,
        ]

        return await (
            super().state.per_workflow(alias)
            if how == IMPORT_reboot_aio_workflows.PER_WORKFLOW
            else super().state.per_iteration(alias)
        ).write(context, _writer, type=str)

    # Node specific methods:
    async def Create(
        self,
        context: IMPORT_reboot_aio_contexts.WriterContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateResponse:
        with IMPORT_reboot_aio_tracing.span(
                state_name=f"{context.state_type_name}('{context.state_id}')",
                span_name="NodeAdaptor Create",
                # The naming above matches Python, but not TypeScript.
                python_specific=True,
                level=IMPORT_reboot_aio_tracing.TraceLevel.CUSTOMER,
        ):
            with IMPORT_reboot_aio_tracing.span(
                state_name=f"{context.state_type_name}('{context.state_id}')",
                span_name="Create and serialize `TrampolineCall`",
                # The naming above matches Python, but not TypeScript.
                python_specific=True,
                level=IMPORT_reboot_aio_tracing.TraceLevel.CUSTOMER,
            ):
                bytes_call = IMPORT_rbt_v1alpha1_nodejs_pb2.TrampolineCall(
                    kind=IMPORT_rbt_v1alpha1_nodejs_pb2.writer,
                    context=IMPORT_rbt_v1alpha1_nodejs_pb2.Context(
                        method='Create',
                        state_id=context.state_id,
                        state_type_name=context.state_type_name,
                        caller_bearer_token=context.caller_bearer_token,
                        cookie=context.cookie,
                        app_internal=context.app_internal,
                        auth=(
                            None if context.auth is None
                            else context.auth.to_proto_bytes()
                        ),
                    ),
                    state=state.SerializeToString(),
                    request=request.SerializeToString(),
                ).SerializeToString()

            cancelled: IMPORT_asyncio.Future[None] = IMPORT_asyncio.Future()

            bytes_result_future: IMPORT_typing.Optional[IMPORT_asyncio.Future[str]] = None

            try:
                with IMPORT_reboot_aio_tracing.span(
                    state_name=f"{context.state_type_name}('{context.state_id}')",
                    span_name="trampoline",
                    # The naming above matches Python, but not TypeScript.
                    python_specific=True,
                    level=IMPORT_reboot_aio_tracing.TraceLevel.CUSTOMER,
                ):
                    bytes_result_future = self._trampoline(  # type: ignore[attr-defined]
                        self._js_servicer_reference,
                        context,
                        cancelled,
                        bytes_call,
                    )
                    # NOTE: we need to `asyncio.shield` so that we can still
                    # correctly wait for this future to complete even if we
                    # are cancelled.
                    assert bytes_result_future is not None
                    bytes_result = await IMPORT_asyncio.shield(bytes_result_future)
            except IMPORT_asyncio.CancelledError:
                cancelled.set_result(None)

                # NOTE: we MUST wait for `bytes_result_future` because this
                # is a `writer` or `transaction` and we CAN NOT execute
                # multiple simultaneously.
                if bytes_result_future is not None:
                    await self._wait_for_cancelled(
                        bytes_result_future,
                        'Node.Create',
                    )

                raise
            except:
                # Make sure we cancel the `cancelled` future either if an
                # exception is thrown or if the result is reeturned so
                # that we don't keep around resources related to it that
                # might cause us to run out of memory or worse, keep Node
                # from exiting because it is waiting for Python.
                cancelled.cancel()
                raise
            else:
                cancelled.cancel()

                with IMPORT_reboot_aio_tracing.span(
                    state_name=f"{context.state_type_name}('{context.state_id}')",
                    span_name="result ParseFromString",
                    # The naming above matches Python, but not TypeScript.
                    python_specific=True,
                    level=IMPORT_reboot_aio_tracing.TraceLevel.CUSTOMER,
                ):
                    result = IMPORT_rbt_v1alpha1_nodejs_pb2.TrampolineResult.FromString(
                        bytes_result
                    )

                if result.HasField('state'):
                    with IMPORT_reboot_aio_tracing.span(
                        state_name=f"{context.state_type_name}('{context.state_id}')",
                        span_name="state ParseFromString",
                        # The naming above matches Python, but not TypeScript.
                        python_specific=True,
                        level=IMPORT_reboot_aio_tracing.TraceLevel.CUSTOMER,
                    ):
                        state.CopyFrom(
                            rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node.FromString(
                                result.state
                            )
                        )

                if result.HasField('status_json'):
                    raise (
                        Node
                        .CreateAborted
                        .from_status(
                            IMPORT_google_protobuf_json_format.Parse(
                                result.status_json,
                                IMPORT_google_rpc_status_pb2.Status(),
                            )
                        )
                    )

                assert result.HasField('response')

                with IMPORT_reboot_aio_tracing.span(
                    state_name=f"{context.state_type_name}('{context.state_id}')",
                    span_name="response ParseFromString",
                ):
                    return rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeCreateResponse.FromString(result.response)
        raise RuntimeError("Unexpected result from Create")

    async def Search(
        self,
        context: IMPORT_reboot_aio_contexts.ReaderContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeSearchRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeSearchResponse:
        bytes_call = IMPORT_rbt_v1alpha1_nodejs_pb2.TrampolineCall(
            kind=IMPORT_rbt_v1alpha1_nodejs_pb2.reader,
            context=IMPORT_rbt_v1alpha1_nodejs_pb2.Context(
                method='Search',
                state_id=context.state_id,
                state_type_name=context.state_type_name,
                caller_bearer_token=context.caller_bearer_token,
                cookie=context.cookie,
                app_internal=context.app_internal,
                auth=(
                    None if context.auth is None
                    else context.auth.to_proto_bytes()
                ),
            ),
            state=state.SerializeToString(),
            request=request.SerializeToString(),
        ).SerializeToString()

        cancelled: IMPORT_asyncio.Future[None] = IMPORT_asyncio.Future()

        bytes_result_future: IMPORT_typing.Optional[IMPORT_asyncio.Future[str]] = None

        try:
            bytes_result_future = self._trampoline(  # type: ignore[attr-defined]
                self._js_servicer_reference,
                context,
                cancelled,
                bytes_call,
            )
            # NOTE: we need to `asyncio.shield` so that we can still
            # correctly wait for this future to complete even if we
            # are cancelled.
            assert bytes_result_future is not None
            bytes_result = await IMPORT_asyncio.shield(bytes_result_future)
        except IMPORT_asyncio.CancelledError:
            cancelled.set_result(None)

            # NOTE: unlike for a `writer` or `transaction`, we DO NOT
            # _need_ to wait for `bytes_result_future` because this is a
            # reader and we can execute multiple readers
            # simultaneously. That being said, we still want to give
            # good feedback that the RPC has been cancelled, and there
            # is no harm waiting because other readers can still be
            # called.
            if bytes_result_future is not None:
                await self._wait_for_cancelled(
                    bytes_result_future,
                    'Node.Search',
                )

            raise
        except:
            # Make sure we cancel the `cancelled` future either if an
            # exception is thrown or if the result is reeturned so
            # that we don't keep around resources related to it that
            # might cause us to run out of memory or worse, keep Node
            # from exiting because it is waiting for Python.
            cancelled.cancel()
            raise
        else:
            cancelled.cancel()

            result = IMPORT_rbt_v1alpha1_nodejs_pb2.TrampolineResult.FromString(
                bytes_result
            )

            if result.HasField('status_json'):
                raise (
                    Node
                    .SearchAborted
                    .from_status(
                        IMPORT_google_protobuf_json_format.Parse(
                            result.status_json,
                            IMPORT_google_rpc_status_pb2.Status(),
                        )
                    )
                )

            assert result.HasField('response')

            return rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeSearchResponse.FromString(result.response)
        raise RuntimeError("Unexpected result from Search")

    async def Insert(
        self,
        context: IMPORT_reboot_aio_contexts.TransactionContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeInsertRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeInsertResponse:
        with IMPORT_reboot_aio_tracing.span(
                state_name=f"{context.state_type_name}('{context.state_id}')",
                span_name="NodeAdaptor Insert",
                # The naming above matches Python, but not TypeScript.
                python_specific=True,
                level=IMPORT_reboot_aio_tracing.TraceLevel.CUSTOMER,
        ):
            with IMPORT_reboot_aio_tracing.span(
                state_name=f"{context.state_type_name}('{context.state_id}')",
                span_name="Create and serialize `TrampolineCall`",
                # The naming above matches Python, but not TypeScript.
                python_specific=True,
                level=IMPORT_reboot_aio_tracing.TraceLevel.CUSTOMER,
            ):
                bytes_call = IMPORT_rbt_v1alpha1_nodejs_pb2.TrampolineCall(
                    kind=IMPORT_rbt_v1alpha1_nodejs_pb2.transaction,
                    context=IMPORT_rbt_v1alpha1_nodejs_pb2.Context(
                        method='Insert',
                        state_id=context.state_id,
                        state_type_name=context.state_type_name,
                        caller_bearer_token=context.caller_bearer_token,
                        cookie=context.cookie,
                        app_internal=context.app_internal,
                        auth=(
                            None if context.auth is None
                            else context.auth.to_proto_bytes()
                        ),
                    ),
                    state=state.SerializeToString(),
                    request=request.SerializeToString(),
                ).SerializeToString()

            cancelled: IMPORT_asyncio.Future[None] = IMPORT_asyncio.Future()

            bytes_result_future: IMPORT_typing.Optional[IMPORT_asyncio.Future[str]] = None

            try:
                with IMPORT_reboot_aio_tracing.span(
                    state_name=f"{context.state_type_name}('{context.state_id}')",
                    span_name="trampoline",
                    # The naming above matches Python, but not TypeScript.
                    python_specific=True,
                    level=IMPORT_reboot_aio_tracing.TraceLevel.CUSTOMER,
                ):
                    bytes_result_future = self._trampoline(  # type: ignore[attr-defined]
                        self._js_servicer_reference,
                        context,
                        cancelled,
                        bytes_call,
                    )
                    # NOTE: we need to `asyncio.shield` so that we can still
                    # correctly wait for this future to complete even if we
                    # are cancelled.
                    assert bytes_result_future is not None
                    bytes_result = await IMPORT_asyncio.shield(bytes_result_future)
            except IMPORT_asyncio.CancelledError:
                cancelled.set_result(None)

                # NOTE: we MUST wait for `bytes_result_future` because this
                # is a `writer` or `transaction` and we CAN NOT execute
                # multiple simultaneously.
                if bytes_result_future is not None:
                    await self._wait_for_cancelled(
                        bytes_result_future,
                        'Node.Insert',
                    )

                raise
            except:
                # Make sure we cancel the `cancelled` future either if an
                # exception is thrown or if the result is reeturned so
                # that we don't keep around resources related to it that
                # might cause us to run out of memory or worse, keep Node
                # from exiting because it is waiting for Python.
                cancelled.cancel()
                raise
            else:
                cancelled.cancel()

                with IMPORT_reboot_aio_tracing.span(
                    state_name=f"{context.state_type_name}('{context.state_id}')",
                    span_name="result ParseFromString",
                    # The naming above matches Python, but not TypeScript.
                    python_specific=True,
                    level=IMPORT_reboot_aio_tracing.TraceLevel.CUSTOMER,
                ):
                    result = IMPORT_rbt_v1alpha1_nodejs_pb2.TrampolineResult.FromString(
                        bytes_result
                    )

                if result.HasField('state'):
                    with IMPORT_reboot_aio_tracing.span(
                        state_name=f"{context.state_type_name}('{context.state_id}')",
                        span_name="state ParseFromString",
                        # The naming above matches Python, but not TypeScript.
                        python_specific=True,
                        level=IMPORT_reboot_aio_tracing.TraceLevel.CUSTOMER,
                    ):
                        state.CopyFrom(
                            rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node.FromString(
                                result.state
                            )
                        )

                if result.HasField('status_json'):
                    raise (
                        Node
                        .InsertAborted
                        .from_status(
                            IMPORT_google_protobuf_json_format.Parse(
                                result.status_json,
                                IMPORT_google_rpc_status_pb2.Status(),
                            )
                        )
                    )

                assert result.HasField('response')

                with IMPORT_reboot_aio_tracing.span(
                    state_name=f"{context.state_type_name}('{context.state_id}')",
                    span_name="response ParseFromString",
                ):
                    return rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeInsertResponse.FromString(result.response)
        raise RuntimeError("Unexpected result from Insert")

    async def Remove(
        self,
        context: IMPORT_reboot_aio_contexts.TransactionContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRemoveRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRemoveResponse:
        with IMPORT_reboot_aio_tracing.span(
                state_name=f"{context.state_type_name}('{context.state_id}')",
                span_name="NodeAdaptor Remove",
                # The naming above matches Python, but not TypeScript.
                python_specific=True,
                level=IMPORT_reboot_aio_tracing.TraceLevel.CUSTOMER,
        ):
            with IMPORT_reboot_aio_tracing.span(
                state_name=f"{context.state_type_name}('{context.state_id}')",
                span_name="Create and serialize `TrampolineCall`",
                # The naming above matches Python, but not TypeScript.
                python_specific=True,
                level=IMPORT_reboot_aio_tracing.TraceLevel.CUSTOMER,
            ):
                bytes_call = IMPORT_rbt_v1alpha1_nodejs_pb2.TrampolineCall(
                    kind=IMPORT_rbt_v1alpha1_nodejs_pb2.transaction,
                    context=IMPORT_rbt_v1alpha1_nodejs_pb2.Context(
                        method='Remove',
                        state_id=context.state_id,
                        state_type_name=context.state_type_name,
                        caller_bearer_token=context.caller_bearer_token,
                        cookie=context.cookie,
                        app_internal=context.app_internal,
                        auth=(
                            None if context.auth is None
                            else context.auth.to_proto_bytes()
                        ),
                    ),
                    state=state.SerializeToString(),
                    request=request.SerializeToString(),
                ).SerializeToString()

            cancelled: IMPORT_asyncio.Future[None] = IMPORT_asyncio.Future()

            bytes_result_future: IMPORT_typing.Optional[IMPORT_asyncio.Future[str]] = None

            try:
                with IMPORT_reboot_aio_tracing.span(
                    state_name=f"{context.state_type_name}('{context.state_id}')",
                    span_name="trampoline",
                    # The naming above matches Python, but not TypeScript.
                    python_specific=True,
                    level=IMPORT_reboot_aio_tracing.TraceLevel.CUSTOMER,
                ):
                    bytes_result_future = self._trampoline(  # type: ignore[attr-defined]
                        self._js_servicer_reference,
                        context,
                        cancelled,
                        bytes_call,
                    )
                    # NOTE: we need to `asyncio.shield` so that we can still
                    # correctly wait for this future to complete even if we
                    # are cancelled.
                    assert bytes_result_future is not None
                    bytes_result = await IMPORT_asyncio.shield(bytes_result_future)
            except IMPORT_asyncio.CancelledError:
                cancelled.set_result(None)

                # NOTE: we MUST wait for `bytes_result_future` because this
                # is a `writer` or `transaction` and we CAN NOT execute
                # multiple simultaneously.
                if bytes_result_future is not None:
                    await self._wait_for_cancelled(
                        bytes_result_future,
                        'Node.Remove',
                    )

                raise
            except:
                # Make sure we cancel the `cancelled` future either if an
                # exception is thrown or if the result is reeturned so
                # that we don't keep around resources related to it that
                # might cause us to run out of memory or worse, keep Node
                # from exiting because it is waiting for Python.
                cancelled.cancel()
                raise
            else:
                cancelled.cancel()

                with IMPORT_reboot_aio_tracing.span(
                    state_name=f"{context.state_type_name}('{context.state_id}')",
                    span_name="result ParseFromString",
                    # The naming above matches Python, but not TypeScript.
                    python_specific=True,
                    level=IMPORT_reboot_aio_tracing.TraceLevel.CUSTOMER,
                ):
                    result = IMPORT_rbt_v1alpha1_nodejs_pb2.TrampolineResult.FromString(
                        bytes_result
                    )

                if result.HasField('state'):
                    with IMPORT_reboot_aio_tracing.span(
                        state_name=f"{context.state_type_name}('{context.state_id}')",
                        span_name="state ParseFromString",
                        # The naming above matches Python, but not TypeScript.
                        python_specific=True,
                        level=IMPORT_reboot_aio_tracing.TraceLevel.CUSTOMER,
                    ):
                        state.CopyFrom(
                            rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node.FromString(
                                result.state
                            )
                        )

                if result.HasField('status_json'):
                    raise (
                        Node
                        .RemoveAborted
                        .from_status(
                            IMPORT_google_protobuf_json_format.Parse(
                                result.status_json,
                                IMPORT_google_rpc_status_pb2.Status(),
                            )
                        )
                    )

                assert result.HasField('response')

                with IMPORT_reboot_aio_tracing.span(
                    state_name=f"{context.state_type_name}('{context.state_id}')",
                    span_name="response ParseFromString",
                ):
                    return rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRemoveResponse.FromString(result.response)
        raise RuntimeError("Unexpected result from Remove")

    async def Range(
        self,
        context: IMPORT_reboot_aio_contexts.ReaderContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRangeRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRangeResponse:
        bytes_call = IMPORT_rbt_v1alpha1_nodejs_pb2.TrampolineCall(
            kind=IMPORT_rbt_v1alpha1_nodejs_pb2.reader,
            context=IMPORT_rbt_v1alpha1_nodejs_pb2.Context(
                method='Range',
                state_id=context.state_id,
                state_type_name=context.state_type_name,
                caller_bearer_token=context.caller_bearer_token,
                cookie=context.cookie,
                app_internal=context.app_internal,
                auth=(
                    None if context.auth is None
                    else context.auth.to_proto_bytes()
                ),
            ),
            state=state.SerializeToString(),
            request=request.SerializeToString(),
        ).SerializeToString()

        cancelled: IMPORT_asyncio.Future[None] = IMPORT_asyncio.Future()

        bytes_result_future: IMPORT_typing.Optional[IMPORT_asyncio.Future[str]] = None

        try:
            bytes_result_future = self._trampoline(  # type: ignore[attr-defined]
                self._js_servicer_reference,
                context,
                cancelled,
                bytes_call,
            )
            # NOTE: we need to `asyncio.shield` so that we can still
            # correctly wait for this future to complete even if we
            # are cancelled.
            assert bytes_result_future is not None
            bytes_result = await IMPORT_asyncio.shield(bytes_result_future)
        except IMPORT_asyncio.CancelledError:
            cancelled.set_result(None)

            # NOTE: unlike for a `writer` or `transaction`, we DO NOT
            # _need_ to wait for `bytes_result_future` because this is a
            # reader and we can execute multiple readers
            # simultaneously. That being said, we still want to give
            # good feedback that the RPC has been cancelled, and there
            # is no harm waiting because other readers can still be
            # called.
            if bytes_result_future is not None:
                await self._wait_for_cancelled(
                    bytes_result_future,
                    'Node.Range',
                )

            raise
        except:
            # Make sure we cancel the `cancelled` future either if an
            # exception is thrown or if the result is reeturned so
            # that we don't keep around resources related to it that
            # might cause us to run out of memory or worse, keep Node
            # from exiting because it is waiting for Python.
            cancelled.cancel()
            raise
        else:
            cancelled.cancel()

            result = IMPORT_rbt_v1alpha1_nodejs_pb2.TrampolineResult.FromString(
                bytes_result
            )

            if result.HasField('status_json'):
                raise (
                    Node
                    .RangeAborted
                    .from_status(
                        IMPORT_google_protobuf_json_format.Parse(
                            result.status_json,
                            IMPORT_google_rpc_status_pb2.Status(),
                        )
                    )
                )

            assert result.HasField('response')

            return rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeRangeResponse.FromString(result.response)
        raise RuntimeError("Unexpected result from Range")

    async def ReverseRange(
        self,
        context: IMPORT_reboot_aio_contexts.ReaderContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeReverseRangeRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeReverseRangeResponse:
        bytes_call = IMPORT_rbt_v1alpha1_nodejs_pb2.TrampolineCall(
            kind=IMPORT_rbt_v1alpha1_nodejs_pb2.reader,
            context=IMPORT_rbt_v1alpha1_nodejs_pb2.Context(
                method='ReverseRange',
                state_id=context.state_id,
                state_type_name=context.state_type_name,
                caller_bearer_token=context.caller_bearer_token,
                cookie=context.cookie,
                app_internal=context.app_internal,
                auth=(
                    None if context.auth is None
                    else context.auth.to_proto_bytes()
                ),
            ),
            state=state.SerializeToString(),
            request=request.SerializeToString(),
        ).SerializeToString()

        cancelled: IMPORT_asyncio.Future[None] = IMPORT_asyncio.Future()

        bytes_result_future: IMPORT_typing.Optional[IMPORT_asyncio.Future[str]] = None

        try:
            bytes_result_future = self._trampoline(  # type: ignore[attr-defined]
                self._js_servicer_reference,
                context,
                cancelled,
                bytes_call,
            )
            # NOTE: we need to `asyncio.shield` so that we can still
            # correctly wait for this future to complete even if we
            # are cancelled.
            assert bytes_result_future is not None
            bytes_result = await IMPORT_asyncio.shield(bytes_result_future)
        except IMPORT_asyncio.CancelledError:
            cancelled.set_result(None)

            # NOTE: unlike for a `writer` or `transaction`, we DO NOT
            # _need_ to wait for `bytes_result_future` because this is a
            # reader and we can execute multiple readers
            # simultaneously. That being said, we still want to give
            # good feedback that the RPC has been cancelled, and there
            # is no harm waiting because other readers can still be
            # called.
            if bytes_result_future is not None:
                await self._wait_for_cancelled(
                    bytes_result_future,
                    'Node.ReverseRange',
                )

            raise
        except:
            # Make sure we cancel the `cancelled` future either if an
            # exception is thrown or if the result is reeturned so
            # that we don't keep around resources related to it that
            # might cause us to run out of memory or worse, keep Node
            # from exiting because it is waiting for Python.
            cancelled.cancel()
            raise
        else:
            cancelled.cancel()

            result = IMPORT_rbt_v1alpha1_nodejs_pb2.TrampolineResult.FromString(
                bytes_result
            )

            if result.HasField('status_json'):
                raise (
                    Node
                    .ReverseRangeAborted
                    .from_status(
                        IMPORT_google_protobuf_json_format.Parse(
                            result.status_json,
                            IMPORT_google_rpc_status_pb2.Status(),
                        )
                    )
                )

            assert result.HasField('response')

            return rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeReverseRangeResponse.FromString(result.response)
        raise RuntimeError("Unexpected result from ReverseRange")

    async def Stringify(
        self,
        context: IMPORT_reboot_aio_contexts.ReaderContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.Node,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeStringifyRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeStringifyResponse:
        bytes_call = IMPORT_rbt_v1alpha1_nodejs_pb2.TrampolineCall(
            kind=IMPORT_rbt_v1alpha1_nodejs_pb2.reader,
            context=IMPORT_rbt_v1alpha1_nodejs_pb2.Context(
                method='Stringify',
                state_id=context.state_id,
                state_type_name=context.state_type_name,
                caller_bearer_token=context.caller_bearer_token,
                cookie=context.cookie,
                app_internal=context.app_internal,
                auth=(
                    None if context.auth is None
                    else context.auth.to_proto_bytes()
                ),
            ),
            state=state.SerializeToString(),
            request=request.SerializeToString(),
        ).SerializeToString()

        cancelled: IMPORT_asyncio.Future[None] = IMPORT_asyncio.Future()

        bytes_result_future: IMPORT_typing.Optional[IMPORT_asyncio.Future[str]] = None

        try:
            bytes_result_future = self._trampoline(  # type: ignore[attr-defined]
                self._js_servicer_reference,
                context,
                cancelled,
                bytes_call,
            )
            # NOTE: we need to `asyncio.shield` so that we can still
            # correctly wait for this future to complete even if we
            # are cancelled.
            assert bytes_result_future is not None
            bytes_result = await IMPORT_asyncio.shield(bytes_result_future)
        except IMPORT_asyncio.CancelledError:
            cancelled.set_result(None)

            # NOTE: unlike for a `writer` or `transaction`, we DO NOT
            # _need_ to wait for `bytes_result_future` because this is a
            # reader and we can execute multiple readers
            # simultaneously. That being said, we still want to give
            # good feedback that the RPC has been cancelled, and there
            # is no harm waiting because other readers can still be
            # called.
            if bytes_result_future is not None:
                await self._wait_for_cancelled(
                    bytes_result_future,
                    'Node.Stringify',
                )

            raise
        except:
            # Make sure we cancel the `cancelled` future either if an
            # exception is thrown or if the result is reeturned so
            # that we don't keep around resources related to it that
            # might cause us to run out of memory or worse, keep Node
            # from exiting because it is waiting for Python.
            cancelled.cancel()
            raise
        else:
            cancelled.cancel()

            result = IMPORT_rbt_v1alpha1_nodejs_pb2.TrampolineResult.FromString(
                bytes_result
            )

            if result.HasField('status_json'):
                raise (
                    Node
                    .StringifyAborted
                    .from_status(
                        IMPORT_google_protobuf_json_format.Parse(
                            result.status_json,
                            IMPORT_google_rpc_status_pb2.Status(),
                        )
                    )
                )

            assert result.HasField('response')

            return rbt.std.collections.ordered_map.v1.ordered_map_pb2.NodeStringifyResponse.FromString(result.response)
        raise RuntimeError("Unexpected result from Stringify")


class OrderedMapServicerNodeAdaptor(OrderedMap.singleton.Servicer):

    async def _wait_for_cancelled(
        self,
        future: IMPORT_asyncio.Future,
        method: str,
    ):
        while True:
            done, pending = await IMPORT_asyncio.wait(
                [future],
                timeout=5,  # seconds
            )
            # Check if we've timed out and log a warning that their
            # call has been cancelled but it is still running.
            if len(done) == 0:
                logger.warning(
                    f"Call to method '{method}' has been cancelled by the caller, "
                    "BUT WE ARE STILL WAITING for it complete. You can use the promise "
                    "`context.cancelled` to check if the caller has cancelled so you "
                    "don't do unnecessary work or wait for something that may never occur."
                )
                continue
            break

        # Now need to actually `await` the future so that we don't
        # have an unretrieved exception that gets logged.
        #
        # NOTE: this will raise an exception if the method raised even
        # though the call has already been cancelled but it makes it
        # more clear that the method raised so that is why we're not
        # catching and swallowing any exception.
        await future

    def __init__(self):
        self._js_servicer_reference = self._construct_js_servicer()  # type: ignore[attr-defined]

    def authorizer(self) -> IMPORT_typing.Optional[IMPORT_rebootdev.aio.auth.authorizers.Authorizer]:
        return self._construct_authorizer(self._js_servicer_reference)  # type: ignore[attr-defined]

    async def _read(
        self,
        context: IMPORT_reboot_aio_contexts.WorkflowContext,
        json_options: str,
    ) -> str:
        options = IMPORT_json.loads(json_options)

        alias = options.get('alias')

        assert 'how' in options
        how = options['how']
        if how == IMPORT_reboot_aio_workflows.ALWAYS:
            assert alias is None
            return IMPORT_google_protobuf_json_format.MessageToJson(
                await super().state.always().read(context)
            )

        assert how in [
            IMPORT_reboot_aio_workflows.PER_WORKFLOW,
            IMPORT_reboot_aio_workflows.PER_ITERATION,
        ]

        return IMPORT_google_protobuf_json_format.MessageToJson(
            await (
                super().state.per_workflow(alias)
                if how == IMPORT_reboot_aio_workflows.PER_WORKFLOW
                else super().state.per_iteration(alias)
            ).read(context)
        )

    async def _write(
        self,
        context: IMPORT_reboot_aio_contexts.WorkflowContext,
        writer: IMPORT_typing.Callable[[str], IMPORT_typing.Awaitable[str]],
        json_options: str,
    ) -> str:

        async def _writer(state: IMPORT_google_protobuf_message.Message):
            with IMPORT_reboot_aio_tracing.span(
                state_name=f"{context.state_type_name}('{context.state_id}')",
                span_name="_writer on NodeAdaptor",
                # The naming above matches Python, but not TypeScript.
                python_specific=True,
                level=IMPORT_reboot_aio_tracing.TraceLevel.CUSTOMER,
            ):
                json_result_state = await writer(
                    IMPORT_google_protobuf_json_format.MessageToJson(state)
                )

                with IMPORT_reboot_aio_tracing.span(
                    state_name=f"{context.state_type_name}('{context.state_id}')",
                    span_name="_write - State Copy on NodeAdaptor",
                    # The naming above matches Python, but not TypeScript.
                    python_specific=True,
                    level=IMPORT_reboot_aio_tracing.TraceLevel.CUSTOMER,
                ):
                    result_state = IMPORT_json.loads(json_result_state)

                    state.CopyFrom(
                        IMPORT_google_protobuf_json_format.ParseDict(
                            result_state['state'],
                            self.__state_type__(),
                        )
                    )

                    assert 'result' in result_state
                    result = result_state['result']
                    assert type(result) == str
                    return result

        options = IMPORT_json.loads(json_options)

        alias = options.get('alias')

        assert 'how' in options
        how = options['how']

        if how == IMPORT_reboot_aio_workflows.ALWAYS:
            assert alias is None
            return await super().state.always().write(
                context,
                _writer,
            )

        assert how in [
            IMPORT_reboot_aio_workflows.PER_WORKFLOW,
            IMPORT_reboot_aio_workflows.PER_ITERATION,
        ]

        return await (
            super().state.per_workflow(alias)
            if how == IMPORT_reboot_aio_workflows.PER_WORKFLOW
            else super().state.per_iteration(alias)
        ).write(context, _writer, type=str)

    # OrderedMap specific methods:
    async def Create(
        self,
        context: IMPORT_reboot_aio_contexts.TransactionContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapCreateRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapCreateResponse:
        with IMPORT_reboot_aio_tracing.span(
                state_name=f"{context.state_type_name}('{context.state_id}')",
                span_name="NodeAdaptor Create",
                # The naming above matches Python, but not TypeScript.
                python_specific=True,
                level=IMPORT_reboot_aio_tracing.TraceLevel.CUSTOMER,
        ):
            with IMPORT_reboot_aio_tracing.span(
                state_name=f"{context.state_type_name}('{context.state_id}')",
                span_name="Create and serialize `TrampolineCall`",
                # The naming above matches Python, but not TypeScript.
                python_specific=True,
                level=IMPORT_reboot_aio_tracing.TraceLevel.CUSTOMER,
            ):
                bytes_call = IMPORT_rbt_v1alpha1_nodejs_pb2.TrampolineCall(
                    kind=IMPORT_rbt_v1alpha1_nodejs_pb2.transaction,
                    context=IMPORT_rbt_v1alpha1_nodejs_pb2.Context(
                        method='Create',
                        state_id=context.state_id,
                        state_type_name=context.state_type_name,
                        caller_bearer_token=context.caller_bearer_token,
                        cookie=context.cookie,
                        app_internal=context.app_internal,
                        auth=(
                            None if context.auth is None
                            else context.auth.to_proto_bytes()
                        ),
                    ),
                    state=state.SerializeToString(),
                    request=request.SerializeToString(),
                ).SerializeToString()

            cancelled: IMPORT_asyncio.Future[None] = IMPORT_asyncio.Future()

            bytes_result_future: IMPORT_typing.Optional[IMPORT_asyncio.Future[str]] = None

            try:
                with IMPORT_reboot_aio_tracing.span(
                    state_name=f"{context.state_type_name}('{context.state_id}')",
                    span_name="trampoline",
                    # The naming above matches Python, but not TypeScript.
                    python_specific=True,
                    level=IMPORT_reboot_aio_tracing.TraceLevel.CUSTOMER,
                ):
                    bytes_result_future = self._trampoline(  # type: ignore[attr-defined]
                        self._js_servicer_reference,
                        context,
                        cancelled,
                        bytes_call,
                    )
                    # NOTE: we need to `asyncio.shield` so that we can still
                    # correctly wait for this future to complete even if we
                    # are cancelled.
                    assert bytes_result_future is not None
                    bytes_result = await IMPORT_asyncio.shield(bytes_result_future)
            except IMPORT_asyncio.CancelledError:
                cancelled.set_result(None)

                # NOTE: we MUST wait for `bytes_result_future` because this
                # is a `writer` or `transaction` and we CAN NOT execute
                # multiple simultaneously.
                if bytes_result_future is not None:
                    await self._wait_for_cancelled(
                        bytes_result_future,
                        'OrderedMap.Create',
                    )

                raise
            except:
                # Make sure we cancel the `cancelled` future either if an
                # exception is thrown or if the result is reeturned so
                # that we don't keep around resources related to it that
                # might cause us to run out of memory or worse, keep Node
                # from exiting because it is waiting for Python.
                cancelled.cancel()
                raise
            else:
                cancelled.cancel()

                with IMPORT_reboot_aio_tracing.span(
                    state_name=f"{context.state_type_name}('{context.state_id}')",
                    span_name="result ParseFromString",
                    # The naming above matches Python, but not TypeScript.
                    python_specific=True,
                    level=IMPORT_reboot_aio_tracing.TraceLevel.CUSTOMER,
                ):
                    result = IMPORT_rbt_v1alpha1_nodejs_pb2.TrampolineResult.FromString(
                        bytes_result
                    )

                if result.HasField('state'):
                    with IMPORT_reboot_aio_tracing.span(
                        state_name=f"{context.state_type_name}('{context.state_id}')",
                        span_name="state ParseFromString",
                        # The naming above matches Python, but not TypeScript.
                        python_specific=True,
                        level=IMPORT_reboot_aio_tracing.TraceLevel.CUSTOMER,
                    ):
                        state.CopyFrom(
                            rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap.FromString(
                                result.state
                            )
                        )

                if result.HasField('status_json'):
                    raise (
                        OrderedMap
                        .CreateAborted
                        .from_status(
                            IMPORT_google_protobuf_json_format.Parse(
                                result.status_json,
                                IMPORT_google_rpc_status_pb2.Status(),
                            )
                        )
                    )

                assert result.HasField('response')

                with IMPORT_reboot_aio_tracing.span(
                    state_name=f"{context.state_type_name}('{context.state_id}')",
                    span_name="response ParseFromString",
                ):
                    return rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapCreateResponse.FromString(result.response)
        raise RuntimeError("Unexpected result from Create")

    async def Search(
        self,
        context: IMPORT_reboot_aio_contexts.ReaderContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapSearchRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapSearchResponse:
        bytes_call = IMPORT_rbt_v1alpha1_nodejs_pb2.TrampolineCall(
            kind=IMPORT_rbt_v1alpha1_nodejs_pb2.reader,
            context=IMPORT_rbt_v1alpha1_nodejs_pb2.Context(
                method='Search',
                state_id=context.state_id,
                state_type_name=context.state_type_name,
                caller_bearer_token=context.caller_bearer_token,
                cookie=context.cookie,
                app_internal=context.app_internal,
                auth=(
                    None if context.auth is None
                    else context.auth.to_proto_bytes()
                ),
            ),
            state=state.SerializeToString(),
            request=request.SerializeToString(),
        ).SerializeToString()

        cancelled: IMPORT_asyncio.Future[None] = IMPORT_asyncio.Future()

        bytes_result_future: IMPORT_typing.Optional[IMPORT_asyncio.Future[str]] = None

        try:
            bytes_result_future = self._trampoline(  # type: ignore[attr-defined]
                self._js_servicer_reference,
                context,
                cancelled,
                bytes_call,
            )
            # NOTE: we need to `asyncio.shield` so that we can still
            # correctly wait for this future to complete even if we
            # are cancelled.
            assert bytes_result_future is not None
            bytes_result = await IMPORT_asyncio.shield(bytes_result_future)
        except IMPORT_asyncio.CancelledError:
            cancelled.set_result(None)

            # NOTE: unlike for a `writer` or `transaction`, we DO NOT
            # _need_ to wait for `bytes_result_future` because this is a
            # reader and we can execute multiple readers
            # simultaneously. That being said, we still want to give
            # good feedback that the RPC has been cancelled, and there
            # is no harm waiting because other readers can still be
            # called.
            if bytes_result_future is not None:
                await self._wait_for_cancelled(
                    bytes_result_future,
                    'OrderedMap.Search',
                )

            raise
        except:
            # Make sure we cancel the `cancelled` future either if an
            # exception is thrown or if the result is reeturned so
            # that we don't keep around resources related to it that
            # might cause us to run out of memory or worse, keep Node
            # from exiting because it is waiting for Python.
            cancelled.cancel()
            raise
        else:
            cancelled.cancel()

            result = IMPORT_rbt_v1alpha1_nodejs_pb2.TrampolineResult.FromString(
                bytes_result
            )

            if result.HasField('status_json'):
                raise (
                    OrderedMap
                    .SearchAborted
                    .from_status(
                        IMPORT_google_protobuf_json_format.Parse(
                            result.status_json,
                            IMPORT_google_rpc_status_pb2.Status(),
                        )
                    )
                )

            assert result.HasField('response')

            return rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapSearchResponse.FromString(result.response)
        raise RuntimeError("Unexpected result from Search")

    async def Insert(
        self,
        context: IMPORT_reboot_aio_contexts.TransactionContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapInsertRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapInsertResponse:
        with IMPORT_reboot_aio_tracing.span(
                state_name=f"{context.state_type_name}('{context.state_id}')",
                span_name="NodeAdaptor Insert",
                # The naming above matches Python, but not TypeScript.
                python_specific=True,
                level=IMPORT_reboot_aio_tracing.TraceLevel.CUSTOMER,
        ):
            with IMPORT_reboot_aio_tracing.span(
                state_name=f"{context.state_type_name}('{context.state_id}')",
                span_name="Create and serialize `TrampolineCall`",
                # The naming above matches Python, but not TypeScript.
                python_specific=True,
                level=IMPORT_reboot_aio_tracing.TraceLevel.CUSTOMER,
            ):
                bytes_call = IMPORT_rbt_v1alpha1_nodejs_pb2.TrampolineCall(
                    kind=IMPORT_rbt_v1alpha1_nodejs_pb2.transaction,
                    context=IMPORT_rbt_v1alpha1_nodejs_pb2.Context(
                        method='Insert',
                        state_id=context.state_id,
                        state_type_name=context.state_type_name,
                        caller_bearer_token=context.caller_bearer_token,
                        cookie=context.cookie,
                        app_internal=context.app_internal,
                        auth=(
                            None if context.auth is None
                            else context.auth.to_proto_bytes()
                        ),
                    ),
                    state=state.SerializeToString(),
                    request=request.SerializeToString(),
                ).SerializeToString()

            cancelled: IMPORT_asyncio.Future[None] = IMPORT_asyncio.Future()

            bytes_result_future: IMPORT_typing.Optional[IMPORT_asyncio.Future[str]] = None

            try:
                with IMPORT_reboot_aio_tracing.span(
                    state_name=f"{context.state_type_name}('{context.state_id}')",
                    span_name="trampoline",
                    # The naming above matches Python, but not TypeScript.
                    python_specific=True,
                    level=IMPORT_reboot_aio_tracing.TraceLevel.CUSTOMER,
                ):
                    bytes_result_future = self._trampoline(  # type: ignore[attr-defined]
                        self._js_servicer_reference,
                        context,
                        cancelled,
                        bytes_call,
                    )
                    # NOTE: we need to `asyncio.shield` so that we can still
                    # correctly wait for this future to complete even if we
                    # are cancelled.
                    assert bytes_result_future is not None
                    bytes_result = await IMPORT_asyncio.shield(bytes_result_future)
            except IMPORT_asyncio.CancelledError:
                cancelled.set_result(None)

                # NOTE: we MUST wait for `bytes_result_future` because this
                # is a `writer` or `transaction` and we CAN NOT execute
                # multiple simultaneously.
                if bytes_result_future is not None:
                    await self._wait_for_cancelled(
                        bytes_result_future,
                        'OrderedMap.Insert',
                    )

                raise
            except:
                # Make sure we cancel the `cancelled` future either if an
                # exception is thrown or if the result is reeturned so
                # that we don't keep around resources related to it that
                # might cause us to run out of memory or worse, keep Node
                # from exiting because it is waiting for Python.
                cancelled.cancel()
                raise
            else:
                cancelled.cancel()

                with IMPORT_reboot_aio_tracing.span(
                    state_name=f"{context.state_type_name}('{context.state_id}')",
                    span_name="result ParseFromString",
                    # The naming above matches Python, but not TypeScript.
                    python_specific=True,
                    level=IMPORT_reboot_aio_tracing.TraceLevel.CUSTOMER,
                ):
                    result = IMPORT_rbt_v1alpha1_nodejs_pb2.TrampolineResult.FromString(
                        bytes_result
                    )

                if result.HasField('state'):
                    with IMPORT_reboot_aio_tracing.span(
                        state_name=f"{context.state_type_name}('{context.state_id}')",
                        span_name="state ParseFromString",
                        # The naming above matches Python, but not TypeScript.
                        python_specific=True,
                        level=IMPORT_reboot_aio_tracing.TraceLevel.CUSTOMER,
                    ):
                        state.CopyFrom(
                            rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap.FromString(
                                result.state
                            )
                        )

                if result.HasField('status_json'):
                    raise (
                        OrderedMap
                        .InsertAborted
                        .from_status(
                            IMPORT_google_protobuf_json_format.Parse(
                                result.status_json,
                                IMPORT_google_rpc_status_pb2.Status(),
                            )
                        )
                    )

                assert result.HasField('response')

                with IMPORT_reboot_aio_tracing.span(
                    state_name=f"{context.state_type_name}('{context.state_id}')",
                    span_name="response ParseFromString",
                ):
                    return rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapInsertResponse.FromString(result.response)
        raise RuntimeError("Unexpected result from Insert")

    async def Remove(
        self,
        context: IMPORT_reboot_aio_contexts.TransactionContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRemoveRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRemoveResponse:
        with IMPORT_reboot_aio_tracing.span(
                state_name=f"{context.state_type_name}('{context.state_id}')",
                span_name="NodeAdaptor Remove",
                # The naming above matches Python, but not TypeScript.
                python_specific=True,
                level=IMPORT_reboot_aio_tracing.TraceLevel.CUSTOMER,
        ):
            with IMPORT_reboot_aio_tracing.span(
                state_name=f"{context.state_type_name}('{context.state_id}')",
                span_name="Create and serialize `TrampolineCall`",
                # The naming above matches Python, but not TypeScript.
                python_specific=True,
                level=IMPORT_reboot_aio_tracing.TraceLevel.CUSTOMER,
            ):
                bytes_call = IMPORT_rbt_v1alpha1_nodejs_pb2.TrampolineCall(
                    kind=IMPORT_rbt_v1alpha1_nodejs_pb2.transaction,
                    context=IMPORT_rbt_v1alpha1_nodejs_pb2.Context(
                        method='Remove',
                        state_id=context.state_id,
                        state_type_name=context.state_type_name,
                        caller_bearer_token=context.caller_bearer_token,
                        cookie=context.cookie,
                        app_internal=context.app_internal,
                        auth=(
                            None if context.auth is None
                            else context.auth.to_proto_bytes()
                        ),
                    ),
                    state=state.SerializeToString(),
                    request=request.SerializeToString(),
                ).SerializeToString()

            cancelled: IMPORT_asyncio.Future[None] = IMPORT_asyncio.Future()

            bytes_result_future: IMPORT_typing.Optional[IMPORT_asyncio.Future[str]] = None

            try:
                with IMPORT_reboot_aio_tracing.span(
                    state_name=f"{context.state_type_name}('{context.state_id}')",
                    span_name="trampoline",
                    # The naming above matches Python, but not TypeScript.
                    python_specific=True,
                    level=IMPORT_reboot_aio_tracing.TraceLevel.CUSTOMER,
                ):
                    bytes_result_future = self._trampoline(  # type: ignore[attr-defined]
                        self._js_servicer_reference,
                        context,
                        cancelled,
                        bytes_call,
                    )
                    # NOTE: we need to `asyncio.shield` so that we can still
                    # correctly wait for this future to complete even if we
                    # are cancelled.
                    assert bytes_result_future is not None
                    bytes_result = await IMPORT_asyncio.shield(bytes_result_future)
            except IMPORT_asyncio.CancelledError:
                cancelled.set_result(None)

                # NOTE: we MUST wait for `bytes_result_future` because this
                # is a `writer` or `transaction` and we CAN NOT execute
                # multiple simultaneously.
                if bytes_result_future is not None:
                    await self._wait_for_cancelled(
                        bytes_result_future,
                        'OrderedMap.Remove',
                    )

                raise
            except:
                # Make sure we cancel the `cancelled` future either if an
                # exception is thrown or if the result is reeturned so
                # that we don't keep around resources related to it that
                # might cause us to run out of memory or worse, keep Node
                # from exiting because it is waiting for Python.
                cancelled.cancel()
                raise
            else:
                cancelled.cancel()

                with IMPORT_reboot_aio_tracing.span(
                    state_name=f"{context.state_type_name}('{context.state_id}')",
                    span_name="result ParseFromString",
                    # The naming above matches Python, but not TypeScript.
                    python_specific=True,
                    level=IMPORT_reboot_aio_tracing.TraceLevel.CUSTOMER,
                ):
                    result = IMPORT_rbt_v1alpha1_nodejs_pb2.TrampolineResult.FromString(
                        bytes_result
                    )

                if result.HasField('state'):
                    with IMPORT_reboot_aio_tracing.span(
                        state_name=f"{context.state_type_name}('{context.state_id}')",
                        span_name="state ParseFromString",
                        # The naming above matches Python, but not TypeScript.
                        python_specific=True,
                        level=IMPORT_reboot_aio_tracing.TraceLevel.CUSTOMER,
                    ):
                        state.CopyFrom(
                            rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap.FromString(
                                result.state
                            )
                        )

                if result.HasField('status_json'):
                    raise (
                        OrderedMap
                        .RemoveAborted
                        .from_status(
                            IMPORT_google_protobuf_json_format.Parse(
                                result.status_json,
                                IMPORT_google_rpc_status_pb2.Status(),
                            )
                        )
                    )

                assert result.HasField('response')

                with IMPORT_reboot_aio_tracing.span(
                    state_name=f"{context.state_type_name}('{context.state_id}')",
                    span_name="response ParseFromString",
                ):
                    return rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRemoveResponse.FromString(result.response)
        raise RuntimeError("Unexpected result from Remove")

    async def Range(
        self,
        context: IMPORT_reboot_aio_contexts.ReaderContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRangeRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRangeResponse:
        bytes_call = IMPORT_rbt_v1alpha1_nodejs_pb2.TrampolineCall(
            kind=IMPORT_rbt_v1alpha1_nodejs_pb2.reader,
            context=IMPORT_rbt_v1alpha1_nodejs_pb2.Context(
                method='Range',
                state_id=context.state_id,
                state_type_name=context.state_type_name,
                caller_bearer_token=context.caller_bearer_token,
                cookie=context.cookie,
                app_internal=context.app_internal,
                auth=(
                    None if context.auth is None
                    else context.auth.to_proto_bytes()
                ),
            ),
            state=state.SerializeToString(),
            request=request.SerializeToString(),
        ).SerializeToString()

        cancelled: IMPORT_asyncio.Future[None] = IMPORT_asyncio.Future()

        bytes_result_future: IMPORT_typing.Optional[IMPORT_asyncio.Future[str]] = None

        try:
            bytes_result_future = self._trampoline(  # type: ignore[attr-defined]
                self._js_servicer_reference,
                context,
                cancelled,
                bytes_call,
            )
            # NOTE: we need to `asyncio.shield` so that we can still
            # correctly wait for this future to complete even if we
            # are cancelled.
            assert bytes_result_future is not None
            bytes_result = await IMPORT_asyncio.shield(bytes_result_future)
        except IMPORT_asyncio.CancelledError:
            cancelled.set_result(None)

            # NOTE: unlike for a `writer` or `transaction`, we DO NOT
            # _need_ to wait for `bytes_result_future` because this is a
            # reader and we can execute multiple readers
            # simultaneously. That being said, we still want to give
            # good feedback that the RPC has been cancelled, and there
            # is no harm waiting because other readers can still be
            # called.
            if bytes_result_future is not None:
                await self._wait_for_cancelled(
                    bytes_result_future,
                    'OrderedMap.Range',
                )

            raise
        except:
            # Make sure we cancel the `cancelled` future either if an
            # exception is thrown or if the result is reeturned so
            # that we don't keep around resources related to it that
            # might cause us to run out of memory or worse, keep Node
            # from exiting because it is waiting for Python.
            cancelled.cancel()
            raise
        else:
            cancelled.cancel()

            result = IMPORT_rbt_v1alpha1_nodejs_pb2.TrampolineResult.FromString(
                bytes_result
            )

            if result.HasField('status_json'):
                raise (
                    OrderedMap
                    .RangeAborted
                    .from_status(
                        IMPORT_google_protobuf_json_format.Parse(
                            result.status_json,
                            IMPORT_google_rpc_status_pb2.Status(),
                        )
                    )
                )

            assert result.HasField('response')

            return rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapRangeResponse.FromString(result.response)
        raise RuntimeError("Unexpected result from Range")

    async def ReverseRange(
        self,
        context: IMPORT_reboot_aio_contexts.ReaderContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapReverseRangeRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapReverseRangeResponse:
        bytes_call = IMPORT_rbt_v1alpha1_nodejs_pb2.TrampolineCall(
            kind=IMPORT_rbt_v1alpha1_nodejs_pb2.reader,
            context=IMPORT_rbt_v1alpha1_nodejs_pb2.Context(
                method='ReverseRange',
                state_id=context.state_id,
                state_type_name=context.state_type_name,
                caller_bearer_token=context.caller_bearer_token,
                cookie=context.cookie,
                app_internal=context.app_internal,
                auth=(
                    None if context.auth is None
                    else context.auth.to_proto_bytes()
                ),
            ),
            state=state.SerializeToString(),
            request=request.SerializeToString(),
        ).SerializeToString()

        cancelled: IMPORT_asyncio.Future[None] = IMPORT_asyncio.Future()

        bytes_result_future: IMPORT_typing.Optional[IMPORT_asyncio.Future[str]] = None

        try:
            bytes_result_future = self._trampoline(  # type: ignore[attr-defined]
                self._js_servicer_reference,
                context,
                cancelled,
                bytes_call,
            )
            # NOTE: we need to `asyncio.shield` so that we can still
            # correctly wait for this future to complete even if we
            # are cancelled.
            assert bytes_result_future is not None
            bytes_result = await IMPORT_asyncio.shield(bytes_result_future)
        except IMPORT_asyncio.CancelledError:
            cancelled.set_result(None)

            # NOTE: unlike for a `writer` or `transaction`, we DO NOT
            # _need_ to wait for `bytes_result_future` because this is a
            # reader and we can execute multiple readers
            # simultaneously. That being said, we still want to give
            # good feedback that the RPC has been cancelled, and there
            # is no harm waiting because other readers can still be
            # called.
            if bytes_result_future is not None:
                await self._wait_for_cancelled(
                    bytes_result_future,
                    'OrderedMap.ReverseRange',
                )

            raise
        except:
            # Make sure we cancel the `cancelled` future either if an
            # exception is thrown or if the result is reeturned so
            # that we don't keep around resources related to it that
            # might cause us to run out of memory or worse, keep Node
            # from exiting because it is waiting for Python.
            cancelled.cancel()
            raise
        else:
            cancelled.cancel()

            result = IMPORT_rbt_v1alpha1_nodejs_pb2.TrampolineResult.FromString(
                bytes_result
            )

            if result.HasField('status_json'):
                raise (
                    OrderedMap
                    .ReverseRangeAborted
                    .from_status(
                        IMPORT_google_protobuf_json_format.Parse(
                            result.status_json,
                            IMPORT_google_rpc_status_pb2.Status(),
                        )
                    )
                )

            assert result.HasField('response')

            return rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapReverseRangeResponse.FromString(result.response)
        raise RuntimeError("Unexpected result from ReverseRange")

    async def Stringify(
        self,
        context: IMPORT_reboot_aio_contexts.ReaderContext,
        state: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMap,
        request: rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapStringifyRequest,
    ) -> rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapStringifyResponse:
        bytes_call = IMPORT_rbt_v1alpha1_nodejs_pb2.TrampolineCall(
            kind=IMPORT_rbt_v1alpha1_nodejs_pb2.reader,
            context=IMPORT_rbt_v1alpha1_nodejs_pb2.Context(
                method='Stringify',
                state_id=context.state_id,
                state_type_name=context.state_type_name,
                caller_bearer_token=context.caller_bearer_token,
                cookie=context.cookie,
                app_internal=context.app_internal,
                auth=(
                    None if context.auth is None
                    else context.auth.to_proto_bytes()
                ),
            ),
            state=state.SerializeToString(),
            request=request.SerializeToString(),
        ).SerializeToString()

        cancelled: IMPORT_asyncio.Future[None] = IMPORT_asyncio.Future()

        bytes_result_future: IMPORT_typing.Optional[IMPORT_asyncio.Future[str]] = None

        try:
            bytes_result_future = self._trampoline(  # type: ignore[attr-defined]
                self._js_servicer_reference,
                context,
                cancelled,
                bytes_call,
            )
            # NOTE: we need to `asyncio.shield` so that we can still
            # correctly wait for this future to complete even if we
            # are cancelled.
            assert bytes_result_future is not None
            bytes_result = await IMPORT_asyncio.shield(bytes_result_future)
        except IMPORT_asyncio.CancelledError:
            cancelled.set_result(None)

            # NOTE: unlike for a `writer` or `transaction`, we DO NOT
            # _need_ to wait for `bytes_result_future` because this is a
            # reader and we can execute multiple readers
            # simultaneously. That being said, we still want to give
            # good feedback that the RPC has been cancelled, and there
            # is no harm waiting because other readers can still be
            # called.
            if bytes_result_future is not None:
                await self._wait_for_cancelled(
                    bytes_result_future,
                    'OrderedMap.Stringify',
                )

            raise
        except:
            # Make sure we cancel the `cancelled` future either if an
            # exception is thrown or if the result is reeturned so
            # that we don't keep around resources related to it that
            # might cause us to run out of memory or worse, keep Node
            # from exiting because it is waiting for Python.
            cancelled.cancel()
            raise
        else:
            cancelled.cancel()

            result = IMPORT_rbt_v1alpha1_nodejs_pb2.TrampolineResult.FromString(
                bytes_result
            )

            if result.HasField('status_json'):
                raise (
                    OrderedMap
                    .StringifyAborted
                    .from_status(
                        IMPORT_google_protobuf_json_format.Parse(
                            result.status_json,
                            IMPORT_google_rpc_status_pb2.Status(),
                        )
                    )
                )

            assert result.HasField('response')

            return rbt.std.collections.ordered_map.v1.ordered_map_pb2.OrderedMapStringifyResponse.FromString(result.response)
        raise RuntimeError("Unexpected result from Stringify")



############################ Reference Node adapters ############################
# Used by Node.js WeakReference implementations to access Python code and
# vice-versa. Relevant to clients.

class NodeWeakReferenceNodeAdaptor(Node.WeakReference[Node.WeakReference._Schedule]):

    async def _call(  # type: ignore[override]
        self,
        *,
        callable: IMPORT_typing.Callable[[IMPORT_google_protobuf_message.Message], IMPORT_typing.Awaitable],
        aborted_type: type[IMPORT_rebootdev.aio.aborted.Aborted],
        request_type: type[IMPORT_google_protobuf_message.Message],
        json_request: str,
    ) -> str:
        request = request_type()

        try:
            IMPORT_google_protobuf_json_format.Parse(json_request, request)
            response = await callable(request)
        except IMPORT_google_protobuf_json_format.ParseError as parse_error:
            aborted_error = IMPORT_rebootdev.aio.aborted.SystemAborted(
                IMPORT_rbt_v1alpha1.errors_pb2.Unknown(),
                message=f"{parse_error}; "
                       "This is usually caused by a deeply nested protobuf message, which is not supported by protobuf.\n"
                        "See the limits here: https://protobuf.dev/programming-guides/proto-limits/"
                )

            return IMPORT_json.dumps(
                {
                    'status': IMPORT_google_protobuf_json_format.MessageToDict(
                        aborted_error.to_status()
                    )
                }
            )
        except BaseException as exception:
            if isinstance(exception, aborted_type):
                return IMPORT_json.dumps(
                    {
                        'status': IMPORT_google_protobuf_json_format.MessageToDict(
                            exception.to_status()
                        )
                    }
                )
            raise
        else:
            return IMPORT_json.dumps(
                {
                    'response': IMPORT_google_protobuf_json_format.MessageToDict(
                        response
                    )
                }
            )

    async def _schedule(  # type: ignore[override]
        self,
        *,
        method: str,
        context: IMPORT_reboot_aio_contexts.WriterContext | IMPORT_reboot_aio_contexts.TransactionContext | IMPORT_reboot_aio_contexts.WorkflowContext | IMPORT_reboot_aio_external.ExternalContext,
        schedule: IMPORT_reboot_time_DateTimeWithTimeZone,
        idempotency: IMPORT_typing.Optional[IMPORT_reboot_aio_idempotency.Idempotency],
        request_type: type[IMPORT_google_protobuf_message.Message],
        json_request: str,
    ) -> str:
        request = request_type()

        IMPORT_google_protobuf_json_format.Parse(json_request, request)

        if isinstance(context, IMPORT_reboot_aio_contexts.WriterContext):
            task = await getattr(
                NodeServicerTasks(
                    context=context,
                    state_ref=context._state_ref,
                ),
                method,
            )(request, schedule=schedule)

            return IMPORT_json.dumps(
                {
                    'taskId': IMPORT_google_protobuf_json_format.MessageToDict(
                        task.task_id
                    )
                }
            )

        # Add scheduling information to the metadata.
        metadata: IMPORT_reboot_aio_types.GrpcMetadata = (
            (IMPORT_reboot_aio_headers.TASK_SCHEDULE, schedule.isoformat()),
        )

        task_id = await getattr(super()._tasks(context), method)(
            request,
            idempotency=idempotency,
            metadata=metadata,
        )

        return IMPORT_json.dumps(
            {
                'taskId': IMPORT_google_protobuf_json_format.MessageToDict(task_id)
            }
        )

    async def _reader(  # type: ignore[override]
        self,
        method: str,
        context: IMPORT_reboot_aio_contexts.ReaderContext | IMPORT_reboot_aio_contexts.WriterContext | IMPORT_reboot_aio_contexts.TransactionContext | IMPORT_reboot_aio_contexts.WorkflowContext | IMPORT_reboot_aio_external.ExternalContext,
        request_type: type[IMPORT_google_protobuf_message.Message],
        json_request: str,
        json_options: str,
    ) -> str:
        options = IMPORT_json.loads(json_options)

        idempotency: IMPORT_typing.Optional[IMPORT_reboot_aio_idempotency.Idempotency] = None

        if 'idempotency' in options:
            idempotency = IMPORT_reboot_aio_contexts.Context.idempotency(
                alias=options['idempotency'].get('alias'),
                key=options['idempotency'].get('key'),
                each_iteration=options['idempotency'].get('eachIteration'),
                generated=options['idempotency'].get('generated', False),
            )

        method_handle = IMPORT_functools.partial(
            getattr(super()._reader(context), method),
            bearer_token=options.get("bearerToken"),
            idempotency=idempotency,
        )
        return await self._call(
            callable=method_handle,
            aborted_type=getattr(
                Node, method + 'Aborted'
            ),
            request_type=request_type,
            json_request=json_request,
        )

    async def _writer(  # type: ignore[override]
        self,
        method: str,
        context: IMPORT_reboot_aio_contexts.TransactionContext | IMPORT_reboot_aio_contexts.WorkflowContext | IMPORT_reboot_aio_external.ExternalContext,
        request_type: type[IMPORT_google_protobuf_message.Message],
        json_request: str,
        json_options: str,
    ) -> str:
        options = IMPORT_json.loads(json_options)

        idempotency: IMPORT_typing.Optional[IMPORT_reboot_aio_idempotency.Idempotency] = None

        if 'idempotency' in options:
            idempotency = IMPORT_reboot_aio_contexts.Context.idempotency(
                alias=options['idempotency'].get('alias'),
                key=options['idempotency'].get('key'),
                each_iteration=options['idempotency'].get('eachIteration'),
                generated=options['idempotency'].get('generated', False),
            )

        if 'schedule' in options:
            when = IMPORT_google_protobuf_timestamp_pb2.Timestamp()
            when.FromJsonString(options['schedule']['when'])
            return await self._schedule(
                method=method,
                context=context,
                schedule=IMPORT_reboot_time_DateTimeWithTimeZone.from_protobuf_timestamp(when),
                idempotency=idempotency,
                request_type=request_type,
                json_request=json_request,
            )

        method_handle = IMPORT_functools.partial(
            getattr(super()._writer(context), method),
            idempotency=idempotency,
            bearer_token=options.get("bearerToken"),
        )
        return await self._call(
            callable=method_handle,
            aborted_type=getattr(
                Node, method + 'Aborted'
            ),
            request_type=request_type,
            json_request=json_request,
        )

    async def _transaction(  # type: ignore[override]
        self,
        method: str,
        context: IMPORT_reboot_aio_contexts.TransactionContext | IMPORT_reboot_aio_contexts.WorkflowContext | IMPORT_reboot_aio_external.ExternalContext,
        request_type: type[IMPORT_google_protobuf_message.Message],
        json_request: str,
        json_options: str,
    ) -> str:
        options = IMPORT_json.loads(json_options)

        idempotency: IMPORT_typing.Optional[IMPORT_reboot_aio_idempotency.Idempotency] = None

        if 'idempotency' in options:
            idempotency = IMPORT_reboot_aio_contexts.Context.idempotency(
                alias=options['idempotency'].get('alias'),
                key=options['idempotency'].get('key'),
                each_iteration=options['idempotency'].get('eachIteration'),
                generated=options['idempotency'].get('generated', False),
            )

        if 'schedule' in options:
            when = IMPORT_google_protobuf_timestamp_pb2.Timestamp()
            when.FromJsonString(options['schedule']['when'])
            return await self._schedule(
                method=method,
                context=context,
                schedule=IMPORT_reboot_time_DateTimeWithTimeZone.from_protobuf_timestamp(when),
                idempotency=idempotency,
                request_type=request_type,
                json_request=json_request,
            )

        method_handle = IMPORT_functools.partial(
            getattr(super()._workflow(context), method),
            idempotency=idempotency,
            bearer_token=options.get("bearerToken"),
        )
        return await self._call(
            callable=method_handle,
            aborted_type=getattr(
                Node, method + 'Aborted'
            ),
            request_type=request_type,
            json_request=json_request,
        )

    async def _workflow(  # type: ignore[override]
        self,
        method: str,
        context: IMPORT_reboot_aio_contexts.WriterContext | IMPORT_reboot_aio_contexts.TransactionContext | IMPORT_reboot_aio_contexts.WorkflowContext | IMPORT_reboot_aio_external.ExternalContext,
        request_type: type[IMPORT_google_protobuf_message.Message],
        json_request: str,
        json_options: str,
    ) -> str:
        options = IMPORT_json.loads(json_options)

        idempotency: IMPORT_typing.Optional[IMPORT_reboot_aio_idempotency.Idempotency] = None

        if 'idempotency' in options:
            idempotency = IMPORT_reboot_aio_contexts.Context.idempotency(
                alias=options['idempotency'].get('alias'),
                key=options['idempotency'].get('key'),
                each_iteration=options['idempotency'].get('eachIteration'),
                generated=options['idempotency'].get('generated', False),
            )

        assert 'schedule' in options

        when = IMPORT_google_protobuf_timestamp_pb2.Timestamp()
        when.FromJsonString(options['schedule']['when'])

        return await self._schedule(
            method=method,
            context=context,
            schedule=IMPORT_reboot_time_DateTimeWithTimeZone.from_protobuf_timestamp(when),
            idempotency=idempotency,
            request_type=request_type,
            json_request=json_request,
        )

class OrderedMapWeakReferenceNodeAdaptor(OrderedMap.WeakReference[OrderedMap.WeakReference._Schedule]):

    async def _call(  # type: ignore[override]
        self,
        *,
        callable: IMPORT_typing.Callable[[IMPORT_google_protobuf_message.Message], IMPORT_typing.Awaitable],
        aborted_type: type[IMPORT_rebootdev.aio.aborted.Aborted],
        request_type: type[IMPORT_google_protobuf_message.Message],
        json_request: str,
    ) -> str:
        request = request_type()

        try:
            IMPORT_google_protobuf_json_format.Parse(json_request, request)
            response = await callable(request)
        except IMPORT_google_protobuf_json_format.ParseError as parse_error:
            aborted_error = IMPORT_rebootdev.aio.aborted.SystemAborted(
                IMPORT_rbt_v1alpha1.errors_pb2.Unknown(),
                message=f"{parse_error}; "
                       "This is usually caused by a deeply nested protobuf message, which is not supported by protobuf.\n"
                        "See the limits here: https://protobuf.dev/programming-guides/proto-limits/"
                )

            return IMPORT_json.dumps(
                {
                    'status': IMPORT_google_protobuf_json_format.MessageToDict(
                        aborted_error.to_status()
                    )
                }
            )
        except BaseException as exception:
            if isinstance(exception, aborted_type):
                return IMPORT_json.dumps(
                    {
                        'status': IMPORT_google_protobuf_json_format.MessageToDict(
                            exception.to_status()
                        )
                    }
                )
            raise
        else:
            return IMPORT_json.dumps(
                {
                    'response': IMPORT_google_protobuf_json_format.MessageToDict(
                        response
                    )
                }
            )

    async def _schedule(  # type: ignore[override]
        self,
        *,
        method: str,
        context: IMPORT_reboot_aio_contexts.WriterContext | IMPORT_reboot_aio_contexts.TransactionContext | IMPORT_reboot_aio_contexts.WorkflowContext | IMPORT_reboot_aio_external.ExternalContext,
        schedule: IMPORT_reboot_time_DateTimeWithTimeZone,
        idempotency: IMPORT_typing.Optional[IMPORT_reboot_aio_idempotency.Idempotency],
        request_type: type[IMPORT_google_protobuf_message.Message],
        json_request: str,
    ) -> str:
        request = request_type()

        IMPORT_google_protobuf_json_format.Parse(json_request, request)

        if isinstance(context, IMPORT_reboot_aio_contexts.WriterContext):
            task = await getattr(
                OrderedMapServicerTasks(
                    context=context,
                    state_ref=context._state_ref,
                ),
                method,
            )(request, schedule=schedule)

            return IMPORT_json.dumps(
                {
                    'taskId': IMPORT_google_protobuf_json_format.MessageToDict(
                        task.task_id
                    )
                }
            )

        # Add scheduling information to the metadata.
        metadata: IMPORT_reboot_aio_types.GrpcMetadata = (
            (IMPORT_reboot_aio_headers.TASK_SCHEDULE, schedule.isoformat()),
        )

        task_id = await getattr(super()._tasks(context), method)(
            request,
            idempotency=idempotency,
            metadata=metadata,
        )

        return IMPORT_json.dumps(
            {
                'taskId': IMPORT_google_protobuf_json_format.MessageToDict(task_id)
            }
        )

    async def _reader(  # type: ignore[override]
        self,
        method: str,
        context: IMPORT_reboot_aio_contexts.ReaderContext | IMPORT_reboot_aio_contexts.WriterContext | IMPORT_reboot_aio_contexts.TransactionContext | IMPORT_reboot_aio_contexts.WorkflowContext | IMPORT_reboot_aio_external.ExternalContext,
        request_type: type[IMPORT_google_protobuf_message.Message],
        json_request: str,
        json_options: str,
    ) -> str:
        options = IMPORT_json.loads(json_options)

        idempotency: IMPORT_typing.Optional[IMPORT_reboot_aio_idempotency.Idempotency] = None

        if 'idempotency' in options:
            idempotency = IMPORT_reboot_aio_contexts.Context.idempotency(
                alias=options['idempotency'].get('alias'),
                key=options['idempotency'].get('key'),
                each_iteration=options['idempotency'].get('eachIteration'),
                generated=options['idempotency'].get('generated', False),
            )

        method_handle = IMPORT_functools.partial(
            getattr(super()._reader(context), method),
            bearer_token=options.get("bearerToken"),
            idempotency=idempotency,
        )
        return await self._call(
            callable=method_handle,
            aborted_type=getattr(
                OrderedMap, method + 'Aborted'
            ),
            request_type=request_type,
            json_request=json_request,
        )

    async def _writer(  # type: ignore[override]
        self,
        method: str,
        context: IMPORT_reboot_aio_contexts.TransactionContext | IMPORT_reboot_aio_contexts.WorkflowContext | IMPORT_reboot_aio_external.ExternalContext,
        request_type: type[IMPORT_google_protobuf_message.Message],
        json_request: str,
        json_options: str,
    ) -> str:
        options = IMPORT_json.loads(json_options)

        idempotency: IMPORT_typing.Optional[IMPORT_reboot_aio_idempotency.Idempotency] = None

        if 'idempotency' in options:
            idempotency = IMPORT_reboot_aio_contexts.Context.idempotency(
                alias=options['idempotency'].get('alias'),
                key=options['idempotency'].get('key'),
                each_iteration=options['idempotency'].get('eachIteration'),
                generated=options['idempotency'].get('generated', False),
            )

        if 'schedule' in options:
            when = IMPORT_google_protobuf_timestamp_pb2.Timestamp()
            when.FromJsonString(options['schedule']['when'])
            return await self._schedule(
                method=method,
                context=context,
                schedule=IMPORT_reboot_time_DateTimeWithTimeZone.from_protobuf_timestamp(when),
                idempotency=idempotency,
                request_type=request_type,
                json_request=json_request,
            )

        method_handle = IMPORT_functools.partial(
            getattr(super()._writer(context), method),
            idempotency=idempotency,
            bearer_token=options.get("bearerToken"),
        )
        return await self._call(
            callable=method_handle,
            aborted_type=getattr(
                OrderedMap, method + 'Aborted'
            ),
            request_type=request_type,
            json_request=json_request,
        )

    async def _transaction(  # type: ignore[override]
        self,
        method: str,
        context: IMPORT_reboot_aio_contexts.TransactionContext | IMPORT_reboot_aio_contexts.WorkflowContext | IMPORT_reboot_aio_external.ExternalContext,
        request_type: type[IMPORT_google_protobuf_message.Message],
        json_request: str,
        json_options: str,
    ) -> str:
        options = IMPORT_json.loads(json_options)

        idempotency: IMPORT_typing.Optional[IMPORT_reboot_aio_idempotency.Idempotency] = None

        if 'idempotency' in options:
            idempotency = IMPORT_reboot_aio_contexts.Context.idempotency(
                alias=options['idempotency'].get('alias'),
                key=options['idempotency'].get('key'),
                each_iteration=options['idempotency'].get('eachIteration'),
                generated=options['idempotency'].get('generated', False),
            )

        if 'schedule' in options:
            when = IMPORT_google_protobuf_timestamp_pb2.Timestamp()
            when.FromJsonString(options['schedule']['when'])
            return await self._schedule(
                method=method,
                context=context,
                schedule=IMPORT_reboot_time_DateTimeWithTimeZone.from_protobuf_timestamp(when),
                idempotency=idempotency,
                request_type=request_type,
                json_request=json_request,
            )

        method_handle = IMPORT_functools.partial(
            getattr(super()._workflow(context), method),
            idempotency=idempotency,
            bearer_token=options.get("bearerToken"),
        )
        return await self._call(
            callable=method_handle,
            aborted_type=getattr(
                OrderedMap, method + 'Aborted'
            ),
            request_type=request_type,
            json_request=json_request,
        )

    async def _workflow(  # type: ignore[override]
        self,
        method: str,
        context: IMPORT_reboot_aio_contexts.WriterContext | IMPORT_reboot_aio_contexts.TransactionContext | IMPORT_reboot_aio_contexts.WorkflowContext | IMPORT_reboot_aio_external.ExternalContext,
        request_type: type[IMPORT_google_protobuf_message.Message],
        json_request: str,
        json_options: str,
    ) -> str:
        options = IMPORT_json.loads(json_options)

        idempotency: IMPORT_typing.Optional[IMPORT_reboot_aio_idempotency.Idempotency] = None

        if 'idempotency' in options:
            idempotency = IMPORT_reboot_aio_contexts.Context.idempotency(
                alias=options['idempotency'].get('alias'),
                key=options['idempotency'].get('key'),
                each_iteration=options['idempotency'].get('eachIteration'),
                generated=options['idempotency'].get('generated', False),
            )

        assert 'schedule' in options

        when = IMPORT_google_protobuf_timestamp_pb2.Timestamp()
        when.FromJsonString(options['schedule']['when'])

        return await self._schedule(
            method=method,
            context=context,
            schedule=IMPORT_reboot_time_DateTimeWithTimeZone.from_protobuf_timestamp(when),
            idempotency=idempotency,
            request_type=request_type,
            json_request=json_request,
        )

# yapf: enable
