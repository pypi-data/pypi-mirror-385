{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: Corefunctionality for data preparation of sequential data for pytorch,\n",
    "  fastai models\n",
    "output-file: core.html\n",
    "title: Corefunctions\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp data.block\n",
    "#| default_cls_lvl 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from fastai.data.all import *\n",
    "from tsfast.data.core import *\n",
    "from tsfast.data.split import ParentSplitter, ApplyToDict, PercentageSplitter\n",
    "from tsfast.data.transforms import SeqNoiseInjection, Normalize,SeqSlice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dataloaders Creation\n",
    "A Datasets combines all implemented components on item level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def pad_sequence(batch,sorting = False):\n",
    "    '''collate_fn for padding of sequences of different lengths, use in before_batch of databunch, still quite slow'''\n",
    "    #takes list of tuples as input, returns list of tuples\n",
    "    sorted_batch = sorted(batch, key=lambda x: x[0].shape[0], reverse=True) if sorting else batch\n",
    "\n",
    "    pad_func = partial(torch.nn.utils.rnn.pad_sequence,batch_first=True)\n",
    "    padded_tensors = [pad_func([x[tup] for x in sorted_batch]) for tup in range(len(batch[0]))]\n",
    "    padded_list = [retain_types(tuple([tup[entry] for tup in padded_tensors]),batch[0]) for entry in range(len(batch))]\n",
    "    #retain types is important for decoding later back to source items\n",
    "#     import pdb; pdb.set_trace()\n",
    "    \n",
    "    return padded_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Low-Level with Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev.config import get_config\n",
    "from tsfast.data.core import CreateDict, ValidClmContains,DfHDFCreateWindows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_root = get_config().config_file.parent\n",
    "f_path = project_root / 'test_data/WienerHammerstein'\n",
    "hdf_files = get_files(f_path,extensions='.hdf5',recurse=True)\n",
    "tfm_src = CreateDict([ValidClmContains(['valid']),DfHDFCreateWindows(win_sz=100+1,stp_sz=10,clm='u')])\n",
    "src_dicts = tfm_src(hdf_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfm_src = CreateDict([ValidClmContains(['valid']),DfHDFCreateWindows(win_sz=100+1,stp_sz=10,clm='u')])\n",
    "src_dicts = tfm_src(hdf_files)\n",
    "\n",
    "tfms=[  [HDF2Sequence(['u','y']),SeqSlice(l_slc=1),toTensorSequencesInput],\n",
    "        [HDF2Sequence(['y']),SeqSlice(r_slc=-1),toTensorSequencesOutput]]\n",
    "splits = PercentageSplitter()([x['path'] for x in src_dicts])\n",
    "dsrc = Datasets(src_dicts,tfms=tfms,splits=splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "# dsrc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 100, 2])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db = dsrc.dataloaders(bs=128,after_batch=[SeqNoiseInjection(std=[1.1,0.01]),Normalize(axes=[0,1])],before_batch=pad_sequence)\n",
    "db.one_batch()[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Mid-Level with Datablock API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SequenceBlock(TransformBlock):\n",
    "    def __init__(self, seq_extract,padding=False):\n",
    "        return super().__init__(type_tfms=[seq_extract],\n",
    "                                batch_tfms=[Normalize(axes=[0,1])],\n",
    "                                dls_kwargs={} if not padding else {'before_batch': pad_sequence})\n",
    "\n",
    "    @classmethod\n",
    "    @delegates(HDF2Sequence, keep=True)\n",
    "    def from_hdf(cls, clm_names, seq_cls=TensorSequencesInput,padding=False, **kwargs):\n",
    "        return cls(HDF2Sequence(clm_names,to_cls=seq_cls,**kwargs), padding)\n",
    "\n",
    "    @classmethod\n",
    "    def from_numpy(cls, seq_cls=TensorSequencesInput,padding=False, **kwargs):\n",
    "        return cls(ToTensor(enc=seq_cls), padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = DataBlock(blocks=(SequenceBlock.from_hdf(['u','y'],TensorSequencesInput,padding=True,cached=None),\n",
    "                        SequenceBlock.from_hdf(['y'],TensorSequencesOutput,cached=None)),\n",
    "                get_items=tfm_src,\n",
    "                splitter=ApplyToDict(ParentSplitter()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = seq.dataloaders(hdf_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ScalarNormalize(DisplayedTransform):\n",
    "    def __init__(self, mean=None, std=None, axes=(0,)): store_attr()\n",
    "        \n",
    "    @classmethod\n",
    "    def from_stats(cls, mean, std, dim=1, ndim=4, cuda=True): return cls(*broadcast_vec(dim, ndim, mean, std, cuda=cuda))\n",
    "    \n",
    "    def setups(self, dl:DataLoader):\n",
    "        if self.mean is None or self.std is None:\n",
    "            b = dl.one_batch()\n",
    "            for x in b:\n",
    "                if isinstance(x,TensorScalarsInput):\n",
    "                    self.mean,self.std = x.mean(self.axes, keepdim=True),x.std(self.axes, keepdim=True)+1e-7\n",
    "                    return\n",
    "\n",
    "    def encodes(self, x:TensorScalarsInput): \n",
    "        if x.device != self.mean.device:\n",
    "            self.mean = self.mean.to(x.device)\n",
    "            self.std = self.std.to(x.device)\n",
    "        return (x-self.mean) / self.std\n",
    "    \n",
    "    def decodes(self, x:TensorScalarsInput):\n",
    "        if x.device != self.mean.device:\n",
    "            self.mean = self.mean.to(x.device)\n",
    "            self.std = self.std.to(x.device)\n",
    "        return (x*self.std + self.mean)\n",
    "\n",
    "class ScalarBlock(TransformBlock):\n",
    "    def __init__(self, scl_extract):\n",
    "        return super().__init__(type_tfms=[scl_extract],\n",
    "                                batch_tfms=[ScalarNormalize()])\n",
    "\n",
    "    @classmethod\n",
    "    @delegates(HDF_Attrs2Scalars, keep=True)\n",
    "    def from_hdf_attrs(cls, clm_names, scl_cls=TensorScalarsInput, **kwargs):\n",
    "        return cls(HDF_Attrs2Scalars(clm_names,to_cls=scl_cls,**kwargs))\n",
    "\n",
    "    @classmethod\n",
    "    @delegates(HDF_DS2Scalars, keep=True)\n",
    "    def from_hdf_ds(cls, clm_names, scl_cls=TensorScalarsInput, **kwargs):\n",
    "        return cls(HDF_DS2Scalars(clm_names,to_cls=scl_cls,**kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = DataBlock(blocks=(SequenceBlock.from_hdf(['u'],TensorSequencesInput),\n",
    "                        ScalarBlock.from_hdf_ds(['y'],TensorScalarsOutput)),\n",
    "                get_items=tfm_src,\n",
    "                splitter=ApplyToDict(ParentSplitter()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = seq.dataloaders(hdf_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "import nbdev\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
