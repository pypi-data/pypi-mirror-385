# FAI-RL: Foundation of AI - Reinforcement learning Library

A modular, production-ready library designed for **easy training, inference, and evaluation** of language models using reinforcement learning methods. Currently supports: 
- SFT (Supervised Fine-Tuning)
- DPO (Direct Preference Optimization)
- PPO (Proximal Policy Optimization)
- GRPO (Group Relative Preference Optimization)
- GSPO (Group Sequence Policy Optimization)

## ğŸš€ Quick Start

Get started with installation, training, inference, and evaluation in just a few commands:

### ğŸ“¦ Installation

```bash
pip install --extra-index-url https://download.pytorch.org/whl/cu118 FAI-RL
```
ğŸ“˜ PyPI: https://pypi.org/project/FAI-RL/


### Training

Train a model using SFT, DPO, PPO, GRPO, or GSPO:

```bash
# Single GPU training
fai-rl-train --recipe recipes/training/sft/llama3_3B_lora.yaml --num-gpus 1
```

ğŸ“– **[See detailed Training Guide â†’](./trainers/README.md)**

### Inference

Generate responses from your trained models:

```bash
# Run inference with debug mode
fai-rl-inference --recipe recipes/inference/llama3_3B.yaml --debug
```

ğŸ“– **[See detailed Inference Guide â†’](./inference/README.md)**

### Evaluation

Evaluate model performance on benchmarks:

```bash
# Evaluate with debug output
fai-rl-eval --recipe recipes/evaluation/mmlu/llama3_3B.yaml --debug
```

ğŸ“– **[See detailed Evaluation Guide â†’](./evaluations/README.md)**

-----

## Flexible Configuration System
* YAML-based configuration for all training parameters
* Pre-configured recipes for popular models
* DeepSpeed ZeRO-3 integration for distributed training


## ğŸ“ Project Structure

```
FAI-RL/
â”œâ”€â”€ core/                      # Core framework components
â”œâ”€â”€ trainers/                  # Training method implementations
â”œâ”€â”€ inference/                 # Inference components
â”œâ”€â”€ evaluations/               # Evaluation system
â”œâ”€â”€ recipes/                   # Recipe configuration files
â”‚   â”œâ”€â”€ training/              # Training recipes
â”‚   â”œâ”€â”€ inference/             # Inference recipes
â”‚   â””â”€â”€ evaluation/            # Evaluation recipes
â”œâ”€â”€ configs/                   # Core configuration files
â”‚   â””â”€â”€ deepspeed/             # DeepSpeed ZeRO configurations
â”œâ”€â”€ utils/                     # Utility modules
â”œâ”€â”€ logs/                      # Training logs (auto-generated)
â””â”€â”€ outputs/                   # Inference output (auto-generated)
```

-----

## Memory Optimization

FAI-RL supports various techniques to train large models efficiently:

* **Full Fine-tuning:** Train all model parameters (requires most memory)
* **LoRA:** Parameter-efficient training (~10% memory of full fine-tuning)
* **QLoRA:** 4-bit quantized LoRA (train 7B+ models on single consumer GPU)
* **DeepSpeed ZeRO-3:** Distributed training for models that don't fit on single GPU

## ğŸ§ª Tested Environment

This framework has been validated on:

* **Instance:** AWS EC2 p4d.24xlarge
* **GPUs:** 8 x NVIDIA A100-SXM4-80GB (80GB VRAM each)
* **CPU:** 96 vCPUs
* **Memory:** 1152 GiB
* **Storage:** 8TB NVMe SSD
* **Network:** 400 Gbps

## ğŸ›  For Maintainers

To release a new version of FAI-RL:

1. Update version in pyproject.toml:
```bash
[project]
name = "FAI-RL"
version = "__NEW_VERSION__"
```

2. Build and upload the package:
```bash
# Upgrade pip and build tools
pip install --upgrade pip
pip install build twine

# Clean previous builds
rm -rf dist/ build/ *.egg-info

# Build the package
python -m build

# Upload to PyPI
python -m twine upload dist/*
```