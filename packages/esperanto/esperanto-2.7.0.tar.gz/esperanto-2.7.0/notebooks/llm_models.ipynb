{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from esperanto import AIFactory\n",
    "\n",
    "messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What's the capital of France?\"},\n",
    "    ]\n",
    "\n",
    "\n",
    "params = {\n",
    "    \"max_tokens\": 850,\n",
    "    \"temperature\": 1.0,\n",
    "    \"streaming\": False,\n",
    "    \"top_p\": 0.9,\n",
    "    \"structured\": None\n",
    "}\n",
    "\n",
    "models = [\n",
    "    # (\"openai-compatible\", AIFactory.create_language(\"openai-compatible\", \"qwen3:4b\")),\n",
    "    # (\"openrouter\", AIFactory.create_language(\"openrouter\", \"openai/gpt-4o\")),\n",
    "    (\"openai\", AIFactory.create_language(\"openai\", \"gpt-5-mini\")),\n",
    "    # (\"xai\", AIFactory.create_language(\"xai\", \"grok-3\")),\n",
    "    # (\"groq\", AIFactory.create_language(\"groq\", \"llama3-8b-8192\")),\n",
    "    # (\"anthropic\", AIFactory.create_language(\"anthropic\", \"claude-3-5-sonnet-latest\")),\n",
    "    # (\"ollama\", AIFactory.create_language(\"ollama\", \"gemma3:4b\")),\n",
    "    # (\"google\", AIFactory.create_language(\"google\", \"gemini-2.0-flash\")),\n",
    "    # (\"google\", AIFactory.create_language(\"google\", \"gemini-2.5-flash\")),\n",
    "    (\"azure\", AIFactory.create_language(\"azure\", \"o4-mini\")),\n",
    "    # (\"mistral\", AIFactory.create_language(\"mistral\", \"mistral-large-latest\")),\n",
    "    # (\"deepseek\", AIFactory.create_language(\"deepseek\", \"deepseek-chat\")),\n",
    "    # (\"vertex\", AIFactory.create_language(\"vertex\", \"gemini-2.0-flash\")),\n",
    "]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List Models (WIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for model in models:\n",
    "#     try:\n",
    "#         # Create an instance of the provider class\n",
    "#         provider = model[0]\n",
    "#         model_name = model[1]\n",
    "#         print(f\"\\n=== {provider.upper()} Models ===\")\n",
    "#         print(provider.models)\n",
    "#     except Exception as e:\n",
    "#         print(f\"Failed to get models for {provider}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Completion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synchronous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for openai:\n",
      "The capital of France is Paris.\n",
      "The capital of France is Paris.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Results for azure:\n",
      "The capital of France is Paris.\n",
      "The capital of France is Paris.\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for name, llm in models:\n",
    "    try:\n",
    "        print(f\"Results for {llm.provider}:\")\n",
    "        result = llm.chat_complete(messages)\n",
    "        print(result.choices[0].message.content)\n",
    "        \n",
    "        print(result.content)\n",
    "        print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process {name}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asynchronous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for openai:\n",
      "The capital of France is Paris.\n",
      "The capital of France is Paris.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Results for azure:\n",
      "The capital of France is Paris.\n",
      "The capital of France is Paris.\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for name, llm in models:\n",
    "    try:\n",
    "        print(f\"Results for {llm.provider}:\")\n",
    "        result = await llm.achat_complete(messages)\n",
    "        print(result.choices[0].message.content)\n",
    "        print(result.content)\n",
    "        print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process {name}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for openai:\n",
      "[{'name': 'São Paulo', 'state': 'São Paulo'}, {'name': 'Rio de Janeiro', 'state': 'Rio de Janeiro'}, {'name': 'Brasília', 'state': 'Distrito Federal'}]\n",
      "\n",
      "==================================================\n",
      "\n",
      "Results for azure:\n",
      "{'top_3_brazilian_cities': ['São Paulo', 'Rio de Janeiro', 'Brasília']}\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "json_messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Please return the top 3 brazilian cities in JSON format. Dont include ```json```  in the response.\"},\n",
    "    ]\n",
    "\n",
    "\n",
    "for name, llm in models:\n",
    "    try:\n",
    "        print(f\"Results for {llm.provider}:\")\n",
    "        result = llm.chat_complete(json_messages)\n",
    "        try:\n",
    "            json_data = json.loads(result.choices[0].message.content)\n",
    "            print(json_data)\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"Error decoding JSON\")\n",
    "            print(result.choices[0].message.content)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed {name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pydantic (WIP)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# from pydantic import BaseModel\n",
    "# from typing import List\n",
    "\n",
    "# class Country(BaseModel):\n",
    "#     name: str\n",
    "#     population: int\n",
    "\n",
    "# class Response(BaseModel):\n",
    "#     countries: List[Country]\n",
    "\n",
    "# json_messages = [\n",
    "#         {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "#         {\"role\": \"user\", \"content\": \"Please return the top 3 countries in terms of population. Responda no formato JSON.\"},\n",
    "#     ]\n",
    "\n",
    "\n",
    "# for name, config in models.items():\n",
    "#     try:\n",
    "#         llm = config[\"class\"](model_name=config[\"model\"], structured={\"type\": \"json\", \"model\": Response})\n",
    "#         print(f\"Results for {llm.provider}:\")\n",
    "#         result = llm.chat_complete(json_messages)\n",
    "#         try:\n",
    "#             json_data = json.loads(result.choices[0].message.content)\n",
    "#             print(json_data)\n",
    "#         except json.JSONDecodeError:\n",
    "#             print(\"Error decoding JSON\")\n",
    "        \n",
    "#         print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Failed to get models for {name}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synchronous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for openai:\n",
      "id='chatcmpl-CHYUyX6ljFwI77aIRrBwqYyAGyI0h' choices=[StreamChoice(index=0, delta=DeltaMessage(content='', role='assistant', function_call=None, tool_calls=None), finish_reason='None')] model='gpt-5-mini-2025-08-07' created=1758300224 object='chat.completion.chunk'\n",
      "id='chatcmpl-CHYUyX6ljFwI77aIRrBwqYyAGyI0h' choices=[StreamChoice(index=0, delta=DeltaMessage(content='The', role='assistant', function_call=None, tool_calls=None), finish_reason='None')] model='gpt-5-mini-2025-08-07' created=1758300224 object='chat.completion.chunk'\n",
      "id='chatcmpl-CHYUyX6ljFwI77aIRrBwqYyAGyI0h' choices=[StreamChoice(index=0, delta=DeltaMessage(content=' capital', role='assistant', function_call=None, tool_calls=None), finish_reason='None')] model='gpt-5-mini-2025-08-07' created=1758300224 object='chat.completion.chunk'\n",
      "id='chatcmpl-CHYUyX6ljFwI77aIRrBwqYyAGyI0h' choices=[StreamChoice(index=0, delta=DeltaMessage(content=' of', role='assistant', function_call=None, tool_calls=None), finish_reason='None')] model='gpt-5-mini-2025-08-07' created=1758300224 object='chat.completion.chunk'\n",
      "id='chatcmpl-CHYUyX6ljFwI77aIRrBwqYyAGyI0h' choices=[StreamChoice(index=0, delta=DeltaMessage(content=' France', role='assistant', function_call=None, tool_calls=None), finish_reason='None')] model='gpt-5-mini-2025-08-07' created=1758300224 object='chat.completion.chunk'\n",
      "id='chatcmpl-CHYUyX6ljFwI77aIRrBwqYyAGyI0h' choices=[StreamChoice(index=0, delta=DeltaMessage(content=' is', role='assistant', function_call=None, tool_calls=None), finish_reason='None')] model='gpt-5-mini-2025-08-07' created=1758300224 object='chat.completion.chunk'\n",
      "id='chatcmpl-CHYUyX6ljFwI77aIRrBwqYyAGyI0h' choices=[StreamChoice(index=0, delta=DeltaMessage(content=' Paris', role='assistant', function_call=None, tool_calls=None), finish_reason='None')] model='gpt-5-mini-2025-08-07' created=1758300224 object='chat.completion.chunk'\n",
      "id='chatcmpl-CHYUyX6ljFwI77aIRrBwqYyAGyI0h' choices=[StreamChoice(index=0, delta=DeltaMessage(content='.', role='assistant', function_call=None, tool_calls=None), finish_reason='None')] model='gpt-5-mini-2025-08-07' created=1758300224 object='chat.completion.chunk'\n",
      "id='chatcmpl-CHYUyX6ljFwI77aIRrBwqYyAGyI0h' choices=[StreamChoice(index=0, delta=DeltaMessage(content='', role='assistant', function_call=None, tool_calls=None), finish_reason='stop')] model='gpt-5-mini-2025-08-07' created=1758300224 object='chat.completion.chunk'\n",
      "\n",
      "==================================================\n",
      "\n",
      "Results for azure:\n",
      "id='' choices=[] model='o4-mini' created=0 object='chat.completion.chunk'\n",
      "id='chatcmpl-CHYV1HOXL5XxsdmzJF7k9s4DMP8lQ' choices=[StreamChoice(index=0, delta=DeltaMessage(content='', role='assistant', function_call=None, tool_calls=None), finish_reason='None')] model='o4-mini-2025-04-16' created=1758300227 object='chat.completion.chunk'\n",
      "id='chatcmpl-CHYV1HOXL5XxsdmzJF7k9s4DMP8lQ' choices=[StreamChoice(index=0, delta=DeltaMessage(content='The', role='assistant', function_call=None, tool_calls=None), finish_reason='None')] model='o4-mini-2025-04-16' created=1758300227 object='chat.completion.chunk'\n",
      "id='chatcmpl-CHYV1HOXL5XxsdmzJF7k9s4DMP8lQ' choices=[StreamChoice(index=0, delta=DeltaMessage(content=' capital', role='assistant', function_call=None, tool_calls=None), finish_reason='None')] model='o4-mini-2025-04-16' created=1758300227 object='chat.completion.chunk'\n",
      "id='chatcmpl-CHYV1HOXL5XxsdmzJF7k9s4DMP8lQ' choices=[StreamChoice(index=0, delta=DeltaMessage(content=' of', role='assistant', function_call=None, tool_calls=None), finish_reason='None')] model='o4-mini-2025-04-16' created=1758300227 object='chat.completion.chunk'\n",
      "id='chatcmpl-CHYV1HOXL5XxsdmzJF7k9s4DMP8lQ' choices=[StreamChoice(index=0, delta=DeltaMessage(content=' France', role='assistant', function_call=None, tool_calls=None), finish_reason='None')] model='o4-mini-2025-04-16' created=1758300227 object='chat.completion.chunk'\n",
      "id='chatcmpl-CHYV1HOXL5XxsdmzJF7k9s4DMP8lQ' choices=[StreamChoice(index=0, delta=DeltaMessage(content=' is', role='assistant', function_call=None, tool_calls=None), finish_reason='None')] model='o4-mini-2025-04-16' created=1758300227 object='chat.completion.chunk'\n",
      "id='chatcmpl-CHYV1HOXL5XxsdmzJF7k9s4DMP8lQ' choices=[StreamChoice(index=0, delta=DeltaMessage(content=' Paris', role='assistant', function_call=None, tool_calls=None), finish_reason='None')] model='o4-mini-2025-04-16' created=1758300227 object='chat.completion.chunk'\n",
      "id='chatcmpl-CHYV1HOXL5XxsdmzJF7k9s4DMP8lQ' choices=[StreamChoice(index=0, delta=DeltaMessage(content='.', role='assistant', function_call=None, tool_calls=None), finish_reason='None')] model='o4-mini-2025-04-16' created=1758300227 object='chat.completion.chunk'\n",
      "id='chatcmpl-CHYV1HOXL5XxsdmzJF7k9s4DMP8lQ' choices=[StreamChoice(index=0, delta=DeltaMessage(content='', role='assistant', function_call=None, tool_calls=None), finish_reason='stop')] model='o4-mini-2025-04-16' created=1758300227 object='chat.completion.chunk'\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for name, llm in models:\n",
    "    try:\n",
    "        print(f\"Results for {llm.provider}:\")\n",
    "        result = llm.chat_complete(\n",
    "            messages, stream=True\n",
    "        )\n",
    "\n",
    "        for chunk in result:\n",
    "            print(chunk)\n",
    "        print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "    except Exception as e:\n",
    "            print(f\"Failed to process for {name}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for openai:\n",
      "id='chatcmpl-CHYV36RbgqP8iAAr7qtlgxN0vKatr' choices=[StreamChoice(index=0, delta=DeltaMessage(content='', role='assistant', function_call=None, tool_calls=None), finish_reason='None')] model='gpt-5-mini-2025-08-07' created=1758300229 object='chat.completion.chunk'\n",
      "id='chatcmpl-CHYV36RbgqP8iAAr7qtlgxN0vKatr' choices=[StreamChoice(index=0, delta=DeltaMessage(content='Paris', role='assistant', function_call=None, tool_calls=None), finish_reason='None')] model='gpt-5-mini-2025-08-07' created=1758300229 object='chat.completion.chunk'\n",
      "id='chatcmpl-CHYV36RbgqP8iAAr7qtlgxN0vKatr' choices=[StreamChoice(index=0, delta=DeltaMessage(content='.', role='assistant', function_call=None, tool_calls=None), finish_reason='None')] model='gpt-5-mini-2025-08-07' created=1758300229 object='chat.completion.chunk'\n",
      "id='chatcmpl-CHYV36RbgqP8iAAr7qtlgxN0vKatr' choices=[StreamChoice(index=0, delta=DeltaMessage(content='', role='assistant', function_call=None, tool_calls=None), finish_reason='stop')] model='gpt-5-mini-2025-08-07' created=1758300229 object='chat.completion.chunk'\n",
      "\n",
      "==================================================\n",
      "\n",
      "Results for azure:\n",
      "id='' choices=[] model='o4-mini' created=0 object='chat.completion.chunk'\n",
      "id='chatcmpl-CHYV402pA4Mhyk9idPyVaXe9wfN3T' choices=[StreamChoice(index=0, delta=DeltaMessage(content='', role='assistant', function_call=None, tool_calls=None), finish_reason='None')] model='o4-mini-2025-04-16' created=1758300230 object='chat.completion.chunk'\n",
      "id='chatcmpl-CHYV402pA4Mhyk9idPyVaXe9wfN3T' choices=[StreamChoice(index=0, delta=DeltaMessage(content='The', role='assistant', function_call=None, tool_calls=None), finish_reason='None')] model='o4-mini-2025-04-16' created=1758300230 object='chat.completion.chunk'\n",
      "id='chatcmpl-CHYV402pA4Mhyk9idPyVaXe9wfN3T' choices=[StreamChoice(index=0, delta=DeltaMessage(content=' capital', role='assistant', function_call=None, tool_calls=None), finish_reason='None')] model='o4-mini-2025-04-16' created=1758300230 object='chat.completion.chunk'\n",
      "id='chatcmpl-CHYV402pA4Mhyk9idPyVaXe9wfN3T' choices=[StreamChoice(index=0, delta=DeltaMessage(content=' of', role='assistant', function_call=None, tool_calls=None), finish_reason='None')] model='o4-mini-2025-04-16' created=1758300230 object='chat.completion.chunk'\n",
      "id='chatcmpl-CHYV402pA4Mhyk9idPyVaXe9wfN3T' choices=[StreamChoice(index=0, delta=DeltaMessage(content=' France', role='assistant', function_call=None, tool_calls=None), finish_reason='None')] model='o4-mini-2025-04-16' created=1758300230 object='chat.completion.chunk'\n",
      "id='chatcmpl-CHYV402pA4Mhyk9idPyVaXe9wfN3T' choices=[StreamChoice(index=0, delta=DeltaMessage(content=' is', role='assistant', function_call=None, tool_calls=None), finish_reason='None')] model='o4-mini-2025-04-16' created=1758300230 object='chat.completion.chunk'\n",
      "id='chatcmpl-CHYV402pA4Mhyk9idPyVaXe9wfN3T' choices=[StreamChoice(index=0, delta=DeltaMessage(content=' Paris', role='assistant', function_call=None, tool_calls=None), finish_reason='None')] model='o4-mini-2025-04-16' created=1758300230 object='chat.completion.chunk'\n",
      "id='chatcmpl-CHYV402pA4Mhyk9idPyVaXe9wfN3T' choices=[StreamChoice(index=0, delta=DeltaMessage(content='.', role='assistant', function_call=None, tool_calls=None), finish_reason='None')] model='o4-mini-2025-04-16' created=1758300230 object='chat.completion.chunk'\n",
      "id='chatcmpl-CHYV402pA4Mhyk9idPyVaXe9wfN3T' choices=[StreamChoice(index=0, delta=DeltaMessage(content='', role='assistant', function_call=None, tool_calls=None), finish_reason='stop')] model='o4-mini-2025-04-16' created=1758300230 object='chat.completion.chunk'\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for name, llm in models:\n",
    "    try:\n",
    "        print(f\"Results for {llm.provider}:\")\n",
    "        result = await llm.achat_complete(\n",
    "            messages, stream=True\n",
    "        )\n",
    "\n",
    "        async for chunk in result:\n",
    "            print(chunk)\n",
    "        print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process for {name}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synchronous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for openai:\n",
      "The capital of France is Paris.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Results for azure:\n",
      "The capital of France is Paris.\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for name, llm in models:\n",
    "    try:\n",
    "        print(f\"Results for {llm.provider}:\")\n",
    "        model = llm.to_langchain()\n",
    "        response = model.invoke(messages)\n",
    "        print(response.content)\n",
    "        print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process for {name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asynchronous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for openai:\n",
      "The capital of France is Paris.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Results for azure:\n",
      "The capital of France is Paris.\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for name, llm in models:\n",
    "    try:\n",
    "        print(f\"Results for {llm.provider}:\")\n",
    "        model = llm.to_langchain()\n",
    "        response = await model.ainvoke(messages)\n",
    "        print(response.content)\n",
    "        print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process for {name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for openai:\n",
      "content='' additional_kwargs={} response_metadata={} id='run--d9c5dc84-b4f8-4309-8122-bf2f4feff38d'\n",
      "content='The' additional_kwargs={} response_metadata={} id='run--d9c5dc84-b4f8-4309-8122-bf2f4feff38d'\n",
      "content=' capital' additional_kwargs={} response_metadata={} id='run--d9c5dc84-b4f8-4309-8122-bf2f4feff38d'\n",
      "content=' of' additional_kwargs={} response_metadata={} id='run--d9c5dc84-b4f8-4309-8122-bf2f4feff38d'\n",
      "content=' France' additional_kwargs={} response_metadata={} id='run--d9c5dc84-b4f8-4309-8122-bf2f4feff38d'\n",
      "content=' is' additional_kwargs={} response_metadata={} id='run--d9c5dc84-b4f8-4309-8122-bf2f4feff38d'\n",
      "content=' Paris' additional_kwargs={} response_metadata={} id='run--d9c5dc84-b4f8-4309-8122-bf2f4feff38d'\n",
      "content='.' additional_kwargs={} response_metadata={} id='run--d9c5dc84-b4f8-4309-8122-bf2f4feff38d'\n",
      "content='' additional_kwargs={} response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-5-mini-2025-08-07', 'service_tier': 'default'} id='run--d9c5dc84-b4f8-4309-8122-bf2f4feff38d'\n",
      "\n",
      "==================================================\n",
      "\n",
      "Results for azure:\n",
      "content='' additional_kwargs={} response_metadata={} id='run--e4ec21bb-9830-483a-b2e8-12d688931313'\n",
      "content='' additional_kwargs={} response_metadata={} id='run--e4ec21bb-9830-483a-b2e8-12d688931313'\n",
      "content='The' additional_kwargs={} response_metadata={} id='run--e4ec21bb-9830-483a-b2e8-12d688931313'\n",
      "content=' capital' additional_kwargs={} response_metadata={} id='run--e4ec21bb-9830-483a-b2e8-12d688931313'\n",
      "content=' of' additional_kwargs={} response_metadata={} id='run--e4ec21bb-9830-483a-b2e8-12d688931313'\n",
      "content=' France' additional_kwargs={} response_metadata={} id='run--e4ec21bb-9830-483a-b2e8-12d688931313'\n",
      "content=' is' additional_kwargs={} response_metadata={} id='run--e4ec21bb-9830-483a-b2e8-12d688931313'\n",
      "content=' Paris' additional_kwargs={} response_metadata={} id='run--e4ec21bb-9830-483a-b2e8-12d688931313'\n",
      "content='.' additional_kwargs={} response_metadata={} id='run--e4ec21bb-9830-483a-b2e8-12d688931313'\n",
      "content='' additional_kwargs={} response_metadata={'finish_reason': 'stop', 'model_name': 'o4-mini-2025-04-16'} id='run--e4ec21bb-9830-483a-b2e8-12d688931313'\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, llm in models:\n",
    "    try:\n",
    "        # Create a new streaming instance using the factory\n",
    "        streaming_llm = AIFactory.create_language(llm.provider, llm.model_name, config={\"streaming\": True})\n",
    "        print(f\"Results for {streaming_llm.provider}:\")\n",
    "        model = streaming_llm.to_langchain()\n",
    "        response = model.stream(messages)\n",
    "        for chunk in response:\n",
    "            print(chunk)\n",
    "        print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process for {name}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asynchronous Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for openai:\n",
      "content='' additional_kwargs={} response_metadata={} id='run--c1da6755-0b05-4a9c-95e5-8d2425c352b8'\n",
      "content='The' additional_kwargs={} response_metadata={} id='run--c1da6755-0b05-4a9c-95e5-8d2425c352b8'\n",
      "content=' capital' additional_kwargs={} response_metadata={} id='run--c1da6755-0b05-4a9c-95e5-8d2425c352b8'\n",
      "content=' of' additional_kwargs={} response_metadata={} id='run--c1da6755-0b05-4a9c-95e5-8d2425c352b8'\n",
      "content=' France' additional_kwargs={} response_metadata={} id='run--c1da6755-0b05-4a9c-95e5-8d2425c352b8'\n",
      "content=' is' additional_kwargs={} response_metadata={} id='run--c1da6755-0b05-4a9c-95e5-8d2425c352b8'\n",
      "content=' Paris' additional_kwargs={} response_metadata={} id='run--c1da6755-0b05-4a9c-95e5-8d2425c352b8'\n",
      "content='.' additional_kwargs={} response_metadata={} id='run--c1da6755-0b05-4a9c-95e5-8d2425c352b8'\n",
      "content='' additional_kwargs={} response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-5-mini-2025-08-07', 'service_tier': 'default'} id='run--c1da6755-0b05-4a9c-95e5-8d2425c352b8'\n",
      "\n",
      "==================================================\n",
      "\n",
      "Results for azure:\n",
      "content='' additional_kwargs={} response_metadata={} id='run--80f396a2-d898-4403-834c-2a2b434870ee'\n",
      "content='' additional_kwargs={} response_metadata={} id='run--80f396a2-d898-4403-834c-2a2b434870ee'\n",
      "content='The' additional_kwargs={} response_metadata={} id='run--80f396a2-d898-4403-834c-2a2b434870ee'\n",
      "content=' capital' additional_kwargs={} response_metadata={} id='run--80f396a2-d898-4403-834c-2a2b434870ee'\n",
      "content=' of' additional_kwargs={} response_metadata={} id='run--80f396a2-d898-4403-834c-2a2b434870ee'\n",
      "content=' France' additional_kwargs={} response_metadata={} id='run--80f396a2-d898-4403-834c-2a2b434870ee'\n",
      "content=' is' additional_kwargs={} response_metadata={} id='run--80f396a2-d898-4403-834c-2a2b434870ee'\n",
      "content=' Paris' additional_kwargs={} response_metadata={} id='run--80f396a2-d898-4403-834c-2a2b434870ee'\n",
      "content='.' additional_kwargs={} response_metadata={} id='run--80f396a2-d898-4403-834c-2a2b434870ee'\n",
      "content='' additional_kwargs={} response_metadata={'finish_reason': 'stop', 'model_name': 'o4-mini-2025-04-16'} id='run--80f396a2-d898-4403-834c-2a2b434870ee'\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, llm in models:\n",
    "    try:\n",
    "        # Create a new streaming instance using the factory\n",
    "        streaming_llm = AIFactory.create_language(llm.provider, llm.model_name, config={\"streaming\": True})\n",
    "        print(f\"Results for {streaming_llm.provider}:\")\n",
    "        model = streaming_llm.to_langchain()\n",
    "        response = model.astream(messages)\n",
    "        async for chunk in response:\n",
    "            print(chunk)\n",
    "        print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process for {name}: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
