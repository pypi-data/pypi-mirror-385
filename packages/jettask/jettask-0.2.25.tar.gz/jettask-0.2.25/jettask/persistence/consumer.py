"""PostgreSQL Consumer - åŸºäºé€šé…ç¬¦é˜Ÿåˆ—çš„æ–°å®ç°

å®Œå…¨æ›¿æ¢æ—§çš„ consumer.py å®ç°ï¼Œä½¿ç”¨ Jettask é€šé…ç¬¦é˜Ÿåˆ—åŠŸèƒ½ã€‚
"""

import time
import logging
from datetime import datetime, timezone

from jettask import Jettask
from jettask.core.context import TaskContext
from jettask.db.connector import get_pg_engine_and_factory, DBConfig
from .buffer import BatchBuffer
from .persistence import TaskPersistence

logger = logging.getLogger(__name__)


class PostgreSQLConsumer:
    """PostgreSQL Consumer - åŸºäºé€šé…ç¬¦é˜Ÿåˆ—

    æ ¸å¿ƒç‰¹æ€§ï¼š
    1. ä½¿ç”¨ @app.task(queue='*') ç›‘å¬æ‰€æœ‰é˜Ÿåˆ—
    2. ä½¿ç”¨ @app.task(queue='TASK_CHANGES') å¤„ç†çŠ¶æ€æ›´æ–°
    3. æ‰¹é‡ INSERT å’Œ UPDATE
    4. è‡ªåŠ¨é˜Ÿåˆ—å‘ç°ï¼ˆJettask å†…ç½®ï¼‰
    """

    def __init__(
        self,
        pg_config,  # å¯ä»¥æ˜¯å­—å…¸æˆ–é…ç½®å¯¹è±¡
        redis_config,  # å¯ä»¥æ˜¯å­—å…¸æˆ–é…ç½®å¯¹è±¡
        prefix: str = "jettask",
        namespace_id: str = None,
        namespace_name: str = None,
        batch_size: int = 1000,
        flush_interval: float = 5.0
    ):
        """åˆå§‹åŒ– PG Consumer

        Args:
            pg_config: PostgreSQLé…ç½®ï¼ˆå­—å…¸æˆ–å¯¹è±¡ï¼‰
            redis_config: Redisé…ç½®ï¼ˆå­—å…¸æˆ–å¯¹è±¡ï¼‰
            prefix: Redisé”®å‰ç¼€
            node_id: èŠ‚ç‚¹IDï¼ˆå…¼å®¹æ—§æ¥å£ï¼Œä¸ä½¿ç”¨ï¼‰
            namespace_id: å‘½åç©ºé—´ID
            namespace_name: å‘½åç©ºé—´åç§°
            enable_backlog_monitor: æ˜¯å¦å¯ç”¨ç§¯å‹ç›‘æ§ï¼ˆå…¼å®¹æ—§æ¥å£ï¼Œä¸ä½¿ç”¨ï¼‰
            backlog_monitor_interval: ç§¯å‹ç›‘æ§é—´éš”ï¼ˆå…¼å®¹æ—§æ¥å£ï¼Œä¸ä½¿ç”¨ï¼‰
            batch_size: æ‰¹é‡å¤§å°
            flush_interval: åˆ·æ–°é—´éš”ï¼ˆç§’ï¼‰
        """
        self.pg_config = pg_config
        self.redis_config = redis_config
        self.redis_prefix = prefix
        self.namespace_id = namespace_id
        self.namespace_name = namespace_name or "default"

        # æ„å»º Redis URLï¼ˆå…¼å®¹å­—å…¸å’Œå¯¹è±¡ä¸¤ç§æ ¼å¼ï¼‰
        if isinstance(redis_config, dict):
            # å­—å…¸æ ¼å¼ - ä¼˜å…ˆä½¿ç”¨ 'url' å­—æ®µ
            redis_url = redis_config.get('url') or redis_config.get('redis_url')
            if not redis_url:
                # ä»ç‹¬ç«‹å­—æ®µæ„å»º
                password = redis_config.get('password', '')
                host = redis_config.get('host', 'localhost')
                port = redis_config.get('port', 6379)
                db = redis_config.get('db', 0)
                redis_url = f"redis://"
                if password:
                    redis_url += f":{password}@"
                redis_url += f"{host}:{port}/{db}"
        else:
            # å¯¹è±¡æ ¼å¼
            redis_url = f"redis://"
            if hasattr(redis_config, 'password') and redis_config.password:
                redis_url += f":{redis_config.password}@"
            redis_url += f"{redis_config.host}:{redis_config.port}/{redis_config.db}"

        self.redis_url = redis_url
        logger.debug(f"æ„å»º Redis URL: {redis_url}")

        # æ•°æ®åº“å¼•æ“å’Œä¼šè¯ï¼ˆå°†åœ¨ start æ—¶åˆå§‹åŒ–ï¼‰
        self.async_engine = None
        self.AsyncSessionLocal = None
        self.db_manager = None

        # åˆ›å»º Jettask åº”ç”¨
        self.app = Jettask(
            redis_url=redis_url,
            redis_prefix=prefix
        )

        # åˆ›å»ºä¸¤ä¸ªç‹¬ç«‹çš„æ‰¹é‡ç¼“å†²åŒº
        # 1. INSERT ç¼“å†²åŒºï¼ˆç”¨äºæ–°ä»»åŠ¡æŒä¹…åŒ–ï¼‰
        self.insert_buffer = BatchBuffer(
            max_size=batch_size,
            max_delay=flush_interval,
            operation_type='insert'
        )

        # 2. UPDATE ç¼“å†²åŒºï¼ˆç”¨äºä»»åŠ¡çŠ¶æ€æ›´æ–°ï¼‰
        self.update_buffer = BatchBuffer(
            max_size=batch_size // 2,  # çŠ¶æ€æ›´æ–°é€šå¸¸æ›´é¢‘ç¹ï¼Œç”¨è¾ƒå°çš„æ‰¹æ¬¡
            max_delay=flush_interval,
            operation_type='update'
        )

        # æ³¨å†Œä»»åŠ¡
        self._register_tasks()

        # è¿è¡Œæ§åˆ¶
        self._running = False

    def _register_tasks(self):
        """æ³¨å†Œä»»åŠ¡å¤„ç†å™¨"""
        # åˆ›å»ºé—­åŒ…å‡½æ•°æ¥è®¿é—®å®ä¾‹å±æ€§
        consumer = self  # æ•è· self å¼•ç”¨

        @self.app.task(queue='*', auto_ack=False)
        async def _handle_persist_task(ctx: TaskContext, **kwargs):
            return await consumer._do_handle_persist_task(ctx, **kwargs)

        @self.app.task(queue='TASK_CHANGES', auto_ack=False)
        async def _handle_status_update(ctx: TaskContext, **kwargs):
            print(f'[PG Consumer] å¤„ç†çŠ¶æ€æ›´æ–°: {ctx.event_id} {kwargs=}')
            return await consumer._do_handle_status_update(ctx, **kwargs)

    async def _do_handle_persist_task(self, ctx: TaskContext, **kwargs):
        """å¤„ç†ä»»åŠ¡æŒä¹…åŒ–ï¼ˆINSERTï¼‰

        ä½¿ç”¨é€šé…ç¬¦ queue='*' ç›‘å¬æ‰€æœ‰é˜Ÿåˆ—
        Jettask ä¼šè‡ªåŠ¨å‘ç°æ–°é˜Ÿåˆ—å¹¶å¼€å§‹æ¶ˆè´¹

        Args:
            ctx: Jettask è‡ªåŠ¨æ³¨å…¥çš„ä»»åŠ¡ä¸Šä¸‹æ–‡ï¼ˆåŒ…å« queue, event_id ç­‰ï¼‰
            **kwargs: ä»»åŠ¡çš„åŸå§‹æ•°æ®å­—æ®µ
        """
        # è·³è¿‡ TASK_CHANGES é˜Ÿåˆ—ï¼ˆç”±å¦ä¸€ä¸ªä»»åŠ¡å¤„ç†ï¼‰
        if ctx.queue == f'{self.redis_prefix}:QUEUE:TASK_CHANGES':
            await ctx.ack()
            return

        try:
            # æå–çº¯é˜Ÿåˆ—åï¼ˆå»æ‰ prefix:QUEUE: å‰ç¼€ï¼‰
            queue_name = ctx.queue.replace(f'{self.redis_prefix}:QUEUE:', '')

            # è®°å½•çœŸå®çš„é˜Ÿåˆ—åç§°ï¼ˆç”¨äºéªŒè¯é€šé…ç¬¦é˜Ÿåˆ—åŠŸèƒ½ï¼‰
            logger.info(f"[æŒä¹…åŒ–ä»»åŠ¡] å®Œæ•´è·¯å¾„: {ctx.queue}, é˜Ÿåˆ—å: {queue_name}, Stream ID: {ctx.event_id}")

            # æ„å»ºä»»åŠ¡è®°å½•
            trigger_time = kwargs.get('trigger_time', time.time())
            if isinstance(trigger_time, (str, bytes)):
                trigger_time = float(trigger_time)

            priority = kwargs.get('priority', 0)
            if isinstance(priority, (str, bytes)):
                priority = int(priority)

            record = {
                'stream_id': ctx.event_id,
                'queue': ctx.queue.replace(f'{self.redis_prefix}:QUEUE:', ''),
                'task_name': kwargs.get('task_name', 'unknown'),
                'payload': kwargs.get('payload', {}),
                'priority': priority,
                'created_at': datetime.fromtimestamp(trigger_time, tz=timezone.utc),
                'scheduled_task_id': kwargs.get('scheduled_task_id'),
                'namespace': self.namespace_name,
                'source': 'scheduler' if kwargs.get('scheduled_task_id') else 'redis_stream',
            }

            # æ·»åŠ åˆ°ç¼“å†²åŒºï¼ˆä¸ç«‹å³å¤„ç†ï¼Œä¸ç«‹å³ ACKï¼‰
            self.insert_buffer.add(record, ctx)

            # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ·æ–°ï¼ˆæ‰¹é‡å¤§å°æˆ–è¶…æ—¶ï¼‰
            if self.insert_buffer.should_flush():
                await self.insert_buffer.flush(self.db_manager)

            # åŒæ—¶æ£€æŸ¥ UPDATE ç¼“å†²åŒºæ˜¯å¦éœ€è¦åˆ·æ–°ï¼ˆåˆ©ç”¨è¿™æ¬¡æœºä¼šï¼‰
            if self.update_buffer.should_flush():
                await self.update_buffer.flush(self.db_manager)

        except Exception as e:
            logger.error(f"æŒä¹…åŒ–ä»»åŠ¡å¤±è´¥: {e}", exc_info=True)
            # å‡ºé”™ä¹Ÿè¦ ACKï¼Œé¿å…æ¶ˆæ¯å †ç§¯
            await ctx.ack()

    async def _do_handle_status_update(self, ctx: TaskContext, **kwargs):
        """å¤„ç†ä»»åŠ¡çŠ¶æ€æ›´æ–°ï¼ˆUPDATEï¼‰

        æ¶ˆè´¹ TASK_CHANGES é˜Ÿåˆ—ï¼Œæ‰¹é‡æ›´æ–°æ•°æ®åº“ä¸­çš„ä»»åŠ¡çŠ¶æ€

        Args:
            ctx: Jettask è‡ªåŠ¨æ³¨å…¥çš„ä»»åŠ¡ä¸Šä¸‹æ–‡
            **kwargs: ä»»åŠ¡çš„åŸå§‹æ•°æ®å­—æ®µï¼ˆåŒ…å« task_idï¼‰
        """
        try:
            # ä»æ¶ˆæ¯ä¸­è·å– task_id
            task_id = kwargs.get('task_id')
            if not task_id:
                logger.warning(f"TASK_CHANGES æ¶ˆæ¯ç¼ºå°‘ task_id: {ctx.event_id}")
                await ctx.ack()
                return

            # ä» Redis Hash ä¸­è¯»å–å®Œæ•´çš„ä»»åŠ¡çŠ¶æ€ä¿¡æ¯
            # task_id æ ¼å¼: test5:TASK:event_id:queue:task_name
            # æˆ‘ä»¬éœ€è¦æŸ¥è¯¢ Redis Hash è·å–çŠ¶æ€ä¿¡æ¯
            redis_client = ctx.app.async_binary_redis
            # æŸ¥è¯¢ä»»åŠ¡çŠ¶æ€ Hash
            task_info = await redis_client.hgetall(task_id)
            logger.info(f"task_id={task_id!r}")
            logger.info(f"task_info={task_info!r}")
            if not task_info:
                logger.warning(f"æ— æ³•æ‰¾åˆ°ä»»åŠ¡çŠ¶æ€ä¿¡æ¯: {task_id}")
                await ctx.ack()
                return

            # ä» task_id ä¸­æå– event_id (stream_id)
            # task_id æ ¼å¼: prefix:TASK:event_id:queue:task_name
            parts = task_id.split(':')
            if len(parts) >= 3:
                event_id = parts[2]  # æå– event_id
            else:
                logger.error(f"æ— æ•ˆçš„ task_id æ ¼å¼: {task_id}")
                await ctx.ack()
                return

            # è§£æå„ä¸ªå­—æ®µï¼ˆbinary redis è¿”å› bytesï¼‰
            # 1. retries
            retries = task_info.get(b'retries', 0)
            if isinstance(retries, bytes):
                retries = int(retries.decode('utf-8')) if retries else 0
            elif isinstance(retries, str):
                retries = int(retries) if retries else 0

            # 2. started_at
            started_at = task_info.get(b'started_at')
            if started_at:
                if isinstance(started_at, bytes):
                    started_at = float(started_at.decode('utf-8'))
                elif isinstance(started_at, str):
                    started_at = float(started_at)

            # 3. completed_at
            completed_at = task_info.get(b'completed_at')
            if completed_at:
                if isinstance(completed_at, bytes):
                    completed_at = float(completed_at.decode('utf-8'))
                elif isinstance(completed_at, str):
                    completed_at = float(completed_at)

            # 4. consumer
            consumer = task_info.get(b'consumer')
            if consumer:
                if isinstance(consumer, bytes):
                    consumer = consumer.decode('utf-8')

            # 5. status
            status = task_info.get(b'status')
            if status:
                if isinstance(status, bytes):
                    status = status.decode('utf-8')

            # 6. result (ä¿æŒåŸå§‹ bytesï¼Œåœ¨ persistence.py ä¸­è§£æ)
            result = task_info.get(b'result')

            # 7. error/exception
            error = task_info.get(b'exception') or task_info.get(b'error')

            update_record = {
                'stream_id': event_id,
                'status': status,
                'result': result,  # bytes æ ¼å¼ï¼Œç¨åè§£æ
                'error': error,
                'started_at': started_at,
                'completed_at': completed_at,
                'retries': retries,
                'consumer': consumer,
            }

            logger.info(f"update_record={update_record!r}")

            print(f'{update_record=}')
            # æ·»åŠ åˆ°çŠ¶æ€æ›´æ–°ç¼“å†²åŒº
            self.update_buffer.add(update_record, ctx)

            # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ·æ–°ï¼ˆæ‰¹é‡å¤§å°æˆ–è¶…æ—¶ï¼‰
            if self.update_buffer.should_flush():
                await self.update_buffer.flush(self.db_manager)

            # åŒæ—¶æ£€æŸ¥ INSERT ç¼“å†²åŒºæ˜¯å¦éœ€è¦åˆ·æ–°ï¼ˆåˆ©ç”¨è¿™æ¬¡æœºä¼šï¼‰
            if self.insert_buffer.should_flush():
                await self.insert_buffer.flush(self.db_manager)

        except Exception as e:
            logger.error(f"æ›´æ–°ä»»åŠ¡çŠ¶æ€å¤±è´¥: {e}", exc_info=True)
            # å‡ºé”™ä¹Ÿè¦ ACK
            await ctx.ack()

    async def start(self, concurrency: int = 4):
        """å¯åŠ¨ Consumer

        Args:
            concurrency: å¹¶å‘æ•°
        """
        logger.info(f"Starting PostgreSQL consumer (wildcard queue mode)")
        logger.info(f"Namespace: {self.namespace_name} ({self.namespace_id or 'N/A'})")

        # 1. ä½¿ç”¨ connector.py ç»Ÿä¸€ç®¡ç†æ•°æ®åº“è¿æ¥
        # è§£æ PostgreSQL é…ç½®ä¸ºæ ‡å‡† DSN
        dsn = DBConfig.parse_pg_config(self.pg_config)

        # ä½¿ç”¨å…¨å±€å•ä¾‹å¼•æ“å’Œä¼šè¯å·¥å‚
        self.async_engine, self.AsyncSessionLocal = get_pg_engine_and_factory(
            dsn,
            pool_size=50,
            max_overflow=20,
            pool_pre_ping=True,
            pool_recycle=300,
            echo=False
        )

        logger.debug(f"ä½¿ç”¨å…¨å±€ PostgreSQL è¿æ¥æ± : {dsn[:50]}...")

        # 2. åˆå§‹åŒ–ä»»åŠ¡æŒä¹…åŒ–ç®¡ç†å™¨
        self.db_manager = TaskPersistence(
            async_session_local=self.AsyncSessionLocal,
            namespace_id=self.namespace_id,
            namespace_name=self.namespace_name
        )

        # 3. è®¾ç½®è¿è¡ŒçŠ¶æ€
        self._running = True

        # æ³¨æ„ï¼šä¸åœ¨ä¸»è¿›ç¨‹å¯åŠ¨å®šæ—¶åˆ·æ–°ä»»åŠ¡ï¼Œå› ä¸ºç¼“å†²åŒºåœ¨å­è¿›ç¨‹ä¸­
        # åˆ·æ–°é€»è¾‘å·²é›†æˆåˆ°ä»»åŠ¡å¤„ç†å‡½æ•°ä¸­ï¼ˆæ¯æ¬¡å¤„ç†ä»»åŠ¡æ—¶éƒ½ä¼šæ£€æŸ¥æ˜¯å¦éœ€è¦åˆ·æ–°ï¼‰

        # å¯åŠ¨ Workerï¼ˆä½¿ç”¨é€šé…ç¬¦é˜Ÿåˆ—ï¼‰
        logger.info("=" * 60)
        logger.info(f"å¯åŠ¨ PG Consumer (é€šé…ç¬¦é˜Ÿåˆ—æ¨¡å¼)")
        logger.info("=" * 60)
        logger.info(f"å‘½åç©ºé—´: {self.namespace_name} ({self.namespace_id or 'N/A'})")
        logger.info(f"ç›‘å¬é˜Ÿåˆ—: * (æ‰€æœ‰é˜Ÿåˆ—) + TASK_CHANGES (çŠ¶æ€æ›´æ–°)")
        logger.info(f"INSERT æ‰¹é‡: {self.insert_buffer.max_size} æ¡")
        logger.info(f"UPDATE æ‰¹é‡: {self.update_buffer.max_size} æ¡")
        logger.info(f"åˆ·æ–°é—´éš”: {self.insert_buffer.max_delay} ç§’")
        logger.info(f"å¹¶å‘æ•°: {concurrency}")
        logger.info("=" * 60)

        try:
            # å¯åŠ¨ Worker
            # éœ€è¦åŒæ—¶ç›‘å¬ä¸¤ä¸ªé˜Ÿåˆ—ï¼š
            # 1. '*' - é€šé…ç¬¦åŒ¹é…æ‰€æœ‰å¸¸è§„ä»»åŠ¡é˜Ÿåˆ—ï¼ˆINSERTï¼‰
            # 2. 'TASK_CHANGES' - ä¸“é—¨çš„çŠ¶æ€æ›´æ–°é˜Ÿåˆ—ï¼ˆUPDATEï¼‰
            await self.app.start(
                queues=['*', 'TASK_CHANGES'],  # ğŸ¯ å…³é”®ï¼šç›‘å¬æ‰€æœ‰é˜Ÿåˆ— + çŠ¶æ€æ›´æ–°é˜Ÿåˆ—
                concurrency=concurrency
            )
        finally:
            await self.stop()

    async def stop(self):
        """åœæ­¢ Consumer"""
        logger.info("åœæ­¢ PG Consumer...")
        self._running = False

        # æ³¨æ„ï¼šå®šæ—¶åˆ·æ–°ä»»åŠ¡å·²ç§»é™¤ï¼Œåˆ·æ–°é€»è¾‘é›†æˆåœ¨ä»»åŠ¡å¤„ç†ä¸­

        # æœ€ååˆ·æ–°ä¸€æ¬¡ç¼“å†²åŒº
        try:
            if self.insert_buffer.records:
                await self.insert_buffer.flush(self.db_manager)
            if self.update_buffer.records:
                await self.update_buffer.flush(self.db_manager)
        except Exception as e:
            logger.error(f"æœ€ç»ˆåˆ·æ–°å¤±è´¥: {e}")

        # æ³¨æ„ï¼šä¸å…³é—­æ•°æ®åº“å¼•æ“ï¼Œå› ä¸ºå®ƒæ˜¯å…¨å±€å•ä¾‹ï¼Œç”± connector.py ç®¡ç†
        # å¤šä¸ª consumer å®ä¾‹å¯èƒ½å…±äº«åŒä¸€ä¸ªå¼•æ“

        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        insert_stats = self.insert_buffer.get_stats()
        update_stats = self.update_buffer.get_stats()

        logger.info("=" * 60)
        logger.info("PG Consumer ç»Ÿè®¡ä¿¡æ¯")
        logger.info("=" * 60)
        logger.info(f"INSERT: æ€»è®¡ {insert_stats['total_flushed']} æ¡, "
                   f"åˆ·æ–° {insert_stats['flush_count']} æ¬¡, "
                   f"å¹³å‡ {insert_stats['avg_per_flush']} æ¡/æ¬¡")
        logger.info(f"UPDATE: æ€»è®¡ {update_stats['total_flushed']} æ¡, "
                   f"åˆ·æ–° {update_stats['flush_count']} æ¬¡, "
                   f"å¹³å‡ {update_stats['avg_per_flush']} æ¡/æ¬¡")
        logger.info("=" * 60)

        logger.info("PG Consumer å·²åœæ­¢")
