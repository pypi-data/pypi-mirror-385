Metadata-Version: 2.4
Name: chalkdf
Version: 1.3.6
Summary: DataFrame utilities for Chalk AI, backed by libchalk
Author-email: "Chalk AI, Inc." <opensource@chalk.ai>
Project-URL: Homepage, https://chalk.ai
Project-URL: Documentation, https://docs.chalk.ai
Project-URL: Changelog, https://docs.chalk.ai/docs/changelog
Requires-Python: <=3.14,>=3.10
Description-Content-Type: text/markdown
Requires-Dist: frozendict>=2.4.6
Requires-Dist: python-dateutil>=2.8.2
Requires-Dist: pyarrow>=18.0.0
Requires-Dist: ddtrace<3,>=2.6.4
Provides-Extra: chalkpy
Requires-Dist: chalkpy[athena,bigquery,databricks,mysql,postgresql,redshift,snowflake,spanner,sqlite,trino]>=2.89.5; extra == "chalkpy"
Provides-Extra: dev
Requires-Dist: ruff; extra == "dev"
Requires-Dist: pytest; extra == "dev"
Requires-Dist: pytest-cov; extra == "dev"
Requires-Dist: build; extra == "dev"
Requires-Dist: google-cloud-storage>=3; extra == "dev"
Requires-Dist: polars; extra == "dev"
Requires-Dist: setuptools>=65; extra == "dev"
Requires-Dist: flaky>=3.8.1; extra == "dev"
Requires-Dist: chalkpy[athena,bigquery,databricks,mysql,postgresql,redshift,snowflake,spanner,sqlite,trino]>=2.89.5; extra == "dev"
Provides-Extra: bench
Requires-Dist: google-cloud-storage>=3; extra == "bench"
Requires-Dist: polars; extra == "bench"
Requires-Dist: setuptools>=65; extra == "bench"

# chalkdf

DataFrame utilities for building fast, portable data pipelines on top of
Apache Arrow — powered by Chalk’s libchalk execution engine.

The API centers on two concepts:
- `chalkdf.DataFrame`: a lightweight, eager plan that you can materialize to Arrow.
- `chalkdf.LazyFrame`: a serializable, chainable description of a plan that can
  be round‑tripped to/from a protobuf and converted to a `DataFrame` for
  execution.

The library ships with a small expression DSL (`chalkdf.Expr`) and pragmatic
testing helpers (`chalkdf.Testing`) to make it easy to compare results in your
unit tests.


## Installation

- Requires Python 3.10–3.12.
- Depends on `pyarrow` and Chalk’s runtime (`chalkpy`).

Install from PyPI:

```
pip install chalkdf
```


## Quickstart

Below are minimal, self‑contained examples that mirror the public API used in
the tests.

All examples assume:

```
import pyarrow as pa
from chalkdf import DataFrame, Expr

def to_table(df: DataFrame) -> pa.Table:
    batches = list(df.run())
    return pa.Table.from_batches(batches) if batches else pa.table({})
```


### Create from Arrow and select columns

```
tbl = pa.table({"a": [1, 2, 3], "b": [10, 20, 30], "c": [100, 200, 300]})
df = DataFrame.from_arrow(tbl)

out = df.select("c", "a")
print(to_table(out))
# ┌─────┬─────┐
# │  c  │  a  │
# ├─────┼─────┤
# │ 100 │  1  │
# │ 200 │  2  │
# │ 300 │  3  │
# └─────┴─────┘
```


### Add or replace columns with expressions

```
tbl = pa.table({"a": pa.array([1, 2, 3], pa.int64()), "b": pa.array([10, 20, 30], pa.int64())})
df = DataFrame.from_arrow(tbl)

out = df.with_columns(
    {
        "a": df.column("a"),
        "b": df.column("b"),
        "sum": Expr.plus(df.column("a"), df.column("b")),
        "flag": Expr.if_else(
            df.column("a") < Expr.lit(pa.scalar(2, pa.int64())),
            Expr.lit(pa.scalar("small")),
            Expr.lit(pa.scalar("big")),
        ),
    }
)
print(to_table(out))
```


### Filter, slice, and order

```
tbl = pa.table({"a": pa.array([1, 2, 3, 4, 5], pa.int64()), "b": pa.array([10, 20, 30, 40, 50], pa.int64())})
df = DataFrame.from_arrow(tbl)

pred = df.column("a") > Expr.lit(pa.scalar(2, pa.int64()))
out = df.filter(pred).slice(1, 2).order_by(("b", "descending"))
print(to_table(out))
```


### Rename and explode

```
tbl = pa.table({"id": [1, 2], "vals": pa.array([[1, 2], []], type=pa.list_(pa.int64()))})
df = DataFrame.from_arrow(tbl)

renamed = df.rename({"vals": "values"})
exploded = renamed.explode("values")
print(to_table(exploded))  # (1,1), (1,2)
```


### Group by and aggregate

```
tbl = pa.table({"g": [1, 1, 2, 2, 2], "v": pa.array([10, 20, 1, 2, 3], pa.int64())})
df = DataFrame.from_arrow(tbl)

out = df.agg(["g"], df.column("v").sum().alias("v_sum"))
print(to_table(out))  # rows may be unordered
```


### Joins

```
left = DataFrame.from_arrow(pa.table({"key": [1, 2, 3], "x": [10, 20, 30]}))
right = DataFrame.from_arrow(pa.table({"key": [2, 3, 4], "y": [200, 300, 400]}))

joined = left.join(right, on=["key"], how="inner").select("key", "x", "y")
print(to_table(joined))

# Or with an explicit mapping of right->left keys
joined2 = left.join(right, on={"key": "key"}, how="inner")
```


### Scan Parquet files

You can construct a `DataFrame` that scans one or more Parquet files without
loading them eagerly. Use local `file://` URIs (or remote URIs when running in
an environment with appropriate access):

```
df = DataFrame.scan_parquet(
    input_uris=["file:///path/to/data.parquet"],
    num_concurrent_downloads=8,
    max_num_batches_to_buffer=50,
    target_batch_size_bytes=20 * 1024 * 1024,
)

print(list(df.run()))  # yields Arrow RecordBatches
```


## Lazy plans (experimental)

`LazyFrame` records a chain of operations and can round‑trip to a protobuf for
transport or persistence. You can reconstruct the same lazy plan later and
convert it to a `DataFrame` to execute.

```
from chalkdf import LazyFrame
import pyarrow as pa

lf = (
    LazyFrame()
    .select("c1", "c2")
    .slice(0, length=10)
    .filter(Expr.lit(pa.scalar(True)))
)

proto = lf.to_proto()
lf2 = LazyFrame.from_proto(proto)
assert lf == lf2  # structural equality of the recorded plan

df = lf2._convert_to_df()  # convert to a DataFrame for execution
print(list(df.run()))
```

Notes:
- `LazyFrame._convert_to_df()` is currently a private/experimental helper.
- For catalog‑backed tables, use `LazyFrame.from_catalog_table("table_name")`
  in your chain and provide a `ChalkSqlCatalog` when converting.


## Testing helpers

Use `chalkdf.Testing.assert_frame_equal` to compare `DataFrame` results in unit
tests. It materializes both frames to Arrow and supports relaxed comparisons.

```
from chalkdf import Testing

left = DataFrame.from_arrow(pa.table({"a": [1.000001], "b": [2.0]}))
right = DataFrame.from_arrow(pa.table({"a": [1.0], "b": [2.0]}))

Testing.assert_frame_equal(left, right, atol=1e-5, rtol=0.0)

# Ignore row or column order when needed
Testing.assert_frame_equal(left, right, check_row_order=False, check_column_order=False)
```


## Why chalkdf?

- Built on Apache Arrow memory formats for zero‑copy interop.
- Runs on Chalk’s native engine for performance and portability.
- Small, predictable API surface suited for services and batch jobs.
