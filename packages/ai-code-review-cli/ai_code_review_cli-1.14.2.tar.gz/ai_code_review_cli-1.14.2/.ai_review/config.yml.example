# AI Code Review Configuration File Example
# Copy this file to .ai_review/config.yml and customize for your project
#
# ‚ö†Ô∏è  PRIORITY ORDER: CLI args > Environment variables > Config file > Field defaults
# Values in this file are overridden by environment variables and CLI arguments
#
# üìù Note: All environment variables from env.example can be used here in YAML format
# Remove the leading variable names and use lowercase with underscores for keys

#==============================================================================
# Platform Configuration
#==============================================================================

# Platform provider: gitlab, github, or local
# Auto-detected in CI/CD environments, set manually for local use
platform_provider: gitlab

# Platform URLs (for self-hosted instances)
gitlab_url: https://gitlab.com
github_url: https://api.github.com

# Platform tokens (‚ö†Ô∏è NOT RECOMMENDED: use environment variables instead for security)
# gitlab_token: glpat_xxxxxxxxxxxxxxxxxxxx
# github_token: ghp_xxxxxxxxxxxxxxxxxxxx

#==============================================================================
# AI Provider Configuration
#==============================================================================

# AI provider: gemini, anthropic, ollama
ai_provider: gemini

# AI model name for selected provider
ai_model: gemini-2.5-pro

# API key (‚ö†Ô∏è NOT RECOMMENDED: use AI_API_KEY environment variable instead for security)
# ai_api_key: your_api_key_here

# AI model parameters
temperature: 0.1        # 0.0-2.0, lower = more deterministic
max_tokens: 8000        # Maximum response tokens
http_timeout: 5.0       # HTTP timeout in seconds

#==============================================================================
# Ollama Configuration (for local AI)
#==============================================================================

# Ollama server URL (when using ollama provider)
ollama_base_url: http://localhost:11434

#==============================================================================
# SSL Configuration (for internal/self-hosted instances)
#==============================================================================

# SSL certificate verification
ssl_verify: true

# Custom SSL certificate path (for internal instances)
# ssl_cert_path: /path/to/cert.pem

#==============================================================================
# Processing Configuration
#==============================================================================

# Processing limits
max_chars: 100000       # Max characters from diff
max_files: 100          # Max files to process per review

# File filtering (glob patterns)
exclude_patterns:
  - "*.lock"              # All lock files
  - "package-lock.json"   # npm lock file
  - "yarn.lock"           # Yarn lock file
  - "Pipfile.lock"        # Pipenv lock file
  - "poetry.lock"         # Poetry lock file
  - "pnpm-lock.yaml"      # PNPM lock file
  - "*.min.js"            # Minified JavaScript
  - "*.min.css"           # Minified CSS
  - "*.map"               # Source map files
  - "node_modules/**"     # Node modules (top level)
  - "**/node_modules/**"  # Node modules (nested)
  - "__pycache__/**"      # Python cache (top level)
  - "**/__pycache__/**"   # Python cache (nested)
  - "dist/**"             # Build distributions
  - "build/**"            # Build directories
  - "*.egg-info/**"       # Python egg info

#==============================================================================
# Project Context Configuration
#==============================================================================

# Enable loading project context from file
enable_project_context: true

# Path to project context file (relative to repository root)
project_context_file: .ai_review/project.md

# Review format options
include_mr_summary: true    # Include MR Summary section in reviews

#==============================================================================
# Development/Testing Options
#==============================================================================

# Dry run mode (no actual API calls, uses mock responses)
dry_run: false

# Force large context window (24K tokens) for processing big diffs
# Note: Auto-activated when diffs exceed 60K characters
big_diffs: false

# Logging level: DEBUG, INFO, WARNING, ERROR, CRITICAL
log_level: INFO

#==============================================================================
# Example Configurations for Different Scenarios
#==============================================================================

# Example 1: Local development with Ollama
# ai_provider: ollama
# ai_model: qwen2.5-coder:7b
# ollama_base_url: http://localhost:11434
# platform_provider: local
# dry_run: false

# Example 2: GitHub with Anthropic Claude
# ai_provider: anthropic
# ai_model: claude-3-5-sonnet-20241022
# platform_provider: github
# github_url: https://api.github.com
# max_tokens: 4000

# Example 3: Self-hosted GitLab with custom SSL
# platform_provider: gitlab
# gitlab_url: https://gitlab.company.com
# ssl_verify: true
# ssl_cert_path: /etc/ssl/certs/company-ca.pem
# ai_provider: gemini
# ai_model: gemini-2.5-pro

# Example 4: Minimal config for basic usage
# ai_provider: gemini
# ai_model: gemini-2.5-pro
# platform_provider: gitlab

#==============================================================================
# Context7 Integration (Optional)
#==============================================================================

# Context7 provides official library documentation to enhance AI code reviews
# Requires Context7 API key (set via CONTEXT7_API_KEY environment variable)
# Sign up at https://context7.com for API access

context7:
  # Enable Context7 integration for library documentation
  enabled: false

  # Maximum number of libraries to fetch documentation for (default: 3)
  # Limits API calls and processing time. Can be overridden by MAX_LIBRARIES env var
  # Recommended: 3-5 for balanced performance vs coverage
  max_libraries: 3

  # Maximum tokens to fetch per library (100-10000)
  max_tokens_per_library: 2000

  # Timeout for Context7 API calls in seconds (1-60)
  timeout_seconds: 10

  # Priority libraries to fetch documentation for
  # If empty, will auto-detect important libraries from your dependencies
  priority_libraries: []

  # Example configurations for different project types:

  # Web API Project (high priority libraries)
  # max_libraries: 5
  # priority_libraries:
  #   - fastapi
  #   - pydantic
  #   - sqlalchemy
  #   - uvicorn
  #   - aiohttp

  # Data Science Project (focus on core libraries)
  # max_libraries: 4
  # priority_libraries:
  #   - pandas
  #   - numpy
  #   - scikit-learn
  #   - matplotlib

  # Django Project (web framework focused)
  # max_libraries: 5
  # priority_libraries:
  #   - django
  #   - djangorestframework
  #   - celery
  #   - redis
  #   - psycopg2

  # LangChain/AI Project (AI/ML focused)
  # max_libraries: 4
  # priority_libraries:
  #   - langchain
  #   - langchain-community
  #   - openai
  #   - anthropic

  # Performance-focused (minimal API calls)
  # max_libraries: 2
  # priority_libraries:
  #   - fastapi
  #   - pydantic

  # Comprehensive coverage (more API calls, slower)
  # max_libraries: 7
  # priority_libraries: []  # Auto-detect all important libraries
