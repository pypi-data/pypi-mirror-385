
# Prompt Injection Sources

- Willison, Simon. “Design Patterns for Securing LLM Agents against Prompt Injections.” Simon Willison’s Weblog, 13 June 2025. https://simonwillison.net/2025/Jun/13/prompt-injection-design-patterns/.

- Beurer-Kellner, Luca, Beat Buesser, Ana-Maria Crețu, Edoardo Debenedetti, et al. “Design Patterns for Securing LLM Agents against Prompt Injections.” arXiv, June 2025. https://arxiv.org/pdf/2506.08837.

- Sahar Abdelnabi, Kai Greshake, Shailesh Mishra, Christoph Endres, Thorsten Holz, and Mario Fritz. Not What You’ve Signed Up For: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection. In Proceedings of the 16th ACM Workshop on Artificial Intelligence and Security (AISec), 2023.

- Sahar Abdelnabi, Aideen Fay, Giovanni Cherubin, Ahmed Salem, Mario Fritz, and Andrew Paverd. Get my drift? catching llm task drift with activation deltas, 2025a. URL https://arxiv.org/abs/2406.00799.

- Sahar Abdelnabi, Amr Gomaa, Eugene Bagdasarian, Per Ola Kristensson, and Reza Shokri. Firewalls to secure dynamic llm agentic networks, 2025b. URL https://arxiv.org/abs/2502.01822.

- Eugene Bagdasarian, Ren Yi, Sahra Ghalebikesabi, Peter Kairouz, Marco Gruteser, Sewoong Oh, Borja Balle, and Daniel Ramage. AirGapAgent: Protecting privacy-conscious conversational agents, 2024. URL https://arxiv.org/abs/2405.05175.

- Mislav Balunovic, Luca Beurer-Kellner, Marc Fischer, and Martin Vechev. AI agents with formal security guarantees. In ICML 2024 Next Generation of AI Safety Workshop, 2024. URL https://openreview.net/forum?id=c6jNHPksiZ.

- Luca Beurer-Kellner, Marc Fischer, and Martin T. Vechev. Guiding llms the right way: Fast, non-invasive constrained generation. In ICML. OpenReview.net, 2024.

- Maarten AS Boksem and Mattie Tops. Mental fatigue: costs and benefits. Brain research reviews, 59(1), 2008.

- Sizhe Chen, Julien Piet, Chawin Sitawarin, and David Wagner. StruQ: Defending against prompt injection with structured queries. arXiv preprint, 2024. URL https://arxiv.org/abs/2402.06363.

- Benjamin Cohen-Wang, Harshay Shah, Kristian Georgiev, and Aleksander Madry. ContextCite: Attributing model generation to context. In NeurIPS, 2024.

- Jeffrey Dean and Sanjay Ghemawat. MapReduce: Simplified data processing on large clusters. Communications of the ACM, 51(1):107–113, 2008.

- Edoardo Debenedetti, Jie Zhang, Mislav Balunović, Luca Beurer-Kellner, Marc Fischer, and Florian Tramèr. AgentDojo: A Dynamic Environment to Evaluate Prompt Injection Attacks and Defenses for LLM Agents. arXiv preprint, 2024. URL https://arxiv.org/abs/2406.13352.

- Edoardo Debenedetti, Ilia Shumailov, Tianqi Fan, Jamie Hayes, Nicholas Carlini, Daniel Fabian, Christoph Kern, Chongyang Shi, Andreas Terzis, and Florian Tramèr. Defeating Prompt Injections by Design, 2025. URL https://arxiv.org/abs/2503.18813.

- Embrace The Red. Hacking Google Bard: From Prompt Injection to Data Exfiltration. https://embracethered.com/blog/posts/2023/google-bard-data-exfiltration/, 2023.

- Embrace The Red. ASCII Smuggler Tool: Crafting Invisible Text and Decoding Hidden Codes. https://embracethered.com/blog/posts/2024/hiding-and-finding-text-with-unicode-tags/, 2024.

- Embrace The Red. GitHub Copilot Chat: From Prompt Injection to Data Exfiltration (Copirate). https://embracethered.com/blog/posts/2024/github-copilot-chat-prompt-injection-data-exfiltration/, 2024.

- Xiaohan Fu, Zihan Wang, Shuheng Li, Rajesh K. Gupta, Niloofar Mireshghallah, Taylor Berg-Kirkpatrick, and Earlence Fernandes. Misusing tools in large language models with visual adversarial examples. CoRR, abs/2310.03185, 2023.

- Goodside. I genuinely believe prompt engineering is the highest-leverage skill someone can learn in 2022, Sep 2022a. URL https://x.com/goodside/status/1569128808308957185.

- Riley Goodside. Exploiting GPT-3 prompts with malicious inputs that order the model to ignore its previous directions. https://x.com/goodside/status/1569128808308957185, 2022b.

- Daniel Kang, Xuechen Li, Ion Stoica, Carlos Guestrin, Matei Zaharia, and Tatsunori Hashimoto. Exploiting programmatic behavior of llms: Dual-use through standard security attacks. In SP (Workshops), pp. 132–143. IEEE, 2024.

- Yupei Liu, Yuqi Jia, Runpeng Geng, Jinyuan Jia, and Neil Zhenqiang Gong. Formalizing and Benchmarking Prompt Injection Attacks and Defenses. In Proceedings of the 33rd USENIX Security Symposium (USENIX Security ’24), 2024.

- Jennifer M. Logg, Julia A. Minson, and Don A. Moore. Algorithm appreciation: People prefer algorithmic to human judgment. Organizational Behavior and Human Decision Processes, 2019.

- Nicholas Micallef, Mike Just, Lynne Baillie, and Maher Alharby. Stop annoying me! An empirical investigation of the usability of app privacy notifications. In Proceedings of OzCHI ’17, 2017. URL https://doi.org/10.1145/3152771.3156139.

- Shishir G. Patil, Tianjun Zhang, Xin Wang, and Joseph E. Gonzalez. Gorilla: Large language model connected with massive APIs. In NeurIPS, 2024.

- Fabio Perez and Ian Ribeiro. Ignore Previous Prompt: Attack Techniques for Language Models. arXiv preprint, 2022. URL https://arxiv.org/abs/2211.09527.

- ProtectAI.com. Fine-tuned DeBERTa-v3-base for prompt injection detection, 2024. URL https://huggingface.co/ProtectAI/deberta-v3-base-prompt-injection-v2.

- Roman Samoilenko. New prompt injection attack on ChatGPT web version: Markdown images can steal your chat data. https://systemweakness.com/new-prompt-injection-attack-on-chatgpt-web-version-ef717492c5c2, 2024.

- Gregory Schwartzman. Exfiltration of personal information from ChatGPT via prompt injection, 2024. URL https://arxiv.org/abs/2406.00199.

- Shoaib Ahmed Siddiqui, Radhika Gaonkar, Boris Kopf, David Krueger, Andrew Paverd, Ahmed Salem, Shruti Tople, Lukas Wutschitz, Menglin Xia, and Santiago Zanella-Beguelin. Permissive information-flow analysis for large language models, 2024. URL https://arxiv.org/abs/2410.03055.

- Joseph Spracklen, Raveen Wijewickrama, A. H. M. Nazmus Sakib, Anindya Maiti, and Murtuza Jadliwala. We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs. arXiv preprint, 2024. URL https://arxiv.org/abs/2406.10279.

- Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing Properties of Neural Networks, 2014. URL https://arxiv.org/abs/1312.6199.

- Eric Wallace, Kai Xiao, Reimar Leike, Lilian Weng, Johannes Heidecke, and Alex Beutel. The Instruction Hierarchy: Training LLMs to Prioritize Privileged Instructions, 2024.

- Simon Willison. Prompt injection attacks against GPT-3. https://simonwillison.net/2022/Sep/12/prompt-injection/, 2022.

- Simon Willison. Delimiters won’t save you from prompt injection. https://simonwillison.net/2023/May/11/delimiters-wont-save-you/, 2023a.

- Simon Willison. The Dual LLM Pattern for Building AI Assistants That Can Resist Prompt Injection. https://simonwillison.net/2023/Apr/25/dual-llm-pattern/, 2023b.

- Yuhao Wu, Franziska Roesner, Tadayoshi Kohno, Ning Zhang, and Umar Iqbal. IsolateGPT: An Execution Isolation Architecture for LLM-Based Agentic Systems. In NDSS Symposium, 2025.

- Chong Xiang, Tong Wu, Zexuan Zhong, David Wagner, Danqi Chen, and Prateek Mittal. Certifiably Robust RAG against Retrieval Corruption, 2024. URL https://arxiv.org/abs/2405.15556.

- Hui Yang, Sifu Yue, and Yunzhong He. Auto-GPT for Online Decision Making: Benchmarks and Additional Opinions, 2023. URL https://arxiv.org/abs/2306.02224.

- John Yang, Carlos E. Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, and Ofir Press. Swe-agent: Agent-computer interfaces enable automated software engineering. In NeurIPS, 2024.

- Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents. In NeurIPS, 2022.

- Jingwei Yi, Yueqi Xie, Bin Zhu, Emre Kiciman, Guangzhong Sun, Xing Xie, and Fangzhao Wu. Benchmarking and Defending Against Indirect Prompt Injection Attacks on Large Language Models, 2023.

- Andy Zou, Long Phan, Justin Wang, Derek Duenas, Maxwell Lin, Maksym Andriushchenko, J. Zico Kolter, Matt Fredrikson, and Dan Hendrycks. Improving Alignment and Robustness with Circuit Breakers. In NeurIPS, 2024.

- Egor Zverev, Sahar Abdelnabi, Mario Fritz, and Christoph H. Lampert. Can LLMs Separate Instructions From Data? And What Do We Even Mean By That? arXiv preprint, 2024. URL https://arxiv.org/abs/2403.06833.
