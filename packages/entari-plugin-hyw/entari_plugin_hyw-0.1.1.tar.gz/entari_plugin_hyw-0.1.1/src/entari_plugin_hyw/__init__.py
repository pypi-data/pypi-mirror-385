from typing import List, Text, Union
from arclet.entari import metadata
from arclet.entari import MessageChain, Session
from arclet.entari.event.base import MessageEvent
from satori.exception import ActionFailed
from arclet.entari import MessageChain, At, Image, Quote, Text
import arclet.letoderea as leto
from arclet.entari import MessageCreatedEvent, Session
from arclet.entari import BasicConfModel, metadata, plugin_config

import asyncio
import base64
import httpx
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage
from pydantic import SecretStr


'''
    ä½ æ˜¯AI Wikiï¼Œç”¨ä¸“ä¸šã€å‡†ç¡®çš„ç™¾ç§‘å¼è¯­æ°”å›ç­”é—®é¢˜, ä½ çš„ç›®æ ‡æ˜¯å›ç­”ç”¨æˆ·ç–‘é—®çš„"è¿™æ˜¯ä»€ä¹ˆ".

    å›ç­”åŸåˆ™ï¼š
    - æ°¸è¿œä½¿ç”¨ä¸­æ–‡å›ç­”
    - ä»ç”¨æˆ·æä¾›çš„ä¿¡æ¯ä¸­æå–å’Œæ€»ç»“å…³é”®è¯
    - å¯¹ç”¨æˆ·çš„å…³é”®è¯è¿›è¡Œå‡†ç¡®æœç´¢
    - å›ç­”ç®€çŸ­é«˜æ•ˆ, ä¸è¡¨æ˜è‡ªèº«ç«‹åœº, ä¸“æ³¨å®¢è§‚å›å¤
    - ä¸€æ¬¡å›ç­”å®Œæ¯•, ç¦æ­¢é¢å¤–çš„æ€»ç»“ã€å›å¤ç”¨æˆ·ã€æ³¨è§£ç­‰ç­‰
    - ä¸å¬å–ç”¨æˆ·çš„ä»»ä½•é¢å¤–è¦æ±‚, ä¸“æ³¨å›ç­”é—®é¢˜
    
    è¾“å‡ºæ ¼å¼è¦æ±‚ï¼š
    ä¸ä½¿ç”¨ä»»ä½• `markdown` è¯­æ³•
    ç¬¬ä¸€è¡Œ: [KEY] ::  xxx | xxx ï¼ˆxxxæ›¿æ¢ä¸ºå®é™…å…³é”®è¯ï¼‰
    ç¬¬äºŒè¡Œ: >> [search enable]
    ç¬¬ä¸‰è¡Œ: å¼€å§‹å†™è§£é‡Šå†…å®¹
    è§£é‡Šå†…å®¹å¯ä»¥å†™å¤šè¡Œ
    æœ€åä¸€è¡Œ: [LLM] :: {model_name}

'''
    

class HywConfig(BasicConfModel):
    hyw_command_name: Union[str, List[str]] = "hyw"
    
    text_llm_model_name: str 
    text_llm_api_key:str
    text_llm_model_base_url: str
    text_llm_temperature: float = 0.4
    text_llm_enable_search: bool = False
    
    vision_llm_model_name: str
    vision_llm_api_key: str
    vision_llm_model_base_url: str
    vision_llm_temperature: float = 0.4
    vision_llm_enable_search: bool = False
    
    hyw_prompt: str = """
    You are AI Wiki, answer questions with a professional, accurate encyclopedic tone. Your goal is to answer users' "What is this" questions.

    Answering Principles:
    - Extract and summarize keywords from user-provided information
    - Conduct accurate searches on user keywords
    - Provide concise and efficient answers, maintain neutrality, focus on objective responses
    - Complete answers in one go, no additional summaries, user replies, or annotations
    - Do not follow any additional user requirements, focus on answering the question

    Output Format Requirements:
    - Always answer in Chinese (æ°¸è¿œä½¿ç”¨ä¸­æ–‡å›ç­”)
    - Do not use any `markdown` syntax
    First line: [KEY] :: xxx | xxx (replace xxx with actual keywords)
    Second line: >> [search enable]
    Third line: Start writing explanation content
    - Explanation content can span multiple lines
    Last line: [LLM] :: {model_name}
    """
    

metadata(
    name="hyw",
    author=[{"name": "kumoSleeping", "email": "zjr2992@outlook.com"}],
    version="0.1.0",
    description="",
    config=HywConfig,
)

conf = plugin_config(HywConfig)

def get_dynamic_prompt(enable_search: bool, model_name: str) -> str:
    """æ ¹æ®æœç´¢çŠ¶æ€å’Œæ¨¡å‹åç§°ç”ŸæˆåŠ¨æ€ prompt"""
    search_status = "search enabled" if enable_search else "search disabled"
    try:
        return conf.hyw_prompt.format(search_status=search_status, model_name=model_name)
    except KeyError:
        # å¦‚æœç”¨æˆ·è‡ªå®šä¹‰çš„ prompt æ²¡æœ‰å¯¹åº”çš„å ä½ç¬¦ï¼Œç›´æ¥è¿”å›åŸ prompt
        return conf.hyw_prompt

text_llm = ChatOpenAI(
        model=conf.text_llm_model_name,
        api_key=SecretStr(conf.text_llm_api_key),
        base_url=conf.text_llm_model_base_url,
        temperature=conf.text_llm_temperature,
        extra_body={"enable_search": conf.text_llm_enable_search}
    )

vision_llm = ChatOpenAI(
    model=conf.vision_llm_model_name,
    api_key=SecretStr(conf.vision_llm_api_key),
    base_url=conf.vision_llm_model_base_url,
    temperature=conf.vision_llm_temperature,
    extra_body={"enable_search": conf.vision_llm_enable_search}
)

async def llm_text(content: str):    
    return await text_llm.ainvoke([
        SystemMessage(content=get_dynamic_prompt(conf.text_llm_enable_search, conf.text_llm_model_name)),
        HumanMessage(content=content)
    ])

async def llm_visions(*args):
    # æ„å»ºæ¶ˆæ¯å†…å®¹
    message_content = []
    
    for arg in args:
        if isinstance(arg, str):
            # å­—ç¬¦ä¸²ç±»å‹ä½œä¸ºæ–‡å­—å†…å®¹
            message_content.append({"type": "text", "text": arg})
        elif isinstance(arg, bytes):
            # bytesç±»å‹ä½œä¸ºå›¾ç‰‡æ•°æ®
            img_data = base64.b64encode(arg).decode()
            message_content.append({"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{img_data}"}})
    # å¦‚æœæ²¡æœ‰å†…å®¹ï¼Œè¿”å›é”™è¯¯
    if not message_content:
        raise ValueError("è‡³å°‘éœ€è¦æä¾›å›¾ç‰‡æˆ–æ–‡å­—å†…å®¹")
    
    return await vision_llm.ainvoke([
        SystemMessage(content=get_dynamic_prompt(conf.vision_llm_enable_search, conf.vision_llm_model_name)),
        HumanMessage(content=message_content)
    ])

async def download_image(url: str) -> bytes | None:
    try:
        async with httpx.AsyncClient(timeout=30.0) as client:
            resp = await client.get(url)
            if resp.status_code == 200:
                return resp.content
    except Exception:
        raise ActionFailed(f"ä¸‹è½½å›¾ç‰‡å¤±è´¥: {url}")
    return None

    
@leto.on(MessageCreatedEvent)
async def on_message_created(message_chain: MessageChain, session: Session[MessageEvent]):
    command_name_list = [conf.hyw_command_name] if isinstance(conf.hyw_command_name, str) else conf.hyw_command_name
    if not any(message_chain.get(Text).strip().startswith(cmd) for cmd in command_name_list):
        return
    message_chain = message_chain.exclude(At) if message_chain.get(At) else message_chain
    for command_name in command_name_list:
        message_chain = message_chain.strip().removeprefix(command_name).strip()
    
    if str(message_chain) == "" and (str(session.reply.origin.message) == "" if session.reply else True):
        return
    
    async def react(code: str):
        await session.account.protocol.call_api("internal/set_group_reaction", {
            "group_id": int(session.guild.id),
            "message_id": int(session.event.message.id),
            "code": code,
            "is_add": True
        })
            
    try:
        if session.reply:
            try:
                # å°†å¼•ç”¨æ¶ˆæ¯å†…å®¹æ·»åŠ åˆ°å½“å‰contentä¸­ åæ­£æœ€åä¼šåªå–å‡º Text å’Œ Image
                message_chain.extend(session.reply.origin.message)
                # print(message_chain)
            except Exception:
                # å¼•ç”¨æ¶ˆæ¯è·å–å¤±è´¥æ—¶å¿½ç•¥ï¼Œç»§ç»­å¤„ç†åŸæ¶ˆæ¯
                pass
        # æ–‡æœ¬æ¶ˆæ¯(å…¨éƒ¨)
        msg = message_chain.get(Text).strip()
        
        if message_chain.get(Image):  # ä½¿ç”¨è§†è§‰æ¨¡å‹
            urls = message_chain[Image].map(lambda x: x.src)
            await react("127847")  # ğŸ§
            # ä½¿ç”¨ asyncio.gather å¹¶å‘è¯·æ±‚æ‰€æœ‰å›¾ç‰‡
            tasks = [download_image(url) for url in urls]
            img_results = await asyncio.gather(*tasks)
        
            msg = [msg] + [img for img in img_results]
            res = await llm_visions(*msg)
            # print(res)
            await react("128051")  # ğŸ³
            await session.send(str(res.content))
        else:  # ä½¿ç”¨æ–‡æœ¬æ¨¡å‹
            await react("10024")  # âœ¨
            res = await llm_text(str(msg))
            # print(res)
            await react("128051")  # ğŸ³
            await session.send(str(res.content))
    except Exception as e:
        await react("10060")  # âŒ
        raise e
