{"allContent":{"docusaurus-plugin-content-docs":{"default":{"loadedVersions":[{"versionName":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","path":"/docs/","tagsPath":"/docs/tags","editUrl":"https://github.com/your-org/your-repo-name/edit/main/docs/docs","isLast":true,"routePriority":-1,"sidebarFilePath":"C:\\Users\\HP\\PycharmProjects\\cognee-eval\\my-website\\sidebars.js","contentPath":"C:\\Users\\HP\\PycharmProjects\\cognee-eval\\my-website\\docs","docs":[{"id":"getting-started/project-structure","title":"Project Structure","description":"","source":"@site/docs/getting-started/project-structure.md","sourceDirName":"getting-started","slug":"/getting-started/project-structure","permalink":"/docs/getting-started/project-structure","draft":false,"unlisted":false,"editUrl":"https://github.com/your-org/your-repo-name/edit/main/docs/docs/getting-started/project-structure.md","tags":[],"version":"current","frontMatter":{"id":"project-structure","title":"Project Structure"},"sidebar":"docs","previous":{"title":"QuickStart Guide","permalink":"/docs/getting-started/quickstart"},"next":{"title":"End-to-end evaluation tutorial","permalink":"/docs/getting-started/tutorials/end-to-end-eval"}},{"id":"getting-started/quickstart","title":"QuickStart Guide","description":"Vero-Eval is an open-source evaluation framework designed to rigorously assess the performance of Retrieval-Augmented Generation (RAG) pipelines. It provides built-in tracing, logging, and a rich suite of metrics to evaluate each component in the pipeline — from retrieval and reranking to generation — all integrated end to end.","source":"@site/docs/getting-started/quickstart.md","sourceDirName":"getting-started","slug":"/getting-started/quickstart","permalink":"/docs/getting-started/quickstart","draft":false,"unlisted":false,"editUrl":"https://github.com/your-org/your-repo-name/edit/main/docs/docs/getting-started/quickstart.md","tags":[],"version":"current","frontMatter":{"id":"quickstart","title":"QuickStart Guide"},"sidebar":"docs","previous":{"title":"AI Evaluation Framework","permalink":"/docs/"},"next":{"title":"Project Structure","permalink":"/docs/getting-started/project-structure"}},{"id":"getting-started/tutorials/end-to-end-eval","title":"End-to-end evaluation tutorial","description":"This tutorial shows how to run an evaluation suite against a small dataset.","source":"@site/docs/getting-started/tutorials/end-to-end eval.md","sourceDirName":"getting-started/tutorials","slug":"/getting-started/tutorials/end-to-end-eval","permalink":"/docs/getting-started/tutorials/end-to-end-eval","draft":false,"unlisted":false,"editUrl":"https://github.com/your-org/your-repo-name/edit/main/docs/docs/getting-started/tutorials/end-to-end eval.md","tags":[],"version":"current","frontMatter":{"id":"end-to-end-eval","title":"End-to-end evaluation tutorial"},"sidebar":"docs","previous":{"title":"Project Structure","permalink":"/docs/getting-started/project-structure"},"next":{"title":"Metrics overview","permalink":"/docs/metrics/overview"}},{"id":"intro","title":"AI Evaluation Framework","description":"Vero is a comprehensive platform for the evaluation and continuous monitoring of AI pipelines, ensuring optimal performance and enterprise-grade reliability. It empowers developers and teams to transform uncertainty into confidence by providing a straightforward, four-step process to evaluate, monitor, and gain unparalleled confidence in their AI pipelines.","source":"@site/docs/intro.md","sourceDirName":".","slug":"/","permalink":"/docs/","draft":false,"unlisted":false,"editUrl":"https://github.com/your-org/your-repo-name/edit/main/docs/docs/intro.md","tags":[],"version":"current","frontMatter":{"id":"intro","title":"AI Evaluation Framework","slug":"/"},"sidebar":"docs","next":{"title":"QuickStart Guide","permalink":"/docs/getting-started/quickstart"}},{"id":"metrics/generation/alignscore","title":"AlignScore","description":"A factual consistency / alignment metric that evaluates how well a claim (generated or asserted text) is supported by a context (retrieved or original text).","source":"@site/docs/metrics/generation/alignscore.md","sourceDirName":"metrics/generation","slug":"/metrics/generation/alignscore","permalink":"/docs/metrics/generation/alignscore","draft":false,"unlisted":false,"editUrl":"https://github.com/your-org/your-repo-name/edit/main/docs/docs/metrics/generation/alignscore.md","tags":[],"version":"current","frontMatter":{"id":"alignscore","title":"AlignScore"},"sidebar":"docs","previous":{"title":"SEMScore","permalink":"/docs/metrics/generation/semscore"},"next":{"title":"G-Eval","permalink":"/docs/metrics/generation/geval"}},{"id":"metrics/generation/bartscore","title":"BARTScore","description":"A generation evaluation metric that uses a pretrained BART model to assess the quality of generated text against reference text and is a type of comparison score   .","source":"@site/docs/metrics/generation/bartscore.md","sourceDirName":"metrics/generation","slug":"/metrics/generation/bartscore","permalink":"/docs/metrics/generation/bartscore","draft":false,"unlisted":false,"editUrl":"https://github.com/your-org/your-repo-name/edit/main/docs/docs/metrics/generation/bartscore.md","tags":[],"version":"current","frontMatter":{"id":"bartscore","title":"BARTScore"},"sidebar":"docs","previous":{"title":"BERTScore","permalink":"/docs/metrics/generation/bertscore"},"next":{"title":"BleurtScore","permalink":"/docs/metrics/generation/bleurtscore"}},{"id":"metrics/generation/bertscore","title":"BERTScore","description":"It is an automatic evaluation metric for text generation tasks that measures the similarity between candidate and reference texts using contextual embeddings from pre-trained BERT models.","source":"@site/docs/metrics/generation/bertscore.md","sourceDirName":"metrics/generation","slug":"/metrics/generation/bertscore","permalink":"/docs/metrics/generation/bertscore","draft":false,"unlisted":false,"editUrl":"https://github.com/your-org/your-repo-name/edit/main/docs/docs/metrics/generation/bertscore.md","tags":[],"version":"current","frontMatter":{"id":"bertscore","title":"BERTScore"},"sidebar":"docs","previous":{"title":"Metrics overview","permalink":"/docs/metrics/overview"},"next":{"title":"BARTScore","permalink":"/docs/metrics/generation/bartscore"}},{"id":"metrics/generation/bleurtscore","title":"BleurtScore","description":"An advanced metric based on BLEURT that produces a more nuanced weighted similarity score.","source":"@site/docs/metrics/generation/bleurtscore.md","sourceDirName":"metrics/generation","slug":"/metrics/generation/bleurtscore","permalink":"/docs/metrics/generation/bleurtscore","draft":false,"unlisted":false,"editUrl":"https://github.com/your-org/your-repo-name/edit/main/docs/docs/metrics/generation/bleurtscore.md","tags":[],"version":"current","frontMatter":{"id":"bleurtscore","title":"BleurtScore"},"sidebar":"docs","previous":{"title":"BARTScore","permalink":"/docs/metrics/generation/bartscore"},"next":{"title":"ROUGEScore","permalink":"/docs/metrics/generation/rougescore"}},{"id":"metrics/generation/geval","title":"G-Eval","description":"An LLM-based evaluation framework where a large language model directly scores the generated output against criteria such as relevance, consistency, and fluency.","source":"@site/docs/metrics/generation/geval.md","sourceDirName":"metrics/generation","slug":"/metrics/generation/geval","permalink":"/docs/metrics/generation/geval","draft":false,"unlisted":false,"editUrl":"https://github.com/your-org/your-repo-name/edit/main/docs/docs/metrics/generation/geval.md","tags":[],"version":"current","frontMatter":{"id":"geval","title":"G-Eval"},"sidebar":"docs","previous":{"title":"AlignScore","permalink":"/docs/metrics/generation/alignscore"},"next":{"title":"Recall Score","permalink":"/docs/metrics/retriever/recallscore"}},{"id":"metrics/generation/rougescore","title":"ROUGEScore","description":"ROUGE-L focuses on the Longest Common Subsequence (LCS) between generated and reference texts.","source":"@site/docs/metrics/generation/rougescore.md","sourceDirName":"metrics/generation","slug":"/metrics/generation/rougescore","permalink":"/docs/metrics/generation/rougescore","draft":false,"unlisted":false,"editUrl":"https://github.com/your-org/your-repo-name/edit/main/docs/docs/metrics/generation/rougescore.md","tags":[],"version":"current","frontMatter":{"id":"rougescore","title":"ROUGEScore"},"sidebar":"docs","previous":{"title":"BleurtScore","permalink":"/docs/metrics/generation/bleurtscore"},"next":{"title":"SEMScore","permalink":"/docs/metrics/generation/semscore"}},{"id":"metrics/generation/semscore","title":"SEMScore","description":"A metric that measures semantic similarity between candidate (generated) text and reference text using embeddings and cosine similarity.","source":"@site/docs/metrics/generation/semscore.md","sourceDirName":"metrics/generation","slug":"/metrics/generation/semscore","permalink":"/docs/metrics/generation/semscore","draft":false,"unlisted":false,"editUrl":"https://github.com/your-org/your-repo-name/edit/main/docs/docs/metrics/generation/semscore.md","tags":[],"version":"current","frontMatter":{"id":"semscore","title":"SEMScore"},"sidebar":"docs","previous":{"title":"ROUGEScore","permalink":"/docs/metrics/generation/rougescore"},"next":{"title":"AlignScore","permalink":"/docs/metrics/generation/alignscore"}},{"id":"metrics/overview","title":"Metrics overview","description":"The RAG Evaluation Framework supports three classes of metrics:","source":"@site/docs/metrics/overview.md","sourceDirName":"metrics","slug":"/metrics/overview","permalink":"/docs/metrics/overview","draft":false,"unlisted":false,"editUrl":"https://github.com/your-org/your-repo-name/edit/main/docs/docs/metrics/overview.md","tags":[],"version":"current","frontMatter":{"id":"overview","title":"Metrics overview"},"sidebar":"docs","previous":{"title":"End-to-end evaluation tutorial","permalink":"/docs/getting-started/tutorials/end-to-end-eval"},"next":{"title":"BERTScore","permalink":"/docs/metrics/generation/bertscore"}},{"id":"metrics/reranker/cumulative-ndcg","title":"Cumulative NDCG","description":"Unique implementation of NDCG@k that can be used to evaluate the cumulative performance of retriever and reranker.","source":"@site/docs/metrics/reranker/cumulative-ndcg.md","sourceDirName":"metrics/reranker","slug":"/metrics/reranker/cumulative-ndcg","permalink":"/docs/metrics/reranker/cumulative-ndcg","draft":false,"unlisted":false,"editUrl":"https://github.com/your-org/your-repo-name/edit/main/docs/docs/metrics/reranker/cumulative-ndcg.md","tags":[],"version":"current","frontMatter":{"id":"cumulative-ndcg","title":"Cumulative NDCG"},"sidebar":"docs","previous":{"title":"Reranker NDCG","permalink":"/docs/metrics/reranker/rerank-ndcg"}},{"id":"metrics/reranker/meanap","title":"MeanAP","description":"Mean Average Precision: average over queries of the average precision value.","source":"@site/docs/metrics/reranker/map.md","sourceDirName":"metrics/reranker","slug":"/metrics/reranker/meanap","permalink":"/docs/metrics/reranker/meanap","draft":false,"unlisted":false,"editUrl":"https://github.com/your-org/your-repo-name/edit/main/docs/docs/metrics/reranker/map.md","tags":[],"version":"current","frontMatter":{"id":"meanap","title":"MeanAP"},"sidebar":"docs","previous":{"title":"MeanRR","permalink":"/docs/metrics/reranker/meanrr"},"next":{"title":"Reranker NDCG","permalink":"/docs/metrics/reranker/rerank-ndcg"}},{"id":"metrics/reranker/meanrr","title":"MeanRR","description":"Mean Reciprocal Rank: for each query, take reciprocal rank of first relevant item, then average.","source":"@site/docs/metrics/reranker/meanrr.md","sourceDirName":"metrics/reranker","slug":"/metrics/reranker/meanrr","permalink":"/docs/metrics/reranker/meanrr","draft":false,"unlisted":false,"editUrl":"https://github.com/your-org/your-repo-name/edit/main/docs/docs/metrics/reranker/meanrr.md","tags":[],"version":"current","frontMatter":{"id":"meanrr","title":"MeanRR"},"sidebar":"docs","previous":{"title":"Sufficiency Score","permalink":"/docs/metrics/retriever/sufficiencyscore"},"next":{"title":"MeanAP","permalink":"/docs/metrics/reranker/meanap"}},{"id":"metrics/reranker/rerank-ndcg","title":"Reranker NDCG","description":"Normalized Discounted Cumulative Gain for ranking metrics using graded relevance scores.","source":"@site/docs/metrics/reranker/reranker-ndcg.md","sourceDirName":"metrics/reranker","slug":"/metrics/reranker/rerank-ndcg","permalink":"/docs/metrics/reranker/rerank-ndcg","draft":false,"unlisted":false,"editUrl":"https://github.com/your-org/your-repo-name/edit/main/docs/docs/metrics/reranker/reranker-ndcg.md","tags":[],"version":"current","frontMatter":{"id":"rerank-ndcg","title":"Reranker NDCG"},"sidebar":"docs","previous":{"title":"MeanAP","permalink":"/docs/metrics/reranker/meanap"},"next":{"title":"Cumulative NDCG","permalink":"/docs/metrics/reranker/cumulative-ndcg"}},{"id":"metrics/retriever/citationscore","title":"Citation Score","description":"Measures the overlap of retrieved items with ground truth citations.","source":"@site/docs/metrics/retriever/citationscore.md","sourceDirName":"metrics/retriever","slug":"/metrics/retriever/citationscore","permalink":"/docs/metrics/retriever/citationscore","draft":false,"unlisted":false,"editUrl":"https://github.com/your-org/your-repo-name/edit/main/docs/docs/metrics/retriever/citationscore.md","tags":[],"version":"current","frontMatter":{"id":"citationscore","title":"Citation Score"},"sidebar":"docs","previous":{"title":"Precision Score","permalink":"/docs/metrics/retriever/precisionscore"},"next":{"title":"Sufficiency Score","permalink":"/docs/metrics/retriever/sufficiencyscore"}},{"id":"metrics/retriever/precisionscore","title":"Precision Score","description":"Measures how many retrieved items are relevant (i.e. in ground truth).","source":"@site/docs/metrics/retriever/precision.md","sourceDirName":"metrics/retriever","slug":"/metrics/retriever/precisionscore","permalink":"/docs/metrics/retriever/precisionscore","draft":false,"unlisted":false,"editUrl":"https://github.com/your-org/your-repo-name/edit/main/docs/docs/metrics/retriever/precision.md","tags":[],"version":"current","frontMatter":{"id":"precisionscore","title":"Precision Score"},"sidebar":"docs","previous":{"title":"Recall Score","permalink":"/docs/metrics/retriever/recallscore"},"next":{"title":"Citation Score","permalink":"/docs/metrics/retriever/citationscore"}},{"id":"metrics/retriever/recallscore","title":"Recall Score","description":"Measures how many ground truth items are retrieved.","source":"@site/docs/metrics/retriever/recall.md","sourceDirName":"metrics/retriever","slug":"/metrics/retriever/recallscore","permalink":"/docs/metrics/retriever/recallscore","draft":false,"unlisted":false,"editUrl":"https://github.com/your-org/your-repo-name/edit/main/docs/docs/metrics/retriever/recall.md","tags":[],"version":"current","frontMatter":{"id":"recallscore","title":"Recall Score"},"sidebar":"docs","previous":{"title":"G-Eval","permalink":"/docs/metrics/generation/geval"},"next":{"title":"Precision Score","permalink":"/docs/metrics/retriever/precisionscore"}},{"id":"metrics/retriever/sufficiencyscore","title":"Sufficiency Score","description":"Determines whether the retrieved set is sufficient to cover all ground truth items.","source":"@site/docs/metrics/retriever/sufficiencyscore.md","sourceDirName":"metrics/retriever","slug":"/metrics/retriever/sufficiencyscore","permalink":"/docs/metrics/retriever/sufficiencyscore","draft":false,"unlisted":false,"editUrl":"https://github.com/your-org/your-repo-name/edit/main/docs/docs/metrics/retriever/sufficiencyscore.md","tags":[],"version":"current","frontMatter":{"id":"sufficiencyscore","title":"Sufficiency Score"},"sidebar":"docs","previous":{"title":"Citation Score","permalink":"/docs/metrics/retriever/citationscore"},"next":{"title":"MeanRR","permalink":"/docs/metrics/reranker/meanrr"}}],"drafts":[],"sidebars":{"docs":[{"type":"doc","id":"intro"},{"type":"category","label":"Getting Started","items":[{"type":"doc","id":"getting-started/quickstart"},{"type":"doc","id":"getting-started/project-structure"},{"type":"category","label":"Tutorials","items":[{"type":"doc","id":"getting-started/tutorials/end-to-end-eval"}],"collapsed":true,"collapsible":true}],"collapsed":true,"collapsible":true},{"type":"category","label":"Metrics","items":[{"type":"doc","id":"metrics/overview"},{"type":"category","label":"Generation Metrics","items":[{"type":"doc","id":"metrics/generation/bertscore"},{"type":"doc","id":"metrics/generation/bartscore"},{"type":"doc","id":"metrics/generation/bleurtscore"},{"type":"doc","id":"metrics/generation/rougescore"},{"type":"doc","id":"metrics/generation/semscore"},{"type":"doc","id":"metrics/generation/alignscore"},{"type":"doc","id":"metrics/generation/geval"}],"collapsed":true,"collapsible":true},{"type":"category","label":"Retriever Metrics","items":[{"type":"doc","id":"metrics/retriever/recallscore"},{"type":"doc","id":"metrics/retriever/precisionscore"},{"type":"doc","id":"metrics/retriever/citationscore"},{"type":"doc","id":"metrics/retriever/sufficiencyscore"}],"collapsed":true,"collapsible":true},{"type":"category","label":"Reranker Metrics","items":[{"type":"doc","id":"metrics/reranker/meanrr"},{"type":"doc","id":"metrics/reranker/meanap"},{"type":"doc","id":"metrics/reranker/rerank-ndcg"},{"type":"doc","id":"metrics/reranker/cumulative-ndcg"}],"collapsed":true,"collapsible":true}],"collapsed":true,"collapsible":true}]}}]}},"docusaurus-plugin-debug":{},"docusaurus-plugin-svgr":{},"docusaurus-theme-classic":{},"docusaurus-bootstrap-plugin":{},"docusaurus-mdx-fallback-plugin":{}}}