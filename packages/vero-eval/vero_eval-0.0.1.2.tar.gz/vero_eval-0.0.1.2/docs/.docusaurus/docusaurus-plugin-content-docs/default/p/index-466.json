{"version":{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"docs":[{"type":"link","href":"/","label":"RAG Evaluation Framework","docId":"intro","unlisted":false},{"type":"category","label":"Getting Started","items":[{"type":"link","href":"/getting-started/quickstart","label":"QuickStart Guide","docId":"getting-started/quickstart","unlisted":false},{"type":"link","href":"/getting-started/project-structure","label":"Project Structure","docId":"getting-started/project-structure","unlisted":false},{"type":"category","label":"Tutorials","items":[{"type":"link","href":"/getting-started/tutorials/end-to-end-eval","label":"End-to-end evaluation tutorial","docId":"getting-started/tutorials/end-to-end-eval","unlisted":false}],"collapsed":true,"collapsible":true}],"collapsed":true,"collapsible":true},{"type":"category","label":"Metrics","items":[{"type":"link","href":"/metrics/overview","label":"Metrics overview","docId":"metrics/overview","unlisted":false},{"type":"category","label":"Generation Metrics","items":[{"type":"link","href":"/metrics/generation/bertscore","label":"BERTScore","docId":"metrics/generation/bertscore","unlisted":false},{"type":"link","href":"/metrics/generation/bartscore","label":"BARTScore","docId":"metrics/generation/bartscore","unlisted":false},{"type":"link","href":"/metrics/generation/bleurtscore","label":"BleurtScore","docId":"metrics/generation/bleurtscore","unlisted":false},{"type":"link","href":"/metrics/generation/rougescore","label":"ROUGEScore","docId":"metrics/generation/rougescore","unlisted":false},{"type":"link","href":"/metrics/generation/semscore","label":"SEMScore","docId":"metrics/generation/semscore","unlisted":false},{"type":"link","href":"/metrics/generation/alignscore","label":"AlignScore","docId":"metrics/generation/alignscore","unlisted":false},{"type":"link","href":"/metrics/generation/geval","label":"G-Eval","docId":"metrics/generation/geval","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Retriever Metrics","items":[{"type":"link","href":"/metrics/retriever/recallscore","label":"Recall Score","docId":"metrics/retriever/recallscore","unlisted":false},{"type":"link","href":"/metrics/retriever/precisionscore","label":"Precision Score","docId":"metrics/retriever/precisionscore","unlisted":false},{"type":"link","href":"/metrics/retriever/citationscore","label":"Citation Score","docId":"metrics/retriever/citationscore","unlisted":false},{"type":"link","href":"/metrics/retriever/sufficiencyscore","label":"Sufficiency Score","docId":"metrics/retriever/sufficiencyscore","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Reranker Metrics","items":[{"type":"link","href":"/metrics/reranker/meanrr","label":"MeanRR","docId":"metrics/reranker/meanrr","unlisted":false},{"type":"link","href":"/metrics/reranker/meanap","label":"MeanAP","docId":"metrics/reranker/meanap","unlisted":false},{"type":"link","href":"/metrics/reranker/rerank-ndcg","label":"Reranker NDCG","docId":"metrics/reranker/rerank-ndcg","unlisted":false},{"type":"link","href":"/metrics/reranker/cumulative-ndcg","label":"Cumulative NDCG","docId":"metrics/reranker/cumulative-ndcg","unlisted":false}],"collapsed":true,"collapsible":true}],"collapsed":true,"collapsible":true}]},"docs":{"getting-started/project-structure":{"id":"getting-started/project-structure","title":"Project Structure","description":"","sidebar":"docs"},"getting-started/quickstart":{"id":"getting-started/quickstart","title":"QuickStart Guide","description":"Vero-Eval is an open-source evaluation framework designed to rigorously assess the performance of Retrieval-Augmented Generation (RAG) pipelines. It provides built-in tracing, logging, and a rich suite of metrics to evaluate each component in the pipeline — from retrieval and reranking to generation — all integrated end to end.","sidebar":"docs"},"getting-started/tutorials/end-to-end-eval":{"id":"getting-started/tutorials/end-to-end-eval","title":"End-to-end evaluation tutorial","description":"This tutorial shows how to run an evaluation suite against a small dataset.","sidebar":"docs"},"intro":{"id":"intro","title":"RAG Evaluation Framework","description":"A framework for evaluating Retrieval-Augmented Generation (RAG) pipelines with built-in tracing, logging and evaluation metrics.","sidebar":"docs"},"metrics/generation/alignscore":{"id":"metrics/generation/alignscore","title":"AlignScore","description":"A factual consistency / alignment metric that evaluates how well a claim (generated or asserted text) is supported by a context (retrieved or original text).","sidebar":"docs"},"metrics/generation/bartscore":{"id":"metrics/generation/bartscore","title":"BARTScore","description":"A generation evaluation metric that uses a pretrained BART model to assess the quality of generated text against reference text and is a type of comparison score   .","sidebar":"docs"},"metrics/generation/bertscore":{"id":"metrics/generation/bertscore","title":"BERTScore","description":"It is an automatic evaluation metric for text generation tasks that measures the similarity between candidate and reference texts using contextual embeddings from pre-trained BERT models.","sidebar":"docs"},"metrics/generation/bleurtscore":{"id":"metrics/generation/bleurtscore","title":"BleurtScore","description":"An advanced metric based on BLEURT that produces a more nuanced weighted similarity score.","sidebar":"docs"},"metrics/generation/geval":{"id":"metrics/generation/geval","title":"G-Eval","description":"An LLM-based evaluation framework where a large language model directly scores the generated output against criteria such as relevance, consistency, and fluency.","sidebar":"docs"},"metrics/generation/rougescore":{"id":"metrics/generation/rougescore","title":"ROUGEScore","description":"ROUGE-L focuses on the Longest Common Subsequence (LCS) between generated and reference texts.","sidebar":"docs"},"metrics/generation/semscore":{"id":"metrics/generation/semscore","title":"SEMScore","description":"A metric that measures semantic similarity between candidate (generated) text and reference text using embeddings and cosine similarity.","sidebar":"docs"},"metrics/overview":{"id":"metrics/overview","title":"Metrics overview","description":"The RAG Evaluation Framework supports three classes of metrics:","sidebar":"docs"},"metrics/reranker/cumulative-ndcg":{"id":"metrics/reranker/cumulative-ndcg","title":"Cumulative NDCG","description":"Unique implementation of NDCG@k that can be used to evaluate the cumulative performance of retriever and reranker.","sidebar":"docs"},"metrics/reranker/meanap":{"id":"metrics/reranker/meanap","title":"MeanAP","description":"Mean Average Precision: average over queries of the average precision value.","sidebar":"docs"},"metrics/reranker/meanrr":{"id":"metrics/reranker/meanrr","title":"MeanRR","description":"Mean Reciprocal Rank: for each query, take reciprocal rank of first relevant item, then average.","sidebar":"docs"},"metrics/reranker/rerank-ndcg":{"id":"metrics/reranker/rerank-ndcg","title":"Reranker NDCG","description":"Normalized Discounted Cumulative Gain for ranking metrics using graded relevance scores.","sidebar":"docs"},"metrics/retriever/citationscore":{"id":"metrics/retriever/citationscore","title":"Citation Score","description":"Measures the overlap of retrieved items with ground truth citations.","sidebar":"docs"},"metrics/retriever/precisionscore":{"id":"metrics/retriever/precisionscore","title":"Precision Score","description":"Measures how many retrieved items are relevant (i.e. in ground truth).","sidebar":"docs"},"metrics/retriever/recallscore":{"id":"metrics/retriever/recallscore","title":"Recall Score","description":"Measures how many ground truth items are retrieved.","sidebar":"docs"},"metrics/retriever/sufficiencyscore":{"id":"metrics/retriever/sufficiencyscore","title":"Sufficiency Score","description":"Determines whether the retrieved set is sufficient to cover all ground truth items.","sidebar":"docs"}}}}