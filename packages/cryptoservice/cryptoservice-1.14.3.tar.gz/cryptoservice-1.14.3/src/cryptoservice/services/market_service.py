"""å¸‚åœºæ•°æ®æœåŠ¡.

ä¸“æ³¨äºæ ¸å¿ƒAPIåŠŸèƒ½ï¼Œä½¿ç”¨ç»„åˆæ¨¡å¼æ•´åˆå„ä¸ªä¸“ä¸šæ¨¡å—ã€‚
"""

import logging
from datetime import datetime
from pathlib import Path
from typing import Any, cast

from binance import AsyncClient

from cryptoservice.client import BinanceClientFactory
from cryptoservice.config import RetryConfig, settings
from cryptoservice.exceptions import InvalidSymbolError, MarketDataFetchError
from cryptoservice.models import (
    DailyMarketTicker,
    Freq,
    FundingRate,
    FuturesKlineTicker,
    HistoricalKlinesType,
    IntegrityReport,
    LongShortRatio,
    OpenInterest,
    SortBy,
    SpotKlineTicker,
    SymbolTicker,
    UniverseDefinition,
)
from cryptoservice.storage.database import Database
from cryptoservice.utils import DataConverter

# å¯¼å…¥æ–°çš„æ¨¡å—
from .downloaders import KlineDownloader, MetricsDownloader, VisionDownloader
from .processors import CategoryManager, DataValidator, TimeRangeProcessor, UniverseManager

logger = logging.getLogger(__name__)


class MarketDataService:
    """å¸‚åœºæ•°æ®æœåŠ¡å®ç°ç±»."""

    def __init__(self, client: AsyncClient) -> None:
        """åˆå§‹åŒ–å¸‚åœºæ•°æ®æœåŠ¡ (ç§æœ‰æ„é€ å‡½æ•°)."""
        self.client = client
        self.converter = DataConverter()
        self.db: Database | None = None

        # åˆå§‹åŒ–å„ç§ä¸“ä¸šæ¨¡å—
        self.kline_downloader = KlineDownloader(self.client)
        self.metrics_downloader = MetricsDownloader(self.client)
        self.vision_downloader = VisionDownloader(self.client)
        self.data_validator = DataValidator()
        self.universe_manager = UniverseManager(self)
        self.category_manager = CategoryManager()

    @classmethod
    async def create(cls, api_key: str, api_secret: str) -> "MarketDataService":
        """å¼‚æ­¥åˆ›å»ºMarketDataServiceå®ä¾‹."""
        client = await BinanceClientFactory.create_async_client(api_key, api_secret)
        return cls(client)

    async def __aenter__(self) -> "MarketDataService":
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£."""
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb) -> None:
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£ï¼Œç¡®ä¿å®¢æˆ·ç«¯å…³é—­."""
        await BinanceClientFactory.close_client()
        if self.db:
            await self.db.close()

    # ==================== åŸºç¡€å¸‚åœºæ•°æ®API ====================

    async def get_symbol_ticker(self, symbol: str | None = None) -> SymbolTicker | list[SymbolTicker]:
        """è·å–å•ä¸ªæˆ–æ‰€æœ‰äº¤æ˜“å¯¹çš„è¡Œæƒ…æ•°æ®."""
        try:
            ticker = await self.client.get_symbol_ticker(symbol=symbol)
            if not ticker:
                raise InvalidSymbolError(f"Invalid symbol: {symbol}")

            if isinstance(ticker, list):
                return [SymbolTicker.from_binance_ticker(t) for t in ticker]
            return SymbolTicker.from_binance_ticker(ticker)

        except Exception as e:
            logger.error(f"Error fetching ticker for {symbol}: {e}")
            raise MarketDataFetchError(f"Failed to fetch ticker: {e}") from e

    async def get_perpetual_symbols(self, only_trading: bool = True, quote_asset: str = "USDT") -> list[str]:
        """è·å–å½“å‰å¸‚åœºä¸Šæ‰€æœ‰æ°¸ç»­åˆçº¦äº¤æ˜“å¯¹."""
        try:
            logger.info(f"è·å–å½“å‰æ°¸ç»­åˆçº¦äº¤æ˜“å¯¹åˆ—è¡¨ï¼ˆç­›é€‰æ¡ä»¶ï¼š{quote_asset}ç»“å°¾ï¼‰")
            futures_info = await self.client.futures_exchange_info()
            perpetual_symbols = [
                symbol["symbol"]
                for symbol in futures_info["symbols"]
                if symbol["contractType"] == "PERPETUAL"
                and (not only_trading or symbol["status"] == "TRADING")
                and symbol["symbol"].endswith(quote_asset)
            ]

            logger.info(f"æ‰¾åˆ° {len(perpetual_symbols)} ä¸ª{quote_asset}æ°¸ç»­åˆçº¦äº¤æ˜“å¯¹")
            return perpetual_symbols

        except Exception as e:
            logger.error(f"è·å–æ°¸ç»­åˆçº¦äº¤æ˜“å¯¹å¤±è´¥: {e}")
            raise MarketDataFetchError(f"è·å–æ°¸ç»­åˆçº¦äº¤æ˜“å¯¹å¤±è´¥: {e}") from e

    async def get_top_coins(
        self,
        limit: int = settings.DEFAULT_LIMIT,
        sort_by: SortBy = SortBy.QUOTE_VOLUME,
        quote_asset: str | None = None,
    ) -> list[DailyMarketTicker]:
        """è·å–å‰Nä¸ªäº¤æ˜“å¯¹."""
        try:
            tickers = await self.client.get_ticker()
            market_tickers = [DailyMarketTicker.from_binance_ticker(t) for t in tickers]

            if quote_asset:
                market_tickers = [t for t in market_tickers if t.symbol.endswith(quote_asset)]

            return sorted(
                market_tickers,
                key=lambda x: getattr(x, sort_by.value),
                reverse=True,
            )[:limit]

        except Exception as e:
            logger.error(f"Error getting top coins: {e}")
            raise MarketDataFetchError(f"Failed to get top coins: {e}") from e

    async def get_market_summary(self, interval: Freq = Freq.d1) -> dict[str, Any]:
        """è·å–å¸‚åœºæ¦‚è§ˆ."""
        try:
            summary: dict[str, Any] = {"snapshot_time": datetime.now(), "data": {}}
            tickers_result = await self.get_symbol_ticker()
            if isinstance(tickers_result, list):
                tickers = [ticker.to_dict() for ticker in tickers_result]
            else:
                tickers = [tickers_result.to_dict()]
            summary["data"] = tickers

            return summary

        except Exception as e:
            logger.error(f"Error getting market summary: {e}")
            raise MarketDataFetchError(f"Failed to get market summary: {e}") from e

    async def get_historical_klines(
        self,
        symbol: str,
        start_time: str | datetime,
        interval: Freq,
        end_time: str | datetime | None = None,
        klines_type: HistoricalKlinesType = HistoricalKlinesType.SPOT,
    ) -> list["SpotKlineTicker"] | list["FuturesKlineTicker"]:
        """è·å–å†å²è¡Œæƒ…æ•°æ®.

        Args:
            symbol: äº¤æ˜“å¯¹ç¬¦å·
            start_time: å¼€å§‹æ—¶é—´
            end_time: ç»“æŸæ—¶é—´ï¼ˆé»˜è®¤ä¸ºå½“å‰æ—¶é—´ï¼‰
            interval: Kçº¿é¢‘ç‡
            klines_type: Kçº¿ç±»å‹ï¼ˆç°è´§/æœŸè´§ï¼‰

        Returns:
            æ ¹æ® klines_type è¿”å›ä¸åŒç±»å‹:
            - SPOT: list[SpotKlineTicker]
            - FUTURES: list[FuturesKlineTicker]
        """
        try:
            # å¤„ç†æ—¶é—´æ ¼å¼
            if isinstance(start_time, str):
                start_time = datetime.fromisoformat(start_time)
            if end_time is None:
                end_time = datetime.now()
            elif isinstance(end_time, str):
                end_time = datetime.fromisoformat(end_time)

            # è½¬æ¢ä¸ºæ—¶é—´æˆ³
            start_ts = self._date_to_timestamp_start(start_time.strftime("%Y-%m-%d"))
            end_ts = self._date_to_timestamp_end(end_time.strftime("%Y-%m-%d"))

            market_type = "æœŸè´§" if klines_type == HistoricalKlinesType.FUTURES else "ç°è´§"
            logger.info(f"è·å– {symbol} çš„{market_type}å†å²æ•°æ® ({interval.value})")

            ticker_class: type[SpotKlineTicker] | type[FuturesKlineTicker]
            # æ ¹æ®klines_typeé€‰æ‹©APIå’Œè¿”å›ç±»å‹
            if klines_type == HistoricalKlinesType.FUTURES:
                klines = await self.client.futures_klines(
                    symbol=symbol,
                    interval=interval.value,
                    startTime=start_ts,
                    endTime=end_ts,
                    limit=1500,
                )
                ticker_class = FuturesKlineTicker
            else:  # SPOT
                klines = await self.client.get_klines(
                    symbol=symbol,
                    interval=interval.value,
                    startTime=start_ts,
                    endTime=end_ts,
                    limit=1500,
                )
                ticker_class = SpotKlineTicker

            data = list(klines)
            if not data:
                logger.warning(f"æœªæ‰¾åˆ°äº¤æ˜“å¯¹ {symbol} åœ¨æŒ‡å®šæ—¶é—´æ®µå†…çš„æ•°æ®")
                return []

            # æ ¹æ®å¸‚åœºç±»å‹åˆ›å»ºç›¸åº”çš„Tickerå¯¹è±¡
            result = [ticker_class.from_binance_kline(symbol, kline) for kline in data]
            return cast(list[FuturesKlineTicker] | list[SpotKlineTicker], result)

        except Exception as e:
            logger.error(f"Error getting historical data for {symbol}: {e}")
            raise MarketDataFetchError(f"Failed to get historical data: {e}") from e

    # ==================== å¸‚åœºæŒ‡æ ‡API ====================

    async def get_funding_rate(
        self,
        symbol: str,
        start_time: str | datetime | None = None,
        end_time: str | datetime | None = None,
        limit: int = 100,
    ) -> list[FundingRate]:
        """è·å–æ°¸ç»­åˆçº¦èµ„é‡‘è´¹ç‡å†å²."""
        # è½¬æ¢æ—¶é—´æ ¼å¼
        start_time_str = self._convert_time_to_string(start_time) if start_time else ""
        end_time_str = self._convert_time_to_string(end_time) if end_time else ""

        return await self.metrics_downloader.download_funding_rate(
            symbol=symbol,
            start_time=start_time_str,
            end_time=end_time_str,
            limit=limit,
        )

    async def get_open_interest(
        self,
        symbol: str,
        period: str = "5m",
        start_time: str | datetime | None = None,
        end_time: str | datetime | None = None,
        limit: int = 500,
    ) -> list[OpenInterest]:
        """è·å–æ°¸ç»­åˆçº¦æŒä»“é‡æ•°æ®."""
        # è½¬æ¢æ—¶é—´æ ¼å¼
        start_time_str = self._convert_time_to_string(start_time) if start_time else ""
        end_time_str = self._convert_time_to_string(end_time) if end_time else ""

        return await self.metrics_downloader.download_open_interest(
            symbol=symbol,
            period=period,
            start_time=start_time_str,
            end_time=end_time_str,
            limit=limit,
        )

    async def get_long_short_ratio(
        self,
        symbol: str,
        period: str = "5m",
        ratio_type: str = "account",
        start_time: str | datetime | None = None,
        end_time: str | datetime | None = None,
        limit: int = 500,
    ) -> list[LongShortRatio]:
        """è·å–å¤šç©ºæ¯”ä¾‹æ•°æ®."""
        # è½¬æ¢æ—¶é—´æ ¼å¼
        start_time_str = self._convert_time_to_string(start_time) if start_time else ""
        end_time_str = self._convert_time_to_string(end_time) if end_time else ""

        return await self.metrics_downloader.download_long_short_ratio(
            symbol=symbol,
            period=period,
            ratio_type=ratio_type,
            start_time=start_time_str,
            end_time=end_time_str,
            limit=limit,
        )

    # ==================== æ‰¹é‡æ•°æ®ä¸‹è½½ ====================

    async def get_perpetual_data(
        self,
        symbols: list[str],
        start_time: str,
        db_path: Path | str,
        end_time: str | None = None,
        interval: Freq = Freq.h1,
        max_workers: int = 1,
        max_retries: int = 3,
        retry_config: RetryConfig | None = None,
        incremental: bool = True,
    ) -> IntegrityReport:
        """è·å–æ°¸ç»­åˆçº¦æ•°æ®å¹¶å­˜å‚¨."""
        # éªŒè¯å¹¶å‡†å¤‡æ•°æ®åº“æ–‡ä»¶è·¯å¾„
        db_file_path = self._validate_and_prepare_path(db_path, is_file=True)
        end_time = end_time or datetime.now().strftime("%Y-%m-%d")

        # ä½¿ç”¨Kçº¿ä¸‹è½½å™¨
        return await self.kline_downloader.download_multiple_symbols(
            symbols=symbols,
            start_time=start_time,
            end_time=end_time,
            interval=interval,
            db_path=db_file_path,
            max_workers=max_workers,
            retry_config=retry_config or RetryConfig(max_retries=max_retries),
            incremental=incremental,
        )

    async def download_universe_data(
        self,
        universe_file: Path | str,
        db_path: Path | str,
        long_short_ratio_types: list[str],
        retry_config: RetryConfig,
        api_request_delay: float,
        vision_request_delay: float,
        download_market_metrics: bool,
        incremental: bool,
        interval: Freq = Freq.m1,
        max_api_workers: int = 1,
        max_vision_workers: int = 50,
        max_retries: int = 3,
        custom_start_date: str | None = None,
        custom_end_date: str | None = None,
    ) -> None:
        """æŒ‰å‘¨æœŸåˆ†åˆ«ä¸‹è½½universeæ•°æ®.

        Args:
            universe_file: Path to the universe definition file
            db_path: Path to the database file where data will be stored
            long_short_ratio_types: List of long-short ratio data types to download
            retry_config: Custom retry configuration, overrides max_retries
            api_request_delay: Delay in seconds between API requests
            vision_request_delay: Delay in seconds between Vision requests
            download_market_metrics: Whether to download market metrics data
            incremental: Whether to download incremental data,
             if True, only download new data, if False, re-download all data
            interval: Time interval for K-line data (default: 1m)
            max_api_workers: Maximum number of concurrent API workers
            max_vision_workers: Maximum number of concurrent Vision workers
            max_retries: Maximum number of retry attempts for failed requests
            custom_start_date: Custom global start date, will override the start date
             in the universe but must be within the universe time range
            custom_end_date: Custom global end date, will override the end date
             in the universe but must be within the universe time range

        """
        try:
            # éªŒè¯è·¯å¾„
            universe_file_obj = self._validate_and_prepare_path(universe_file, is_file=True)
            db_file_path = self._validate_and_prepare_path(db_path, is_file=True)

            # æ£€æŸ¥universeæ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not universe_file_obj.exists():
                raise FileNotFoundError(f"Universeæ–‡ä»¶ä¸å­˜åœ¨: {universe_file_obj}")

            # åŠ è½½universeå®šä¹‰
            universe_def = UniverseDefinition.load_from_file(universe_file_obj)

            # éªŒè¯å’Œå¤„ç†è‡ªå®šä¹‰æ—¶é—´èŒƒå›´
            if custom_start_date or custom_end_date:
                universe_def = TimeRangeProcessor.apply_custom_time_range(
                    universe_def, custom_start_date, custom_end_date
                )

            logger.info("ğŸ“Š æŒ‰å‘¨æœŸä¸‹è½½æ•°æ®:")
            logger.info(f"   - æ€»å¿«ç…§æ•°: {len(universe_def.snapshots)}")
            logger.info(f"   - æ•°æ®é¢‘ç‡: {interval.value}")
            logger.info(f"   - APIå¹¶å‘çº¿ç¨‹: {max_api_workers}")
            logger.info(f"   - Visionå¹¶å‘çº¿ç¨‹: {max_vision_workers}")
            logger.info(f"   - APIè¯·æ±‚é—´éš”: {api_request_delay}ç§’")
            logger.info(f"   - Visionè¯·æ±‚é—´éš”: {vision_request_delay}ç§’")
            logger.info(f"   - æ•°æ®åº“è·¯å¾„: {db_file_path}")
            logger.info(f"   - ä¸‹è½½å¸‚åœºæŒ‡æ ‡: {download_market_metrics}")

            kline_download_results = []
            # ä¸ºæ¯ä¸ªå‘¨æœŸå•ç‹¬ä¸‹è½½æ•°æ®
            for i, snapshot in enumerate(universe_def.snapshots):
                logger.info(f"ğŸ“… å¤„ç†å¿«ç…§ {i + 1}/{len(universe_def.snapshots)}: {snapshot.effective_date}")

                # ä¸‹è½½Kçº¿æ•°æ®
                kline_download_results.append(
                    await self.get_perpetual_data(
                        symbols=snapshot.symbols,
                        start_time=snapshot.start_date,
                        end_time=snapshot.end_date,
                        db_path=db_file_path,
                        interval=interval,
                        max_workers=max_api_workers,
                        max_retries=max_retries,
                        retry_config=retry_config,
                        incremental=incremental,
                    )
                )

                # ä¸‹è½½å¸‚åœºæŒ‡æ ‡æ•°æ®
                if download_market_metrics:
                    logger.info("   ğŸ“ˆ å¼€å§‹ä¸‹è½½å¸‚åœºæŒ‡æ ‡æ•°æ®...")
                    await self._download_market_metrics_for_snapshot(
                        snapshot=snapshot,
                        db_path=db_file_path,
                        api_request_delay=api_request_delay,
                        vision_request_delay=vision_request_delay,
                        max_api_workers=max_api_workers,
                        max_vision_workers=max_vision_workers,
                    )

                logger.info(f"   âœ… å¿«ç…§ {snapshot.effective_date} ä¸‹è½½å®Œæˆ")

            logger.info("ğŸ‰ universeæ•°æ®ä¸‹è½½ç»“æœå®Œæ•´æ€§æŠ¥å‘Š: ")
            for result in kline_download_results:
                logger.info(result)
            logger.info(f"ğŸ“ æ•°æ®å·²ä¿å­˜åˆ°: {db_file_path}")

        except Exception as e:
            logger.error(f"æŒ‰å‘¨æœŸä¸‹è½½universeæ•°æ®å¤±è´¥: {e}")
            raise MarketDataFetchError(f"æŒ‰å‘¨æœŸä¸‹è½½universeæ•°æ®å¤±è´¥: {e}") from e

    # ==================== Universeç®¡ç† ====================

    async def define_universe(
        self,
        start_date: str,
        end_date: str,
        t1_months: int,
        t2_months: int,
        t3_months: int,
        output_path: Path | str,
        top_k: int | None = None,
        top_ratio: float | None = None,
        description: str | None = None,
        delay_days: int = 7,
        api_delay_seconds: float = 1.0,
        batch_delay_seconds: float = 3.0,
        batch_size: int = 5,
        quote_asset: str = "USDT",
    ) -> UniverseDefinition:
        """å®šä¹‰universeå¹¶ä¿å­˜åˆ°æ–‡ä»¶."""
        return await self.universe_manager.define_universe(
            start_date=start_date,
            end_date=end_date,
            t1_months=t1_months,
            t2_months=t2_months,
            t3_months=t3_months,
            output_path=output_path,
            top_k=top_k,
            top_ratio=top_ratio,
            description=description,
            delay_days=delay_days,
            api_delay_seconds=api_delay_seconds,
            batch_delay_seconds=batch_delay_seconds,
            batch_size=batch_size,
            quote_asset=quote_asset,
        )

    # ==================== åˆ†ç±»ç®¡ç† ====================

    def get_symbol_categories(self) -> dict[str, list[str]]:
        """è·å–å½“å‰æ‰€æœ‰äº¤æ˜“å¯¹çš„åˆ†ç±»ä¿¡æ¯."""
        return self.category_manager.get_symbol_categories()

    def get_all_categories(self) -> list[str]:
        """è·å–æ‰€æœ‰å¯èƒ½çš„åˆ†ç±»æ ‡ç­¾."""
        return self.category_manager.get_all_categories()

    def create_category_matrix(
        self, symbols: list[str], categories: list[str] | None = None
    ) -> tuple[list[str], list[str], list[list[int]]]:
        """åˆ›å»º symbols å’Œ categories çš„å¯¹åº”çŸ©é˜µ."""
        categories_list = categories if categories is not None else []
        return self.category_manager.create_category_matrix(symbols, categories_list)

    def save_category_matrix_csv(
        self,
        output_path: Path | str,
        symbols: list[str],
        date_str: str | None = None,
        categories: list[str] | None = None,
    ) -> None:
        """å°†åˆ†ç±»çŸ©é˜µä¿å­˜ä¸º CSV æ–‡ä»¶."""
        date_str_value = date_str if date_str is not None else ""
        categories_list = categories if categories is not None else []
        self.category_manager.save_category_matrix_csv(
            output_path=output_path,
            symbols=symbols,
            date_str=date_str_value,
            categories=categories_list,
        )

    def download_and_save_categories_for_universe(
        self,
        universe_file: Path | str,
        output_path: Path | str,
    ) -> None:
        """ä¸º universe ä¸­çš„æ‰€æœ‰äº¤æ˜“å¯¹ä¸‹è½½å¹¶ä¿å­˜åˆ†ç±»ä¿¡æ¯."""
        self.category_manager.download_and_save_categories_for_universe(
            universe_file=universe_file,
            output_path=output_path,
        )

    async def check_symbol_exists_on_date(self, symbol: str, date: str) -> bool:
        """æ£€æŸ¥æŒ‡å®šæ—¥æœŸæ˜¯å¦å­˜åœ¨è¯¥äº¤æ˜“å¯¹."""
        try:
            # å°†æ—¥æœŸè½¬æ¢ä¸ºæ—¶é—´æˆ³èŒƒå›´
            start_time = self._date_to_timestamp_start(date)
            end_time = self._date_to_timestamp_end(date)

            # å°è¯•è·å–è¯¥æ—¶é—´èŒƒå›´å†…çš„Kçº¿æ•°æ®
            klines = await self.client.futures_klines(
                symbol=symbol,
                interval="1d",
                startTime=start_time,
                endTime=end_time,
                limit=1,
            )

            # å¦‚æœæœ‰æ•°æ®ï¼Œè¯´æ˜è¯¥æ—¥æœŸå­˜åœ¨è¯¥äº¤æ˜“å¯¹
            return bool(klines and len(klines) > 0)

        except Exception as e:
            logger.debug(f"æ£€æŸ¥äº¤æ˜“å¯¹ {symbol} åœ¨ {date} æ˜¯å¦å­˜åœ¨æ—¶å‡ºé”™: {e}")
            return False

    # ==================== ç§æœ‰è¾…åŠ©æ–¹æ³• ====================

    async def _download_market_metrics_for_snapshot(
        self,
        snapshot,
        db_path: Path,
        api_request_delay: float,
        vision_request_delay: float,
        max_api_workers: int,
        max_vision_workers: int,
    ) -> None:
        """ä¸ºå•ä¸ªå¿«ç…§ä¸‹è½½å¸‚åœºæŒ‡æ ‡æ•°æ®."""
        try:
            # åˆå§‹åŒ–æ•°æ®åº“è¿æ¥
            if self.db is None:
                self.db = Database(db_path)

            symbols = snapshot.symbols
            start_time = snapshot.start_date
            end_time = snapshot.end_date

            # ä¸‹è½½Visionæ•°æ®ï¼ˆæŒä»“é‡ã€å¤šç©ºæ¯”ä¾‹ï¼‰
            logger.info("      ğŸ“Š ä½¿ç”¨ Binance Vision ä¸‹è½½å¸‚åœºæŒ‡æ ‡æ•°æ®...")
            await self.vision_downloader.download_metrics_batch(
                symbols=symbols,
                start_date=start_time,
                end_date=end_time,
                db_path=str(db_path),
                request_delay=vision_request_delay,
                max_workers=max_vision_workers,
            )

            # ä¸‹è½½Metrics APIæ•°æ®ï¼ˆèµ„é‡‘è´¹ç‡ï¼‰
            logger.info("      ğŸ’° ä½¿ç”¨ Binance API ä¸‹è½½èµ„é‡‘è´¹ç‡æ•°æ®...")
            await self.metrics_downloader.download_funding_rate_batch(
                symbols=symbols,
                start_time=start_time,
                end_time=end_time,
                db_path=str(db_path),
                request_delay=api_request_delay,
                max_workers=max_api_workers,  # é™åˆ¶å¹¶å‘ä»¥é¿å…APIé™åˆ¶
            )

            logger.info("      âœ… å¸‚åœºæŒ‡æ ‡æ•°æ®ä¸‹è½½å®Œæˆ")

        except Exception as e:
            logger.error(f"ä¸‹è½½å¸‚åœºæŒ‡æ ‡æ•°æ®å¤±è´¥: {e}")
            raise MarketDataFetchError(f"ä¸‹è½½å¸‚åœºæŒ‡æ ‡æ•°æ®å¤±è´¥: {e}") from e

    def _validate_and_prepare_path(self, path: Path | str, is_file: bool = False, file_name: str | None = None) -> Path:
        """éªŒè¯å¹¶å‡†å¤‡è·¯å¾„."""
        if not path:
            raise ValueError("è·¯å¾„ä¸èƒ½ä¸ºç©ºï¼Œå¿…é¡»æ‰‹åŠ¨æŒ‡å®š")

        path_obj = Path(path)

        # å¦‚æœæ˜¯æ–‡ä»¶è·¯å¾„ï¼Œç¡®ä¿çˆ¶ç›®å½•å­˜åœ¨
        if is_file:
            if path_obj.is_dir():
                path_obj = path_obj.joinpath(file_name) if file_name else path_obj
            else:
                path_obj.parent.mkdir(parents=True, exist_ok=True)
        else:
            # å¦‚æœæ˜¯ç›®å½•è·¯å¾„ï¼Œç¡®ä¿ç›®å½•å­˜åœ¨
            path_obj.mkdir(parents=True, exist_ok=True)

        return path_obj

    def _date_to_timestamp_start(self, date: str) -> str:
        """å°†æ—¥æœŸå­—ç¬¦ä¸²è½¬æ¢ä¸ºå½“å¤©å¼€å§‹çš„æ—¶é—´æˆ³ï¼ˆUTCï¼‰."""
        from cryptoservice.utils import date_to_timestamp_start

        return str(date_to_timestamp_start(date))

    def _date_to_timestamp_end(self, date: str) -> str:
        """å°†æ—¥æœŸå­—ç¬¦ä¸²è½¬æ¢ä¸ºæ¬¡æ—¥å¼€å§‹çš„æ—¶é—´æˆ³ï¼ˆUTCï¼‰."""
        from cryptoservice.utils import date_to_timestamp_end

        return str(date_to_timestamp_end(date))

    def _convert_time_to_string(self, time_value: str | datetime | None) -> str:
        """å°†æ—¶é—´å€¼è½¬æ¢ä¸ºå­—ç¬¦ä¸²æ ¼å¼."""
        if time_value is None:
            return ""
        if isinstance(time_value, str):
            return time_value
        if isinstance(time_value, datetime):
            return time_value.strftime("%Y-%m-%d")
