[eval]
description = "Test response accuracy"
type = "accuracy"
targets.agents = ["*"]
targets.tools = []

# Exact string match
[[eval.cases]]
prompt = "What is 2+2?"
output = "4"

# Substring containment (inline syntax) - matches if output contains "Paris"
[[eval.cases]]
prompt = "What is the capital of France?"
output = { contains = "Paris" }

# Substring containment (dotted syntax) - matches if output contains "capital"
[[eval.cases]]
prompt = "What is the capital of France?"
output.contains = "capital"

# Multiple substring options (inline) - matches if ANY are found
[[eval.cases]]
prompt = "Tell me the weather"
output = { contains_any = ["sunny", "cloudy", "rainy", "temperature"] }

# Multiple substring options (dotted) - matches if ANY are found
[[eval.cases]]
prompt = "Tell me the weather"
output.contains_any = ["sunny", "cloudy", "rainy", "temperature"]

# Prefix matching (dotted syntax)
[[eval.cases]]
prompt = "Greet the user"
output.startswith = ["Hello", "Hi", "Hey"]

# Regex matching (dotted syntax)
[[eval.cases]]
prompt = "Give me a phone number"
output.match = '''^\d{3}-\d{3}-\d{4}$'''

# Semantic similarity (dotted syntax)
[[eval.cases]]
prompt = "What is the capital of France?"
output.similar = "The capital city of France is Paris."
output.threshold = 0.75

# Tool evaluation with schema validation (TOML table format)
[[eval.cases]]
context = { city = "San Francisco", api_key = "test_key" }

[eval.cases.output.schema.temperature]
type = "float"

[eval.cases.output.schema.condition]
type = "str"

[eval.cases.output.schema.humidity]
type = "int"
min = 0
max = 100
