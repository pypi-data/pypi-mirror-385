[eval]
description = "LLM evaluation of response quality"
type = "llm"
targets.agents = ["*"]
targets.tools = []

# LLM configuration
[eval.llm]
model = "gpt-4"
prompt = """
Evaluate the helpfulness and accuracy of this response on a scale of 1-10.
Consider: relevance, clarity, completeness, and correctness.
"""
output_schema = """
{
  "type": "object",
  "required": ["score", "reasoning"],
  "properties": {
    "score": {"type": "number", "minimum": 1, "maximum": 10},
    "reasoning": {"type": "string"}
  }
}
"""

# Test cases with score thresholds
[[eval.cases]]
prompt = "I need help with my account"
score = { min = 7 }

[[eval.cases]]
prompt = "How do I configure SSL?"
score = { min = 6, max = 9 }

[[eval.cases]]
prompt = "Calculate 2+2"
score = { equal = 10 }